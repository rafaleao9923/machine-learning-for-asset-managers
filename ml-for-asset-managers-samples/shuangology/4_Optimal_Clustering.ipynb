{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distinguish several types of clustering algorithms, including the following:\n",
    "\n",
    "1 Connectivity: This clustering is based on distance connectivity, like hier- archical clustering. For an example in finance, see López de Prado (2016).\n",
    "\n",
    "2 Centroids: These algorithms perform a vector quantization, like k-means. For an example in finance, see López de Prado and Lewis (2018).\n",
    "\n",
    "3 Distribution: Clusters are formed using statistical distributions, e.g., a mixture of Gaussians.\n",
    "\n",
    "4 Density: These algorithms search for connected dense regions in the data space. Examples include DBSCAN and OPTICS.\n",
    "\n",
    "5 Subspace: Clusters are modeled on two dimensions, features and observa- tions. An example is biclustering (also known as coclustering). For instance, they can help identify similarities in subsets of instruments and time periods simultaneously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T15:12:02.475159Z",
     "start_time": "2020-09-09T15:12:02.468896Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_path = os.path.split(os.getcwd())[0]\n",
    "if nb_path not in sys.path:\n",
    "    sys.path.append(nb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T15:15:56.924157Z",
     "start_time": "2020-09-09T15:15:55.283405Z"
    }
   },
   "outputs": [],
   "source": [
    "import CovMatrix\n",
    "corr0,eVal0,eVec0 = CovMatrix.init_para()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T10:35:40.740566Z",
     "start_time": "2020-09-10T10:35:40.727381Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 4.1 BASE CLUSTERING\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def clusterKMeansBase(corr0, maxNumClusters=10, n_init=10):\n",
    "    x, silh = ((1-corr0.fillna(0))/2.)**.5, pd.Series()  # observations matrix\n",
    "    for init in range(n_init):\n",
    "        for i in range(2, maxNumClusters+1):\n",
    "            kmeans_ = KMeans(n_clusters=i, n_jobs=1, n_init=1)\n",
    "            kmeans_ = kmeans_.fit(x)\n",
    "            silh_ = silhouette_samples(x, kmeans_.labels_)\n",
    "            stat = (silh_.mean()/silh_.std(), silh.mean()/silh.std())\n",
    "            if np.isnan(stat[1]) or stat[0] > stat[1]:\n",
    "                silh, kmeans = silh_, kmeans_\n",
    "    newIdx = np.argsort(kmeans.labels_)\n",
    "    corr1 = corr0.iloc[newIdx]  # reorder rows\n",
    "\n",
    "    corr1 = corr1.iloc[:, newIdx]  # reorder columns\n",
    "    clstrs = {i: corr0.columns[np.where(kmeans.labels_ == i)[0]].tolist()\n",
    "              for i in np.unique(kmeans.labels_)}  # cluster members\n",
    "    silh = pd.Series(silh, index=x.index)\n",
    "    return corr1, clstrs, silh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T10:36:10.396076Z",
     "start_time": "2020-09-10T10:35:56.398771Z"
    }
   },
   "outputs": [],
   "source": [
    "corr1,clstrs,silh = clusterKMeansBase(pd.DataFrame(corr0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T10:36:15.285575Z",
     "start_time": "2020-09-10T10:36:15.271596Z"
    }
   },
   "outputs": [],
   "source": [
    "# a new (reduced) observations matrix out of the elements that compose the K1 clusters, and rerun the base clustering algorithm on that reduced correlation matrix. Doing so will return a, possibly new, clustering for those elements in K1. To check its efficacy, we compare the average cluster quality before and after reclustering those elements in K1. If the average cluster quality improves, we return the accepted clustering from the base clustering concate- nated with the new clustering for the redone nodes.\n",
    "\n",
    "# SNIPPET 4.2 TOP-LEVEL OF CLUSTERING\n",
    "from sklearn.metrics import silhouette_samples\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def makeNewOutputs(corr0, clstrs, clstrs2):\n",
    "    clstrsNew = {}\n",
    "    for i in clstrs.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())] = list(clstrs[i])\n",
    "    for i in clstrs2.keys():\n",
    "        clstrsNew[len(clstrsNew.keys())] = list(clstrs2[i])\n",
    "    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]\n",
    "    corrNew = corr0.loc[newIdx, newIdx]\n",
    "    x = ((1-corr0.fillna(0))/2.)**.5\n",
    "    kmeans_labels = np.zeros(len(x.columns))\n",
    "    for i in clstrsNew.keys():\n",
    "        idxs = [x.index.get_loc(k) for k in clstrsNew[i]]\n",
    "        kmeans_labels[idxs] = i\n",
    "    silhNew = pd.Series(silhouette_samples(x, kmeans_labels), index=x.index)\n",
    "    return corrNew, clstrsNew, silhNew\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "# for real stock data, n_init start from 10 ; for simulated data, n_init=3 would speed up the process\n",
    "def clusterKMeansTop(corr0, maxNumClusters=None, n_init=3):\n",
    "    if maxNumClusters == None:\n",
    "        maxNumClusters = corr0.shape[1]-1\n",
    "    corr1, clstrs, silh = clusterKMeansBase(corr0, maxNumClusters=min(\n",
    "        maxNumClusters, corr0.shape[1]-1), n_init=n_init)\n",
    "    clusterTstats = {i: np.mean(silh[clstrs[i]]) /\n",
    "                     np.std(silh[clstrs[i]]) for i in clstrs.keys()}\n",
    "    tStatMean = sum(clusterTstats.values())/len(clusterTstats)\n",
    "    redoClusters = [i for i in clusterTstats.keys() if\n",
    "                    clusterTstats[i] < tStatMean]\n",
    "    if len(redoClusters) <= 1:\n",
    "        return corr1, clstrs, silh\n",
    "    else:\n",
    "        keysRedo = [j for i in redoClusters for j in clstrs[i]]\n",
    "        corrTmp = corr0.loc[keysRedo, keysRedo]\n",
    "        tStatMean = np.mean([clusterTstats[i] for i in redoClusters])\n",
    "        corr2, clstrs2, silh2 = clusterKMeansTop(corrTmp,\n",
    "                                                 maxNumClusters=min(maxNumClusters,\n",
    "                                                                    corrTmp.shape[1]-1), n_init=n_init)\n",
    "    # Make new outputs, if necessary\n",
    "    corrNew, clstrsNew, silhNew = makeNewOutputs(corr0,\n",
    "                                                 {i: clstrs[i] for i in clstrs.keys(\n",
    "                                                 ) if i not in redoClusters},\n",
    "                                                 clstrs2)\n",
    "    newTstatMean = np.mean([np.mean(silhNew[clstrsNew[i]]) /\n",
    "                            np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])\n",
    "    if newTstatMean <= tStatMean:\n",
    "        return corr1, clstrs, silh\n",
    "    else:\n",
    "        return corrNew, clstrsNew, silhNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T17:01:57.217448Z",
     "start_time": "2020-09-09T17:01:57.201227Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 4.3 RANDOM BLOCK CORRELATION MATRIX CREATION\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import block_diag\n",
    "from sklearn.utils import check_random_state\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def getCovSub(nObs, nCols, sigma, random_state=None):\n",
    "    # Sub correl matrix\n",
    "    rng = check_random_state(random_state)\n",
    "    if nCols == 1:\n",
    "        return np.ones((1, 1))\n",
    "    ar0 = rng.normal(size=(nObs, 1))\n",
    "    ar0 = np.repeat(ar0, nCols, axis=1)\n",
    "    ar0 += rng.normal(scale=sigma, size=ar0.shape)\n",
    "    ar0 = np.cov(ar0, rowvar=False)\n",
    "    return ar0\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def getRndBlockCov(nCols, nBlocks, minBlockSize=1, sigma=1.,\n",
    "                   random_state=None):\n",
    "    # Generate a block random correlation matrix\n",
    "    rng = check_random_state(random_state)\n",
    "    parts = rng.choice(range(1, nCols-(minBlockSize-1)*nBlocks),\n",
    "                       nBlocks-1, replace=False)\n",
    "    parts.sort()\n",
    "    parts = np.append(parts, nCols-(minBlockSize-1)*nBlocks)\n",
    "    # random number of the cols in each block\n",
    "    parts = np.append(parts[0], np.diff(parts))-1+minBlockSize\n",
    "    cov = None\n",
    "    for nCols_ in parts:\n",
    "        cov_ = getCovSub(int(max(nCols_*(nCols_+1)/2., 100)),\n",
    "                         nCols_, sigma, random_state=rng)\n",
    "        if cov is None:\n",
    "            cov = cov_.copy()\n",
    "        else:\n",
    "            cov = block_diag(cov, cov_)\n",
    "    return cov\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def randomBlockCorr(nCols, nBlocks, random_state=None,\n",
    "                    minBlockSize=1):\n",
    "    # Form block corr\n",
    "    rng = check_random_state(random_state)\n",
    "    cov0 = getRndBlockCov(\n",
    "        nCols, nBlocks, minBlockSize=minBlockSize, sigma=.5, random_state=rng)\n",
    "    cov1 = getRndBlockCov(nCols, 1, minBlockSize=minBlockSize,\n",
    "                          sigma=1., random_state=rng)  # add noise\n",
    "    cov0 += cov1\n",
    "    corr0 = CovMatrix.cov2corr(cov0)\n",
    "    corr0 = pd.DataFrame(corr0)\n",
    "    return corr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T09:56:45.924959Z",
     "start_time": "2020-09-10T09:56:45.574809Z"
    }
   },
   "outputs": [],
   "source": [
    "# simulated a cov matraix with blocks\n",
    "import seaborn as sns\n",
    "corr_blk_simu = randomBlockCorr(100, 10)\n",
    "sns.heatmap(corr_blk_simu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T10:36:45.718412Z",
     "start_time": "2020-09-10T10:36:39.518374Z"
    }
   },
   "outputs": [],
   "source": [
    "# clustered using clusterKMeansTop\n",
    "sns.heatmap(clusterKMeansTop(corr_blk_simu)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
