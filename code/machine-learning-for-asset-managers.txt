// ---------------------------------------------------

// setup.py
// samatix/setup.py
# Copyright 2020 Ayoub ENNASSIRI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from setuptools import setup
from Cython.Build import cythonize

setup(
    name='Machine Learning for Asset Managers',
    ext_modules=cythonize("src/runner/pipeline.pyx"),
    zip_safe=False,
)


// ---------------------------------------------------

// portfolio_construction.py
// samatix/notebooks/portfolio_construction.py
# Generated from: portfolio_construction.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

import sys
sys.path.append('../src')

from src.testing.fixtures import CorrelationFactory

%matplotlib inline
plt.style.use('ggplot')
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = 20,10


# # Portfolio Construction
# ## Covariance Instability


cf = CorrelationFactory(n_cols=4, n_blocks=2, sigma_b=0, seed=3)

cov = cf.get_rnd_block_cov(n_blocks=2, sigma=0.5)

corr = cf.cov2corr(cov)

f = plt.figure(figsize=(19, 15))
plt.matshow(corr, fignum=f.number)
plt.xticks(range(corr.shape[1]), fontsize=14, rotation=45)
plt.yticks(range(corr.shape[1]), fontsize=14)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
plt.title('Correlation Matrix after KMeansBase', fontsize=16);


eigenvalues, eigenvectos = np.linalg.eigh(corr)

print(f"The condition number {max(eigenvalues) / min(eigenvalues)}")


# The condition number increases if :
# - Make one block greater
# - Increase the intrablock correlation
#
# The stability of the Markowitz's solution can be linked to a few dominant clusters within a correlation matrix.
#
# The solution is by optimizing the dominant clusters separately.


# ## The Nested Clustered Optimization Algorithm



// ---------------------------------------------------

// denoising_detoining.py
// samatix/notebooks/denoising_detoining.py
# Generated from: denoising_detoining.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns


import sys
sys.path.append('../src')

%matplotlib inline
plt.style.use('ggplot')
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = 20,10


# ## Testing the Marcenko-Pastur Theorem
#
# All the procedures are defined in the src.data.clean package.


from src.data.clean import *


x = np.random.normal(size=(10000, 1000))
eval0, _ = get_pca(np.corrcoef(x, rowvar=False))
mp = MarcenkoPastur(points=1000)
pdf0 = mp.pdf(var=1., q=x.shape[0]/float(x.shape[1]))
pdf1 = fit_kde(np.diag(eval0), bandwidth=0.01, x=pdf0.index.values.reshape(-1, 1))


plt.plot(pdf0.index.values, pdf0)
plt.plot(pdf0.index.values, pdf1)
plt.title('A visualization of the Marcenko-Pastur Theorem', fontsize=16)


# ### Comments:


# ## Fitting the Marcenko-Pastur Distribution


from src.testing.fixtures import CorrelationFactory

alpha = 0.995
columns_number = 1000
facts_number = 100
q = 10

cf = CorrelationFactory(n_cols=columns_number)

covariance = np.cov(np.random.normal(size=(columns_number*q, columns_number)), rowvar=False)
covariance = alpha*covariance + (1-alpha)*cf.get_rnd_covariance(facts_number=facts_number)

corr0 = cf.cov2corr(covariance)
eval0, evec0 = get_pca(corr0)

mp = MarcenkoPastur()
emax0, var0 = mp.fit(np.diag(eval0), q, bandwidth=0.01)
facts_number0 = eval0.shape[0] - np.diag(eval0)[::-1].searchsorted(emax0)


# This method helps us discriminate between eigenvalues associated with noise components and eigenvalues associated
# with signal components.


pdf0 = mp.pdf(var=var0, q=q)
nn, bb, patches=plt.hist(eval0.diagonal(),
                         bins=1000,
                         density=True)
plt.plot(pdf0.index.values, pdf0)
plt.title('Fitting the Marcenko-Pastur PDF on a noisy covariance matrix', fontsize=16)

plt.ylim(top=nn[1:].max() * 1.1)
plt.show()


# ## Denoising
#
# ### Denoising with the constant residual eigen value method


# Reusing the same results from before
corr = mp.denoise(eval0, evec0)
eval1, evec1 = get_pca(corr)


plt.plot(np.log(eval1.diagonal()))
plt.plot(np.log(eval0.diagonal()))


plt.title('A visualization of the Marcenko-Pastur Theorem', fontsize=16)



# ### Denoising with the shrinkage method


corr = mp.denoise(eval0, evec0, method="shrink", alpha=0.5)
eval2, evec2 = get_pca(corr)

plt.plot(np.log(eval2.diagonal()))
plt.plot(np.log(eval0.diagonal()))
plt.title('A visualization of the Marcenko-Pastur Theorem', fontsize=16)


# ## Experimental Results
#
# In the experimental results, we estimate a minimum variance portfolio with and without denoising
# then compare it with the true covariance matrix.


from scipy.linalg import block_diag
from sklearn.covariance import LedoitWolf

from src.utils import corr2cov, cov2corr

def form_block_matrix(n_blocks, b_size, b_corr):
    """
    Form a block matrix of size b_size and correlation b_corr

    :param n_blocks: Number of blocks
    :type n_blocks: int
    :param b_size: Size of the blocks
    :type b_size: int
    :param b_corr: Correlation
    :type b_corr: float
    :return: A matrix of size (n_blocks*b_size, n_blocks*b_size)
    :rtype: np.ndarray
    """
    block = np.ones((b_size, b_size)) * b_corr
    block[range(b_size), range(b_size)] = 1 
    corr = block_diag(*([block]*n_blocks))
    return corr 


def form_true_matrix(n_blocks, b_size, b_corr):
    """
    Form the true covariance matrix

    :param n_blocks:
    :type n_blocks:
    :param b_size:
    :type b_size:
    :param b_corr:
    :type b_corr:
    :return:
    :rtype:
    """
    corr0 = form_block_matrix(n_blocks=n_blocks, b_size=b_size, b_corr=b_corr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(.05, .2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0

def sim_cov_mu(mu0, cov0, n_obs, shrink=False):
    """
    Simulates a random matrix X of size n_obs * len(cov0) from a covariance matrix
    If shrink = True, the function fperforms a Ledoit-Wolf shrinkage of the
    empirical covariance matrix

    :param mu0: Vector of means
    :type mu0: np.ndarray
    :param cov0: Covariance matrix
    :type cov0: np.ndarray
    :param n_obs: Number of observations to simulate
    :type n_obs: int
    :param shrink: Shrink or not
    :type shrink: bool
    :return:
    :rtype:
    """
    X = np.random.multivariate_normal(mu0.flatten(), cov0, size=n_obs)
    mu1 = X.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(X).covariance_
    else:
        cov1 = np.cov(X, rowvar=0)
    return mu1, cov1

def denoise_cov(cov0, q, bandwidth):
    corr0 = cov2corr(cov0)
    eval0, evec0 = get_pca(corr0)
    mp = MarcenkoPastur()
    mp.fit(eigenvalues=np.diag(eval0), q=q, bandwidth=bandwidth)
    corr1 = mp.denoise(eigenvalues=eval0, eigenvectors=evec0)
    cov1 = corr2cov(corr1, np.diag(cov0)**.5)
    return cov1

def opt_port(cov, mu=None):
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = inv @ mu
    w /= ones.T @ w
    return w


n_blocks = 10
b_size = 50 
b_corr = 0.5

mu0, cov0 = form_true_matrix(n_blocks=n_blocks, b_size=b_size, b_corr=b_corr)


n_obs = 1000
n_trials = 1000
bandwidth = .01
shrink = False
min_var_portfolio = True


w1 = pd.DataFrame(columns=range(cov0.shape[0]),
                  index=range(n_trials))

w1_d = w1.copy(deep=True)

np.random.seed(0)

for i in range(n_trials):
    mu1, cov1 = sim_cov_mu(mu0=mu0, cov0=cov0, n_obs=n_obs, shrink=shrink)
    if min_var_portfolio:
        mu1 = None

    cov1_d = denoise_cov(cov0=cov1, q=n_obs*1./cov1.shape[1], bandwidth=bandwidth)
    w1.loc[i] = opt_port(cov1, mu1).flatten()
    w1_d.loc[i] = opt_port(cov1_d, mu1).flatten()



# ### Results Evaluation
# After computing the true minimum variance portfolio drived from the true covariance matrix,
# we compute the root-mean-squre errors (RMSE) with and without denoising.


w0 = opt_port(cov0, None if min_var_portfolio else mu0) # True minimum var portfolio
w0 = np.repeat(w0.T, w1.shape[0], axis=0)

rmse = np.mean((w1-w0).values.flatten()**2)**.5
rmse_denoised = np.mean((w1_d-w0).values.flatten()**2)**.5
print(f"RMSE = {rmse:e} / RMSE denoised {rmse_denoised:e}")



# ## Exercises


# 1. Implement in python the detoning method described in Section2.6.



// ---------------------------------------------------

// feature_importance.py
// samatix/notebooks/feature_importance.py
# Generated from: feature_importance.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
plt.style.use('ggplot')
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = 20,10


# # Feature Importance Analysis
# ## P-Value Caveats Demonstrations
# P-Values has the following pitfalls:
# - Rely on many strong assumptions
# - Produces noisy estimates
# - Is a probability of something we don't need: Given $H_0$ and an estimated $\hat{\beta}$, p-values
# estimates the probability of obtaining a result equal or more extreme than $\hat{\beta}$, subject to $H_0$ being true.
# We are more interested in the probability of $H_0$ being true given an observed $\hat{\beta}$
# - Not generalizable out of sample


from sklearn.datasets import make_classification

def get_test_data(n_features=100, n_informative=25, n_redundant=25,
                  n_samples = 10000, random_state = 0, sigma_std = .0):
    """
    Generates a random set of informed, redundant and noise explanatory variables
    :param n_features: Total number of features
    :type n_features: int
    :param n_informative: Number of informative features (subset of the features)
    :type n_informative: int
    :param n_redundant: Number of redundant features (subset of features)
    :type n_redundant: int
    :param n_samples: samples number
    :type n_samples: int
    :param random_state: random state
    :type random_state: float
    :param sigma_std: standard deviation
    :type sigma_std: float
    :return:  X : Dataframe of the generated samples.
              y : Series of the integer labels for class membership of each sample.
    :rtype: (pd.DataFrame, pd.Series)
    """
    np.random.seed(random_state)

    # Generate a random n-class classification problem
    X,y = make_classification(
        n_samples=n_samples, n_features=n_features-n_redundant, n_informative=n_informative,
        n_redundant=0, shuffle=False, random_state=random_state
    )

    # Rename the columns
    cols = [f'I_{i}' for i in range(n_informative)]
    cols += [f'N_{i}' for i in range(n_features - n_informative - n_redundant)]
    X = pd.DataFrame(X, columns=cols)
    y = pd.Series(y)

    # Create the redundant features
    i = np.random.choice(range(n_informative), size=n_redundant)
    for k, j in enumerate(i):
        X[f'R_{k}'] = X[f'I_{j}'] + np.random.normal(size=X.shape[0]) * sigma_std

    return X,y


import statsmodels.discrete.discrete_model as sm

X, y = get_test_data(
    n_features=40, n_informative=5, n_redundant=30, n_samples=10000,
    sigma_std=0.1
)

ols = sm.Logit(y, X).fit()



# Generate some sequential data
pvalues_sorted = ols.pvalues.sort_values()
sns.barplot(x=pvalues_sorted.index, y=pvalues_sorted.values, palette="rocket")


# - Only four out of the thirty five non-noise features are deemed statistically significant : I_1, R_29, R_27, I_3.
# - Noise features are ranked as relatively important
# - Fourteen of the features ranked as least important are not noise


# ## Mean-Decrease Impurity


def feat_imp_mdi(clf, feat_names):
    """
    Feature importance beased on IS mean impurity reduction
    :param clf: a built estimator using an ensemble method
    :type clf: e.g BaggingClassifier
    :param feat_names: The features names
    :type feat_names: Array like
    :return: An importance dataframe with the index as the column names and mean / std as the columns
    :rtype: pd.DataFrame
    """
    # Loop through all the estimators (DecisionTreeClassifier) and get the feature importance array
    # (Gini Importance)
    df0 = {
        i: tree.feature_importances_ for i, tree in enumerate(clf.estimators_)
    }
    df0 = pd.DataFrame.from_dict(df0, orient="index")
    df0.columns = feat_names
    df0 = df0.replace(0, np.nan) # beacause max_features=1

    imp = pd.concat(
        {
            'mean': df0.mean(),
            'std': df0.std() * df0.shape[0]**-.5
        },
        axis=1
    )

    imp /= imp['mean'].sum()
    return imp


from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X, y = get_test_data(
    n_features=40, n_informative=5, n_redundant=30, n_samples=10000,
    sigma_std=0.1
)

clf = DecisionTreeClassifier(
    criterion='entropy',
    max_features=1,
    class_weight='balanced',
    min_weight_fraction_leaf=0
)

clf = BaggingClassifier(base_estimator=clf, n_estimators=1000,
                        max_features=1., max_samples=1.)

clf_fit = clf.fit(X,y)

imp = feat_imp_mdi(clf=clf_fit, feat_names=X.columns).sort_values(by=['mean'])



ax = sns.barplot(x=imp.index, y=imp['mean'].values, palette="rocket")
ax.errorbar(imp.index, imp['mean'], yerr=imp['std'], ls='none', fmt='-o')


# - MDI does a good job as noisy features are less important
# - A small number of nonnoisy features appear to be much more important than their peers (substitution effect)
#
# Out of the 4 p-values pitfalls, the MDI method deals with three: 
# - No need for strong distributional assumptions
# - Variance of MDI estimates can be reduced by increasing the number of trees in ensemble methods in general or random forest in particular -> less noisy results 
# - The goal of the the tree-based classifiers is not to estimate the coefficients of a given algebraic equation. Estimating the probability of $H_0$ is irrelevant. MDI looks for important features in general. 
# - MDI doesn't fully solve the out of sample caveat encountered with p-values. 


# ## Mean-Decrease Accuracy


from sklearn.metrics import log_loss
from sklearn.model_selection import KFold

def feat_imp_mda(clf, X, y, n_splits=10):
    """
    Feature importance based on OOS score reduction
    :param clf: a classifier using an ensemble method
    :type clf: e.g BaggingClassifier
    :param X: The training input samples. Sparse matrices are accepted only if
            they are supported by the base estimator.
    :type X: {array-like, sparse matrix} of shape (n_samples, n_features)
    :param y: The target values (class labels in classification, real numbers in
            regression).
    :type y: array-like of shape (n_samples,)
    :param n_splits: Number of folds. Must be at least 2.
    :type n_splits: int
    :return: An importance dataframe with the index as the column names and mean / std as the columns
    :rtype: pd.DataFrame
    """
    # feat importance based on OOS score reduction
    cv_gen = KFold(n_splits=n_splits) 
    score_0, score_1 = pd.Series(), pd.DataFrame(columns=X.columns)
    for i,(train,test) in enumerate(cv_gen.split(X=X)):
        X0, y0 = X.iloc[train,:], y.iloc[train]
        X1, y1 = X.iloc[test,:], y.iloc[test]
        fit = clf.fit(X=X0, y=y0) # the fit occurs here 
        prob = fit.predict_proba(X1) # prediction before shuffling 
        score_0.loc[i] = -log_loss(y1,prob, labels=clf.classes_)
        for j in X.columns:
            X1_ = X1.copy(deep=True) 
            np.random.shuffle(X1_[j].values) # shuffle one column 
            prob = fit.predict_proba(X1_) # prediction after shuffling 
            score_1.loc[i,j] = -log_loss(y1, prob, labels=clf.classes_)
    imp = (-1 * score_1).add(score_0, axis=0)
    imp = imp/(-1 * score_1)
    imp = pd.concat(
        {
            'mean':imp.mean(),
            'std':imp.std()*imp.shape[0]**-.5
        },
        axis=1
    ) # CLT 
    return imp


X, y = get_test_data(
    n_features=40, n_informative=5, n_redundant=30, n_samples=10000,
    sigma_std=0.1
)

clf = DecisionTreeClassifier(
    criterion='entropy', max_features=1,
    class_weight='balanced', min_weight_fraction_leaf=0
)
clf = BaggingClassifier(
    base_estimator=clf, n_estimators=1000, max_features=1., 
    max_samples=1., oob_score=False
) 

imp=feat_imp_mda(clf,X,y,10).sort_values(by=['mean'])


ax = sns.barplot(x=imp.index, y=imp['mean'].values, palette="rocket")
ax.errorbar(imp.index, imp['mean'], yerr=imp['std'], ls='none', fmt='-o')


# - Noise features are ranked last 
# - Noise features are also deemed unimportant in magnitude, with MDA values of essentially zero 


# ## Probability-Weighted Accuracy


# #TODO: Implement the PWA


# # Substitution Effects


# ## Orthogonalization


#TODO: Implement orthogonalization example


# ## Cluster Feature Importance
# 1- Features clustering: after projecting the observed features into a metric space (correlation or other
# non linear metrics), we perform a clustering. For each cluster k=1,...,K, we replace the features included
# in the cluster with residual features, where those residual features do not contain information from features
# outside the cluster k.
# 2- Cluster importance: We use the outcome from step 1 (clusters and features) to apply MDI or MDA on groups
# of similar features rather on individual features.


def group_mean_std(df0, clusters):
    out = pd.DataFrame(columns=['mean','std'])
    for i,j in clusters.items():
        df1 = df0[j].sum(axis=1)
        out.loc[f'C_{i}','mean'] = df1.mean()
        out.loc[f'C_{i}','std'] = df1.std()*df1.shape[0]**-.5
    return out

def feat_imp_mdi_clustered(fit, feat_names, clusters):
    df0 = {
        i:tree.feature_importances_
        for i,tree in enumerate(fit.estimators_)
    }
    df0 = pd.DataFrame.from_dict(df0, orient='index')
    df0.columns = feat_names
    df0 = df0.replace(0,np.nan) # because max_features=1
    imp = group_mean_std(df0, clusters)
    imp /= imp['mean'].sum()
    return imp


from sklearn.metrics import log_loss
from sklearn.model_selection import KFold

def feat_imp_mda_clustered(clf, X, y, clusters, n_splits=10):
    cv_gen = KFold(n_splits=n_splits)
    score_0, score_1 = pd.Series(),pd.DataFrame(columns=clusters.keys())
    for i, (train,test) in enumerate(cv_gen.split(X=X)):
        X0, y0 = X.iloc[train,:],y.iloc[train]
        X1, y1 = X.iloc[test,:],y.iloc[test]
        fit = clf.fit(X=X0, y=y0)
        prob = fit.predict_proba(X1)
        score_0.loc[i] = -log_loss(y1,prob,labels=clf.classes_)
        for j in score_1.columns:
            X1_ = X1.copy(deep=True)
            for k in clusters[j]:
                np.random.shuffle(X1_[k].values) # shuffle cluster
            prob = fit.predict_proba(X1_)
            score_1.loc[i,j] = -log_loss(y1, prob, labels=clf.classes_)
    imp = (-1 * score_1).add(score_0, axis=0)
    imp = imp/(-1 * score_1)
    imp = pd.concat(
        {
            'mean': imp.mean(),
            'std': imp.std()*imp.shape[0]**-.5
        },
        axis=1
    )
    imp.index=[f'C_{i}' for i in imp.index]
    return imp


# ## Experimental Results


# Generate the test data and using KMeansBase for clustering
from src.cluster import KMeansBase

X, y = get_test_data(
    n_features=40, n_informative=5, n_redundant=30, n_samples=10000,
    sigma_std=0.1
)

clustering = KMeansBase(max_n_clusters=10, random_state=0).fit(X.corr())


# Getting the clusters dictionary
clusters = {}
for i in clustering.labels_:
    clusters[i] = X.columns[np.where(clustering.labels_ == i)]


# Get the sorted columns from KMeans
columns = [X.columns.tolist()[i] for i in np.argsort(clustering.labels_)]
X_sorted = X.reindex(columns, axis=1)


f = plt.figure(figsize=(19, 15))
plt.matshow(X_sorted.corr(), fignum=f.number)
plt.xticks(range(X_sorted.shape[1]), X_sorted.columns, fontsize=14, rotation=45)
plt.yticks(range(X_sorted.shape[1]), X_sorted.columns, fontsize=14)
cb = plt.colorbar()
cb.ax.tick_params(labelsize=14)
plt.title('Correlation Matrix after KMeansBase', fontsize=16);


# ### Clustered MDI


clf = DecisionTreeClassifier(
    criterion='entropy', max_features=1,
    class_weight='balanced', min_weight_fraction_leaf=0
)
clf = BaggingClassifier(
    base_estimator=clf, n_estimators=1000, max_features=1.,
    max_samples=1., oob_score=False
)
fit = clf.fit(X, y)

imp = feat_imp_mdi_clustered(fit=fit, feat_names=X.columns, clusters=clusters).sort_values(by=['mean'])


ax = sns.barplot(x=imp.index, y=imp['mean'].values, palette="rocket")
ax.errorbar(imp.index, imp['mean'], yerr=imp['std'], ls='none', fmt='-o')


# **Remarks**
# - The last cluster C_1 is associated with the noise features and all other clusters are associated
# with informative and redundant features
# - In contrast with the result without clustering, some non-noise features appeared to be much more important
# than others, even after taking into consideration the standard deviation around the mean values


# ### Clustered MDA


clf = DecisionTreeClassifier(
    criterion='entropy', max_features=1,
    class_weight='balanced', min_weight_fraction_leaf=0
)
clf = BaggingClassifier(
    base_estimator=clf, n_estimators=1000, max_features=1.,
    max_samples=1., oob_score=False
)

imp = feat_imp_mda_clustered(clf=clf, X=X, y=y, clusters=clusters, n_splits=10).sort_values(by=['mean'])


ax = sns.barplot(x=imp.index, y=imp['mean'].values, palette="rocket")
ax.errorbar(imp.index, imp['mean'], yerr=imp['std'], ls='none', fmt='-o')


# **Remarks**
# - The last cluster C_1 is associated with the noise features and has zero importance (noise)
# - Same remarks as for the clustered MDI features importance review


# # Conclusions
# - Using p-values to evaluate the significance of explanatory variables has major flaws
# - ML offers feature importance methods that overcome most or all of the p-values flaws
# - MDI/MDA assess the importance of features robustly (without making strong assumptions on the distribution)
# - The perception that ML tools are black-boxes and classical tools is false
# - Important to consider carefully what are we interested in explaining or predicting.


# # Exercises
# **1-** Consider a medical test with a false positive rate $ \alpha = P[x>\tau|H_0]$
# where H_0 is the null hypothesis (the patient is healthy),
# $x$ is the observed measurement, and $\tau$ is the significance threshold.
#
# A test is run on a random patient and comes back positive (the null hypothesis is rejected).
#
# What is the probability that the patient truly has the condition?
#
# **a.** Is it $1-\alpha = P[x≤\tau|H_0]$ (the confidence of the test) ?
# No, because it stands for the probability to have negative test when the patient is healthy
#
# **b.** Is it $1-\beta = P[x>\tau|H_1]$ (the power,or recall,of the test)?
# No, because this measure stands for the probability to have a true positive.
#
# **c.** Or is it $P[H_1|x>\tau]$ (the precision of the test)? Yes as the later stands for the probability that the
# patient has the condition knowing that his test came back positive.
#
# **d.** Of the above, what do p-values measure ? p-values is the lowest significance level on
# which we can reject H0 (patient is healthy). If we compute the P-value p,
# we reject on a significance level $\alpha$ if $p ≤ \alpha$
#
# **e.** In finance,the analogous situation is to test whether a variable is involved
# in a phenomenon. Do p-values tell us anything about the probability that
# the variable is relevant, given the observed evidence? No as explained in cavet#3
#
# **2-** Consider a medical test where $\alpha = 0.01$, $\beta = 0$, and the probability of the condition is
# $P[H_1] = 0.001$. The test has full recall and a very high confidence.
# What is the probability that a positive-tested patient is actually sick?
#
# From Bayes' theorem:
#
# $$\frac{P[H_1 | x > \tau]}{P[x > \tau | H_1]} = \frac{P[H_1]}{P[x > \tau]} $$
#
# From the law of total probability:
#
# $$P[H_1 | x > \tau] = \frac{P[H_1] \times P[x > \tau | H_1]}{P[x > \tau|H_0] \times P[H_0] + P[x > \tau|H_1] \times P[H_1]}$$
#
# With $\beta = 0$
#
# $$P[H_1 | x > \tau] = \frac{P[H_1]}{\alpha \times (1-P[H_1]) + P[H_1]}$$


p_h1= 0.001
alpha = 0.01
p_sick =  p_h1/(alpha*(1-p_h1)+p_h1)


# The probability found is of 0.091 which means that we have a 10% chance that the patient is sick
# if the test is positive.
#
# Why is it much lower than $1 - \alpha$ and $1 - \beta$?
#
# It is much lower because the probability that the test is positive is $P(x>\tau) = 0.01099$ due to a high positive rate ($\alpha$). Thus we need to test confidence.
#
# What is the probability that a patient is actually sick after testing positive twice on independent tests?
#
# We have :
# $$\frac{P[H_1 | T_1 \cap T_2]}{P[ T_1 \cap T_2| H_1]} = \frac{P[H_1]}{P[T_1 \cap T_2]} $$
#
# Which leads us to : (warning : recheck the calculation)
#
# $$P[H_1 | T_1 \cap T_2] = \frac{P[H_1] \times P[ T_1 \cap T_2| H_1]}{P[ T_1] \times P[ T_2]} $$


# **3-** Rerun the examples in Sections 6.3.1 and 6.3.2,where this time you pass an argument sigmaStd=0
# to the getTestData function. How do Figures 6.2 and 6.3 look now?
# What causes the difference, if there is one?


# MDI
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier

X, y = get_test_data(
    n_features=40, n_informative=5, n_redundant=30, n_samples=10000
)

clf = DecisionTreeClassifier(
    criterion='entropy',
    max_features=1,
    class_weight='balanced',
    min_weight_fraction_leaf=0
)

clf = BaggingClassifier(base_estimator=clf, n_estimators=1000,
                        max_features=1., max_samples=1.)

clf_fit = clf.fit(X,y)

imp = feat_imp_mdi(clf=clf_fit, feat_names=X.columns).sort_values(by=['mean'])

ax = sns.barplot(x=imp.index, y=imp['mean'].values, palette="rocket")
ax.errorbar(imp.index, imp['mean'], yerr=imp['std'], ls='none', fmt='-o')


# MDA
clf = DecisionTreeClassifier(
    criterion='entropy', max_features=1,
    class_weight='balanced', min_weight_fraction_leaf=0
)
clf = BaggingClassifier(
    base_estimator=clf, n_estimators=1000, max_features=1.,
    max_samples=1., oob_score=False
)

imp=feat_imp_mda(clf,X,y,10).sort_values(by=['mean'])

ax = sns.barplot(x=imp.index, y=imp['mean'].values, palette="rocket")
ax.errorbar(imp.index, imp['mean'], yerr=imp['std'], ls='none', fmt='-o')


# We can see that the
# - For MDI and MDA, we don't see many differences compared to the initial setup
# - I would have expected the features importance to have a piecewise form



// ---------------------------------------------------

// distance_metrics.py
// samatix/notebooks/distance_metrics.py
# Generated from: distance_metrics.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# ## Linear Dependency 
#
# - Correlations are good for quantifying the **linear** codependency between random variables.
#
# - This form of codependency (linearity) can be measured using different distance metrics:
#
# $$ d_{|p|}[X,Y] = \sqrt{1/2\times(1-\rho[X,Y])}$$
#
# or
#
# $$ d_{|p|}[X,Y] = \sqrt{(1-\rho[X,Y])}$$
#
# ## Non Linear Dependency 
#
# - When the variables X and Y are bound by a non linear relationship, the above distance metric misdjuges the similarity of these variables.
#
# - For nonlinear cases, the **normalised variable of information** is a more appropriate distance
# metric. It allows us to answer the questions regarding the unique information contributed
# by a random variable, without having to make functional assumptions. Given that many ML algorithms
# do not impose a functional form on the data, it makes sense to use them in conjunction
# with entropy-based features.
#
#
#


import numpy as np
import pandas as pd 

import scipy.stats as ss
from sklearn.metrics import mutual_info_score
import matplotlib.pyplot as plt

from src import entropy


size, seed = 5000, 0

np.random.seed(seed)

x = np.random.normal(size=size)
e = np.random.normal(size=size)

fig, axs = plt.subplots(3, figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')

# marker symbol
y = 0*x + e
nmi = entropy.mutual_info(x, y, norm=True)
corr = np.corrcoef(x,y)[0,1]
axs[0].scatter(x, y)
axs[0].set_title(r"$y = 0*x + e$")
axs[0].text(0.1, 0.9, f"nmi={nmi}; \n corr={corr}",
     horizontalalignment='center',
     verticalalignment='center',
     transform = axs[0].transAxes)

y = 100*x + e
nmi = entropy.mutual_info(x, y, norm=True)
corr = np.corrcoef(x,y)[0,1]
axs[1].scatter(x, y)
axs[1].set_title(r"$y = 100*x + e$")
axs[1].text(0.1, 0.9, f"nmi={nmi}; \n corr={corr}",
     horizontalalignment='center',
     verticalalignment='center',
     transform = axs[1].transAxes)

y = 100*abs(x) + e
nmi = entropy.mutual_info(x, y, norm=True)
corr = np.corrcoef(x,y)[0,1]
axs[2].scatter(x, y)
axs[2].set_title(r"$y = 100*|x| + e$")
axs[2].text(0.1, 0.9, f"nmi={nmi}; \n corr={corr}",
     horizontalalignment='center',
     verticalalignment='center',
     transform = axs[2].transAxes)


# # Exercises
# 1. We draw 1000 observations from a bivariate Normal Distribution with unit standard deviations and a correlation coefficient $\rho \in \{ -1, -.5, 0, .5, 1 \}$
# 2. In this section, we discretize the observations and compute H[X], H[Y], H[X,Y], H[X|Y], I[X,Y], VI[X,Y], VI_norm[X,Y] using 


def show_result(results_final):
    """
    Function to plot the results that are in dict format  
    results_final = {
        'N=1000': { # Curves labels
            '0': { # Rho 
                'H[X]': ...
            }
        }
    }
    """
    measures = results_final['N=1000'][0].keys()

    fig, axs = plt.subplots(len(measures)//2+1, 2,  figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k', constrained_layout=True)
    # Are H[X], H[Y], affected by rho ? Yes as per the results below
    for result_desc, results in results_final.items():
        for axs_id, measure in enumerate(measures):
            axs[axs_id//2, axs_id%2].plot(rhos, tuple(v[measure] for v in results.values()), label=result_desc)
            axs[axs_id//2, axs_id%2].set_title(measure)
            axs[axs_id//2, axs_id%2].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)


size, seed = 1000, 0
np.random.seed(seed)
bins = 10
results_final = {}
results_temp = {}
#rhos = (-0.9999, -.75, -.5, -.25, 0, .25, .5, .75, 0.9999)
rhos = np.arange(-0.99, 0.99, 0.1)

for rho in rhos:
    x, y = np.random.multivariate_normal([0, 0], [[1, rho], [rho, 1]], size=size).T
    results_temp[rho] = {
        "H[X]": entropy.marginal(x, bins),
        "H[Y]": entropy.marginal(y, bins),
        "H[X,Y]": entropy.joint(x, y, bins=bins),
        "H[X|Y]": entropy.conditional(x, y, bins=bins),
        "I[X,Y]": entropy.mutual_info(x, y, bins=bins),
        "VI[X,Y]": entropy.variation_info(x, y, bins=bins),
        "VI_N[X,Y]": entropy.variation_info(x, y, bins=bins, norm=True)
    }
    
results_final['N=1000'] = results_temp


# 2. Exercise 1 with 1 million observations


size, seed = int(1e6), 0
np.random.seed(seed)
bins = 10
results_temp = {}

for rho in rhos:
    x, y = np.random.multivariate_normal([0, 0], [[1, rho], [rho, 1]], size=size).T
    results_temp[rho] = {
        "H[X]": entropy.marginal(x, bins),
        "H[Y]": entropy.marginal(y, bins),
        "H[X,Y]": entropy.joint(x, y, bins=bins),
        "H[X|Y]": entropy.conditional(x, y, bins=bins),
        "I[X,Y]": entropy.mutual_info(x, y, bins=bins),
        "VI[X,Y]": entropy.variation_info(x, y, bins=bins),
        "VI_N[X,Y]": entropy.variation_info(x, y, bins=bins, norm=True)
    }
results_final['N=1e6'] = results_temp


# 3. Exercise 1 with 1 million observations


size, seed = int(1e6), 0
np.random.seed(seed)

results_temp = {}

for rho in rhos:
    x, y = np.random.multivariate_normal([0, 0], [[1, rho], [rho, 1]], size=size).T
    bins = None
    results_temp[rho] = {
        "H[X]": entropy.marginal(x, bins=bins),
        "H[Y]": entropy.marginal(y, bins=bins),
        "H[X,Y]": entropy.joint(x, y, bins=bins),
        "H[X|Y]": entropy.conditional(x, y, bins=bins),
        "I[X,Y]": entropy.mutual_info(x, y, bins=bins),
        "VI[X,Y]": entropy.variation_info(x, y, bins=bins),
        "VI_N[X,Y]": entropy.variation_info(x, y, bins=bins, norm=True)
    }
    bins = entropy.num_bins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    print(f"Optimal Bins = {bins} \tfor rho = {rho}")
results_final['N=1e6 with optimal bining'] = results_temp


# There is an issue with the np.histogram2 function that leads to a memory leak for an a bivariate normal distribution
# with a rho equal to 1 or -1. When $\rho$ is near from 1, the bias cancellation becomes very difficult to achieve 
# because of the higher influence of the term $$


show_result(results_final)


# - $H[X]$ and $H[Y]$ are slightly impacted with the change in the correlation. For the higher number of observations and with the optimal bining, the marginal entropies are higher. 
# - $H[X,Y]$ that stands for the joint entropy is minimal for positively or negatively correlated observations. We see a decrease in the joint entropy with more positive or negative correlation. The joint formula used with the optimal bining is not the same as the one described in the Machine Learning for Asset Managers book (snippet 3.3). In this calculation, H[X,Y] = H[X] + H[Y] - I[X,Y] where the marginal entropy is calculated using the univariate optimal bining and the mutual information is calculated with the bivariate optimal bining. When using the same formula as in snippet 3.3, the H[X,Y] had the same behavior as the marginal entropy (slighlty fixed value) which is not expected. The next step is to use a direct JE (joint entropy) estimator. 
# - $H[X|Y]$ is zero when the observations are positively or negatively correlated. The conditional entropy is the uncertainty in X if we are told the value of Y. The same observation can be made for the variation of information metric $VI[X,Y]$, $\widetilde{VI}[X,Y]$
# - $I[X,Y]$ has the opposite behavior of the variation of information in the sense that the mutual information is the information gain in X that results from knowing Y.


for result_desc, results in results_final.items():
    print(f"\nResults for {result_desc}")
    print(pd.DataFrame.from_dict(results))



// ---------------------------------------------------

// financial_labels.py
// samatix/notebooks/financial_labels.py
# Generated from: financial_labels.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Financial Labels


import sys
sys.path.append('../src')

import numpy as np
import pandas as pd 
    
import matplotlib as mpl
import matplotlib.pyplot as plt

from src.data import parse, bars

%matplotlib inline
plt.style.use('ggplot')
plt.style.use('ggplot')
plt.rcParams['figure.figsize'] = 10,7


# **Question 1** Given the time series of E-mini S&P 500 futures, compute labels on one minute
# time bars using the fixed horizon method, where $\tau$ is set at two standard deviations of one-minute returns


# Retrieving and cleaning the time series of E-mini S&P 500 futures.
#
# Download the data from: http://www.kibot.com/buy.aspx at the: "Free historical data for your data quality analysis" section
#
# We have the data from the WDC stock and the iShares IVE ETF: https://www.ishares.com/us/products/239728/ishares-sp-500-value-etf
#
# Tick Data info from kibot
# http://www.kibot.com/support.aspx#data_format
#
# - The order of the fields in the tick files (with bid/ask prices) is: Date,Time,Price,Bid,Ask,Size.
# - The bid/ask prices are recorded whenever a trade occurs and they represent the "national best bid and offer" (NBBO) prices across multiple exchanges and ECNs.
# - For each trade, current best bid/ask values are recorded together with the transaction price and volume. Trade records are not aggregated and all transactions are included in their consecutive order.
# - The order of fields in our regular tick files (without bid/ask) is: Date,Time,Price,Size.
# - The order of fields in our 1, 5 or 10 second files is: Date,Time,Open,High,Low,Close,Volume. It is the same format used in our minute files.
# - The stocks and ETFs data includes pre-market (8:00-9:30 a.m. ET), regular (9:30 a.m.-4:00 p.m. ET.) and after market (4:00-6:30 p.m. ET) sessions.
# - Trading for SPY (SPDR S&P 500 ETF) and some other liquid ETFs and stocks usually starts at 4 a.m and ends at 8 p.m. ET.


def prepare_data(raw_data_path):
    df = pd.read_csv(raw_data_path,
                     header=None,
                     names=['day', 'time', 'price', 'bid', 'ask', 'vol'])
    df['date'] = pd.to_datetime(df['day'] + df['time'],
                                format='%m/%d/%Y%H:%M:%S')
    df['dollar_vol'] = df['price']*df['vol']
    df = df.set_index('date')
    df = df.drop(['day', 'time'],
                 axis=1)
    return df


raw_data_path = '../data/IVE_tickbidask.txt'
df = prepare_data(raw_data_path)

fig, ax = plt.subplots()
_ = ax.boxplot(df.price)


# Labels computation on one minute time bars using the fixed horizon method, where
# $\tau = 2 \times \sigma$ where $\sigma$ is the standard deviations of one-minute returns


with open('../data/IVE_tickbidask.txt') as file_input: 
    pass


# The labels' Distribution


# The labels' distribution across all days, for each hour of the trading session


# Comment 



// ---------------------------------------------------

// optimal_clustering.py
// samatix/notebooks/optimal_clustering.py
# Generated from: optimal_clustering.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Optimal Clustering


import numpy as np
import pandas as pd
from sklearn.neighbors.kde import KernelDensity
import matplotlib.pyplot as plt


# 1. What is the main difference between outputs from hierarchical and partitionning clustering algorithms. Why can't the output from the latter be converted into the output of the former ?
#
# The output from a partinioning clustering algorithm (for instance, k-means) is a partition whereas the output from the hierarchical clustering algorithms is a tree known as a dendrogram as well. We can get from the tree multiple partitions but we cannot get from a partition, a hierarchical clustering. The reason is that the later doesn't provide enough information (in distance or similarity) on how other groups in the hierarchical tree are distanced. 


# 2. Is MSCI's GICS classification system an example of hierarchical or partitionning clustering ? Using the appropriate algorithm on a correlation matrix, try to replicate the MSCI classification. To compare the clustering output with MSCI's, use the clustering distance introduction in Section 3.
#
# THe GICS® classification system is an example of hierarchical clustering (more information on how the clustering is made can be found under [here](https://www.msci.com/documents/1296102/11185224/GICS+Methodology+2020.pdf/9caadd09-790d-3d60-455b-2a1ed5d1e48c?t=1578405935658). 
#
# We can use a hierarchical clustering on the companies returns.


# Download the data from IEX 

# Disclaimer, it would have been much more precise to use bar data to construct our correlation matrix 

# Retrieve the company's categorisation 

# Build the hierarchical clustering and compare


# 3. Modify Code Snippets 4.1 and 4.2 to work with a spectral biclustering algorithm. Do you get fundamentally different results ? 


çç


# 3. Modify Code Snippets 4.1 and 4.2 to work with a spectral biclustering algorithm. Do you get fundamentally different results ? 


çç



// ---------------------------------------------------

// utils.py
// samatix/src/utils.py
import numpy as np


def cov2corr(cov):
    """
    Derive the correlation matrix from a covariance matrix
    :param cov: covariance matrix
    :type cov: ndarray
    :return: correlation matrix
    :rtype: ndarray
    """
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    return corr


def corr2cov(corr, std):
    """
    Derive the covariance matrix from a correlation matrix
    :param corr: correlation matrix
    :type corr: np.ndarray
    :param std:
    :type std:
    :return:
    :rtype:
    """
    return corr * np.outer(std, std)


// ---------------------------------------------------

// __init__.py
// samatix/src/__init__.py


// ---------------------------------------------------

// test_fixtures.py
// samatix/src/test_fixtures.py
import unittest

import numpy as np
import numpy.testing as npt

from src.testing.fixtures import CorrelationFactory


class CorrelationFactoryTestCase(unittest.TestCase):

    def setUp(self) -> None:
        self.cf = CorrelationFactory(
            n_cols=10,
            n_blocks=4,
            sigma_b=0.5,
            sigma_n=1,
            seed=None
        )

    def test_get_cov_sub(self):
        sub_cov = self.cf.get_cov_sub(
            n_obs=2, n_cols=2, sigma=1.
        )
        self.assertEqual(sub_cov.shape, (2, 2))

    def test_get_rnd_block_cov(self):
        random_block_cov = self.cf.get_rnd_block_cov(n_blocks=5, sigma=1)
        self.assertEqual(random_block_cov.shape, (10, 10))
        self.assertLessEqual(np.count_nonzero(random_block_cov), 50)

    def test_random_block_corr(self):
        corr = self.cf.random_block_corr()
        self.assertEqual(corr.shape, (10, 10))
        npt.assert_almost_equal(corr.diagonal().min(), 1)
        npt.assert_almost_equal(corr.diagonal().max(), 1)


if __name__ == '__main__':
    unittest.main()


// ---------------------------------------------------

// test_clustering.py
// samatix/src/test_clustering.py
import unittest

import numpy as np
import numpy.testing as npt

from src.testing.fixtures import CorrelationFactory
from src.cluster import KMeansBase, KMeansHL


class KmeansBaseTestCase(unittest.TestCase):
    def test_clustering(self):
        corr = np.array(
            [
                [1, 0.9, -0.4, 0, 0],
                [0.9, 1, -0.3, 0.1, 0],
                [-0.4, -0.3, 1, -0.1, 0],
                [0, 0.1, -0.1, 1, 0],
                [0, 0, 0, 0, 1],

            ]
        )
        kmeans = KMeansBase(max_n_clusters=4, random_state=0).fit(corr)

        # Assert the best quality calculation
        npt.assert_almost_equal(kmeans.quality, 1.188441935313023)

        # TODO: Review the Silhouette Calculation
        # Assert that the optimal number of clusters is 2
        self.assertEqual(len(set(kmeans.labels_)), 2)
        # Assert that the 1 and 2 belong to the same cluster as
        # they are both correlated
        self.assertEqual(kmeans.labels_[0], kmeans.labels_[1])


class KmeansHLTestCase(unittest.TestCase):
    def test_clustering(self):
        corr0 = CorrelationFactory(
            n_cols=20,
            n_blocks=4,
            seed=13
        ).random_block_corr()

        cluster = KMeansHL(n_init=1, random_state=13)
        cluster.fit(corr=corr0)

        npt.assert_equal(cluster.labels_,
                         [1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 3, 3, 3, 3, 2,
                          2, 2, 2])


if __name__ == '__main__':
    unittest.main()


// ---------------------------------------------------

// test_labels.py
// samatix/src/test_labels.py
import unittest
import datetime as dt

import pandas as pd
import pandas.testing as pdt
import numpy as np

from src import labels


class TripleBarrierTestCase(unittest.TestCase):
    def setUp(self) -> None:
        self.time_index = pd.date_range('2020-08-01 08:00:00', periods=10,
                                        freq='s')
        self.tl = pd.Series(pd.NaT, index=self.time_index)

        self.tl_dyn = pd.Series(
            [
                dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                dt.datetime.fromisoformat("2020-08-01 08:00:04"),
                dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                dt.datetime.fromisoformat("2020-08-01 08:00:06"),
                dt.datetime.fromisoformat("2020-08-01 08:00:07"),
                dt.datetime.fromisoformat("2020-08-01 08:00:08"),
                dt.datetime.fromisoformat("2020-08-01 08:00:09"),
                dt.datetime.fromisoformat("2020-08-01 08:00:10"),
                dt.datetime.fromisoformat("2020-08-01 08:00:11"),
                dt.datetime.fromisoformat("2020-08-01 08:00:12"),
            ],
            index=self.time_index
        )

        self.prices = pd.Series(data=[10, 10.1, 10.2, 12, 10.1,
                                      9, 9.1, 9.2, 9.3, 9.4],
                                index=self.time_index)

        self.target = pd.Series(data=[0.1, 0.1, 0.1, 0.1, 0.1,
                                      0.1, 0.1, 0.1, 0.1, 0.1],
                                index=self.time_index)
        self.side_ = pd.Series(1., index=self.target.index)

        self.events = pd.concat({'tl': self.tl, 'target': self.target,
                                 'side': self.side_},
                                axis=1).dropna(subset=['target'])

        self.events_dyn = pd.concat({'tl': self.tl_dyn, 'target': self.target,
                                     'side': self.side_},
                                    axis=1).dropna(subset=['target'])

    def test_simulate_no_barrier(self):
        triple_barrier = labels.TripleBarrier()
        out_calculated = triple_barrier.simulate(prices=self.prices,
                                                 events=self.events,
                                                 molecule=self.events.index)
        expected_data = {
            'tl': [pd.NaT for _ in range(10)],
            'sl': [pd.NaT for _ in range(10)],
            'pt': [pd.NaT for _ in range(10)]
        }
        out_expected = pd.DataFrame(
            data=expected_data, index=self.events.index
        )
        pdt.assert_frame_equal(out_expected, out_calculated)

    def test_simulate_up_barrier(self):
        triple_barrier = labels.TripleBarrier(barrier_up=1)
        out_calculated = triple_barrier.simulate(prices=self.prices,
                                                 events=self.events,
                                                 molecule=self.events.index)
        expected_data = {
            'tl': [pd.NaT for _ in range(10)],
            'sl': [pd.NaT for _ in range(10)],
            'pt': [dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                   *(pd.NaT for _ in range(7))]
        }
        out_expected = pd.DataFrame(
            data=expected_data, index=self.events.index
        )
        pdt.assert_frame_equal(out_expected, out_calculated)

    def test_simulate_down_barrier(self):
        triple_barrier = labels.TripleBarrier(barrier_down=1)
        out_calculated = triple_barrier.simulate(prices=self.prices,
                                                 events=self.events,
                                                 molecule=self.events.index)
        expected_data = {
            'tl': [pd.NaT for _ in range(10)],
            'sl': [pd.NaT,
                   dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:04"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                   *(pd.NaT for _ in range(5))],
            'pt': [pd.NaT for _ in range(10)]
        }
        out_expected = pd.DataFrame(
            data=expected_data, index=self.events.index
        )
        pdt.assert_frame_equal(out_expected, out_calculated)

    def test_simulate_time_limit(self):
        triple_barrier = labels.TripleBarrier()
        out_calculated = triple_barrier.simulate(prices=self.prices,
                                                 events=self.events_dyn,
                                                 molecule=self.events.index)
        expected_data = {
            'tl': [
                dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                dt.datetime.fromisoformat("2020-08-01 08:00:04"),
                dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                dt.datetime.fromisoformat("2020-08-01 08:00:06"),
                dt.datetime.fromisoformat("2020-08-01 08:00:07"),
                dt.datetime.fromisoformat("2020-08-01 08:00:08"),
                dt.datetime.fromisoformat("2020-08-01 08:00:09"),
                dt.datetime.fromisoformat("2020-08-01 08:00:10"),
                dt.datetime.fromisoformat("2020-08-01 08:00:11"),
                dt.datetime.fromisoformat("2020-08-01 08:00:12"),
            ],
            'sl': [pd.NaT for _ in range(10)],
            'pt': [pd.NaT for _ in range(10)]
        }
        out_expected = pd.DataFrame(
            data=expected_data, index=self.events.index
        )
        pdt.assert_frame_equal(out_expected, out_calculated)

    def test_simulate_time_limit_barriers(self):
        triple_barrier = labels.TripleBarrier(barrier_up=1, barrier_down=1)
        out_calculated = triple_barrier.simulate(prices=self.prices,
                                                 events=self.events_dyn,
                                                 molecule=self.events.index)
        expected_data = {
            'tl': [
                dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                dt.datetime.fromisoformat("2020-08-01 08:00:04"),
                dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                dt.datetime.fromisoformat("2020-08-01 08:00:06"),
                dt.datetime.fromisoformat("2020-08-01 08:00:07"),
                dt.datetime.fromisoformat("2020-08-01 08:00:08"),
                dt.datetime.fromisoformat("2020-08-01 08:00:09"),
                dt.datetime.fromisoformat("2020-08-01 08:00:10"),
                dt.datetime.fromisoformat("2020-08-01 08:00:11"),
                dt.datetime.fromisoformat("2020-08-01 08:00:12"),
            ],
            'sl': [pd.NaT,
                   pd.NaT,
                   dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:04"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                   *(pd.NaT for _ in range(5))],
            'pt': [dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                   dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                   *(pd.NaT for _ in range(7))]
        }
        out_expected = pd.DataFrame(
            data=expected_data, index=self.events.index
        )
        pdt.assert_frame_equal(out_expected, out_calculated)

    def test_get_events_time_limit_barriers(self):
        triple_barrier = labels.TripleBarrier(barrier_up=1, barrier_down=1)
        events_calculated = triple_barrier.get_events(
            prices=self.prices,
            time_events=self.prices.index,
            target=self.target,
            tl=self.tl_dyn
        )
        expected_data = {
            'tl': [
                dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                dt.datetime.fromisoformat("2020-08-01 08:00:03"),
                dt.datetime.fromisoformat("2020-08-01 08:00:04"),
                dt.datetime.fromisoformat("2020-08-01 08:00:05"),
                dt.datetime.fromisoformat("2020-08-01 08:00:08"),
                dt.datetime.fromisoformat("2020-08-01 08:00:09"),
                dt.datetime.fromisoformat("2020-08-01 08:00:10"),
                dt.datetime.fromisoformat("2020-08-01 08:00:11"),
                dt.datetime.fromisoformat("2020-08-01 08:00:12"),
            ],
            'target': [0.1 for _ in range(10)]
        }
        events_expected = pd.DataFrame(
            data=expected_data, index=self.events.index
        )

        pdt.assert_frame_equal(events_expected, events_calculated)

    def test_get_bins_time_limit_barriers(self):
        data = {
            'tl': [
                dt.datetime.fromisoformat("2020-08-01 08:00:03"), #0
                dt.datetime.fromisoformat("2020-08-01 08:00:03"), #1
                dt.datetime.fromisoformat("2020-08-01 08:00:03"), #2
                dt.datetime.fromisoformat("2020-08-01 08:00:04"), #3
                dt.datetime.fromisoformat("2020-08-01 08:00:05"), #4
                dt.datetime.fromisoformat("2020-08-01 08:00:08"), #5
                dt.datetime.fromisoformat("2020-08-01 08:00:09"), #6
                dt.datetime.fromisoformat("2020-08-01 08:00:10"), #7
                dt.datetime.fromisoformat("2020-08-01 08:00:11"), #8
                dt.datetime.fromisoformat("2020-08-01 08:00:12"), #9
            ],
            'target': [0.1 for _ in range(10)]
        }
        events = pd.DataFrame(
            data=data, index=self.events.index
        )

        triple_barrier = labels.TripleBarrier()

        out_calculated = triple_barrier.get_bins(events=events,
                                                 prices=self.prices)

        data_expected = {
            'ret': [12 / 10 - 1,
                    12 / 10.1 - 1,
                    12 / 10.2 - 1 ,
                    10.1 / 12 - 1,
                    9 / 10.1 - 1,
                    9.3/9 - 1,
                    9.4/9.1 - 1,
                    np.nan,
                    np.nan,
                    np.nan],
            'bin': [1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, np.nan, np.nan,
                    np.nan]
        }
        out_expected = pd.DataFrame(
            data=data_expected, index=self.events.index
        )

        pdt.assert_frame_equal(out_expected, out_calculated)


if __name__ == '__main__':
    unittest.main()


// ---------------------------------------------------

// test_entropy.py
// samatix/src/test_entropy.py
import unittest
import numpy as np
import numpy.testing as npt

from src import entropy


class EntropyTestCase(unittest.TestCase):

    def setUp(self) -> None:
        self.x = np.array([
            -1.068975469981432, 0.37745946782651796, -1.4503714157560206,
            -2.0189938521856945, -0.6720045848322777, 1.0585123584971843,
            0.10590926320793637, 2.8321554887980236, -1.6415040483953403,
            0.8256354839964547
        ])
        self.e = np.array([
            -0.4355421091328046, 0.08072721876416557, -0.18228820347023844,
            0.1553520158613207, -0.07595958194802123, -1.5300711428677072,
            -1.482275653452137, -0.035086362949407486, -1.3101091248694603,
            -0.7693024441943448
        ])
        self.zeros = np.zeros(10)

    def test_histogram2d(self):
        h = entropy.histogram2d(self.e, self.x)
        npt.assert_array_equal(h, [[1., 2., 0.],
                                   [1., 1., 0.],
                                   [3., 1., 1.]])

    def test_marginal(self):
        marginal = entropy.marginal(self.x, bins=10)
        self.assertEqual(marginal, 1.8866967846580784)
        # Test the marginal entropy for
        self.assertEqual(entropy.marginal(self.zeros), 0)

    def test_joint(self):
        joint = entropy.joint(self.x, self.e)
        self.assertEqual(joint, 1.8343719702816235)

        # H(X,Y) = H(Y,X)
        self.assertEqual(joint, entropy.joint(self.e, self.x))

        # H(X,Y) <= H(X) + H(Y)
        self.assertLessEqual(
            joint,
            entropy.marginal(self.x) + entropy.marginal(self.e)
        )

        # H(X,X) = H(X)
        npt.assert_almost_equal(
            entropy.joint(self.x, self.x),
            entropy.marginal(self.x)
        )

    def test_mutual_info(self):
        y = 0 * self.x + self.e
        mi = entropy.mutual_info(self.x, y, bins=5)
        nmi = entropy.mutual_info(self.x, y, bins=5, norm=True)
        corr = np.corrcoef(self.x, y)[0, 1]

        self.assertEqual(mi, 0.5343822308972674)

        # No correlation and normalized mutual information is low (small
        # observations set)
        self.assertEqual(corr, -0.08756232304451231)
        self.assertEqual(nmi, 0.4175336691560972)

        y = 100 * self.x + self.e
        nmi = entropy.mutual_info(self.x, y, bins=5, norm=True)
        corr = np.corrcoef(self.x, y)[0, 1]

        # Linear correlation between x and y both the correlation and
        # normalized mutual information are close to 1
        self.assertEqual(corr, 0.9999901828471118)
        self.assertEqual(nmi, 1.0000000000000002)

        y = 100 * abs(self.x) + self.e
        nmi = entropy.mutual_info(self.x, y, bins=5, norm=True)
        corr = np.corrcoef(self.x, y)[0, 1]

        # Non linear correlation between x and y. Correlation is close to 0
        # but the normalized mutual information says otherwise
        self.assertEqual(corr, 0.13607916658759206)
        self.assertEqual(nmi, 0.6090771016090842)

    def test_conditional(self):
        conditional = entropy.conditional(self.x, self.e)

        # H(X) >= H(X|Y)
        self.assertGreaterEqual(entropy.marginal(self.x), conditional)

        # H(X|X) = 0
        npt.assert_almost_equal(entropy.conditional(self.x, self.x), 0)

        npt.assert_almost_equal(conditional, 0.8047189562170498)

    def test_variation_info(self):
        # The variation of information is interpreted as the uncertainty
        # we expected in one variable if we are told the value of other
        y = 0 * self.x + self.e
        vi = entropy.variation_info(self.x, y, bins=5)
        nvi = entropy.variation_info(self.x, y, bins=5, norm=True)
        corr = np.corrcoef(self.x, y)[0, 1]

        self.assertEqual(vi, 1.6295734259847894)

        # No correlation and the variation of information is high because
        # both observations are taken from random variables
        self.assertEqual(corr, -0.08756232304451231)
        self.assertEqual(nvi, 0.7530530585514705)

        y = 100 * self.x + self.e
        nvi = entropy.variation_info(self.x, y, bins=5, norm=True)
        corr = np.corrcoef(self.x, y)[0, 1]

        # Correlated variables. The normalized variation of information is very
        # low because both observations are correlated
        self.assertEqual(corr, 0.9999901828471118)
        self.assertEqual(nvi, -3.130731934134016e-16)

        y = 100 * abs(self.x) + self.e
        nvi = entropy.variation_info(self.x, y, bins=5, norm=True)
        corr = np.corrcoef(self.x, y)[0, 1]

        # Non linear correlation between x and y. The normalized
        # variation of information is somehow low (but not as we wish but this
        # is due to the fact that our sample is small)
        self.assertEqual(corr, 0.13607916658759206)
        self.assertEqual(nvi, 0.5734188849985834)

    def test_num_bins(self):
        # For marginal entropy the following bining optimal for the H(X)
        # estimator
        numb_bins = entropy.num_bins(n_obs=10)
        self.assertEqual(numb_bins, 3)

        numb_bins = entropy.num_bins(n_obs=100)
        self.assertEqual(numb_bins, 7)

        # For joint entropy with zero correlation
        numb_bins = entropy.num_bins(n_obs=10, corr=0)
        self.assertEqual(numb_bins, 3)

        # For joint entropy with total correlation
        numb_bins = entropy.num_bins(n_obs=10, corr=1)

        # For joint entropy with 0.5 correlation
        numb_bins = entropy.num_bins(n_obs=10, corr=0.99)
        self.assertEqual(numb_bins, 7)

        # The number of optimal bining increases when the correlation is high


if __name__ == '__main__':
    unittest.main()


// ---------------------------------------------------

// entropy.py
// samatix/src/entropy.py
import logging

import numpy as np
import scipy.stats as ss
from sklearn.metrics import mutual_info_score


def histogram2d(x, y, bins=None):
    if bins is None:
        bins = num_bins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    histogram, *_ = np.histogram2d(x, y, bins=bins)
    return histogram


def marginal(x, bins=None):
    """
    Marginal entropy H[X] = -sum(Ni/N * log(Ni/N))
    :param x: input data
    :type x: array_like
    :param bins: the number of equal-width bins in the given range (10, 
    by default)
    :type bins: int, optional
    :return: entropy is calculated as ``S = -sum(pk * log(pk), axis=axis)``.
    :rtype: float
    """
    if bins is None:
        bins = num_bins(x.shape[0])
    histogram, *_ = np.histogram(x, bins=bins)
    return ss.entropy(histogram)


def joint(x, y, bins=None):
    """
    Joint entropy H[X,Y] = H[X] + H[Y] - I[X,Y]
    :param x: X observations
    :type x: array_like
    :param y: Y observations
    :type y: array_like
    :param bins: the number of equal-width bins in the given range (10, 
    by default)
    :type bins: int, optional
    :return: Joint entropy
    :rtype: float
    """
    # TODO : Use a better JE without using the mutual info function

    return marginal(x, bins=bins) + marginal(y, bins=bins) \
        - mutual_info(x, y, bins=bins)


def mutual_info(x, y, bins=None, norm=False):
    """
    Mutual Information : The informational gain in X that results from 
    knowing the value of Y
    :param x: X observations
    :type x: array_like
    :param y: Y observations
    :type y: array_like
    :param bins: the number of equal-width bins in the given range (10, 
    by default)
    :type bins: int, optional
    :param norm: Parameter to get the normalized version of the measure or 
    not (False, by default)
    :type norm: bool, optional
    :return: Mutual Information I[X,Y] = H[X] - H[X|Y]
    :rtype:
    """

    if bins is None:
        bins = num_bins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])

    mi = mutual_info_score(
        None, None, contingency=histogram2d(x, y, bins=bins)
    )

    if norm:
        return mi / min(marginal(x, bins=bins),
                        marginal(y, bins=bins))
    return mi


def conditional(x, y, bins=None):
    """
    Conditional entrop H(X|Y) = H(X,Y) - H(Y)
    :param x: X observations
    :type x: array_like
    :param y: Y observations
    :type y: array_like
    :param bins: the number of equal-width bins in the given range (None,
    by default)
    :type bins: int, optional
    :return: conditional entropy
    :rtype: float
    """
    return joint(x, y, bins=bins) - marginal(y, bins=bins)


def variation_info(x, y, bins=None, norm=False):
    """
    Variation info VI(X,Y) = H(X|Y) + H(Y|X) = H(X) + H(Y) - 2 I(X,Y)
    :param x: X observations
    :type x: array_like
    :param y: Y observations
    :type y: array_like
    :param bins: the number of equal-width bins in the given range (10, 
    by default)
    :type bins: int, optional
    :param norm: Parameter to get the normalized version of the measure or 
    not (False, by default)
    :type norm: bool, optional
    :return: variation info
    :rtype: float
    """
    if bins is None:
        bins = num_bins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])

    i_xy = mutual_info(x, y, bins=bins)
    h_x = marginal(x, bins=bins)
    h_y = marginal(y, bins=bins)

    v_xy = h_x + h_y - 2 * i_xy
    if norm:
        h_xy = h_x + h_y - i_xy
        return v_xy / h_xy
    return v_xy


def num_bins(n_obs, corr=None):
    """
    Optimal number of bins for discretization as described in :
    Abdenour Hacine-Gharbi, Philippe Ravier, Rachid Harba, Tayeb Mohamadi,
    Low bias histogram-based estimation of mutual information for feature 
    selection,
    Pattern Recognition Letters,
    Volume 33, Issue 10,
    2012,
    Pages 1302-1308,
    ISSN 0167-8655,
    https://doi.org/10.1016/j.patrec.2012.02.022.
    (http://www.sciencedirect.com/science/article/pii/S0167865512000761) 
    :param n_obs: number of observations
    :type n_obs: int
    :param corr: Correlation between X, Y 
    :type corr: float
    :return: Optimal number of bins
    :rtype: int
    """
    if corr is None:
        z = (8 + 324 * n_obs + 12 * (
                36 * n_obs + 729 * n_obs ** 2) ** 0.5) ** (1 / 3.)

        b = round(z / 6. + 2. / (3 * z) + 1. / 3)
    else:
        try:
            b = round(
                2 ** (-.5) * (1 + (
                        1 + 24 * n_obs / (1 - corr ** 2)) ** .5)
                ** .5)

        except (ZeroDivisionError, OverflowError):
            logging.error(
                f"To use the optimal bining for joint entropy, "
                f"the correlation should not be equal to 1 or -1. "
                f"The correlation given is equal to {corr}"
            )
            return num_bins(n_obs)

    return int(b)


// ---------------------------------------------------

// labels.py
// samatix/src/labels.py
import statsmodels.api as sml
import numpy as np
import pandas as pd


# TODO: Implement Meta Labeling

def t_value_lin(sample):
    """
    t value from a linear trend
    :param sample:
    :type sample:
    :return:
    :rtype:
    """
    x = np.ones((sample.shape[0]), 2)
    x[:, 1] = np.arange(sample.shape[0])
    ols = sml.OLS(sample, x).fit()
    return ols.tvalues[1]


def getDailyVol(close, span0=100):
    # daily vol, reindexed to close
    df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))
    df0 = df0[df0 > 0]
    df0 = pd.Series(close.index[df0 - 1],
                    index=close.index[close.shape[0] - df0.shape[0]:])

    # daily returns
    df0 = close.loc[df0.index] / close.loc[df0.values].values - 1
    df0 = df0.ewm(span=span0).std()
    return df0


class TripleBarrier:
    def __init__(self, barrier_up=0, barrier_down=0, min_return=-999):
        """
        :param min_return: The minimum target return required for running
        a triple barrier search.
        :type min_return: float
        :param barrier_up: non-negative float value that is used for setting
        the upper barrier. If 0 there will be no upper barrier
        :type barrier_up: float
        :param barrier_down: non-negative float value that is used for setting
        the inferior barrier. If 0 there will be no inferior barrier
        :type barrier_down: float
        """
        self.barrier_up = barrier_up
        self.barrier_down = barrier_down
        self.min_return = min_return

    def get_events(self, prices, time_events, target, tl=False):
        """

        :param prices: A pandas series of prices
        :type prices: pd.Series
        :param time_events: The pandas timeindex containing the timestamps
        that will seed every triple barrier.
        These are the timestamps selected by the sampling procedures
        discussed in Chapter 2, Section 2.5.
        :type time_events:
        :param target: A pandas series of targets, expressed in terms of
        absolute returns.
        :type target: pd.Series

        :param tl: A pandas series with the timestamps of the vertical
        barriers. We pass a False when we want to disable vertical barriers.
        :type tl: pd.Series or Boolean
        :return: pd.DataFrame with the time index and a tl and target columns
        and the next date when one of the triple barriers is hit
        :rtype: pd.DataFrame
        """
        # Get target
        target_filtered = target.loc[time_events]
        target_filtered = target_filtered[
            target_filtered > self.min_return]  # minRet
        # Get tl (max holding period)
        if tl is False:
            tl = pd.Series(pd.NaT, index=time_events)
        # Form events object, apply stop loss on tl
        side_ = pd.Series(1., index=target_filtered.index)
        events = pd.concat({'tl': tl,
                            'target': target_filtered,
                            'side': side_},
                           axis=1).dropna(subset=['target'])

        df0 = self.simulate(prices=prices, events=events,
                            molecule=events.index)
        events['tl'] = df0.dropna(how='all').min(axis=1)  # pd.min ignores nan
        events = events.drop('side', axis=1)
        return events

    def simulate(self, prices, events, molecule):
        """
        Apply stop loss/profit taking, if it takes place before tl
        (end of event)
        :param prices: Prices
        :type prices: pd.Series
        :param events: A pandas dataframe, with columns:
            tl: The timestamp of vertical barrier. When the value is np.nan,
            there will not be a vertical barrier.
            target: The unit width of the horizontal barriers
        :type events: pd.DataFrame
        :param molecule: A list with the subset of event indices
        that will be processed by a single thread.
        :type molecule: pd.DataFrame
        :return: pd.DataFrame with the time index and 3 columns of
        when one of the barriers is hit
        :rtype: pd.DataFrame
        """
        #
        events_ = events.loc[molecule]
        out = events_[['tl']].copy(deep=True)
        if self.barrier_up > 0:
            pt = self.barrier_up * events_['target']
        else:
            pt = pd.Series(index=events.index)  # NaNs
        if self.barrier_down > 0:
            sl = -self.barrier_down * events_['target']
        else:
            sl = pd.Series(index=events.index)  # NaNs
        for loc, tl in events_['tl'].fillna(prices.index[-1]).items():
            df0 = prices[loc:tl]  # path prices
            df0 = (df0 / prices[loc] - 1) * events_.at[
                loc, 'side']  # path returns
            out.loc[loc, 'sl'] = df0[
                df0 < sl[loc]].index.min()  # earliest stop loss.
            out.loc[loc, 'pt'] = df0[
                df0 > pt[loc]].index.min()  # earliest profit taking.
        return out

    @staticmethod
    def get_bins(events, prices):

        # 1) prices aligned with events
        events_ = events.dropna(subset=['tl'])
        px = events_.index.union(events_['tl'].values).drop_duplicates()
        px = prices.reindex(px, method='bfill')
        # 2) create out object
        out = pd.DataFrame(index=events_.index)
        out['ret'] = px.loc[events_['tl'].values].values / px.loc[
            events_.index] - 1
        out['bin'] = np.sign(out['ret'])
        return out


class TrendScan:
    def simulate(self, prices, molecule, span):
        """
        Derive labels from the sign of t-value of the linear trend
        :param prices: Time series of {x_t}
        :type prices: array like
        :param molecule: the index of the observations we wish to label
        :type molecule: array like
        :param span: set of values of L, the look forward period
        :type span: array like
        """
        out = pd.DataFrame(
            index=molecule,
            columns=['tl', 'tval', 'bin']
        )
        horizons = np.xrange(*span)
        for dt in molecule:
            df = pd.Series()
            iloc = prices.index.get_loc(dt)
            if iloc + max(horizons) > prices.shape[0]:
                continue
            for horizon in horizons:
                dt1 = prices.index[iloc + horizon - 1]
                df1 = prices.loc[dt:dt1]
                df.loc[dt1] = t_value_lin(df1.values)

            dt1 = df.replace([-np.inf, np.inf, np.nan], 0).abs().idxmax()
            out.loc[dt, ['tl', 'tval', 'bin']] = df.index[-1], df[
                dt1], np.sign(df[dt1])
        out['tl'] = pd.to_datetime(out['tl'])
        out['bin'] = pd.to_numeric(out['bin'], downcast='signed')
        return out.dropna(subset=['bin'])


// ---------------------------------------------------

// cluster.py
// samatix/src/cluster.py
from itertools import groupby
from operator import itemgetter
import logging

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples

logger = logging.Logger(__name__)


class KMeansBase(KMeans):
    def __init__(self, max_n_clusters=None, n_init=10, **kwargs):
        """
        KMeans Base Clustering method
        This kmeans algorithm doesn't require a
        :param max_n_clusters:
        :type max_n_clusters:
        :param n_init:
        :type n_init:
        :param kwargs:
        :type kwargs:
        """
        self.max_n_clusters = max_n_clusters
        self.n_init = n_init
        self.quality = None
        self.silhouette = None
        super().__init__(n_init=n_init, **kwargs)

    def fit(self, corr, y=None, sample_weight=None):
        if self.max_n_clusters is None:
            self.max_n_clusters = corr.shape[1] - 1

        # Transform the correlation matrix to X
        X = ((1 - np.nan_to_num(corr)) / 2) ** 0.5
        # Init the best figures
        best_cluster_centers_ = None
        best_labels_ = None
        best_inertia_ = None
        best_n_iter_ = None
        best_silhouette = None
        best_quality = -1

        for init in range(self.n_init):
            for i in range(2, self.max_n_clusters + 1):
                logging.info(f"Clustering iteration: init: {init}, "
                             f"max_cluster: {i}")
                self.n_clusters = i
                super().fit(X, y=y, sample_weight=sample_weight)
                silhouette = silhouette_samples(X, self.labels_)
                quality = silhouette.mean() / silhouette.std()
                if best_silhouette is None or quality > best_quality:
                    best_silhouette = silhouette
                    best_cluster_centers_ = self.cluster_centers_
                    best_labels_ = self.labels_.copy()
                    best_inertia_ = self.inertia_
                    best_n_iter_ = self.n_iter_
                    best_quality = quality

        # We keep only the best kmeans data
        self.cluster_centers_ = best_cluster_centers_
        self.labels_ = best_labels_
        self.inertia_ = best_inertia_
        self.n_iter_ = best_n_iter_
        self.quality = best_quality
        self.silhouette = best_silhouette
        self.n_clusters = len(best_labels_)
        return self

    @property
    def clusters(self):
        cluster = {}
        for i in self.labels_:
            cluster[i] = np.where(self.labels_ == i)[0]
        return cluster


class KMeansHL(KMeansBase):
    @staticmethod
    def eval_scores(labels, silhouette):
        clusters = groupby(
            sorted(zip(labels, silhouette)),
            itemgetter(0)
        )

        for key, data in clusters:
            silhouettes = tuple(data)
            if silhouettes is not None:
                mean = np.mean(silhouettes, axis=0)[1]
                vol = np.std(silhouettes, axis=0)[1]
                if vol != 0.:
                    yield key, mean / vol
                else:
                    yield key, 0.
            else:
                yield key, None

    def merge(self, sub_other, corr, cluster_redo, clusters_scores_avg):
        rows_idx, = np.where(
            np.isin(self.labels_, cluster_redo)
        )
        new_labels = self.labels_.copy()
        new_labels[rows_idx] = sub_other.labels_ + max(self.labels_)
        X = ((1 - np.nan_to_num(corr)) / 2.) ** .5
        new_silhouette = silhouette_samples(X, new_labels)
        new_quality = new_silhouette.mean() / new_silhouette.std()
        clusters_scores = tuple(self.eval_scores(new_labels, new_silhouette))
        _, new_clusters_scores_avg = np.mean(np.nan_to_num(clusters_scores),
                                             axis=0)
        logger.info(f"A solution with score {new_clusters_scores_avg} using "
                    f"KMeans HL found compared to the original score  "
                    f"{clusters_scores_avg}")
        if new_clusters_scores_avg > clusters_scores_avg:
            # TODO: Trigger recalculation of kmeans attributes from new labels
            self.labels_ = new_labels
            self.quality = new_quality
            self.silhouette = new_silhouette

    def fit(self, corr, y=None, sample_weight=None):
        # Initial clustering
        super().fit(corr=corr, y=y, sample_weight=sample_weight)

        # Second clustering
        clusters_scores = tuple(self.eval_scores(self.labels_,
                                                 self.silhouette))
        _, clusters_scores_avg = np.mean(np.nan_to_num(clusters_scores),
                                         axis=0)
        cluster_redo = tuple(
            key for key, score in clusters_scores
            if score < clusters_scores_avg
        )

        if len(cluster_redo) > 1:
            # Redo elements in bad quality clusters
            rows_idx, = np.where(np.isin(self.labels_, cluster_redo))
            # Get the sub-correlation matrix with indexes in rows_idx
            corr_sub = corr[rows_idx[:, None], rows_idx]
            kmeans_sub = KMeansHL(
                max_n_clusters=min(
                    self.max_n_clusters,
                    corr_sub.shape[1] - 1),
                n_init=self.n_init,
                init=self.init,
                max_iter=self.max_iter,
                tol=self.tol,
                verbose=self.verbose,
                random_state=self.random_state,
                copy_x=self.copy_x,
                algorithm=self.algorithm
            ).fit(corr_sub)
            self.merge(sub_other=kmeans_sub, corr=corr,
                       cluster_redo=cluster_redo,
                       clusters_scores_avg=clusters_scores_avg)
        return self


// ---------------------------------------------------

// __init__.py
// samatix/src/testing/__init__.py


// ---------------------------------------------------

// fixtures.py
// samatix/src/testing/fixtures.py
import os
import random
import datetime as dt
from collections import namedtuple
import tempfile
import unittest

import numpy as np
from scipy.linalg import block_diag
from sklearn.utils import check_random_state

from src.data.models import Quote, Tick


class CorrelationFactory:
    def __init__(self, n_cols=None, n_blocks=None, seed=None,
                 min_block_size=1, sigma_b=0.5, sigma_n=1):
        """
        Generate a subcorrelation matrix
        :param n_cols: Number of columns
        :type n_cols: int
        :param n_blocks: Number of correlation blocks
        :type n_blocks: int
        :param sigma_b: Standard deviation (spread or "width") of the
            blocks distributions. Must be non-negative.
        :type sigma_b: float
        :param sigma_n: Standard deviation (spread or "width") of the
            global noise. Must be non-negative. Can be set to 0 to remove the
            noise
        :type sigma_n: float
        :param seed: Seed for random variables generation
        :type seed: None | int | instance of np.RandomState
        """
        self.n_cols = n_cols
        self.n_blocks = n_blocks
        self.random_state = check_random_state(seed)
        self.min_block_size = min_block_size
        self.sigma_b = sigma_b
        self.sigma_n = sigma_n

    def get_cov_sub(self, n_obs, n_cols, sigma):
        """
        :return: Generate a random sub covariance matrix from
            randomly generated observations.
        :rtype: ndarray
        """
        if n_cols == 1:
            return np.ones((1, 1))
        sub_correl = self.random_state.normal(size=(n_obs, 1))
        sub_correl = np.repeat(sub_correl, n_cols, axis=1)
        sub_correl += self.random_state.normal(
            scale=sigma,
            size=sub_correl.shape
        )
        return np.cov(sub_correl, rowvar=False)

    def get_rnd_block_cov(self, n_blocks, sigma=1):
        """
        Generate a block random covariance matrix
        :param sigma: Standard deviation (spread or "width") of the 
            distribution. Must be non-negative.
        :type sigma: float
        :return: 
        :rtype: 
        """
        parts = self.random_state.choice(
            range(1, self.n_cols - (self.min_block_size - 1) * n_blocks),
            n_blocks - 1,
            replace=False
        )
        parts.sort()
        parts = np.append(
            parts,
            self.n_cols - (self.min_block_size - 1) * n_blocks
        )
        parts = np.append(parts[0], np.diff(parts)) - 1 + self.min_block_size
        cov = None
        for n_cols_ in parts:
            cov_ = self.get_cov_sub(
                int(max(n_cols_ * (n_cols_ + 1) / 2., 100)),
                n_cols_, sigma)
            if cov is None:
                cov = cov_.copy()
            else:
                # Create a block diagonal matrix from provided arrays
                cov = block_diag(cov, cov_)
        return cov

    @staticmethod
    def cov2corr(cov):
        """
        Derive the correlation matrix from a covariance matrix
        :param cov: covariance matrix
        :type cov: ndarray
        :return: correlation matrix
        :rtype: ndarray
        """
        std = np.sqrt(np.diag(cov))
        corr = cov / np.outer(std, std)
        corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
        return corr

    def random_block_corr(self):
        """
        For a block correlation with a noise part
        :return:
        :rtype:
        """
        cov0 = self.get_rnd_block_cov(sigma=self.sigma_b,
                                      n_blocks=self.n_blocks)
        cov1 = self.get_rnd_block_cov(sigma=self.sigma_n,
                                      n_blocks=1)  # add noise
        cov0 += cov1
        corr0 = self.cov2corr(cov0)
        return corr0

    def get_rnd_covariance(self, facts_number=1):
        """
        Get a random covariance and add signal to it
        :param facts_number: the number of factors
        :type facts_number: int
        :return: a full rank covariance matrix
        :rtype: np.array
        """
        w = np.random.normal(size=(self.n_cols, facts_number))
        covariance = np.dot(w, w.T)
        covariance += np.diag(np.random.uniform(size=self.n_cols))
        return covariance


class TickFactory:
    def __init__(self, tick_number=10, price_average=100, price_volatility=0.1,
                 start_time=dt.datetime.now() - dt.timedelta(minutes=10),
                 end_time=dt.datetime.now(), store_ticks=False):
        """
        Tick factory
        :param tick_number: Number of ticks to generate
        :type tick_number: int
        :param price_average: Average price
        :type price_average: float
        :param price_volatility: Average volatility
        :type price_volatility: float
        :param start_time: Start time defaulted to now - 10 minutes
        :type start_time: dt.datetime
        :param end_time: End time defaulted to now
        :type end_time: dt.datetime
        :param store_ticks: option to store or not ticks in the self instance
        :type store_ticks: bool
        """
        self.tick_number = tick_number
        self.price_average = price_average
        self.price_volatility = price_volatility
        self.price_now = price_average
        self.time_now = start_time
        self.time_step = (end_time - start_time) / tick_number
        self.time_delta = None
        self.store_ticks = store_ticks
        self.ticks = []

    def random_step(self, start, delta):
        """
        This function returns a random time between start and start + delta
        :param start: start time
        :type start: dt.datetime
        :param delta: time delta
        :type delta: dt.timedelta
        :return: random step
        :rtype: dt.datetime
        """
        self.time_delta = dt.timedelta(
            seconds=random.uniform(0, delta.total_seconds())
        )
        return start + self.time_delta

    def random_quote(self, price_current, delta, volatility):
        """

        :param price_current: Current price that will be used to calculate
        the new quote
        :type price_current: float
        :param delta: Time delta
        :type delta:
        :param volatility:
        :type volatility:
        :return:
        :rtype:
        """
        price = price_current + \
                random.choice(
                    (-1, 1)) * delta.total_seconds() ** 0.5 * volatility
        return Quote(
            price=price,
            bid=price - 0.1 * random.random(),
            ask=price + 0.1 * random.random()
        )

    def generate_one_tick(self):
        """

        :return: A tick in string format e.g
        "06/19/2020,16:00:00,109.34,109.32,109.38,379\n"
        :rtype: str
        """
        tick_time = self.random_step(start=self.time_now,
                                     delta=self.time_step)
        quote = self.random_quote(
            price_current=self.price_now,
            delta=self.time_delta,
            volatility=self.price_volatility
        )
        tick = Tick(
            time=tick_time,
            price=quote.price,
            bid=quote.bid,
            ask=quote.ask,
            quantity=random.uniform(100, 200)
        )
        if self.store_ticks:
            self.ticks.append(tick)

        self.price_now = tick.price
        self.time_now = tick.time
        return (f"{tick.time.strftime('%m/%d/%Y,%H:%M:%S')},"
                f"{tick.price},{tick.bid},{tick.ask},{tick.quantity}")

    def generate_all_ticks(self):
        """
        Generator function to generate multiple ticks (total equal to
        tick_number)
        :return: yields a generated tick
        :rtype: str
        """
        for _ in range(self.tick_number):
            yield self.generate_one_tick()

    def to_file(self, output_file):
        with open(output_file, 'w') as o:
            for tick in self.generate_all_ticks():
                o.write(tick)


class BaseTestCase(unittest.TestCase):
    TICK_DATA = (
        "06/19/2020,16:00:00,109.34,109.32,109.38,500\n"
        "06/19/2020,16:03:13,109.37,109.37,112.66,1700\n"
        "06/19/2020,16:03:13,109.37,109.37,112.66,750\n"
        "06/19/2020,16:03:13,109.37,109.37,112.66,250\n"
        "wrong_line\n"
        "06/19/2020,16:03:13,109.37,109.37,112.66,1000\n"
        "06/19/2020,16:03:13,109.37,109.37,112.66,750\n"
        "06/19/2020,16:03:14,109.37,109.37,110.54,500"
    )

    TICK_DATA_PARSED = [
        Tick(time=dt.datetime(2020, 6, 19, 16, 0),
             price=109.34, bid=109.32, ask=109.38, quantity=500),
        Tick(time=dt.datetime(2020, 6, 19, 16, 3, 13),
             price=109.37, bid=109.37, ask=112.66, quantity=1700),
        Tick(time=dt.datetime(2020, 6, 19, 16, 3, 13),
             price=109.37, bid=109.37, ask=112.66, quantity=750),
        Tick(time=dt.datetime(2020, 6, 19, 16, 3, 13),
             price=109.37, bid=109.37, ask=112.66, quantity=250),
        Tick(time=dt.datetime(2020, 6, 19, 16, 3, 13),
             price=109.37, bid=109.37, ask=112.66, quantity=1000),
        Tick(time=dt.datetime(2020, 6, 19, 16, 3, 13),
             price=109.37, bid=109.37, ask=112.66, quantity=750),
        Tick(time=dt.datetime(2020, 6, 19, 16, 3, 14),
             price=109.37, bid=109.37, ask=110.54, quantity=500)
    ]

    @classmethod
    def setUpClass(cls) -> None:
        cls.ticks_factory = TickFactory()

    def setUp(self):
        self.test_files = {
            'input_file': self.generate_temp_file(
                contents=self.ticks_factory.generate_all_ticks()
            ),
            'output_file': self.generate_temp_file()
        }

    def tearDown(self):
        for test_file in self.test_files.values():
            if os.path.exists(test_file):
                os.remove(test_file)

    def generate_temp_file(self, contents=None):
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            if contents is not None:
                for content in contents:
                    temp_file.write(content.encode('utf-8'))
            return temp_file.name


// ---------------------------------------------------

// __init__.py
// samatix/src/data/__init__.py


// ---------------------------------------------------

// models.py
// samatix/src/data/models.py
# Copyright 2020 Ayoub ENNASSIRI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from collections import namedtuple

Tick = namedtuple('Tick', ('time', 'price', 'bid', 'ask', 'quantity'))
Quote = namedtuple('Quote', ('price', 'bid', 'ask'))


// ---------------------------------------------------

// test_parse.py
// samatix/src/data/test_parse.py
# Copyright 2020 Ayoub ENNASSIRI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import os
import tempfile
import unittest

import pandas as pd

from src.data.parse import ParseTickDataFn
from src.testing.fixtures import BaseTestCase


class TickDataSetTest(BaseTestCase):
    def setUp(self):
        self.test_files = {
            'input_file': self.generate_temp_file(contents=self.TICK_DATA),
            'output_file': self.generate_temp_file()
        }

    def tearDown(self):
        for test_file in self.test_files.values():
            if os.path.exists(test_file):
                os.remove(test_file)

    def generate_temp_file(self, contents=None):
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            if contents is not None:
                temp_file.write(contents.encode('utf-8'))
            return temp_file.name

    def test_output_file(self):
        with open(self.test_files['input_file'], 'r') as file_input:
            data_raw = (ParseTickDataFn().process(file_input.readlines()))

        self.assertEqual(list(data_raw), self.TICK_DATA_PARSED)


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    unittest.main()


// ---------------------------------------------------

// bars.py
// samatix/src/data/bars.py
# Copyright 2020 Ayoub ENNASSIRI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import numpy as np
import pandas as pd

from src.runner import pipeline


class TickBarFn(pipeline.DoFn):
    """
    Parse the tick objects into Tick Bars
    """

    def __init__(self, threshold=10):
        """
        Tick Bar Function
        :param threshold: The number of ticks at which we extract the bid ask
        :type threshold: int
        """
        self.ticks_processed = 0
        self.buffer = 0
        self.threshold = threshold

    def process(self, elements):
        for e in elements:
            self.buffer += 1
            self.ticks_processed += 1
            if self.buffer == self.threshold:
                self.buffer = 0
                yield e


class VolumeBarFn(pipeline.DoFn):
    """
    Parse the tick objects into volume bars
    """

    def __init__(self, threshold=10000):
        """
        Volume Bar Function
        :param threshold: The accumulated volume threshold at which we extract
        the bid ask
        :type threshold: float
        """
        self.ticks_processed = 0
        self.buffer = 0
        self.threshold = threshold

    def process(self, elements):
        for e in elements:
            self.buffer += e.quantity * e.price
            self.ticks_processed += 1
            if self.buffer >= self.threshold:
                self.buffer = 0
                yield e


// ---------------------------------------------------

// test_bars.py
// samatix/src/data/test_bars.py
# Copyright 2020 Ayoub ENNASSIRI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import unittest

from src.testing.fixtures import BaseTestCase, TickFactory
from src.data import bars


class TickBarTestCase(BaseTestCase):
    @classmethod
    def setUpClass(cls) -> None:
        cls.ticks_factory = TickFactory(
            tick_number=100,
            store_ticks=True
        )

    def test_tick_bar_data(self):

        expected_ticks = []
        count = 1
        for tick in self.ticks_factory.ticks:
            if count % 10 == 0:
                expected_ticks.append(tick)
            count += 1

        self.assertEqual(
            expected_ticks,
            list(
                bars.TickBarFn(threshold=10).process(
                    self.ticks_factory.ticks
                )
            )
        )


class VolumeBarTestCase(BaseTestCase):
    @classmethod
    def setUpClass(cls) -> None:
        cls.ticks_factory = TickFactory(
            tick_number=2,
            store_ticks=True
        )

    def test_volume_bar_data(self):
        expected_ticks = []
        volume_threshold = 100000
        volume = 0
        for tick in self.ticks_factory.ticks:
            volume += tick.price * tick.quantity
            if volume >= volume_threshold:
                expected_ticks.append(tick)
                volume = 0
        logging.info(f"Volume threshold = {volume_threshold}")
        logging.info(expected_ticks)

        self.assertEqual(
            expected_ticks,
            list(bars.VolumeBarFn(threshold=volume_threshold).process(
                self.ticks_factory.ticks
            ))
        )

    def test_sample_volume_bar_data(self):
        self.assertEqual(
            [self.TICK_DATA_PARSED[1], self.TICK_DATA_PARSED[3],
             self.TICK_DATA_PARSED[4], self.TICK_DATA_PARSED[6]],
            list(
                bars.VolumeBarFn(threshold=100000).process(
                    self.TICK_DATA_PARSED
                )
            )
        )


if __name__ == '__main__':
    logging.getLogger().setLevel(logging.INFO)
    unittest.main()


// ---------------------------------------------------

// clean.py
// samatix/src/data/clean.py
import logging

import numpy as np
import pandas as pd
# from sklearn.neighbors.kde import KernelDensity
from sklearn.neighbors import KernelDensity
from scipy.optimize import minimize

from src.utils import cov2corr


class MarcenkoPastur:
    def __init__(self, points=1000):
        """
        Marcenko-Pastur

        :param points:
        :type points: int
        :return:The Marcenko-Pastur probability density function
        :rtype: pd.Series
        """
        self.points = points
        self.eigen_max = None

    def pdf(self, var, q):
        """
        :param var: The variance
        :type var: float
        :param q: N/T number of observations on the number of dates
        :type q: float
        :return: 
        :rtype: 
        """
        if isinstance(var, np.ndarray):
            var = var.item()
        eigen_min = var * (1 - (1. / q) ** .5) ** 2
        eigen_max = var * (1 + (1. / q) ** .5) ** 2
        eigen_values = np.linspace(eigen_min,
                                   eigen_max,
                                   self.points)
        pdf = q / (2 * np.pi * var * eigen_values) * (
                (eigen_max - eigen_values) * (
                eigen_values - eigen_min)) ** .5
        pdf = pd.Series(pdf, index=eigen_values)
        return pdf

    def err_pdfs(self, var, eigenvalues, q, bandwidth):
        pdf0 = self.pdf(var, q)
        pdf1 = fit_kde(
            eigenvalues, bandwidth,
            x=pdf0.index.values.reshape(-1, 1)
        )
        sse = np.sum((pdf1 - pdf0) ** 2)
        return sse

    def fit(self, eigenvalues, q, bandwidth):
        func = lambda *x: self.err_pdfs(*x)
        x0 = 0.5
        out = minimize(func, x0,
                       args=(eigenvalues, q, bandwidth),
                       bounds=((1E-5, 1 - 1E-5),))

        if out['success']:
            var = out['x'][0]
        else:
            var = 1
        eigen_max = var * (1 + (1. / q) ** 0.5) ** 2
        self.eigen_max = eigen_max
        return eigen_max, var

    def facts_number(self, eigenvalues):
        if self.eigen_max is not None:
            return eigenvalues.shape[0] - \
                   np.diag(eigenvalues)[::-1].searchsorted(self.eigen_max)
        else:
            raise ValueError(f"Eigen max is not calculated. Please "
                             f"run the fit method before calculating the "
                             f"facts number")

    def _denoise_constant_residual(self, eigenvalues, eigenvectors):
        facts_number = self.facts_number(eigenvalues)
        eigenvalues_ = eigenvalues.diagonal().copy()
        # Denoising by making constant the eigen values past facts_number
        eigenvalues_[facts_number:] = eigenvalues_[
                                      facts_number:].sum() / float(
            eigenvalues_.shape[0] - facts_number)
        eigenvalues_ = np.diag(eigenvalues_)
        cov = eigenvectors @ eigenvalues_ @ eigenvectors.T
        # Rescaling
        return cov2corr(cov)

    def _denoise_shrink(self, eigenvalues, eigenvectors, alpha=0):

        # Eigenvalues and eigenvectors corresponding
        # to the eigenvalues less than the max value
        facts_number = self.facts_number(eigenvalues)

        eigenvalues_l = eigenvalues[:facts_number, :facts_number]
        eigenvectors_l = eigenvectors[:, :facts_number]

        # Eigenvalues and eigenvectors corresponding
        # to the eigenvalues more than the max value

        eigenvalues_r = eigenvalues[facts_number:, facts_number:]
        eigenvectors_r = eigenvectors[:, facts_number:]

        corr_l = eigenvectors_l @ eigenvalues_l @ eigenvectors_l.T
        corr_r = eigenvectors_r @ eigenvalues_r @ eigenvectors_r.T

        return corr_l + alpha * corr_r + (1 - alpha) * np.diag(
            corr_r.diagonal())

    def denoise(self, eigenvalues, eigenvectors, method="constant", alpha=0):
        """
        Remove noise from corr by fixing random eigenvalues
        :param eigenvalues:
        :type eigenvalues:
        :param eigenvectors:
        :type eigenvectors:
        :param method:
        :type method: str
        :param alpha:
        :type alpha:
        :return:
        :rtype:
        """
        if method == "constant":
            return self._denoise_constant_residual(eigenvalues, eigenvectors)
        elif method == "shrink":
            return self._denoise_shrink(eigenvalues, eigenvectors, alpha=alpha)
        else:
            raise ValueError(f"The only available denoising methods are "
                             f"'constant' or 'shrink'. The method provided is "
                             f"{method}")

    def detone(self, eigenvalues, eigenvectors):
        # Test if the correlation matrix has a market component
        eigenvalues_m = eigenvalues[0, 0]
        eigenvectors_m = eigenvectors[:, 0]

        cov = (eigenvectors @ eigenvalues @ eigenvectors.T) - \
              (eigenvectors_m @ eigenvalues_m @ eigenvectors_m.T)

        return cov2corr(cov)


def get_pca(matrix):
    """
    Function to retrieve the eigenvalues and eigenvector from a Hermitian
    matrix
    :param matrix: Hermitian matrix
    :type matrix: np.matrix or np.ndarray
    :return:
    :rtype:
    """
    eigenvalues, eigenvectors = np.linalg.eigh(matrix)
    indices = eigenvalues.argsort()[::-1]
    eigenvalues, eigenvectors = eigenvalues[indices], eigenvectors[:, indices]
    eigenvalues = np.diagflat(eigenvalues)
    return eigenvalues, eigenvectors


def fit_kde(obs, bandwidth=0.25, kernel='gaussian', x=None):
    """
    Fit kernel to a series of observations and derive the probability of obs
    :param obs:
    :type obs:
    :param bandwidth:
    :type bandwidth:
    :param kernel:
    :type kernel:
    :param x: The array of values on which the fit KDE will be evaluated
    :type x: array like
    :return:
    :rtype:
    """
    if len(obs.shape) == 1:
        obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(obs)
    if x is None:
        x = np.unique(obs).reshape(-1, 1)
    log_prob = kde.score_samples(x)
    pdf = pd.Series(np.exp(log_prob), index=x.flatten())
    return pdf


// ---------------------------------------------------

// parse.py
// samatix/src/data/parse.py
# Copyright 2020 Ayoub ENNASSIRI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import datetime as dt
import csv
import logging

from src.data.models import Tick
from src.runner import pipeline


class ParseTickDataFn(pipeline.DoFn):
    """
    Parse the raw tick data events into a tick object
    """

    def __init__(self):
        self.ticks_counter = 0
        self.errors_parse_num = 0

    def process(self, elements):
        for e in elements:
            try:
                row = list(csv.reader([e]))[0]
                self.ticks_counter += 1
                yield Tick(
                    time=dt.datetime.strptime(
                        f"{row[0]},{row[1]}",
                        '%m/%d/%Y,%H:%M:%S'
                    ),
                    price=float(row[2]),
                    bid=float(row[3]),
                    ask=float(row[4]),
                    quantity=float(row[5])
                )
            except:
                self.errors_parse_num += 1
                logging.error(f"Parsing error of element = {e}")


// ---------------------------------------------------

// test_pipeline.py
// samatix/src/runner/test_pipeline.py
import unittest
import timeit
import logging

from src.runner import pipeline

logging.basicConfig(level=logging.DEBUG)


class Function1(pipeline.DoFn):
    def __init__(self):
        self.processed = 0

    def process(self, elements):
        for e in elements:
            self.processed += 1
            yield e * 2


class Function2(pipeline.DoFn):
    def __init__(self):
        self.processed = 0

    def process(self, elements):
        for e in elements:
            self.processed += 1
            yield e * 3


class PipelineTestCase(unittest.TestCase):
    def test_dofn(self):
        fct_1 = Function1()
        fct_2 = Function2()

        fct_o = fct_1 | fct_2

        result = (fct_o.process(range(1, 10)))

        self.assertEqual(
            tuple(result),
            tuple(e * 2 * 3 for e in range(1, 10))
        )

        self.assertEqual(fct_1.processed, 9)
        self.assertEqual(fct_2.processed, 9)

    def test_performance(self):
        def perf():
            fct_1 = Function1()
            fct_2 = Function2()

            fct_o = fct_1 | fct_2
            x = tuple(fct_o.process(range(1, 100)))

        perf_n = 1000
        perf_t = timeit.timeit(perf, number=perf_n)
        logging.info(f"Pipeline Test Case Performance = {perf_t / perf_n}")
        self.assertLess(perf_t / perf_n, 1e-4)


if __name__ == '__main__':
    unittest.main()


// ---------------------------------------------------

// __init__.py
// samatix/src/runner/__init__.py


// ---------------------------------------------------

// 5_financial_lables.py
// shuangology/5_financial_lables.py
# Generated from: 5_financial_lables.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# SNIPPET 5.1 T-VALUE OF A LINEAR TREND
import statsmodels.api as sm1
import numpy as np
import pandas as pd
# ---------------------------------------------------


def tValLinR(close):
    # tValue of the beta coefficient of the trend on time index from a linear trend (it shows the significance of )
    x = np.ones((close.shape[0], 2))
    x[:, 1] = np.arange(close.shape[0])
    ols = sm1.OLS(close, x).fit()
    return ols.tvalues[1]


# SNIPPET 5.2 IMPLEMENTATION OF THE TREND-SCANNING METHOD
def getBinsFromTrend(molecule, close, span):
    '''
    Derive labels from the sign of t-value of linear trend
    Output includes:
        - t1: End time for the identified trend
        - tVal: t-value associated with the estimated trend coefficient 
        - bin: Sign of the trend
    '''
    out = pd.DataFrame(index=molecule, columns=['t1', 'tVal', 'bin'])
    hrzns = range(*span)
    for dt0 in molecule:
        df0 = pd.Series()
        iloc0 = close.index.get_loc(dt0)
        if iloc0+max(hrzns) > close.shape[0]:
            continue
        for hrzn in hrzns:
            dt1 = close.index[iloc0+hrzn-1]
            df1 = close.loc[dt0:dt1]
            df0.loc[dt1] = tValLinR(df1.values)
        dt1 = df0.replace([-np.inf, np.inf, np.nan], 0).abs().idxmax()
        out.loc[dt0, ['t1', 'tVal', 'bin']
                ] = df0.index[-1], df0[dt1], np.sign(df0[dt1])  # prevent leakage
    out['t1'] = pd.to_datetime(out['t1'])
    out['bin'] = pd.to_numeric(out['bin'], downcast='signed')
    return out.dropna(subset=['bin'])


# SNIPPET 5.3 TESTING THE TREND-SCANNING LABELING ALGORITHM
import matplotlib.pyplot as plt
df0 = pd.Series(np.random.normal(0, .1, 100)).cumsum()
df1 = getBinsFromTrend(df0.index, df0, [3, 10, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(df1.index, df0.loc[df1.index].values,
           c=df1['bin'].values, cmap='viridis')
ax.set_title('Simu without Sine trend')


df0 = pd.Series(np.random.normal(0, .1, 100)).cumsum()
df0 += np.sin(np.linspace(0, 10, df0.shape[0]))
df1 = getBinsFromTrend(df0.index, df0, [3, 10, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
ax.scatter(df1.index, df0.loc[df1.index].values,
           c=df1['tVal'].values, cmap='viridis')
ax.set_title('Simu with Sine trend')
# mpl.savefig('fig 5.1.png');
# mpl.clf();
# mpl.close()
# mpl.scatter(df1.index,df0.loc[df1.index].values,cmap='viridis')



// ---------------------------------------------------

// denoise.py
// shuangology/denoise.py
from sklearn.covariance import LedoitWolf
from scipy.linalg import block_diag
from scipy.optimize import minimize
from sklearn.neighbors.kde import KernelDensity
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# %% md

# The Marcenko-Pastur Theorem


# %% md

# SNIPPET 2.1

# %%


# ---------------------------------------------------

def mpPDF(var, q, pts):
    # Marcenko-Pastur pdf
    # q=T/N
    # when var= 1, C = T^-1 X'X  is the correlation matrix associated with X
    # lambda+ =,lambda- = eMax, eMin
    eMin, eMax = var*(1-(1./q)**.5)**2, var*(1+(1./q)**.5)**2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5
    # pdf = pdf.ravel()
    pdf = pd.Series(pdf, index=eVal)
    return pdf


# %% md

# SNIPPET 2.2

# %%

# ---------------------------------------------------


def getPCA(matrix):
    # Get eVal,eVec from a Hermitian matrix
    eVal, eVec = np.linalg.eigh(matrix)
    indices = eVal.argsort()[::-1]  # arguments for sorting eVal desc
    eVal, eVec = eVal[indices], eVec[:, indices]
    eVal = np.diagflat(eVal)
    return eVal, eVec
# ---------------------------------------------------


def fitKDE(obs, bWidth=.25, kernel='gaussian', x=None):
    # Fit kernel to a series of obs, and derive the prob of obs
    # x is the array of values on which the fit KDE will be evaluated
    if len(obs.shape) == 1:
        obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    if x is None:
        x = np.unique(obs).reshape(-1, 1)
    if len(x.shape) == 1:
        x = x.reshape(-1, 1)
    logProb = kde.score_samples(x)  # log(density)
    pdf = pd.Series(np.exp(logProb), index=x.flatten())
    return pdf


# %%

# ---------------------------------------------------
x = np.random.normal(size=(10000, 1000))
eVal0, eVec0 = getPCA(np.corrcoef(x, rowvar=False)
                      )  # each column is a variable
pdf0 = mpPDF(1., q=x.shape[0]/float(x.shape[1]), pts=1000)
pdf1 = fitKDE(np.diag(eVal0), bWidth=.01)  # empirical pdf
ax = plt.figure().add_subplot(111)
ax.plot(pdf0, label='Marcenko-Pastur')
ax.plot(pdf1, linestyle='--', label='Empirical:KDE')
ax.set_xlabel(r'$\lambda$')
ax.set_ylabel(r'prob[$\lambda$]')
ax.legend()

# %% md


# Random Matrix with Signal (not perfectly random)

# %%

# SNIPPET 2.3 ADD SIGNAL TO A RANDOM COVARIANCE MATRIX
def getRndCov(nCols, nFacts):
    w = np.random.normal(size=(nCols, nFacts))
    cov = np.dot(w, w.T)  # random cov matrix, however not full rank
    cov += np.diag(np.random.uniform(size=nCols))  # full rank cov
    return cov
# ---------------------------------------------------


def cov2corr(cov):
    # Derive the correlation matrix from a covariance matrix
    std = np.sqrt(np.diag(cov))
    corr = cov/np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    return corr


# ---------------------------------------------------
alpha, nCols, nFact, q = .995, 1000, 100, 10
cov = np.cov(np.random.normal(size=(nCols*q, nCols)), rowvar=False)
cov = alpha*cov+(1-alpha)*getRndCov(nCols, nFact)  # noise+signal
corr0 = cov2corr(cov)
eVal0, eVec0 = getPCA(corr0)

# %%

# SNIPPET 2.4 FITTING THE MARCENKO–PASTUR PDF
# ---------------------------------------------------


def errPDFs(var, eVal, q, bWidth, pts=1000):
    # Fit error
    var = var[0]
    pdf0 = mpPDF(var, q, pts)  # theoretical pdf
    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)  # empirical pdf
    # import pdb; pdb.set_trace()
    sse = np.sum((pdf1-pdf0)**2)
    return sse
# ---------------------------------------------------


def findMaxEval(eVal, q, bWidth):
    # Find max random eVal by fitting Marcenko’s dist
    out = minimize(lambda *x: errPDFs(*x), .5,
                   args=(eVal, q, bWidth), bounds=((1E-5, 1-1E-5),))
    if out['success']:
        var = out['x'][0]
    else:
        var = 1
    eMax = var*(1+(1./q)**.5)**2
    return eMax, var


# ---------------------------------------------------
eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth=.01)
nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)

# nFacts0 gives the number of the eigenvalue is assumed to be important (cutoff level lambda+ adjusted for the presence of nonrandom eigenvectors)

# %%

# ---------------------------------------------------
# Fitting the Marcenko–Pastur PDF on a noisy covariance matrix.
# estimate the sigma for Marcenko-Pastur dist
bWidth = 0.01
out = minimize(lambda *x: errPDFs(*x), .5,
               args=(np.diag(eVal0), q, bWidth), bounds=((1E-5, 1-1E-5),))
if out['success']:
    var = out['x'][0]
else:
    var = 1

pdf0 = mpPDF(var, q, pts=1000)  # Marcenko-Pastur dist
pdf1 = fitKDE(np.diag(eVal0), bWidth=.01)  # empirical pdf
ax = plt.figure().add_subplot(111)
ax.plot(pdf0, label='Marcenko-Pastur dist')
ax.bar(pdf1.index, pdf1.values, width=bWidth,
       label='Empirical dist', color='darkorange')
ax.set_xlabel(r'$\lambda$')
ax.set_ylabel(r'prob[$\lambda$]')
ax.legend()

# %% md

# 2.5 Denoising

# %% md

# 2.5.1 Constant Residual Eigenvalue Method

setting a constant eigenvalue for all random eigenvectors.

# %%


def denoisedCorr(eVal, eVec, nFacts):
    # Remove noise from corr by fixing random eigenvalues
    eVal_ = np.diag(eVal).copy()
    eVal_[nFacts:] = eVal_[nFacts:].sum()/float(eVal_.shape[0] -
                                                nFacts)  # average the rest
    eVal_ = np.diag(eVal_)
    corr1 = np.dot(eVec, eVal_).dot(eVec.T)
    corr1 = cov2corr(corr1)
    return corr1


# ---------------------------------------------------
corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
eVal1, eVec1 = getPCA(corr1)

# %%

# A comparison of eigenvalues before and after applying the residual eigenvalue method.
ax = plt.figure().add_subplot(111)
ax.plot(np.diagonal(eVal0), label='Original eigen-function')
ax.plot(np.diagonal(eVal1),
        label='Denoised eigen-function (Constant Residual)', linestyle='--')
ax.legend()
ax.set_yscale('log')
ax.set_xlabel('Eigenvalue number')
ax.set_ylabel('Eigenvalue (log-scale)')

# %% md

# 2.5.2 Targeted Shrinkage
$\alpha$ regulates the amount fo shrinkage among the eigen vectors

# %%

# SNIPPET 2.6 DENOISING BY TARGETED SHRINKAGE


def denoisedCorr2(eVal, eVec, nFacts, alpha=0):
    # Remove noise from corr through targeted shrinkage
    eValL, eVecL = eVal[:nFacts, :nFacts], eVec[:, :nFacts]
    eValR, eVecR = eVal[nFacts:, nFacts:], eVec[:, nFacts:]
    corr0 = np.dot(eVecL, eValL).dot(eVecL.T)
    corr1 = np.dot(eVecR, eValR).dot(eVecR.T)
    corr2 = corr0+alpha*corr1+(1-alpha)*np.diag(np.diag(corr1))
    return corr2


# ---------------------------------------------------
corr1 = denoisedCorr2(eVal0, eVec0, nFacts0, alpha=.5)
eVal1, eVec1 = getPCA(corr1)

# %%

# A comparison of eigenvalues before and after applying the residual eigenvalue method.
ax = plt.figure().add_subplot(111)
ax.plot(np.diagonal(eVal0), label='Original eigen-function')
ax.plot(np.diagonal(eVal1),
        label='Denoised eigen-function (targeted shrinkage)', linestyle='--')
ax.legend()
ax.set_yscale('log')
ax.set_xlabel('Eigenvalue number')
ax.set_ylabel('Eigenvalue (log-scale)')

# %% md

# Experimental Results
# 2.7.1 Minimum Variance Portfolio

# %%


def corr2cov(corr, std):
    # Derive the covariance matrix from a correlation matrix
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    cov = np.outer(std, std)*corr
    return cov

# %%


# SNIPPET 2.7 GENERATING A BLOCK-DIAGONAL COVARIANCE MATRIX AND A VECTOR OF MEANS
def formBlockMatrix(nBlocks, bSize, bCorr):
    block = np.ones((bSize, bSize))*bCorr
    block[range(bSize), range(bSize)] = 1
    corr = block_diag(*([block]*nBlocks))
    return corr
# ---------------------------------------------------


def formTrueMatrix(nBlocks, bSize, bCorr):
    # In each block, the variances are drawn from a uniform distribution bounded between 5% and 20%; the vector of means is drawn from a Normal distribution with mean and standard deviation equal to the standard deviation from the covariance matrix
    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(.05, .2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0


# ---------------------------------------------------
nBlocks, bSize, bCorr = 10, 50, .5
np.random.seed(0)
mu0, cov0 = formTrueMatrix(nBlocks, bSize, bCorr)

# %%

# SNIPPET 2.8 GENERATING THE EMPIRICAL COVARIANCE MATRIX


def simCovMu(mu0, cov0, nObs, shrink=False):
    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)
    mu1 = x.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(x).covariance_
    else:
        cov1 = np.cov(x, rowvar=0)
    return mu1, cov1

# %%

# SNIPPET 2.9 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX


def deNoiseCov(cov0, q, bWidth):
    corr0 = cov2corr(cov0)
    eVal0, eVec0 = getPCA(corr0)
    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth)
    nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)
    corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
    cov1 = corr2cov(corr1, np.diag(cov0)**.5)
    return cov1

# %%

# SNIPPET 2.10 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX


def optPort(cov, mu=None):  # optimal portfolio for minimum variance
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = np.dot(inv, mu)
    w /= np.dot(ones.T, w)
    return w


// ---------------------------------------------------

// 2_Denoising_and_Detoning_aider.py
// shuangology/2_Denoising_and_Detoning_aider.py
# 2_Denoising_and_Detoning.py
# Implementation of code snippets and exercises from Chapter 2 of "Machine Learning for Asset Managers"

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from sklearn.neighbors.kde import KernelDensity
from scipy.optimize import minimize
from sklearn.covariance import LedoitWolf
from scipy.linalg import block_diag
import bs4 as bs
import requests
import yfinance as yf
import datetime
import scipy.optimize as opt


def mpPDF(var, q, pts):
    eMin, eMax = var * (1 - (1. / q) ** .5) ** 2, var * \
        (1 + (1. / q) ** .5) ** 2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q / (2 * np.pi * var * eVal) * ((eMax - eVal) * (eVal - eMin)) ** .5
    pdf = pd.Series(pdf, index=eVal)
    return pdf


def getPCA(matrix):
    eVal, eVec = np.linalg.eigh(matrix)
    indices = eVal.argsort()[::-1]
    eVal, eVec = eVal[indices], eVec[:, indices]
    eVal = np.diagflat(eVal)
    return eVal, eVec


def fitKDE(obs, bWidth=.25, kernel='gaussian', x=None):
    if len(obs.shape) == 1:
        obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    if x is None:
        x = np.unique(obs).reshape(-1, 1)
    if len(x.shape) == 1:
        x = x.reshape(-1, 1)
    logProb = kde.score_samples(x)
    pdf = pd.Series(np.exp(logProb), index=x.flatten())
    return pdf


def getRndCov(nCols, nFacts):
    w = np.random.normal(size=(nCols, nFacts))
    cov = np.dot(w, w.T)
    cov += np.diag(np.random.uniform(size=nCols))
    return cov


def cov2corr(cov):
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1
    return corr


def errPDFs(var, eVal, q, bWidth, pts=1000):
    var = var[0]
    pdf0 = mpPDF(var, q, pts)
    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)
    sse = np.sum((pdf1 - pdf0) ** 2)
    return sse


def findMaxEval(eVal, q, bWidth):
    out = minimize(lambda *x: errPDFs(*x), .5, args=(eVal,
                   q, bWidth), bounds=((1E-5, 1 - 1E-5),))
    if out['success']:
        var = out['x'][0]
    else:
        var = 1
    eMax = var * (1 + (1. / q) ** .5) ** 2
    return eMax, var


def denoisedCorr(eVal, eVec, nFacts):
    eVal_ = np.diag(eVal).copy()
    eVal_[nFacts:] = eVal_[nFacts:].sum() / float(eVal_.shape[0] - nFacts)
    eVal_ = np.diag(eVal_)
    corr1 = np.dot(eVec, eVal_).dot(eVec.T)
    corr1 = cov2corr(corr1)
    return corr1


def denoisedCorr2(eVal, eVec, nFacts, alpha=0):
    eValL, eVecL = eVal[:nFacts, :nFacts], eVec[:, :nFacts]
    eValR, eVecR = eVal[nFacts:, nFacts:], eVec[:, nFacts:]
    corr0 = np.dot(eVecL, eValL).dot(eVecL.T)
    corr1 = np.dot(eVecR, eValR).dot(eVecR.T)
    corr2 = corr0 + alpha * corr1 + (1 - alpha) * np.diag(np.diag(corr1))
    return corr2


def corr2cov(corr, std):
    corr[corr < -1], corr[corr > 1] = -1, 1
    cov = np.outer(std, std) * corr
    return cov


def formBlockMatrix(nBlocks, bSize, bCorr):
    block = np.ones((bSize, bSize)) * bCorr
    block[range(bSize), range(bSize)] = 1
    corr = block_diag(*([block] * nBlocks))
    return corr


def formTrueMatrix(nBlocks, bSize, bCorr):
    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(.05, .2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0


def simCovMu(mu0, cov0, nObs, shrink=False):
    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)
    mu1 = x.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(x).covariance_
    else:
        cov1 = np.cov(x, rowvar=0)
    return mu1, cov1


def deNoiseCov(cov0, q, bWidth):
    corr0 = cov2corr(cov0)
    eVal0, eVec0 = getPCA(corr0)
    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth)
    nFacts0 = eVal0.shape[0] - np.diag(eVal0)[::-1].searchsorted(eMax0)
    corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
    cov1 = corr2cov(corr1, np.diag(cov0) ** .5)
    return cov1


def optPort(cov, mu=None):
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = np.dot(inv, mu)
    w /= np.dot(ones.T, w)
    return w


def portfolio_volatility(weights, day_returns, cov_matrix):
    return np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))


def portfolio_annualised_performance(weights, day_returns, cov_matrix):
    returns = np.sum(day_returns.T.dot(weights)) * 252
    std = np.sqrt(np.dot(weights, np.dot(cov_matrix, weights))) * np.sqrt(252)
    return std, returns


def random_portfolios(num_portfolios, day_returns, cov_matrix):
    results = np.zeros((2, num_portfolios))
    weights_record = []
    for i in range(num_portfolios):
        weights = np.random.normal(0.1, 0.1, len(day_returns))
        weights /= np.sum(weights)
        weights_record.append(weights)
        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(
            weights, day_returns, cov_matrix)
        results[0, i] = portfolio_std_dev
        results[1, i] = portfolio_return
    return results, weights_record


def efficient_return(day_returns, cov_matrix, target):
    num_assets = len(day_returns)
    args = (day_returns, cov_matrix)

    def portfolio_return(weights):
        return portfolio_annualised_performance(weights, day_returns, cov_matrix)[1]

    constraints = ({'type': 'eq', 'fun': lambda x: portfolio_return(x) - target},
                   {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
    bounds = tuple((0, 1) for asset in range(num_assets))
    result = opt.minimize(portfolio_volatility, num_assets * [
                          1. / num_assets, ], args=args, method='SLSQP', bounds=bounds, constraints=constraints)
    return result


def efficient_frontier(day_returns, cov_matrix, returns_range):
    efficients = []
    for ret in tqdm(returns_range, desc='calculating efficient frontier using optimization method:'):
        efficients.append(efficient_return(day_returns, cov_matrix, ret))
    return efficients


def ef_with_random_portfolio_opt(day_returns, cov_matrix, num_portfolios, ax=None, return_plot=True):
    results, weights = random_portfolios(
        num_portfolios, day_returns, cov_matrix)
    target = np.linspace(
        max(np.min(results[1]), 0), np.quantile(results[1], 0.7), 30)
    efficient_portfolios = efficient_frontier(day_returns, cov_matrix, target)
    frontier = [p['fun'] for p in efficient_portfolios]

    if return_plot:
        if not ax:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111)
        ax.plot(frontier, target, color='black',
                linewidth=2, label='efficient frontier')
        ax.scatter(results[0, :], results[1, :], marker='o', s=10, alpha=0.3)
        ax.set_title(
            'Calculated Portfolio Optimization based on Efficient Frontier')
        ax.set_xlabel('annualised volatility')
        ax.set_ylabel('annualised returns')
        ax.legend()
    return ax, [frontier, target]


def efficient_return_simu(results, target):
    results = pd.DataFrame(results.T).sort_values(by=1)
    closiest_idx = np.argmin(np.abs(results[1] - target))
    data_target = results[1][closiest_idx]
    target_range_min = min(data_target * 0.95, data_target * 1.05)
    target_range_max = max(data_target * 0.95, data_target * 1.05)
    sub_results = results.loc[(results[1] <= target_range_max) & (
        results[1] >= target_range_min), 0:2]
    return min(sub_results[0])


def efficient_frontier_emp(day_returns, cov_matrix, num_portfolios, returns_range, random_seed=0):
    efficients = []
    np.random.seed(random_seed)
    results, weights = random_portfolios(
        num_portfolios, day_returns, cov_matrix)
    for ret in returns_range:
        efficients.append(efficient_return_simu(results, ret))
    return efficients


def ef_with_random_portfolio_simu(day_returns, cov_matrix, num_portfolios, ax=None, return_plot=True, random_seed=0, mean_frontier=None):
    results, weights = random_portfolios(
        num_portfolios, day_returns, cov_matrix)
    target_start = 0
    if True:
        target_end = 400
    else:
        target_end = results[1][results[0] == np.max(results[0])][0]

    target = np.linspace(target_start, target_end, 30)
    efficient_portfolios = efficient_frontier_emp(
        day_returns, cov_matrix, num_portfolios, target, random_seed=random_seed)

    if return_plot:
        if not ax:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111)
        if mean_frontier:
            ax.plot(mean_frontier[0], mean_frontier[1], color='black',
                    linewidth=2, label='mean efficient frontier')
        else:
            ax.plot(efficient_portfolios, target, color='black',
                    linewidth=2, label='efficient frontier')
        ax.scatter(results[0, :], results[1, :], marker='o', s=10, alpha=0.3)
        ax.set_title(
            'Calculated Portfolio Optimization based on Efficient Frontier')
        ax.set_xlabel('annualised volatility')
        ax.set_ylabel('annualised returns')
        ax.legend()
    else:
        ax = None
    return ax, [efficient_portfolios, target]


def MC_ef_frontier(day_returns, cov_matrix, itertimes=100, random_seed=3):
    np.random.seed(random_seed)
    target_ret = pd.DataFrame(day_returns.T)
    target_cov = cov_matrix
    frontier = []
    for i in tqdm(range(itertimes)):
        ax, frontier_ret = ef_with_random_portfolio_simu(
            target_ret, target_cov, 1000, return_plot=False, random_seed=i)
        frontier.append(frontier_ret[0])
    mean_frontier_vol = np.mean(frontier, axis=0)
    mean_frontier_ret = frontier_ret[1]
    return mean_frontier_vol, mean_frontier_ret


def error_mean_ef_frontier(frontiers, mean_frontier):
    err = []
    for frontier in frontiers:
        err.append(np.std(frontier - mean_frontier))
    return np.var(err)


if __name__ == "__main__":
    nBlocks, bSize, bCorr = 10, 50, .5
    np.random.seed(0)
    mu0, cov0 = formTrueMatrix(nBlocks, bSize, bCorr)
    print("True Matrix formed with mean and covariance.")


// ---------------------------------------------------

// 2_Denoising_and_Detoning.py
// shuangology/2_Denoising_and_Detoning.py
# Generated from: 2_Denoising_and_Detoning.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Chapter 2 Denoising and Detoning
#
# reduce the noise and enhance the signal included in an empirical covariance matrix.


import numpy as np,pandas as pd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm


# ## The Marcenko-Pastur Theorem


# ### SNIPPET 2.1



# ---------------------------------------------------

def mpPDF(var, q, pts):
    # Marcenko-Pastur pdf
    # q=T/N
    # when var= 1, C = T^-1 X'X  is the correlation matrix associated with X
    # lambda+ =,lambda- = eMax, eMin
    eMin, eMax = var*(1-(1./q)**.5)**2, var*(1+(1./q)**.5)**2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5
    # pdf = pdf.ravel()
    pdf = pd.Series(pdf, index=eVal)
    return pdf


# ### SNIPPET 2.2


from sklearn.neighbors.kde import KernelDensity
# ---------------------------------------------------


def getPCA(matrix):
    # Get eVal,eVec from a Hermitian matrix
    eVal, eVec = np.linalg.eigh(matrix)
    indices = eVal.argsort()[::-1]  # arguments for sorting eVal desc
    eVal, eVec = eVal[indices], eVec[:, indices]
    eVal = np.diagflat(eVal)
    return eVal, eVec
# ---------------------------------------------------


def fitKDE(obs, bWidth=.25, kernel='gaussian', x=None):
    # Fit kernel to a series of obs, and derive the prob of obs
    # x is the array of values on which the fit KDE will be evaluated
    if len(obs.shape) == 1:
        obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    if x is None:
        x = np.unique(obs).reshape(-1, 1)
    if len(x.shape) == 1:
        x = x.reshape(-1, 1)
    logProb = kde.score_samples(x)  # log(density)
    pdf = pd.Series(np.exp(logProb), index=x.flatten())
    return pdf


# ---------------------------------------------------
x = np.random.normal(size=(10000, 1000))
eVal0, eVec0 = getPCA(np.corrcoef(x, rowvar=False)
                      )  # each column is a variable
pdf0 = mpPDF(1., q=x.shape[0]/float(x.shape[1]), pts=1000)
pdf1 = fitKDE(np.diag(eVal0), bWidth=.01)  # empirical pdf
ax = plt.figure().add_subplot(111)
ax.plot(pdf0, label='Marcenko-Pastur')
ax.plot(pdf1, linestyle='--', label='Empirical:KDE')
ax.set_xlabel(r'$\lambda$')
ax.set_ylabel(r'prob[$\lambda$]')
ax.legend()


#
# ## Random Matrix with Signal (not perfectly random)


# SNIPPET 2.3 ADD SIGNAL TO A RANDOM COVARIANCE MATRIX
def getRndCov(nCols, nFacts):
    w = np.random.normal(size=(nCols, nFacts))
    cov = np.dot(w, w.T)  # random cov matrix, however not full rank
    cov += np.diag(np.random.uniform(size=nCols))  # full rank cov
    return cov
# ---------------------------------------------------


def cov2corr(cov):
    # Derive the correlation matrix from a covariance matrix
    std = np.sqrt(np.diag(cov))
    corr = cov/np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    return corr


# ---------------------------------------------------
alpha, nCols, nFact, q = .995, 1000, 100, 10
cov = np.cov(np.random.normal(size=(nCols*q, nCols)), rowvar=False)
cov = alpha*cov+(1-alpha)*getRndCov(nCols, nFact)  # noise+signal
corr0 = cov2corr(cov)
eVal0, eVec0 = getPCA(corr0)


# SNIPPET 2.4 FITTING THE MARCENKO–PASTUR PDF
from scipy.optimize import minimize
# ---------------------------------------------------


def errPDFs(var, eVal, q, bWidth, pts=1000):
    # Fit error
    var = var[0]
    pdf0 = mpPDF(var, q, pts)  # theoretical pdf
    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)  # empirical pdf
    # import pdb; pdb.set_trace()
    sse = np.sum((pdf1-pdf0)**2)
    return sse
# ---------------------------------------------------


def findMaxEval(eVal, q, bWidth):
    # Find max random eVal by fitting Marcenko’s dist
    out = minimize(lambda *x: errPDFs(*x), .5,
                   args=(eVal, q, bWidth), bounds=((1E-5, 1-1E-5),))
    if out['success']:
        var = out['x'][0]
    else:
        var = 1
    eMax = var*(1+(1./q)**.5)**2
    return eMax, var


# ---------------------------------------------------
eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth=.01)
nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)

# nFacts0 gives the number of the eigenvalue is assumed to be important 
# (cutoff level lambda+ adjusted for the presence of nonrandom eigenvectors)


# ---------------------------------------------------
# Fitting the Marcenko–Pastur PDF on a noisy covariance matrix.
# estimate the sigma for Marcenko-Pastur dist
bWidth = 0.01
out = minimize(lambda *x: errPDFs(*x), .5,
               args=(np.diag(eVal0), q, bWidth), bounds=((1E-5, 1-1E-5),))
if out['success']:
    var = out['x'][0]
else:
    var = 1

pdf0 = mpPDF(var, q, pts=1000)  # Marcenko-Pastur dist
pdf1 = fitKDE(np.diag(eVal0), bWidth=.01)  # empirical pdf
ax = plt.figure().add_subplot(111)
ax.plot(pdf0, label='Marcenko-Pastur dist')
ax.bar(pdf1.index, pdf1.values, width=bWidth,
       label='Empirical dist', color='darkorange')
ax.set_xlabel(r'$\lambda$')
ax.set_ylabel(r'prob[$\lambda$]')
ax.legend()


# ## 2.5 Denoising


# ### 2.5.1 Constant Residual Eigenvalue Method
#
# setting a constant eigenvalue for all random eigenvectors.


def denoisedCorr(eVal, eVec, nFacts):
    # Remove noise from corr by fixing random eigenvalues
    eVal_ = np.diag(eVal).copy()
    eVal_[nFacts:] = eVal_[nFacts:].sum()/float(eVal_.shape[0] -
                                                nFacts)  # average the rest
    eVal_ = np.diag(eVal_)
    corr1 = np.dot(eVec, eVal_).dot(eVec.T)
    corr1 = cov2corr(corr1)
    return corr1


# ---------------------------------------------------
corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
eVal1, eVec1 = getPCA(corr1)


# A comparison of eigenvalues before and after applying the residual eigenvalue method.
ax = plt.figure().add_subplot(111)
ax.plot(np.diagonal(eVal0), label='Original eigen-function')
ax.plot(np.diagonal(eVal1),
        label='Denoised eigen-function (Constant Residual)', linestyle='--')
ax.legend()
ax.set_yscale('log')
ax.set_xlabel('Eigenvalue number')
ax.set_ylabel('Eigenvalue (log-scale)')


# ### 2.5.2 Targeted Shrinkage
# $\alpha$ regulates the amount fo shrinkage among the eigen vectors


# SNIPPET 2.6 DENOISING BY TARGETED SHRINKAGE
def denoisedCorr2(eVal, eVec, nFacts, alpha=0):
    # Remove noise from corr through targeted shrinkage
    eValL, eVecL = eVal[:nFacts, :nFacts], eVec[:, :nFacts]
    eValR, eVecR = eVal[nFacts:, nFacts:], eVec[:, nFacts:]
    corr0 = np.dot(eVecL, eValL).dot(eVecL.T)
    corr1 = np.dot(eVecR, eValR).dot(eVecR.T)
    corr2 = corr0+alpha*corr1+(1-alpha)*np.diag(np.diag(corr1))
    return corr2


# ---------------------------------------------------
corr1 = denoisedCorr2(eVal0, eVec0, nFacts0, alpha=.5)
eVal1, eVec1 = getPCA(corr1)


# A comparison of eigenvalues before and after applying the residual eigenvalue method.
ax = plt.figure().add_subplot(111)
ax.plot(np.diagonal(eVal0), label='Original eigen-function')
ax.plot(np.diagonal(eVal1),
        label='Denoised eigen-function (targeted shrinkage)', linestyle='--')
ax.legend()
ax.set_yscale('log')
ax.set_xlabel('Eigenvalue number')
ax.set_ylabel('Eigenvalue (log-scale)')


# # Experimental Results
# ## 2.7.1 Minimum Variance Portfolio


def corr2cov(corr, std):
    # Derive the covariance matrix from a correlation matrix
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    cov = np.outer(std, std)*corr
    return cov



# SNIPPET 2.7 GENERATING A BLOCK-DIAGONAL COVARIANCE MATRIX AND A VECTOR OF MEANS
from sklearn.covariance import LedoitWolf
from scipy.linalg import block_diag


def formBlockMatrix(nBlocks, bSize, bCorr):
    block = np.ones((bSize, bSize))*bCorr
    block[range(bSize), range(bSize)] = 1
    corr = block_diag(*([block]*nBlocks))
    return corr
# ---------------------------------------------------


def formTrueMatrix(nBlocks, bSize, bCorr):
    # In each block, the variances are drawn from a uniform distribution bounded between 5% and 20%; the vector of means is drawn from a Normal distribution with mean and standard deviation equal to the standard deviation from the covariance matrix
    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(.05, .2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0


# ---------------------------------------------------
nBlocks, bSize, bCorr = 10, 50, .5
np.random.seed(0)
mu0, cov0 = formTrueMatrix(nBlocks, bSize, bCorr)


# SNIPPET 2.8 GENERATING THE EMPIRICAL COVARIANCE MATRIX
def simCovMu(mu0, cov0, nObs, shrink=False):
    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)
    mu1 = x.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(x).covariance_
    else:
        cov1 = np.cov(x, rowvar=0)
    return mu1, cov1


# SNIPPET 2.9 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX
def deNoiseCov(cov0, q, bWidth):
    corr0 = cov2corr(cov0)
    eVal0, eVec0 = getPCA(corr0)
    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth)
    nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)
    corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
    cov1 = corr2cov(corr1, np.diag(cov0)**.5)
    return cov1


# SNIPPET 2.10 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX
def optPort(cov, mu=None):  # optimal portfolio for minimum variance
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = np.dot(inv, mu)
    w /= np.dot(ones.T, w)
    return w


# ---------------------------------------------------
nObs, nTrials, bWidth, shrink, minVarPortf = 1000, 100, .01, False, True
w1 = w1_s = pd.DataFrame(columns=range(cov0.shape[0]),
                         index=range(nTrials), dtype=float)
w1_d = w1.copy(deep=True)
w1_s_d = w1_s.copy(deep=True)
np.random.seed(0)
for i in tqdm(range(nTrials)):
    mu1, cov1 = simCovMu(mu0, cov0, nObs, shrink=True)
    if minVarPortf:
        mu1 = None
    cov1_d = deNoiseCov(cov1, nObs*1./cov1.shape[1], bWidth)
    w1_s.loc[i] = optPort(cov1, mu1).flatten()
    w1_s_d.loc[i] = optPort(cov1_d, mu1).flatten()


for i in tqdm(range(nTrials)):
    mu1, cov1 = simCovMu(mu0, cov0, nObs, shrink=False)
    if minVarPortf:
        mu1 = None
    cov1_d = deNoiseCov(cov1, nObs*1./cov1.shape[1], bWidth)
    w1.loc[i] = optPort(cov1, mu1).flatten()
    w1_d.loc[i] = optPort(cov1_d, mu1).flatten()


# SNIPPET 2.11 ROOT-MEAN-SQUARE ERRORS
w0 = optPort(cov0, None if minVarPortf else mu0)
w0 = np.repeat(w0.T, w1.shape[0], axis=0)
# RMSE  not shrunk not denoised
rmsd = np.mean((w1-w0).values.flatten()**2)**.5
rmsd_d = np.mean((w1_d-w0).values.flatten()**2)**.5  # RMSE not shrunk denoised
rmsd_s = np.mean((w1_s-w0).values.flatten()**2)**.5  # RMSE shrunk not denoised
rmsd_s_d = np.mean((w1_s_d-w0).values.flatten()**2)**.5  # RMSE shrunk denoised

res_tab = pd.DataFrame(columns=['Note denoised', 'Denoised'], index=[
                       'Not shrunk', 'Shrunk'], data=np.array([[rmsd, rmsd_d], [rmsd_s, rmsd_s_d]]))
res_tab


# # Exercise


# 2.9 Exercise
# Download the historical SP500 stocks daily closing price for the most recent year
import bs4 as bs
import requests
import yfinance as yf
import datetime

resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
soup = bs.BeautifulSoup(resp.text, 'lxml')
table = soup.find('table', {'class': 'wikitable sortable'})
tickers = []
for row in table.findAll('tr')[1:]:
    ticker = row.findAll('td')[0].text
    tickers.append(ticker)

tickers = [s.replace('\n', '') for s in tickers]
popular_tickers = ['AAPL', 'AMZN', 'FB', 'GOOGL', 'SPY']
data = yf.download(tickers, period="ytd", auto_adjust=True, threads=True)
data = data.T.reset_index()


# get the log return time series and calculate the covariance matrix for the SP 500 stocks
cls = data[data.level_0 == 'Close']
cls = cls.drop(['level_0'], axis='columns')

cls = cls.set_index(['level_1'])
cls = cls.T
cls.index = pd.to_datetime(cls.index)

logret = cls.apply(np.log, axis=0).diff().drop(
    pd.to_datetime(['2020-01-02']), axis=0).fillna(0)


# 2.a Covaraince matrix ; condition number of the correlation matrix

covmat = np.cov(logret, rowvar=False)  # compute the covariance matrix
Codnum = np.linalg.cond(covmat)  # condition number of correlation matrix
Codnum


# 2.b one hundread efficient frontiers

def portfolio_annualised_performance(weights, day_returns, cov_matrix):
    returns = np.sum(day_returns.T.dot(weights)) * 252
    std = np.sqrt(np.dot(weights, np.dot(cov_matrix, weights))) * np.sqrt(252)
    return std, returns


def random_portfolios(num_portfolios, day_returns, cov_matrix):
    '''
    Return performance of required number of random portfolios
    '''
    results = np.zeros((2, num_portfolios))
    weights_record = []
    for i in range(num_portfolios):
        # mean and standard deviation of the alternative vectors of expected returns
        mu, sigma = 0.1, 0.1
        weights = np.random.normal(mu, sigma, len(day_returns))
        weights /= np.sum(weights)
        weights_record.append(weights)
        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(
            weights, day_returns, cov_matrix)
        results[0, i] = portfolio_std_dev
        results[1, i] = portfolio_return
    return results, weights_record


# Way 1 to get the efficient frontier: using optimization techniques to solve the efficient frontier
# Time consuming
import scipy.optimize as opt


# target is the target value for returning the efficient frontier axis (similiar to a y-axis value )
def efficient_return(day_returns, cov_matrix, target):
    num_assets = len(day_returns)
    args = (day_returns, cov_matrix)

    def portfolio_return(weights):
        return portfolio_annualised_performance(weights, day_returns, cov_matrix)[1]

    constraints = ({'type': 'eq', 'fun': lambda x: portfolio_return(x) - target},
                   {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})
    bounds = tuple((0, 1) for asset in range(num_assets))
    result = opt.minimize(portfolio_volatility, num_assets*[
                          1./num_assets,], args=args, method='SLSQP', bounds=bounds, constraints=constraints)
    return result


def efficient_frontier(day_returns, cov_matrix, returns_range):  # return efficient frontier
    efficients = []
    for ret in tqdm(returns_range, desc='calculating efficient frontier using optimization method:'):
        efficients.append(efficient_return(day_returns, cov_matrix, ret))
    return efficients


def ef_with_random_portfolio_opt(day_returns, cov_matrix, num_portfolios, ax=None, return_plot=True):
    results, weights = random_portfolios(
        num_portfolios, day_returns, cov_matrix)

    target = np.linspace(
        max(np.min(results[1]), 0), np.quantile(results[1], 0.7), 30)
    efficient_portfolios = efficient_frontier(day_returns, cov_matrix, target)
    frontier = [p['fun'] for p in efficient_portfolios]

    if return_plot:
        if not ax:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111)

            # ax.legend(labelspacing=0.8)
        ax.plot(frontier, target, color='black',
                linewidth=2, label='efficient frontier')

        ax.scatter(results[0, :], results[1, :], marker='o', s=10, alpha=0.3)
        ax.set_title(
            'Calculated Portfolio Optimization based on Efficient Frontier')
        ax.set_xlabel('annualised volatility')
        ax.set_ylabel('annualised returns')
        ax.legend()

    return ax, [frontier, target]



target_ret = pd.DataFrame(logret.T)  # select 50 stocks to construct portfolio
target_cov = np.cov(target_ret)
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111)
ax, frontier_ret = ef_with_random_portfolio_opt(
    target_ret, target_cov, 1000, ax=ax)


# Way 2: monte-carlo method, direct infer from the efficient frontier from the simulated data


def efficient_return_simu(results, target):

    # return efficient frontier
    # use +-5% area of the target, return the nearest min
    results = pd.DataFrame(results.T).sort_values(by=1)
    closiest_idx = np.argmin(np.abs(results[1]-target))
    data_target = results[1][closiest_idx]
    target_range_min = min(data_target*0.95, data_target*1.05)
    target_range_max = max(data_target*0.95, data_target*1.05)
    sub_results = results.loc[(results[1] <= target_range_max) & (
        results[1] >= target_range_min), 0:2]

    return min(sub_results[0])


def efficient_frontier_emp(day_returns, cov_matrix, num_portfolios,  returns_range, random_seed=0):
    efficients = []
    np.random.seed(random_seed)
    results, weights = random_portfolios(
        num_portfolios, day_returns, cov_matrix)
    for ret in returns_range:
        efficients.append(efficient_return_simu(results, ret))
    return efficients


def ef_with_random_portfolio_simu(day_returns, cov_matrix, num_portfolios, ax=None, return_plot=True, random_seed=0, mean_frontier=None):
    results, weights = random_portfolios(
        num_portfolios, day_returns, cov_matrix)

    # target_start = max(results[1][results[0]==np.min(results[0])][0],0)
    target_start = 0
    if True:  # not return_plot:
        target_end = 400
    else:
        target_end = results[1][results[0] == np.max(results[0])][0]

    target = np.linspace(target_start, target_end, 30)
    efficient_portfolios = None
    efficient_portfolios = efficient_frontier_emp(
        day_returns, cov_matrix, num_portfolios, target, random_seed=random_seed)

    if return_plot:
        if not ax:
            fig = plt.figure(figsize=(10, 7))
            ax = fig.add_subplot(111)

            # ax.legend(labelspacing=0.8)
        if mean_frontier:
            ax.plot(mean_frontier[0], mean_frontier[1], color='black',
                    linewidth=2, label='mean efficient frontier')
        else:
            ax.plot(efficient_portfolios, target, color='black',
                    linewidth=2, label='efficient frontier')

        ax.scatter(results[0, :], results[1, :], marker='o', s=10, alpha=0.3)
        ax.set_title(
            'Calculated Portfolio Optimization based on Efficient Frontier')
        ax.set_xlabel('annualised volatility')
        ax.set_ylabel('annualised returns')
        ax.legend()
    else:
        ax = None

    return ax, [efficient_portfolios, target]


np.random.seed(3)
target_ret = pd.DataFrame(logret.T)
target_cov = np.cov(target_ret)
fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111)
ax, frontier_ret = ef_with_random_portfolio_simu(
    target_ret, target_cov, 1000, ax=ax)


def MC_ef_frontier(day_returns, cov_matrix, itertimes=100, random_seed=3):
    np.random.seed(random_seed)
    target_ret = pd.DataFrame(day_returns.T)
    target_cov = cov_matrix
    frontier = []
    for i in tqdm(range(itertimes)):
        ax, frontier_ret = ef_with_random_portfolio_simu(
            target_ret, target_cov, 1000, return_plot=False, random_seed=i)
        frontier.append(frontier_ret[0])
    mean_frontier_vol = np.mean(frontier, axis=0)
    mean_frontier_ret = frontier_ret[1]


MC_ef_frontier(logret,covmat,itertimes = 100,random_seed = 3)


fig = plt.figure(figsize=(10, 7))
ax = fig.add_subplot(111)
ax, _ = ef_with_random_portfolio_simu(target_ret, target_cov, 1000, ax=ax,
                                      return_plot=True, random_seed=0, mean_frontier=[mean_frontier, frontier_ret[1]])


# variance of the errors against the mean efficient frontier
def error_mean_ef_frontier(frontiers, mean_frontier):
    err = []
    for frontier in frontiers:
        err.append(np.std(frontier-mean_frontier))
    return np.var(err)


error_mean_ef_frontier(frontier_ret, mean_frontier)


# exercise 3
corr3 = cov2corr(covmat)
eVal3, eVec3 = getPCA(corr3)
# Fitting the Marcenko–Pastur PDF on a noisy covariance matrix.
# estimate the sigma for Marcenko-Pastur dist
bWidth = 0.01
out = minimize(lambda *x: errPDFs(*x), .5,
               args=(np.diag(eVal3), q, bWidth), bounds=((1E-5, 1-1E-5),))
if out['success']:
    var = out['x'][0]
else:
    var = 1
print('-'*10)
print(r'value of $\sigma^2$ implied by Marcenko-Pastur distribution: ')
pdf0 = mpPDF(var, q, pts=1000)  # Marcenko-Pastur dist
pdf3 = fitKDE(np.diag(eVal3), bWidth=.01)  # empirical pdf
ax = plt.figure().add_subplot(111)
ax.plot(pdf0, label='Marcenko-Pastur dist')
ax.bar(pdf3.index, pdf3.values, width=bWidth,
       label='Empirical dist', color='darkorange')
ax.set_xlabel(r'$\lambda$')
ax.set_ylabel(r'prob[$\lambda$]')
ax.legend()



// ---------------------------------------------------

// 4_Optimal_Clustering.py
// shuangology/4_Optimal_Clustering.py
# Generated from: 4_Optimal_Clustering.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# distinguish several types of clustering algorithms, including the following:
#
# 1 Connectivity: This clustering is based on distance connectivity, like hier- archical clustering. For an example in finance, see López de Prado (2016).
#
# 2 Centroids: These algorithms perform a vector quantization, like k-means. For an example in finance, see López de Prado and Lewis (2018).
#
# 3 Distribution: Clusters are formed using statistical distributions, e.g., a mixture of Gaussians.
#
# 4 Density: These algorithms search for connected dense regions in the data space. Examples include DBSCAN and OPTICS.
#
# 5 Subspace: Clusters are modeled on two dimensions, features and observa- tions. An example is biclustering (also known as coclustering). For instance, they can help identify similarities in subsets of instruments and time periods simultaneously.
#


import os
import sys
nb_path = os.path.split(os.getcwd())[0]
if nb_path not in sys.path:
    sys.path.append(nb_path)


import CovMatrix
corr0,eVal0,eVec0 = CovMatrix.init_para()



# SNIPPET 4.1 BASE CLUSTERING
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples
# ---------------------------------------------------


def clusterKMeansBase(corr0, maxNumClusters=10, n_init=10):
    x, silh = ((1-corr0.fillna(0))/2.)**.5, pd.Series()  # observations matrix
    for init in range(n_init):
        for i in range(2, maxNumClusters+1):
            kmeans_ = KMeans(n_clusters=i, n_jobs=1, n_init=1)
            kmeans_ = kmeans_.fit(x)
            silh_ = silhouette_samples(x, kmeans_.labels_)
            stat = (silh_.mean()/silh_.std(), silh.mean()/silh.std())
            if np.isnan(stat[1]) or stat[0] > stat[1]:
                silh, kmeans = silh_, kmeans_
    newIdx = np.argsort(kmeans.labels_)
    corr1 = corr0.iloc[newIdx]  # reorder rows

    corr1 = corr1.iloc[:, newIdx]  # reorder columns
    clstrs = {i: corr0.columns[np.where(kmeans.labels_ == i)[0]].tolist()
              for i in np.unique(kmeans.labels_)}  # cluster members
    silh = pd.Series(silh, index=x.index)
    return corr1, clstrs, silh


corr1,clstrs,silh = clusterKMeansBase(pd.DataFrame(corr0))


# a new (reduced) observations matrix out of the elements that compose the K1 clusters, and rerun the base clustering algorithm on that reduced correlation matrix. Doing so will return a, possibly new, clustering for those elements in K1. To check its efficacy, we compare the average cluster quality before and after reclustering those elements in K1. If the average cluster quality improves, we return the accepted clustering from the base clustering concate- nated with the new clustering for the redone nodes.

# SNIPPET 4.2 TOP-LEVEL OF CLUSTERING
from sklearn.metrics import silhouette_samples
# ---------------------------------------------------


def makeNewOutputs(corr0, clstrs, clstrs2):
    clstrsNew = {}
    for i in clstrs.keys():
        clstrsNew[len(clstrsNew.keys())] = list(clstrs[i])
    for i in clstrs2.keys():
        clstrsNew[len(clstrsNew.keys())] = list(clstrs2[i])
    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]
    corrNew = corr0.loc[newIdx, newIdx]
    x = ((1-corr0.fillna(0))/2.)**.5
    kmeans_labels = np.zeros(len(x.columns))
    for i in clstrsNew.keys():
        idxs = [x.index.get_loc(k) for k in clstrsNew[i]]
        kmeans_labels[idxs] = i
    silhNew = pd.Series(silhouette_samples(x, kmeans_labels), index=x.index)
    return corrNew, clstrsNew, silhNew

# ---------------------------------------------------


# for real stock data, n_init start from 10 ; for simulated data, n_init=3 would speed up the process
def clusterKMeansTop(corr0, maxNumClusters=None, n_init=3):
    if maxNumClusters == None:
        maxNumClusters = corr0.shape[1]-1
    corr1, clstrs, silh = clusterKMeansBase(corr0, maxNumClusters=min(
        maxNumClusters, corr0.shape[1]-1), n_init=n_init)
    clusterTstats = {i: np.mean(silh[clstrs[i]]) /
                     np.std(silh[clstrs[i]]) for i in clstrs.keys()}
    tStatMean = sum(clusterTstats.values())/len(clusterTstats)
    redoClusters = [i for i in clusterTstats.keys() if
                    clusterTstats[i] < tStatMean]
    if len(redoClusters) <= 1:
        return corr1, clstrs, silh
    else:
        keysRedo = [j for i in redoClusters for j in clstrs[i]]
        corrTmp = corr0.loc[keysRedo, keysRedo]
        tStatMean = np.mean([clusterTstats[i] for i in redoClusters])
        corr2, clstrs2, silh2 = clusterKMeansTop(corrTmp,
                                                 maxNumClusters=min(maxNumClusters,
                                                                    corrTmp.shape[1]-1), n_init=n_init)
    # Make new outputs, if necessary
    corrNew, clstrsNew, silhNew = makeNewOutputs(corr0,
                                                 {i: clstrs[i] for i in clstrs.keys(
                                                 ) if i not in redoClusters},
                                                 clstrs2)
    newTstatMean = np.mean([np.mean(silhNew[clstrsNew[i]]) /
                            np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])
    if newTstatMean <= tStatMean:
        return corr1, clstrs, silh
    else:
        return corrNew, clstrsNew, silhNew


# SNIPPET 4.3 RANDOM BLOCK CORRELATION MATRIX CREATION
import numpy as np
import pandas as pd
from scipy.linalg import block_diag
from sklearn.utils import check_random_state
# ---------------------------------------------------


def getCovSub(nObs, nCols, sigma, random_state=None):
    # Sub correl matrix
    rng = check_random_state(random_state)
    if nCols == 1:
        return np.ones((1, 1))
    ar0 = rng.normal(size=(nObs, 1))
    ar0 = np.repeat(ar0, nCols, axis=1)
    ar0 += rng.normal(scale=sigma, size=ar0.shape)
    ar0 = np.cov(ar0, rowvar=False)
    return ar0
# ---------------------------------------------------


def getRndBlockCov(nCols, nBlocks, minBlockSize=1, sigma=1.,
                   random_state=None):
    # Generate a block random correlation matrix
    rng = check_random_state(random_state)
    parts = rng.choice(range(1, nCols-(minBlockSize-1)*nBlocks),
                       nBlocks-1, replace=False)
    parts.sort()
    parts = np.append(parts, nCols-(minBlockSize-1)*nBlocks)
    # random number of the cols in each block
    parts = np.append(parts[0], np.diff(parts))-1+minBlockSize
    cov = None
    for nCols_ in parts:
        cov_ = getCovSub(int(max(nCols_*(nCols_+1)/2., 100)),
                         nCols_, sigma, random_state=rng)
        if cov is None:
            cov = cov_.copy()
        else:
            cov = block_diag(cov, cov_)
    return cov
# ---------------------------------------------------


def randomBlockCorr(nCols, nBlocks, random_state=None,
                    minBlockSize=1):
    # Form block corr
    rng = check_random_state(random_state)
    cov0 = getRndBlockCov(
        nCols, nBlocks, minBlockSize=minBlockSize, sigma=.5, random_state=rng)
    cov1 = getRndBlockCov(nCols, 1, minBlockSize=minBlockSize,
                          sigma=1., random_state=rng)  # add noise
    cov0 += cov1
    corr0 = CovMatrix.cov2corr(cov0)
    corr0 = pd.DataFrame(corr0)
    return corr0


# simulated a cov matraix with blocks
import seaborn as sns
corr_blk_simu = randomBlockCorr(100, 10)
sns.heatmap(corr_blk_simu)


# clustered using clusterKMeansTop
sns.heatmap(clusterKMeansTop(corr_blk_simu)[0])



// ---------------------------------------------------

// 3_Distance_Metrics.py
// shuangology/3_Distance_Metrics.py
# Generated from: 3_Distance_Metrics.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Chapter 3 Distance Metrics
#
# Look beyond correlations to understand codependency


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm


# SNIPPET 3.1 MARGINAL, JOINT, CONDITIONAL ENTROPIES, AND MUTUAL INFORMATION
import numpy as np
import scipy.stats as ss
from sklearn.metrics import mutual_info_score

x = np.random.random(100)
y = np.random.random(100)
bins = 100
# The bi-dimensional histogram of samples x and y. Values in x are histogrammed along the first dimension and values in y are histogrammed along the second dimension.
cXY = np.histogram2d(x, y, bins)[0]
hX = ss.entropy(np.histogram(x, bins)[0])  # marginal
hY = ss.entropy(np.histogram(y, bins)[0])  # marginal
iXY = mutual_info_score(None, None, contingency=cXY)
iXYn = iXY/min(hX, hY)  # normalized mutual information
hXY = hX+hY-iXY  # joint
hX_Y = hXY-hY  # conditional
hY_X = hXY-hX  # conditional

print('**'*8+'\n')
print('hX marginal entropy: {}'.format(hX))
print('hY marginal entropy: {}'.format(hY))
print('iXY mutual info score: {}'.format(iXY))
print('iXYn normalized mutual information: {}'.format(iXYn))
print('hX_Y cross entropy between x and y : {}'.format(hX_Y))
print('hY_X cross entropy between y and x : {}'.format(hY_X))


# SNIPPET 3.2 MUTUAL INFORMATION, VARIATION OF INFORMATION, AND NORMALIZED VARIATION OF INFORMATION
def varInfo(x, y, bins, norm=False):
    # variation of information
    cXY = np.histogram2d(x, y, bins)[0]
    iXY = mutual_info_score(None, None, contingency=cXY)
    hX = ss.entropy(np.histogram(x, bins)[0])  # marginal
    hY = ss.entropy(np.histogram(y, bins)[0])  # marginal
    vXY = hX+hY-2*iXY  # variation of information
    if norm:
        hXY = hX+hY-iXY  # joint
        vXY /= hXY  # normalized variation of information
    return vXY


# SNIPPET 3.3 VARIATION OF INFORMATION ON DISCRETIZED CONTINUOUS RANDOM VARIABLES
def numBins(nObs, corr=None):
    # Optimal number of bins for discretization
    if corr is None:  # univariate case
        z = (8+324*nObs+12*(36*nObs+729*nObs**2)**.5)**(1/3.)
        b = round(z/6.+2./(3*z)+1./3)
    else:  # bivariate case
        if (1.-corr**2) == 0:
            corr = np.sign(corr)*(np.abs(corr)-1e-5)
        b = round(2**-.5*(1+(1+24*nObs/(1.-corr**2))**.5)**.5)
    return int(b)
# ---------------------------------------------------


def varInfo_optBIn(x, y, norm=False):  # Discretized and with optimal bin value
    # variation of information
    bXY = numBins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    cXY = np.histogram2d(x, y, bXY)[0]
    iXY = mutual_info_score(None, None, contingency=cXY)
    hX = ss.entropy(np.histogram(x, bXY)[0])  # marginal
    hY = ss.entropy(np.histogram(y, bXY)[0])  # marginal
    vXY = hX+hY-2*iXY  # variation of information
    if norm:
        hXY = hX+hY-iXY  # joint
        vXY /= hXY  # normalized variation of information
    return vXY


# SNIPPET 3.4 CORRELATION AND NORMALIZED MUTUAL INFORMATION OF TWO INDEPENDENT GAUSSIAN RANDOM VARIABLES
def mutualInfo(x, y, norm=False):
    # mutual information
    bXY = numBins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    cXY = np.histogram2d(x, y, bXY)[0]
    iXY = mutual_info_score(None, None, contingency=cXY)
    if norm:
        hX = ss.entropy(np.histogram(x, bXY)[0])  # marginal
        hY = ss.entropy(np.histogram(y, bXY)[0])  # marginal
        iXY /= min(hX, hY)  # normalized mutual information
    return iXY


# ---------------------------------------------------
size, seed = 5000, 0
np.random.seed(seed)
x = np.random.normal(size=size)
e = np.random.normal(size=size)
y = 0*x+e
nmi = mutualInfo(x, y, True)
corr = np.corrcoef(x, y)[0, 1]


# Exercise 3.13.1


bins = 10

rho_list = [-1, -0.5, 0, 0.5, 1]
hX = hY = hXY = hX_Y = iXY = viXY = vi_t_XY = np.zeros(len(rho_list))
for i in tqdm(range(len(rho_list))):
    rho = rho_list[i]
    mu, sigma = 0, 1
    rr = np.random.normal(mu, sigma, size=(2, 1000))
    x, y_ = rr[0, :], rr[1, :]
    y = rho * x+np.sqrt(1-rho**2)*y_

    # The bi-dimensional histogram of samples x and y. Values in x are histogrammed along the first dimension and values in y are histogrammed along the second dimension.
    cXY = np.histogram2d(x, y, bins)[0]
    hX[i] = ss.entropy(np.histogram(x, bins)[0])  # marginal
    hY[i] = ss.entropy(np.histogram(y, bins)[0])  # marginal
    iXY[i] = mutual_info_score(None, None, contingency=cXY)
    hXY[i] = hX[i]+hY[i]-iXY[i]  # joint
    hX_Y[i] = hXY[i]-hY[i]  # conditional
    viXY[i] = varInfo(x, y, bins)
    vi_t_XY[i] = varInfo_optBIn(x, y)

fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(rho_list, hX, label='H[X]')
ax.plot(rho_list, hY, label='H[Y]')
# ax.plot(rho_list, hXY, label='H[X,Y]')
# ax.plot(rho_list, hX_Y, label='H[X|Y]')
# ax.plot(rho_list, iXY, label='I[X,Y]')
# ax.plot(rho_list, viXY, label='VI[X,Y]')
# ax.plot(rho_list, vi_t_XY, label=r'$\tilde{VI}$[X,Y]')
ax.legend()


cXY = np.histogram2d(x, y, bins)[0]
cXY


rr = np.random.normal(mu, sigma, size=(2, 1000))
rr[0,:].shape



// ---------------------------------------------------

// CovMatrix.py
// shuangology/CovMatrix.py
#!/usr/bin/env python
# coding: utf-8

# # Chapter 2 Denoising and Detoning
#
# reduce the noise and enhance the signal included in an empirical covariance matrix.

# In[1]:


from sklearn.covariance import LedoitWolf
from scipy.linalg import block_diag
from scipy.optimize import minimize
from sklearn.neighbors.kde import KernelDensity
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm

# ## The Marcenko-Pastur Theorem
#

# ### SNIPPET 2.1

# In[2]:


# ---------------------------------------------------

def mpPDF(var, q, pts):
    # Marcenko-Pastur pdf
    # q=T/N
    # when var= 1, C = T^-1 X'X  is the correlation matrix associated with X
    # lambda+ =,lambda- = eMax, eMin
    eMin, eMax = var*(1-(1./q)**.5)**2, var*(1+(1./q)**.5)**2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5
    # pdf = pdf.ravel()
    pdf = pd.Series(pdf, index=eVal)
    return pdf


# ### SNIPPET 2.2

# In[3]:


# ---------------------------------------------------


def getPCA(matrix):
    # Get eVal,eVec from a Hermitian matrix
    eVal, eVec = np.linalg.eigh(matrix)
    indices = eVal.argsort()[::-1]  # arguments for sorting eVal desc
    eVal, eVec = eVal[indices], eVec[:, indices]
    eVal = np.diagflat(eVal)
    return eVal, eVec
# ---------------------------------------------------


def fitKDE(obs, bWidth=.25, kernel='gaussian', x=None):
    # Fit kernel to a series of obs, and derive the prob of obs
    # x is the array of values on which the fit KDE will be evaluated
    if len(obs.shape) == 1:
        obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    if x is None:
        x = np.unique(obs).reshape(-1, 1)
    if len(x.shape) == 1:
        x = x.reshape(-1, 1)
    logProb = kde.score_samples(x)  # log(density)
    pdf = pd.Series(np.exp(logProb), index=x.flatten())
    return pdf


#
# ## Random Matrix with Signal (not perfectly random)

# In[5]:


# SNIPPET 2.3 ADD SIGNAL TO A RANDOM COVARIANCE MATRIX
def getRndCov(nCols, nFacts):
    w = np.random.normal(size=(nCols, nFacts))
    cov = np.dot(w, w.T)  # random cov matrix, however not full rank
    cov += np.diag(np.random.uniform(size=nCols))  # full rank cov
    return cov
# ---------------------------------------------------


def cov2corr(cov):
    # Derive the correlation matrix from a covariance matrix
    std = np.sqrt(np.diag(cov))
    corr = cov/np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    return corr


# SNIPPET 2.4 FITTING THE MARCENKO–PASTUR PDF
# ---------------------------------------------------
def errPDFs(var, eVal, q, bWidth, pts=1000):
    # Fit error
    var = var[0]
    pdf0 = mpPDF(var, q, pts)  # theoretical pdf
    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)  # empirical pdf
    # import pdb; pdb.set_trace()
    sse = np.sum((pdf1-pdf0)**2)
    return sse
# ---------------------------------------------------


def findMaxEval(eVal, q, bWidth):
    # Find max random eVal by fitting Marcenko’s dist
    out = minimize(lambda *x: errPDFs(*x), .5,
                   args=(eVal, q, bWidth), bounds=((1E-5, 1-1E-5),))
    if out['success']:
        var = out['x'][0]
    else:
        var = 1
    eMax = var*(1+(1./q)**.5)**2
    return eMax, var

# ax = plt.figure().add_subplot(111)
# ax.plot(pdf0,label= 'Marcenko-Pastur')
# ax.plot(pdf1,linestyle = '--',label= 'Empirical:KDE')
# ax.set_xlabel(r'$\lambda$')
# ax.set_ylabel(r'prob[$\lambda$]')
# ax.legend()


# ---------------------------------------------------

# ---------------------------------------------------


# nFacts0 gives the number of the eigenvalue is assumed to be important (cutoff level lambda+ adjusted for the presence of nonrandom eigenvectors)


# In[7]:


# ---------------------------------------------------
# Fitting the Marcenko–Pastur PDF on a noisy covariance matrix.
# estimate the sigma for Marcenko-Pastur dist
# bWidth=0.01
# out=minimize(lambda *x: errPDFs(*x),.5,args=(np.diag(eVal0),q,bWidth),bounds=((1E-5,1-1E-5),))
# if out['success']:
#     var=out['x'][0]
# else:
#     var=1
#
# pdf0=mpPDF(var,q,pts=1000) # Marcenko-Pastur dist
# pdf1=fitKDE(np.diag(eVal0),bWidth=.01) # empirical pdf
# ax = plt.figure().add_subplot(111)
# ax.plot(pdf0,label= 'Marcenko-Pastur dist')
# ax.bar(pdf1.index,pdf1.values,width = bWidth,label= 'Empirical dist',color = 'darkorange')
# ax.set_xlabel(r'$\lambda$')
# ax.set_ylabel(r'prob[$\lambda$]')
# ax.legend()


# ## 2.5 Denoising

# ### 2.5.1 Constant Residual Eigenvalue Method
#
# setting a constant eigenvalue for all random eigenvectors.

# In[8]:


def denoisedCorr(eVal, eVec, nFacts):
    # Remove noise from corr by fixing random eigenvalues
    eVal_ = np.diag(eVal).copy()
    eVal_[nFacts:] = eVal_[nFacts:].sum()/float(eVal_.shape[0] -
                                                nFacts)  # average the rest
    eVal_ = np.diag(eVal_)
    corr1 = np.dot(eVec, eVal_).dot(eVec.T)
    corr1 = cov2corr(corr1)
    return corr1
# ---------------------------------------------------


# In[9]:


# # A comparison of eigenvalues before and after applying the residual eigenvalue method.
# ax = plt.figure().add_subplot(111)
# ax.plot(np.diagonal(eVal0),label = 'Original eigen-function')
# ax.plot(np.diagonal(eVal1),label = 'Denoised eigen-function (Constant Residual)',linestyle = '--')
# ax.legend()
# ax.set_yscale('log')
# ax.set_xlabel('Eigenvalue number')
# ax.set_ylabel('Eigenvalue (log-scale)')


# ### 2.5.2 Targeted Shrinkage
# $\alpha$ regulates the amount fo shrinkage among the eigen vectors

# In[10]:


# SNIPPET 2.6 DENOISING BY TARGETED SHRINKAGE
def denoisedCorr2(eVal, eVec, nFacts, alpha=0.0):
    # Remove noise from corr through targeted shrinkage
    eValL, eVecL = eVal[:nFacts, :nFacts], eVec[:, :nFacts]
    eValR, eVecR = eVal[nFacts:, nFacts:], eVec[:, nFacts:]
    corr0 = np.dot(eVecL, eValL).dot(eVecL.T)
    corr1 = np.dot(eVecR, eValR).dot(eVecR.T)
    corr2 = corr0+alpha*corr1+(1-alpha)*np.diag(np.diag(corr1))
    return corr2
# ---------------------------------------------------


# In[11]:


# # A comparison of eigenvalues before and after applying the residual eigenvalue method.
# ax = plt.figure().add_subplot(111)
# ax.plot(np.diagonal(eVal0),label = 'Original eigen-function')
# ax.plot(np.diagonal(eVal1),label = 'Denoised eigen-function (targeted shrinkage)',linestyle = '--')
# ax.legend()
# ax.set_yscale('log')
# ax.set_xlabel('Eigenvalue number')
# ax.set_ylabel('Eigenvalue (log-scale)')


# # Experimental Results
# ## 2.7.1 Minimum Variance Portfolio

# In[12]:


def corr2cov(corr, std):
    # Derive the covariance matrix from a correlation matrix
    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error
    cov = np.outer(std, std)*corr
    return cov


# In[13]:


# SNIPPET 2.7 GENERATING A BLOCK-DIAGONAL COVARIANCE MATRIX AND A VECTOR OF MEANS
def formBlockMatrix(nBlocks, bSize, bCorr):
    block = np.ones((bSize, bSize))*bCorr
    block[range(bSize), range(bSize)] = 1
    corr = block_diag(*([block]*nBlocks))
    return corr
# ---------------------------------------------------


def formTrueMatrix(nBlocks, bSize, bCorr):
    # In each block, the variances are drawn from a uniform distribution bounded between 5% and 20%; the vector of means is drawn from a Normal distribution with mean and standard deviation equal to the standard deviation from the covariance matrix
    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(.05, .2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0


# ---------------------------------------------------
nBlocks, bSize, bCorr = 10, 50, .5
np.random.seed(0)
mu0, cov0 = formTrueMatrix(nBlocks, bSize, bCorr)


# In[14]:


# SNIPPET 2.8 GENERATING THE EMPIRICAL COVARIANCE MATRIX
def simCovMu(mu0, cov0, nObs, shrink=False):
    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)
    mu1 = x.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(x).covariance_
    else:
        cov1 = np.cov(x, rowvar=0)
    return mu1, cov1


# In[15]:


# SNIPPET 2.9 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX
def deNoiseCov(cov0, q, bWidth):
    corr0 = cov2corr(cov0)
    eVal0, eVec0 = getPCA(corr0)
    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth)
    nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)
    corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
    cov1 = corr2cov(corr1, np.diag(cov0)**.5)
    return cov1


# In[16]:


# SNIPPET 2.10 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX
def optPort(cov, mu=None):  # optimal portfolio for minimum variance
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = np.dot(inv, mu)
    w /= np.dot(ones.T, w)
    return w
# #---------------------------------------------------
# nObs,nTrials,bWidth,shrink,minVarPortf=1000,100,.01,False,True
# w1= w1_s=pd.DataFrame(columns=range(cov0.shape[0]),
# index=range(nTrials),dtype=float)
# w1_d=w1.copy(deep=True)
# w1_s_d = w1_s.copy(deep = True)
# np.random.seed(0)
# for i in tqdm(range(nTrials)):
#     mu1,cov1=simCovMu(mu0,cov0,nObs,shrink=True)
#     if minVarPortf:
#         mu1=None
#     cov1_d=deNoiseCov(cov1,nObs*1./cov1.shape[1],bWidth)
#     w1_s.loc[i]=optPort(cov1,mu1).flatten()
#     w1_s_d.loc[i]=optPort(cov1_d,mu1).flatten()
#
#
# for i in tqdm(range(nTrials)):
#     mu1,cov1=simCovMu(mu0,cov0,nObs,shrink=False)
#     if minVarPortf:
#         mu1=None
#     cov1_d=deNoiseCov(cov1,nObs*1./cov1.shape[1],bWidth)
#     w1.loc[i]=optPort(cov1,mu1).flatten()
#     w1_d.loc[i]=optPort(cov1_d,mu1).flatten()


# In[18]:


# #SNIPPET 2.11 ROOT-MEAN-SQUARE ERRORS
# w0=optPort(cov0,None if minVarPortf else mu0)
# w0=np.repeat(w0.T,w1.shape[0],axis=0)
# rmsd=np.mean((w1-w0).values.flatten()**2)**.5 # RMSE  not shrunk not denoised
# rmsd_d=np.mean((w1_d-w0).values.flatten()**2)**.5 # RMSE not shrunk denoised
# rmsd_s=np.mean((w1_s-w0).values.flatten()**2)**.5 # RMSE shrunk not denoised
# rmsd_s_d=np.mean((w1_s_d-w0).values.flatten()**2)**.5 # RMSE shrunk denoised
#
# res_tab = pd.DataFrame(columns = ['Note denoised','Denoised'],index = ['Not shrunk','Shrunk'],data = np.array([[rmsd,rmsd_d],[rmsd_s,rmsd_s_d]]))
#

# #--------------------------------------------------------

alpha, nCols, nFact, q = .995, 1000, 100, 10


def init_para():

    cov = np.cov(np.random.normal(size=(nCols*q, nCols)), rowvar=False)
    cov = alpha*cov+(1-alpha)*getRndCov(nCols, nFact)  # noise+signal
    corr0 = cov2corr(cov)
    eVal0, eVec0 = getPCA(corr0)
    return corr0, eVal0, eVec0


def denoise_init(method=1):
    corr0, eVal0, eVec0 = init_para()
    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth=.01)
    nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)

    if method == 1:
        corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
        eVal1, eVec1 = getPCA(corr1)
    else:
        corr1 = denoisedCorr2(eVal0, eVec0, nFacts0, alpha=.5)
        eVal1, eVec1 = getPCA(corr1)
    return corr0, eVal1, eVec1


// ---------------------------------------------------

// distance-metrics-improvement.py
// shuangology/claude/distance-metrics-improvement.py
"""
Distance Metrics for Financial Analysis.

This module implements various distance and dependency metrics including mutual information,
variation of information, and entropy-based measures with improved numerical stability
and error handling.
"""

from typing import Tuple, Optional, Union, Dict
from dataclasses import dataclass
import numpy as np
import scipy.stats as ss
from sklearn.metrics import mutual_info_score

EPSILON = 1e-10  # Small constant for numerical stability

@dataclass
class EntropyMetrics:
    """Container for entropy-based metrics."""
    marginal_entropy_x: float
    marginal_entropy_y: float
    mutual_information: float
    normalized_mutual_information: float
    conditional_entropy_x_y: float
    conditional_entropy_y_x: float
    joint_entropy: float

def validate_inputs(x: np.ndarray, y: np.ndarray) -> None:
    """Validate input arrays for calculations."""
    if x.size == 0 or y.size == 0:
        raise ValueError("Input arrays cannot be empty")
    if x.size != y.size:
        raise ValueError("Input arrays must have same length")
    if np.any(np.isnan(x)) or np.any(np.isnan(y)):
        raise ValueError("Input arrays contain NaN values")
    if np.any(np.isinf(x)) or np.any(np.isinf(y)):
        raise ValueError("Input arrays contain infinite values")

def compute_histogram(x: np.ndarray, bins: int) -> Tuple[np.ndarray, np.ndarray]:
    """Compute histogram with optimal bin edges."""
    hist, edges = np.histogram(x, bins=bins)
    hist = hist.astype(float) + EPSILON  # Avoid zero counts
    hist /= hist.sum()  # Normalize
    return hist, edges

def compute_entropy(hist: np.ndarray) -> float:
    """Compute entropy from histogram."""
    return ss.entropy(hist)

def compute_joint_histogram(x: np.ndarray, y: np.ndarray, 
                          bins: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Compute 2D histogram with optimal bin edges."""
    hist, x_edges, y_edges = np.histogram2d(x, y, bins=bins)
    hist = hist.astype(float) + EPSILON
    hist /= hist.sum()
    return hist, x_edges, y_edges

def optimal_bin_count(n_obs: int, corr: Optional[float] = None) -> int:
    """
    Calculate optimal number of bins for discretization using established rules.
    
    Args:
        n_obs: Number of observations
        corr: Correlation coefficient for bivariate case
        
    Returns:
        Optimal number of bins
        
    Raises:
        ValueError: If correlation is invalid or observations count is too low
    """
    if n_obs < 2:
        raise ValueError("Need at least 2 observations")
        
    if corr is None:
        # Freedman-Diaconis rule for univariate case
        z = (8 + 324*n_obs + 12*(36*n_obs + 729*n_obs**2)**0.5)**(1/3.)
        b = round(z/6. + 2./(3*z) + 1./3)
    else:
        # Handle edge cases for correlation
        if abs(corr) > 1:
            raise ValueError("Correlation must be between -1 and 1")
        if abs(1 - corr**2) < EPSILON:
            corr = np.sign(corr) * (1 - EPSILON)
        # Modified rule for bivariate case
        b = round(2**-0.5 * (1 + (1 + 24*n_obs/(1 - corr**2))**0.5)**0.5)
    
    return max(int(b), 2)  # Ensure at least 2 bins

def calculate_entropy_metrics(x: np.ndarray, y: np.ndarray, 
                            bins: Optional[int] = None) -> EntropyMetrics:
    """
    Calculate comprehensive entropy-based metrics between two variables.
    
    Args:
        x: First variable
        y: Second variable
        bins: Number of bins (if None, calculated optimally)
        
    Returns:
        EntropyMetrics object containing all metrics
        
    Raises:
        ValueError: For invalid inputs
    """
    validate_inputs(x, y)
    
    if bins is None:
        bins = optimal_bin_count(x.size, np.corrcoef(x, y)[0, 1])
    
    # Compute histograms
    hist_x, _ = compute_histogram(x, bins)
    hist_y, _ = compute_histogram(y, bins)
    joint_hist, _, _ = compute_joint_histogram(x, y, bins)
    
    # Calculate entropy metrics
    hX = compute_entropy(hist_x)
    hY = compute_entropy(hist_y)
    iXY = mutual_info_score(None, None, contingency=joint_hist)
    
    # Handle edge case where both entropies are near zero
    min_entropy = min(hX, hY)
    if min_entropy < EPSILON:
        iXYn = 0.0
    else:
        iXYn = iXY / min_entropy
    
    hXY = hX + hY - iXY
    hX_Y = hXY - hY
    hY_X = hXY - hX
    
    return EntropyMetrics(
        marginal_entropy_x=float(hX),
        marginal_entropy_y=float(hY),
        mutual_information=float(iXY),
        normalized_mutual_information=float(iXYn),
        conditional_entropy_x_y=float(hX_Y),
        conditional_entropy_y_x=float(hY_X),
        joint_entropy=float(hXY)
    )

def variation_information(x: np.ndarray, y: np.ndarray, 
                         bins: Optional[int] = None,
                         normalize: bool = False) -> float:
    """
    Calculate variation of information between two variables.
    
    Args:
        x: First variable
        y: Second variable
        bins: Number of bins (if None, calculated optimally)
        normalize: Whether to normalize the result
        
    Returns:
        Variation of information value
        
    Raises:
        ValueError: For invalid inputs
    """
    metrics = calculate_entropy_metrics(x, y, bins)
    vXY = metrics.marginal_entropy_x + metrics.marginal_entropy_y - 2 * metrics.mutual_information
    
    if normalize and metrics.joint_entropy > EPSILON:
        vXY /= metrics.joint_entropy
        
    return float(vXY)

def mutual_information(x: np.ndarray, y: np.ndarray,
                      bins: Optional[int] = None,
                      normalize: bool = False) -> float:
    """
    Calculate mutual information between two variables.
    
    Args:
        x: First variable
        y: Second variable
        bins: Number of bins (if None, calculated optimally)
        normalize: Whether to normalize the result
        
    Returns:
        Mutual information value
        
    Raises:
        ValueError: For invalid inputs
    """
    metrics = calculate_entropy_metrics(x, y, bins)
    if normalize:
        return float(metrics.normalized_mutual_information)
    return float(metrics.mutual_information)

def generate_correlated_normals(size: int, rho: float,
                              mu: float = 0, sigma: float = 1,
                              random_state: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate correlated normal random variables.
    
    Args:
        size: Number of samples
        rho: Correlation coefficient
        mu: Mean
        sigma: Standard deviation
        random_state: Random seed for reproducibility
        
    Returns:
        Tuple of (x, y) correlated variables
        
    Raises:
        ValueError: If parameters are invalid
    """
    if abs(rho) > 1:
        raise ValueError("Correlation coefficient must be between -1 and 1")
    if sigma <= 0:
        raise ValueError("Standard deviation must be positive")
    if size < 1:
        raise ValueError("Size must be positive")
        
    if random_state is not None:
        np.random.seed(random_state)
        
    rr = np.random.normal(mu, sigma, size=(2, size))
    x, y_ = rr[0, :], rr[1, :]
    y = rho * x + np.sqrt(1 - rho**2) * y_
    
    return x, y

def compute_distance_matrix(data: np.ndarray,
                          metric: str = 'variation_information',
                          normalize: bool = False) -> np.ndarray:
    """
    Compute pairwise distance matrix using specified metric.
    
    Args:
        data: Matrix where each column is a variable
        metric: Distance metric to use ('variation_information' or 'mutual_information')
        normalize: Whether to normalize the results
        
    Returns:
        Distance matrix
        
    Raises:
        ValueError: For invalid inputs or unknown metric
    """
    if data.ndim != 2:
        raise ValueError("Input must be 2D array")
    if metric not in ['variation_information', 'mutual_information']:
        raise ValueError("Unknown metric")
        
    n = data.shape[1]
    dist_matrix = np.zeros((n, n))
    
    for i in range(n):
        for j in range(i+1, n):
            if metric == 'variation_information':
                dist = variation_information(data[:, i], data[:, j], normalize=normalize)
            else:
                dist = mutual_information(data[:, i], data[:, j], normalize=normalize)
            dist_matrix[i, j] = dist_matrix[j, i] = dist
            
    return dist_matrix

// ---------------------------------------------------

// denoising-bonus.py
// shuangology/claude/denoising-bonus.py
"""
Enhanced denoising and portfolio optimization with robust implementation.
"""

import numpy as np
import pandas as pd
from scipy import linalg, optimize, cluster
from sklearn.covariance import LedoitWolf, OAS
from numba import njit
from typing import Tuple, List, Optional, Dict
import warnings

def validate_returns(returns: np.ndarray) -> None:
    """Validate returns data."""
    if np.isnan(returns).any():
        raise ValueError("Returns contain NaN values")
    if np.isinf(returns).any():
        raise ValueError("Returns contain infinite values")

def project_to_pd(matrix: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:
    """Project matrix to nearest positive definite matrix."""
    eigvals, eigvecs = linalg.eigh(matrix)
    eigvals = np.maximum(eigvals, epsilon)
    return eigvecs @ np.diag(eigvals) @ eigvecs.T

def factor_model_denoise(returns: np.ndarray, n_factors: Optional[int] = None) -> np.ndarray:
    """
    Denoise using statistical factor model with positive definiteness guarantee.
    """
    T, N = returns.shape
    if n_factors is None:
        n_factors = min(3, N // 3)  # Conservative default
    if n_factors >= min(T, N):
        raise ValueError(f"n_factors ({n_factors}) must be less than min(T,N)={min(T,N)}")
        
    U, s, Vh = linalg.svd(returns, full_matrices=False)
    s[n_factors:] = 0
    denoised_returns = U @ np.diag(s) @ Vh
    cov = denoised_returns.T @ denoised_returns / T
    return project_to_pd(cov)

def adaptive_thresholding(corr: np.ndarray, q: float, method: str = 'power') -> np.ndarray:
    """
    Enhanced adaptive thresholding with multiple methods.
    """
    eigvals, eigvecs = linalg.eigh(corr)
    if method == 'power':
        threshold = (q**0.5 + 1/q**0.5)**2
        weights = np.where(eigvals > threshold, 1.0, 
                         (eigvals/threshold)**2)
    else:  # Linear
        threshold = q**0.5
        weights = np.where(eigvals > threshold, 1.0, 
                         eigvals/threshold)
    
    filtered = eigvals * weights
    denoised = eigvecs @ np.diag(filtered) @ eigvecs.T
    return project_to_pd(denoised)

def shrinkage_ensemble(returns: np.ndarray, 
                      lookback: int = 252) -> np.ndarray:
    """
    Adaptive shrinkage ensemble based on out-of-sample performance.
    """
    if len(returns) < lookback:
        raise ValueError("Insufficient data for lookback period")
        
    def get_risk(est: np.ndarray, test_rets: np.ndarray) -> float:
        w = np.ones(est.shape[0]) / est.shape[0]  # Equal weights
        return np.sqrt(w @ est @ w) - np.std(test_rets @ w)
    
    # Calculate weights based on historical performance
    lw = LedoitWolf()
    oas = OAS()
    lw_errors = []
    oas_errors = []
    
    for t in range(lookback, len(returns), lookback//4):
        train = returns[t-lookback:t]
        test = returns[t:t+lookback//4]
        
        lw.fit(train)
        oas.fit(train)
        
        lw_errors.append(abs(get_risk(lw.covariance_, test)))
        oas_errors.append(abs(get_risk(oas.covariance_, test)))
    
    # Compute adaptive weights
    lw_weight = 1 / (1 + np.mean(lw_errors))
    oas_weight = 1 / (1 + np.mean(oas_errors))
    total = lw_weight + oas_weight
    
    # Final estimation
    lw.fit(returns)
    oas.fit(returns)
    return (lw_weight * lw.covariance_ + 
            oas_weight * oas.covariance_) / total

def hierarchical_risk_parity(cov: np.ndarray) -> np.ndarray:
    """
    Hierarchical Risk Parity using proper clustering.
    """
    corr = cov / np.sqrt(np.outer(np.diag(cov), np.diag(cov)))
    dist = np.sqrt(2 * (1 - corr))
    
    # Hierarchical clustering
    linkage = cluster.hierarchy.linkage(dist, method='single')
    sort_ix = cluster.hierarchy.leaves_list(linkage)
    
    # Sort covariance by cluster order
    cov = cov[sort_ix][:,sort_ix]
    
    # Recursive bisection
    def bisect(cov: np.ndarray, sort_ix: np.ndarray) -> np.ndarray:
        if len(sort_ix) == 1:
            return np.array([1.])
        
        mid = len(sort_ix) // 2
        left_ix = sort_ix[:mid]
        right_ix = sort_ix[mid:]
        
        # Recursive weights
        left_w = bisect(cov[left_ix][:,left_ix], left_ix)
        right_w = bisect(cov[right_ix][:,right_ix], right_ix)
        
        # Combine weights
        left_risk = np.sqrt(left_w @ cov[left_ix][:,left_ix] @ left_w)
        right_risk = np.sqrt(right_w @ cov[right_ix][:,right_ix] @ right_w)
        factor = 1 / (left_risk + right_risk)
        
        return np.concatenate([
            left_w * factor * left_risk,
            right_w * factor * right_risk
        ])
    
    weights = bisect(cov, np.arange(len(cov)))
    
    # Reorder weights to original order
    inv_sort_ix = np.argsort(sort_ix)
    return weights[inv_sort_ix]

def risk_parity_portfolio(cov: np.ndarray, 
                         tol: float = 1e-12) -> np.ndarray:
    """
    Risk parity with gradient information and robust optimization.
    """
    n = len(cov)
    
    def obj_func(w: np.ndarray) -> Tuple[float, np.ndarray]:
        w = w.reshape(-1,1)
        risk = np.sqrt(w.T @ cov @ w)
        risk_contrib = w * (cov @ w) / risk
        grad = cov @ w / risk - risk_contrib / w / risk
        return np.std(risk_contrib), grad.flatten()
    
    # Initialize with equal risk contribution
    x0 = np.ones(n) / np.sqrt(np.diag(cov))
    x0 /= x0.sum()
    
    # Optimization constraints
    constraints = [
        {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},
        {'type': 'ineq', 'fun': lambda x: x}  # Non-negative weights
    ]
    
    try:
        result = optimize.minimize(
            lambda w: obj_func(w)[0],
            x0,
            jac=lambda w: obj_func(w)[1],
            method='SLSQP',
            constraints=constraints,
            bounds=[(0, 1)] * n,
            options={'ftol': tol, 'maxiter': 1000}
        )
        if not result.success:
            warnings.warn(f"Optimization failed: {result.message}")
        return np.clip(result.x, 0, 1)
    except:
        warnings.warn("Optimization failed, returning equal weights")
        return np.ones(n) / n

class PortfolioOptimizer:
    """Enhanced portfolio optimization with robust implementation."""
    
    def __init__(self, returns: np.ndarray):
        validate_returns(returns)
        self.returns = returns
        self.T, self.N = returns.shape
        self.q = self.T / self.N
        
    def denoise_covariance(self, method: str = 'factor', 
                          **kwargs) -> np.ndarray:
        """Denoising with expanded options and error handling."""
        try:
            if method == 'factor':
                return factor_model_denoise(self.returns, **kwargs)
            elif method == 'shrinkage':
                return shrinkage_ensemble(self.returns, **kwargs)
            elif method == 'adaptive':
                sample_cov = np.cov(self.returns, rowvar=False)
                sample_corr = sample_cov / np.sqrt(
                    np.outer(np.diag(sample_cov), np.diag(sample_cov)))
                denoised_corr = adaptive_thresholding(
                    sample_corr, self.q, **kwargs)
                std = np.sqrt(np.diag(sample_cov))
                return project_to_pd(denoised_corr * np.outer(std, std))
            else:
                raise ValueError(f"Unknown method: {method}")
        except Exception as e:
            warnings.warn(f"Denoising failed: {str(e)}. Using sample covariance.")
            return np.cov(self.returns, rowvar=False)
    
    def optimize_portfolio(self, method: str = 'hrp', 
                         denoising: str = 'factor', 
                         **kwargs) -> np.ndarray:
        """Portfolio optimization with enhanced error handling."""
        try:
            cov = self.denoise_covariance(denoising, **kwargs)
            
            if method == 'hrp':
                return hierarchical_risk_parity(cov)
            elif method == 'min_corr':
                corr = cov / np.sqrt(np.outer(np.diag(cov), np.diag(cov)))
                w = minimum_correlation_portfolio(corr)
                return np.clip(w, 0, 1)  # Ensure valid weights
            elif method == 'risk_parity':
                return risk_parity_portfolio(cov, **kwargs)
            else:
                raise ValueError(f"Unknown method: {method}")
        except Exception as e:
            warnings.warn(f"Optimization failed: {str(e)}. Using equal weights.")
            return np.ones(self.N) / self.N
    
    def backtest_portfolio(self, method: str = 'hrp',
                          denoising: str = 'factor',
                          lookback: int = 252,
                          rebalance_freq: int = 21,
                          **kwargs) -> pd.DataFrame:
        """Enhanced backtesting with performance metrics."""
        results = []
        weights_history = []
        
        for t in range(lookback, len(self.returns), rebalance_freq):
            window = self.returns[t-lookback:t]
            weights = self.optimize_portfolio(
                method, denoising, **kwargs)
            weights_history.append(weights)
            
            # Forward returns
            fwd_rets = self.returns[t:t+rebalance_freq]
            port_rets = fwd_rets @ weights
            
            results.append(pd.DataFrame({
                'returns': port_rets,
                'cumulative': (1 + port_rets).cumprod(),
                'drawdown': 1 - (1 + port_rets).cumprod() / \
                    (1 + port_rets).cumprod().cummax()
            }, index=pd.RangeIndex(t, t+len(port_rets))))
        
        results_df = pd.concat(results)
        
        # Add performance metrics
        ann_ret = results_df['returns'].mean() * 252
        ann_vol = results_df['returns'].std() * np.sqrt(252)
        sharpe = ann_ret / ann_vol
        max_dd = results_df['drawdown'].max()
        
        print(f"\nPerformance Metrics:")
        print(f"Annual Return: {ann_ret:.2%}")
        print(f"Annual Volatility: {ann_vol:.2%}")
        print(f"Sharpe Ratio: {sharpe:.2f}")
        print(f"Maximum Drawdown: {max_dd:.2%}")
        
        return results_df

// ---------------------------------------------------

// denoising-detoning-improvement.py
// shuangology/claude/denoising-detoning-improvement.py
"""
Advanced methods for denoising financial covariance matrices and portfolio optimization.

This module implements the Marcenko-Pastur theorem and various denoising techniques
for financial analysis, with optimized performance and robust error handling.
"""

from typing import Tuple, Optional, Union, Any
import warnings
import numpy as np
import pandas as pd
from scipy import linalg
from scipy.optimize import minimize
from sklearn.covariance import LedoitWolf
from sklearn.neighbors import KernelDensity

# Default parameters
DEFAULT_BANDWIDTH = 0.25
DEFAULT_ALPHA = 0.5
DEFAULT_POINTS = 1000
DEFAULT_Q = 10
MIN_EIGENVAL = 1e-8


def validate_matrix(matrix: np.ndarray) -> None:
    """Validate matrix properties."""
    if not isinstance(matrix, np.ndarray):
        raise TypeError("Input must be numpy array")
    if matrix.shape[0] != matrix.shape[1]:
        raise ValueError("Matrix must be square")
    if not np.allclose(matrix, matrix.T):
        raise ValueError("Matrix must be symmetric")


def mpPDF(variance: float, q: float, points: int = DEFAULT_POINTS) -> pd.Series:
    """Calculate Marcenko-Pastur probability density function."""
    if variance <= 0 or q <= 0:
        raise ValueError("Variance and q must be positive")

    eMin = variance * (1 - (1/q)**0.5)**2
    eMax = variance * (1 + (1/q)**0.5)**2
    eVal = np.linspace(eMin, eMax, points)
    pdf = q/(2 * np.pi * variance * eVal) * \
        ((eMax - eVal) * (eVal - eMin))**0.5
    return pd.Series(pdf, index=eVal)


def getPCA(matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Perform PCA with robust eigenvalue computation."""
    validate_matrix(matrix)
    try:
        eVal, eVec = linalg.eigh(matrix)
        if np.any(eVal < -MIN_EIGENVAL):
            warnings.warn("Matrix has significant negative eigenvalues")
        indices = eVal.argsort()[::-1]
        return np.diagflat(eVal[indices]), eVec[:, indices]
    except linalg.LinAlgError:
        raise ValueError("Failed to compute eigendecomposition")


def fitKDE(obs: np.ndarray, bandwidth: float = DEFAULT_BANDWIDTH,
           kernel: str = 'gaussian', x: Optional[np.ndarray] = None) -> pd.Series:
    """Fit kernel density estimation with vectorized operations."""
    obs = np.asarray(obs).reshape(-1, 1)
    x = np.unique(obs) if x is None else np.asarray(x).reshape(-1, 1)

    kde = KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(obs)
    logProb = kde.score_samples(x)
    return pd.Series(np.exp(logProb), index=x.flatten())


def getRndCov(n_cols: int, n_facts: int) -> np.ndarray:
    """Generate random covariance matrix using efficient operations."""
    if n_cols <= 0 or n_facts <= 0:
        raise ValueError("Dimensions must be positive")

    w = np.random.normal(size=(n_cols, n_facts))
    cov = np.einsum('ik,jk->ij', w, w)
    cov += np.diag(np.random.uniform(size=n_cols))
    return cov


def cov2corr(cov: np.ndarray) -> np.ndarray:
    """Convert covariance to correlation matrix using vectorized operations."""
    validate_matrix(cov)
    std = np.sqrt(np.diag(cov))
    if np.any(std < MIN_EIGENVAL):
        raise ValueError("Near-zero standard deviations detected")

    corr = cov / np.outer(std, std)
    np.clip(corr, -1, 1, out=corr)
    return corr


def corr2cov(corr: np.ndarray, std: np.ndarray) -> np.ndarray:
    """Convert correlation to covariance matrix with validation."""
    validate_matrix(corr)
    if np.any(std < MIN_EIGENVAL):
        raise ValueError("Invalid standard deviations")

    cov = np.einsum('i,ij,j->ij', std, corr, std)
    return cov


def errPDFs(params: np.ndarray, eVal: np.ndarray, q: float,
            bandwidth: float, points: int = DEFAULT_POINTS) -> float:
    """Calculate PDF fitting error with optimized operations."""
    variance = params[0]
    if variance <= 0:
        return np.inf

    pdf0 = mpPDF(variance, q, points)
    pdf1 = fitKDE(eVal, bandwidth, x=pdf0.index.values)
    return np.sum((pdf1 - pdf0)**2)


def findMaxEval(eVal: np.ndarray, q: float, bandwidth: float) -> Tuple[float, float]:
    """Find maximum eigenvalue with robust optimization."""
    result = minimize(
        errPDFs,
        x0=[0.5],
        args=(eVal, q, bandwidth),
        method='trust-constr',
        bounds=[(MIN_EIGENVAL, 1-MIN_EIGENVAL)],
        options={'gtol': 1e-6}
    )

    if not result.success:
        warnings.warn("Optimization failed to converge")

    variance = result.x[0]
    return variance * (1 + (1./q)**0.5)**2, variance


def _denoise_base(eVal: np.ndarray, eVec: np.ndarray, n_facts: int,
                  method: str = 'constant', alpha: float = DEFAULT_ALPHA) -> np.ndarray:
    """Base denoising implementation for both methods."""
    if method == 'constant':
        eVal_new = np.diag(eVal).copy()
        eVal_new[n_facts:] = eVal_new[n_facts:].mean()
        return np.einsum('ij,j,kj->ik', eVec, eVal_new, eVec)
    elif method == 'shrinkage':
        eValL, eVecL = eVal[:n_facts, :n_facts], eVec[:, :n_facts]
        eValR, eVecR = eVal[n_facts:, n_facts:], eVec[:, n_facts:]
        corr0 = np.einsum('ij,jk,lj->il', eVecL, eValL, eVecL)
        corr1 = np.einsum('ij,jk,lj->il', eVecR, eValR, eVecR)
        return corr0 + alpha * corr1 + (1 - alpha) * np.diag(np.diag(corr1))
    else:
        raise ValueError("Invalid denoising method")


def denoisedCorr(eVal: np.ndarray, eVec: np.ndarray, n_facts: int) -> np.ndarray:
    """Denoise correlation matrix using constant residual method."""
    corr = _denoise_base(eVal, eVec, n_facts, method='constant')
    return cov2corr(corr)


def denoisedCorr2(eVal: np.ndarray, eVec: np.ndarray, n_facts: int,
                  alpha: float = DEFAULT_ALPHA) -> np.ndarray:
    """Denoise correlation matrix using targeted shrinkage."""
    return _denoise_base(eVal, eVec, n_facts, method='shrinkage', alpha=alpha)


def formBlockMatrix(n_blocks: int, block_size: int, block_corr: float) -> np.ndarray:
    """Form block correlation matrix with validation."""
    if not (0 <= block_corr <= 1):
        raise ValueError("Block correlation must be between 0 and 1")

    block = np.full((block_size, block_size), block_corr)
    np.fill_diagonal(block, 1)
    return linalg.block_diag(*([block] * n_blocks))


def optPort(cov: np.ndarray, mu: Optional[np.ndarray] = None) -> np.ndarray:
    """Calculate optimal portfolio weights using robust solver."""
    validate_matrix(cov)
    n = cov.shape[0]
    ones = np.ones((n, 1))
    mu = ones if mu is None else mu.reshape(-1, 1)

    try:
        w = linalg.solve(cov, mu, assume_a='sym')
        return w / (ones.T @ w)
    except linalg.LinAlgError:
        raise ValueError("Singular covariance matrix detected")


def simCovMu(mu0: np.ndarray, cov0: np.ndarray, n_obs: int,
             shrink: bool = False) -> Tuple[np.ndarray, np.ndarray]:
    """Simulate covariance matrix and means with validation."""
    validate_matrix(cov0)

    try:
        x = np.random.multivariate_normal(mu0.flatten(), cov0, size=n_obs)
        mu1 = x.mean(axis=0).reshape(-1, 1)
        cov1 = LedoitWolf().fit(x).covariance_ if shrink else np.cov(x, rowvar=0)
        return mu1, cov1
    except (ValueError, np.linalg.LinAlgError) as e:
        raise ValueError(f"Simulation failed: {str(e)}")


// ---------------------------------------------------

// distance-metrics.py
// shuangology/claude/distance-metrics.py
"""
Distance Metrics for Financial Analysis.

This module implements various distance and dependency metrics including mutual information,
variation of information, and entropy-based measures.
"""

from typing import Tuple, Optional, Union
import numpy as np
import scipy.stats as ss
from sklearn.metrics import mutual_info_score

def calculate_entropy_metrics(x: np.ndarray, y: np.ndarray, bins: int = 100) -> dict:
    """
    Calculate various entropy-based metrics between two variables.
    
    Args:
        x: First variable
        y: Second variable
        bins: Number of bins for histogram
        
    Returns:
        Dictionary containing entropy metrics
    """
    cXY = np.histogram2d(x, y, bins)[0]
    hX = ss.entropy(np.histogram(x, bins)[0])
    hY = ss.entropy(np.histogram(y, bins)[0])
    iXY = mutual_info_score(None, None, contingency=cXY)
    iXYn = iXY / min(hX, hY)
    hXY = hX + hY - iXY
    hX_Y = hXY - hY
    hY_X = hXY - hX
    
    return {
        'marginal_entropy_x': hX,
        'marginal_entropy_y': hY,
        'mutual_information': iXY,
        'normalized_mutual_information': iXYn,
        'conditional_entropy_x_y': hX_Y,
        'conditional_entropy_y_x': hY_X,
        'joint_entropy': hXY
    }

def variation_information(x: np.ndarray, y: np.ndarray, bins: int, 
                         normalize: bool = False) -> float:
    """
    Calculate variation of information between two variables.
    
    Args:
        x: First variable
        y: Second variable
        bins: Number of bins for histogram
        normalize: Whether to normalize the result
        
    Returns:
        Variation of information value
    """
    cXY = np.histogram2d(x, y, bins)[0]
    iXY = mutual_info_score(None, None, contingency=cXY)
    hX = ss.entropy(np.histogram(x, bins)[0])
    hY = ss.entropy(np.histogram(y, bins)[0])
    vXY = hX + hY - 2 * iXY
    
    if normalize:
        hXY = hX + hY - iXY
        vXY /= hXY
        
    return vXY

def optimal_bin_count(n_obs: int, corr: Optional[float] = None) -> int:
    """
    Calculate optimal number of bins for discretization.
    
    Args:
        n_obs: Number of observations
        corr: Correlation coefficient for bivariate case
        
    Returns:
        Optimal number of bins
    """
    if corr is None:
        z = (8 + 324*n_obs + 12*(36*n_obs + 729*n_obs**2)**0.5)**(1/3.)
        b = round(z/6. + 2./(3*z) + 1./3)
    else:
        if abs(1 - corr**2) < 1e-5:
            corr = np.sign(corr) * (abs(corr) - 1e-5)
        b = round(2**-0.5 * (1 + (1 + 24*n_obs/(1 - corr**2))**0.5)**0.5)
    return int(b)

def variation_information_optimal(x: np.ndarray, y: np.ndarray, 
                                normalize: bool = False) -> float:
    """
    Calculate variation of information using optimal bin count.
    
    Args:
        x: First variable
        y: Second variable
        normalize: Whether to normalize the result
        
    Returns:
        Variation of information value
    """
    bins = optimal_bin_count(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    return variation_information(x, y, bins, normalize)

def mutual_information(x: np.ndarray, y: np.ndarray, 
                      normalize: bool = False) -> float:
    """
    Calculate mutual information between two variables.
    
    Args:
        x: First variable
        y: Second variable
        normalize: Whether to normalize the result
        
    Returns:
        Mutual information value
    """
    bins = optimal_bin_count(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    cXY = np.histogram2d(x, y, bins)[0]
    iXY = mutual_info_score(None, None, contingency=cXY)
    
    if normalize:
        hX = ss.entropy(np.histogram(x, bins)[0])
        hY = ss.entropy(np.histogram(y, bins)[0])
        iXY /= min(hX, hY)
        
    return iXY

def generate_correlated_normals(size: int, rho: float, 
                              mu: float = 0, sigma: float = 1) -> Tuple[np.ndarray, np.ndarray]:
    """
    Generate correlated normal random variables.
    
    Args:
        size: Number of samples
        rho: Correlation coefficient
        mu: Mean
        sigma: Standard deviation
        
    Returns:
        Tuple of (x, y) correlated variables
    """
    rr = np.random.normal(mu, sigma, size=(2, size))
    x, y_ = rr[0, :], rr[1, :]
    y = rho * x + np.sqrt(1 - rho**2) * y_
    return x, y


// ---------------------------------------------------

// optimal-clustering.py
// shuangology/claude/optimal-clustering.py
"""
Optimal Clustering Implementation.

This module implements various clustering algorithms for financial correlation matrices,
including hierarchical clustering, K-means, and random block generation utilities.
"""

from typing import Dict, Tuple, List, Optional, Union
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples
from sklearn.utils import check_random_state
from scipy.linalg import block_diag

def clusterKMeansBase(corr0: pd.DataFrame, 
                      maxNumClusters: int = 10,
                      n_init: int = 10) -> Tuple[pd.DataFrame, Dict, pd.Series]:
    """
    Perform base K-means clustering on correlation matrix.
    
    Args:
        corr0: Correlation matrix
        maxNumClusters: Maximum number of clusters to try
        n_init: Number of initialization attempts
        
    Returns:
        Tuple of (reordered correlation matrix, cluster assignments, silhouette scores)
    """
    # Transform correlations to distances
    x = ((1 - corr0.fillna(0)) / 2.)**0.5
    best_silh = pd.Series()
    best_kmeans = None
    
    for init in range(n_init):
        for n_clusters in range(2, maxNumClusters + 1):
            kmeans = KMeans(n_clusters=n_clusters, n_init=1)
            kmeans.fit(x)
            silh = silhouette_samples(x, kmeans.labels_)
            
            # Compare clustering quality
            stat = (silh.mean() / silh.std())
            if not best_silh.size or stat > (best_silh.mean() / best_silh.std()):
                best_silh = pd.Series(silh, index=x.index)
                best_kmeans = kmeans

    # Reorder correlation matrix
    idx = np.argsort(best_kmeans.labels_)
    corr1 = corr0.iloc[idx].iloc[:, idx]
    
    # Create cluster assignments
    clusters = {i: corr0.columns[np.where(best_kmeans.labels_ == i)[0]].tolist()
               for i in np.unique(best_kmeans.labels_)}
    
    return corr1, clusters, best_silh

def makeNewOutputs(corr0: pd.DataFrame,
                   clstrs: Dict,
                   clstrs2: Dict) -> Tuple[pd.DataFrame, Dict, pd.Series]:
    """
    Combine results from multiple clustering operations.
    
    Args:
        corr0: Original correlation matrix
        clstrs: First set of clusters
        clstrs2: Second set of clusters
        
    Returns:
        Tuple of (new correlation matrix, combined clusters, new silhouette scores)
    """
    # Combine clusters
    combined_clusters = {}
    for i, cluster in clstrs.items():
        combined_clusters[len(combined_clusters)] = list(cluster)
    for i, cluster in clstrs2.items():
        combined_clusters[len(combined_clusters)] = list(cluster)
    
    # Reorder correlation matrix
    new_idx = [j for i in combined_clusters.values() for j in i]
    corr_new = corr0.loc[new_idx, new_idx]
    
    # Calculate silhouette scores
    x = ((1 - corr0.fillna(0)) / 2.)**0.5
    labels = np.zeros(len(x.columns))
    for i, members in combined_clusters.items():
        idxs = [x.index.get_loc(k) for k in members]
        labels[idxs] = i
        
    silh_new = pd.Series(silhouette_samples(x, labels), index=x.index)
    return corr_new, combined_clusters, silh_new

def clusterKMeansTop(corr0: pd.DataFrame,
                     maxNumClusters: Optional[int] = None,
                     n_init: int = 3) -> Tuple[pd.DataFrame, Dict, pd.Series]:
    """
    Perform recursive optimal clustering.
    
    Args:
        corr0: Correlation matrix
        maxNumClusters: Maximum number of clusters
        n_init: Number of initialization attempts
        
    Returns:
        Tuple of (clustered correlation matrix, cluster assignments, silhouette scores)
    """
    if maxNumClusters is None:
        maxNumClusters = corr0.shape[1] - 1
        
    # Initial clustering
    corr1, clusters, silh = clusterKMeansBase(
        corr0,
        maxNumClusters=min(maxNumClusters, corr0.shape[1] - 1),
        n_init=n_init
    )
    
    # Calculate cluster quality
    cluster_stats = {
        i: np.mean(silh[clusters[i]]) / np.std(silh[clusters[i]])
        for i in clusters
    }
    mean_stat = np.mean(list(cluster_stats.values()))
    
    # Find clusters to redo
    redo_clusters = [i for i, stat in cluster_stats.items() if stat < mean_stat]
    
    if len(redo_clusters) <= 1:
        return corr1, clusters, silh
        
    # Recluster poor quality clusters
    redo_keys = [j for i in redo_clusters for j in clusters[i]]
    corr_tmp = corr0.loc[redo_keys, redo_keys]
    
    corr2, clusters2, silh2 = clusterKMeansTop(
        corr_tmp,
        maxNumClusters=min(maxNumClusters, corr_tmp.shape[1] - 1),
        n_init=n_init
    )
    
    # Combine results
    keep_clusters = {i: clusters[i] for i in clusters if i not in redo_clusters}
    corr_new, clusters_new, silh_new = makeNewOutputs(
        corr0, keep_clusters, clusters2
    )
    
    # Compare quality
    new_stats = [
        np.mean(silh_new[clusters_new[i]]) / np.std(silh_new[clusters_new[i]])
        for i in clusters_new
    ]
    
    if np.mean(new_stats) <= mean_stat:
        return corr1, clusters, silh
    return corr_new, clusters_new, silh_new

def getCovSub(nObs: int,
              nCols: int,
              sigma: float,
              random_state: Optional[int] = None) -> np.ndarray:
    """
    Generate random submatrix for block correlation matrix.
    
    Args:
        nObs: Number of observations
        nCols: Number of columns
        sigma: Standard deviation of noise
        random_state: Random seed
        
    Returns:
        Covariance submatrix
    """
    rng = check_random_state(random_state)
    
    if nCols == 1:
        return np.ones((1, 1))
        
    # Generate correlated data
    base = rng.normal(size=(nObs, 1))
    data = np.repeat(base, nCols, axis=1)
    data += rng.normal(scale=sigma, size=data.shape)
    
    return np.cov(data, rowvar=False)

def getRndBlockCov(nCols: int,
                   nBlocks: int,
                   minBlockSize: int = 1,
                   sigma: float = 1.0,
                   random_state: Optional[int] = None) -> np.ndarray:
    """
    Generate random block covariance matrix.
    
    Args:
        nCols: Total number of columns
        nBlocks: Number of blocks
        minBlockSize: Minimum block size
        sigma: Noise standard deviation
        random_state: Random seed
        
    Returns:
        Block covariance matrix
    """
    rng = check_random_state(random_state)
    
    # Generate random block sizes
    available_space = nCols - (minBlockSize - 1) * nBlocks
    split_points = rng.choice(range(1, available_space), nBlocks - 1, replace=False)
    split_points.sort()
    split_points = np.append(split_points, available_space)
    
    # Calculate block sizes
    block_sizes = np.append(split_points[0], np.diff(split_points)) - 1 + minBlockSize
    
    # Generate blocks
    cov_blocks = []
    for size in block_sizes:
        n_obs = int(max(size * (size + 1) / 2, 100))
        cov_block = getCovSub(n_obs, size, sigma, random_state=rng)
        cov_blocks.append(cov_block)
        
    return block_diag(*cov_blocks)

def randomBlockCorr(nCols: int,
                    nBlocks: int,
                    minBlockSize: int = 1,
                    random_state: Optional[int] = None) -> pd.DataFrame:
    """
    Generate random block correlation matrix.
    
    Args:
        nCols: Number of columns
        nBlocks: Number of blocks
        minBlockSize: Minimum block size
        random_state: Random seed
        
    Returns:
        Random block correlation matrix
    """
    rng = check_random_state(random_state)
    
    # Generate base structure
    cov_structure = getRndBlockCov(
        nCols, nBlocks,
        minBlockSize=minBlockSize,
        sigma=0.5,
        random_state=rng
    )
    
    # Add noise
    cov_noise = getRndBlockCov(
        nCols, 1,
        minBlockSize=minBlockSize,
        sigma=1.0,
        random_state=rng
    )
    
    cov_total = cov_structure + cov_noise
    
    # Convert to correlation matrix
    std = np.sqrt(np.diag(cov_total))
    corr = cov_total / np.outer(std, std)
    corr = pd.DataFrame(corr)
    
    return corr

// ---------------------------------------------------

// denoising-detoning.py
// shuangology/claude/denoising-detoning.py
"""
Denoising and Detoning Functions for Financial Analysis.

This module provides implementations of the Marcenko-Pastur theorem and various
denoising methods for financial covariance matrices, along with portfolio optimization utilities.
"""

from typing import Tuple, Optional, List, Union
import numpy as np
import pandas as pd
from scipy.linalg import block_diag
from scipy.optimize import minimize
from sklearn.covariance import LedoitWolf
from sklearn.neighbors import KernelDensity


def mpPDF(var: float, q: float, pts: int = 1000) -> pd.Series:
    """
    Calculate Marcenko-Pastur probability density function.

    Args:
        var: Variance parameter
        q: Ratio T/N (time samples / variables)
        pts: Number of points for PDF calculation

    Returns:
        PDF values as pandas Series
    """
    eMin, eMax = var * (1 - (1/q)**0.5)**2, var * (1 + (1/q)**0.5)**2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q/(2 * np.pi * var * eVal) * ((eMax - eVal) * (eVal - eMin))**0.5
    return pd.Series(pdf, index=eVal)


def getPCA(matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Perform PCA on a given matrix.

    Args:
        matrix: Input matrix for PCA

    Returns:
        Tuple of (eigenvalues matrix, eigenvectors matrix)
    """
    eVal, eVec = np.linalg.eigh(matrix)
    indices = eVal.argsort()[::-1]
    eVal, eVec = eVal[indices], eVec[:, indices]
    eVal = np.diagflat(eVal)
    return eVal, eVec


def fitKDE(obs: np.ndarray, bWidth: float = 0.25, kernel: str = 'gaussian',
           x: Optional[np.ndarray] = None) -> pd.Series:
    """
    Fit kernel density estimation to observations.

    Args:
        obs: Observations to fit
        bWidth: Bandwidth parameter
        kernel: Kernel type
        x: Optional evaluation points

    Returns:
        Fitted PDF as pandas Series
    """
    if len(obs.shape) == 1:
        obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    if x is None:
        x = np.unique(obs).reshape(-1, 1)
    if len(x.shape) == 1:
        x = x.reshape(-1, 1)
    logProb = kde.score_samples(x)
    return pd.Series(np.exp(logProb), index=x.flatten())


def getRndCov(nCols: int, nFacts: int) -> np.ndarray:
    """
    Generate random covariance matrix.

    Args:
        nCols: Number of columns
        nFacts: Number of factors

    Returns:
        Random covariance matrix
    """
    w = np.random.normal(size=(nCols, nFacts))
    cov = np.dot(w, w.T)
    cov += np.diag(np.random.uniform(size=nCols))
    return cov


def cov2corr(cov: np.ndarray) -> np.ndarray:
    """
    Convert covariance matrix to correlation matrix.

    Args:
        cov: Covariance matrix

    Returns:
        Correlation matrix
    """
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1
    return corr


def corr2cov(corr: np.ndarray, std: np.ndarray) -> np.ndarray:
    """
    Convert correlation matrix to covariance matrix.

    Args:
        corr: Correlation matrix
        std: Standard deviations

    Returns:
        Covariance matrix
    """
    corr[corr < -1], corr[corr > 1] = -1, 1
    return np.outer(std, std) * corr


def errPDFs(var: List[float], eVal: np.ndarray, q: float,
            bWidth: float, pts: int = 1000) -> float:
    """
    Calculate error between theoretical and empirical PDFs.

    Args:
        var: Variance parameter (as list for optimization)
        eVal: Eigenvalues
        q: T/N ratio
        bWidth: Bandwidth for KDE
        pts: Number of points for PDF

    Returns:
        Sum of squared errors
    """
    var = var[0]
    pdf0 = mpPDF(var, q, pts)
    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)
    return np.sum((pdf1 - pdf0) ** 2)


def findMaxEval(eVal: np.ndarray, q: float, bWidth: float) -> Tuple[float, float]:
    """
    Find maximum eigenvalue through optimization.

    Args:
        eVal: Eigenvalues
        q: T/N ratio
        bWidth: Bandwidth for KDE

    Returns:
        Tuple of (maximum eigenvalue, optimal variance)
    """
    out = minimize(lambda *x: errPDFs(*x), 0.5, args=(eVal, q, bWidth),
                   bounds=((1E-5, 1-1E-5),))
    var = out['x'][0] if out['success'] else 1
    eMax = var * (1 + (1./q)**0.5)**2
    return eMax, var


def denoisedCorr(eVal: np.ndarray, eVec: np.ndarray, nFacts: int) -> np.ndarray:
    """
    Denoise correlation matrix using constant residual method.

    Args:
        eVal: Eigenvalues matrix
        eVec: Eigenvectors matrix
        nFacts: Number of factors to preserve

    Returns:
        Denoised correlation matrix
    """
    eVal_ = np.diag(eVal).copy()
    eVal_[nFacts:] = eVal_[nFacts:].sum() / float(eVal_.shape[0] - nFacts)
    eVal_ = np.diag(eVal_)
    corr1 = np.dot(eVec, eVal_).dot(eVec.T)
    return cov2corr(corr1)


def denoisedCorr2(eVal: np.ndarray, eVec: np.ndarray, nFacts: int,
                  alpha: float = 0) -> np.ndarray:
    """
    Denoise correlation matrix using targeted shrinkage.

    Args:
        eVal: Eigenvalues matrix
        eVec: Eigenvectors matrix
        nFacts: Number of factors to preserve
        alpha: Shrinkage parameter

    Returns:
        Denoised correlation matrix
    """
    eValL, eVecL = eVal[:nFacts, :nFacts], eVec[:, :nFacts]
    eValR, eVecR = eVal[nFacts:, nFacts:], eVec[:, nFacts:]
    corr0 = np.dot(eVecL, eValL).dot(eVecL.T)
    corr1 = np.dot(eVecR, eValR).dot(eVecR.T)
    return corr0 + alpha * corr1 + (1 - alpha) * np.diag(np.diag(corr1))


def formBlockMatrix(nBlocks: int, bSize: int, bCorr: float) -> np.ndarray:
    """
    Form block matrix for covariance structure.

    Args:
        nBlocks: Number of blocks
        bSize: Block size
        bCorr: Block correlation

    Returns:
        Block correlation matrix
    """
    block = np.ones((bSize, bSize)) * bCorr
    block[range(bSize), range(bSize)] = 1
    return block_diag(*([block] * nBlocks))


def formTrueMatrix(nBlocks: int, bSize: int, bCorr: float) -> Tuple[np.ndarray, np.ndarray]:
    """
    Form true matrix with block structure.

    Args:
        nBlocks: Number of blocks
        bSize: Block size
        bCorr: Block correlation

    Returns:
        Tuple of (means vector, covariance matrix)
    """
    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(0.05, 0.2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0


def deNoiseCov(cov0: np.ndarray, q: float, bWidth: float) -> np.ndarray:
    """
    Denoise covariance matrix.

    Args:
        cov0: Original covariance matrix
        q: T/N ratio
        bWidth: Bandwidth for KDE

    Returns:
        Denoised covariance matrix
    """
    corr0 = cov2corr(cov0)
    eVal0, eVec0 = getPCA(corr0)
    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth)
    nFacts0 = eVal0.shape[0] - np.diag(eVal0)[::-1].searchsorted(eMax0)
    corr1 = denoisedCorr(eVal0, eVec0, nFacts0)
    return corr2cov(corr1, np.diag(cov0)**0.5)


def optPort(cov: np.ndarray, mu: Optional[np.ndarray] = None) -> np.ndarray:
    """
    Calculate optimal portfolio weights.

    Args:
        cov: Covariance matrix
        mu: Optional expected returns vector

    Returns:
        Optimal portfolio weights
    """
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    mu = ones if mu is None else mu
    w = np.dot(inv, mu)
    return w / np.dot(ones.T, w)


def simCovMu(mu0: np.ndarray, cov0: np.ndarray, nObs: int,
             shrink: bool = False) -> Tuple[np.ndarray, np.ndarray]:
    """
    Simulate covariance matrix and means.

    Args:
        mu0: True means
        cov0: True covariance
        nObs: Number of observations
        shrink: Whether to use shrinkage

    Returns:
        Tuple of (simulated means, simulated covariance)
    """
    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)
    mu1 = x.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(x).covariance_
    else:
        cov1 = np.cov(x, rowvar=0)
    return mu1, cov1


// ---------------------------------------------------

// financial_lables_ok.py
// shuangology/claude/financial_lables_ok.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
# from scipy import stats
from statsmodels.tsa.stattools import adfuller
from concurrent.futures import ThreadPoolExecutor
from typing import Tuple, List

def t_value_linear_trend(close: np.ndarray) -> float:
    """Calculate t-value for linear trend."""
    x = np.column_stack((np.ones(len(close)), np.arange(len(close))))
    return sm.OLS(close, x).fit().tvalues[1]

def polynomial_trend(close: np.ndarray, degree: int = 2) -> float:
    """Calculate polynomial trend t-value."""
    x = np.arange(len(close))
    coeffs = np.polyfit(x, close, degree)
    y_pred = np.polyval(coeffs, x)
    residuals = close - y_pred
    mse = np.sum(residuals**2) / (len(close) - degree - 1)
    return coeffs[0] / np.sqrt(mse)

def rolling_mean_test(close: np.ndarray, window: int = 20) -> float:
    """Perform rolling mean trend test."""
    if len(close) < window * 2:
        return 0
    first_half = close[:window].mean()
    second_half = close[-window:].mean()
    pooled_std = np.sqrt(np.var(close[:window], ddof=1)/window + 
                        np.var(close[-window:], ddof=1)/window)
    return (second_half - first_half) / pooled_std if pooled_std != 0 else 0

def get_trend_labels(molecule: pd.DatetimeIndex, close: pd.Series, 
                    span: Tuple[int, int], 
                    methods: List[str] = ['linear', 'poly', 'rolling', 'adf']) -> pd.DataFrame:
    """Get trend labels using multiple methods."""
    out = pd.DataFrame(index=molecule, 
                      columns=['t1', 'tVal', 'bin', 'poly_trend', 'rolling_trend', 'adf_trend'])
    hrzns = range(*span)
    
    def process_date(dt0):
        iloc0 = close.index.get_loc(dt0)
        if iloc0 + max(hrzns) > len(close):
            return None
            
        results = {}
        for hrzn in hrzns:
            dt1 = close.index[iloc0 + hrzn - 1]
            subset = close[dt0:dt1].values
            
            if len(subset) < 2:
                continue
                
            results[dt1] = {
                'linear': t_value_linear_trend(subset),
                'poly': polynomial_trend(subset),
                'rolling': rolling_mean_test(subset),
                'adf': -adfuller(subset)[0]  # Negative ADF stat for consistency
            }
        
        if not results:
            return None
            
        # Aggregate results for each method
        method_results = {method: pd.Series({dt: res[method] 
                         for dt, res in results.items()})
                         for method in ['linear', 'poly', 'rolling', 'adf']}
        
        max_dt = max(method_results['linear'].abs().idxmax(),
                    method_results['poly'].abs().idxmax())
        
        return pd.Series({
            't1': max_dt,
            'tVal': method_results['linear'].loc[max_dt],
            'bin': np.sign(method_results['linear'].loc[max_dt]),
            'poly_trend': method_results['poly'].loc[max_dt],
            'rolling_trend': method_results['rolling'].loc[max_dt],
            'adf_trend': method_results['adf'].loc[max_dt]
        })

    with ThreadPoolExecutor() as executor:
        results = list(executor.map(process_date, molecule))
    
    for dt0, result in zip(molecule, results):
        if result is not None:
            out.loc[dt0] = result
    
    out['t1'] = pd.to_datetime(out['t1'])
    out['bin'] = out['bin'].astype('int8')
    return out.dropna(subset=['bin'])

def plot_trend_labels(close: pd.Series, labels: pd.DataFrame, title: str):
    """Plot trend labels with multiple indicators."""
    import matplotlib.pyplot as plt
    
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)
    
    # Price and linear trend
    scatter = ax1.scatter(labels.index, close.loc[labels.index], 
                         c=labels['bin'], cmap='viridis')
    ax1.plot(close.index, close.values, color='gray', alpha=0.5)
    ax1.set_title(f'{title} - Price and Linear Trend')
    
    # Trend indicators
    for col, color in zip(['poly_trend', 'rolling_trend', 'adf_trend'], 
                         ['blue', 'green', 'red']):
        ax2.plot(labels.index, labels[col], 
                label=col.replace('_', ' ').title(), 
                color=color, alpha=0.7)
    
    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)
    ax2.legend()
    ax2.set_title('Trend Indicators')
    
    plt.tight_layout()
    return fig, (ax1, ax2)

// ---------------------------------------------------

// financial_lables.py
// shuangology/deepseek/financial_lables.py
import numpy as np
import pandas as pd
import statsmodels.api as sm
from scipy import stats
from statsmodels.tsa.stattools import adfuller
from concurrent.futures import ProcessPoolExecutor
from typing import Tuple, List, Optional, Dict
from functools import lru_cache
from numba import jit
import matplotlib.pyplot as plt
from dataclasses import dataclass
from scipy.stats import norm

@dataclass
class TrendResult:
    """Data class for storing trend analysis results."""
    t_value: float
    confidence_interval: Tuple[float, float]
    trend_strength: float

class AdaptiveWindow:
    """Adaptive window size based on volatility."""
    def __init__(self, min_window: int = 10, max_window: int = 50):
        self.min_window = min_window
        self.max_window = max_window
    
    def get_window(self, data: np.ndarray) -> int:
        """Calculate adaptive window size based on volatility."""
        volatility = np.std(np.diff(data))
        # Scale window size inversely with volatility
        window = int(self.max_window / (1 + volatility))
        return max(self.min_window, min(window, self.max_window))

@jit(nopython=True)
def fast_rolling_stats(data: np.ndarray, window: int) -> Tuple[np.ndarray, np.ndarray]:
    """Optimized rolling mean and std calculation using Numba."""
    n = len(data)
    means = np.zeros(n - window + 1)
    stds = np.zeros(n - window + 1)
    
    for i in range(n - window + 1):
        window_data = data[i:i+window]
        means[i] = np.mean(window_data)
        stds[i] = np.std(window_data)
    
    return means, stds

@lru_cache(maxsize=1024)
def cached_t_value_linear_trend(close_tuple: Tuple[float, ...]) -> TrendResult:
    """Cached version of t-value calculation with confidence intervals."""
    close = np.array(close_tuple)
    x = np.column_stack((np.ones(len(close)), np.arange(len(close))))
    model = sm.OLS(close, x).fit()
    
    t_value = model.tvalues[1]
    conf_int = model.conf_int(alpha=0.05)[1]
    trend_strength = abs(t_value) / np.sqrt(len(close))
    
    return TrendResult(t_value, conf_int, trend_strength)

@jit(nopython=True)
def fast_polynomial_trend(close: np.ndarray, degree: int = 2) -> float:
    """Optimized polynomial trend calculation using Numba."""
    x = np.arange(len(close))
    coeffs = np.polyfit(x, close, degree)
    y_pred = np.polyval(coeffs, x)
    residuals = close - y_pred
    mse = np.sum(residuals**2) / (len(close) - degree - 1)
    return coeffs[0] / np.sqrt(mse)

class TrendAnalyzer:
    """Main class for trend analysis with caching and adaptive windows."""
    def __init__(self, cache_size: int = 1024):
        self.cache_size = cache_size
        self.adaptive_window = AdaptiveWindow()
        self._clear_cache()
    
    def _clear_cache(self):
        """Clear all internal caches."""
        cached_t_value_linear_trend.cache_clear()
    
    def analyze_trend(self, close: np.ndarray) -> Dict[str, TrendResult]:
        """Comprehensive trend analysis with multiple methods."""
        window = self.adaptive_window.get_window(close)
        close_tuple = tuple(close)  # For caching
        
        results = {
            'linear': cached_t_value_linear_trend(close_tuple),
            'poly': TrendResult(
                fast_polynomial_trend(close),
                (-np.inf, np.inf),  # Placeholder for now
                abs(fast_polynomial_trend(close)) / np.sqrt(len(close))
            )
        }
        
        # Add rolling analysis if enough data
        if len(close) >= window * 2:
            means, stds = fast_rolling_stats(close, window)
            z_stat, p_value = stats.ranksums(means[:window], means[-window:])
            conf_int = norm.interval(0.95, loc=z_stat, scale=np.sqrt(1/window))
            results['rolling'] = TrendResult(z_stat, conf_int, abs(z_stat))
        
        return results

def get_trend_labels(
    molecule: pd.DatetimeIndex,
    close: pd.Series,
    span: Tuple[int, int],
    confidence_level: float = 0.95
) -> pd.DataFrame:
    """Enhanced trend labeling with confidence intervals and adaptive windows."""
    analyzer = TrendAnalyzer()
    out = pd.DataFrame(index=molecule, 
                      columns=['t1', 'tVal', 'bin', 'confidence_lower', 
                              'confidence_upper', 'trend_strength'])
    
    def process_date(dt0):
        iloc0 = close.index.get_loc(dt0)
        if iloc0 + span[1] > len(close):
            return None
        
        results = {}
        for hrzn in range(*span):
            dt1 = close.index[iloc0 + hrzn - 1]
            subset = close[dt0:dt1].values
            
            if len(subset) < 2:
                continue
            
            trend_results = analyzer.analyze_trend(subset)
            
            # Combine results using weighted ensemble
            weights = {'linear': 0.4, 'poly': 0.3, 'rolling': 0.3}
            combined_t = sum(res.t_value * weights.get(method, 0) 
                           for method, res in trend_results.items())
            
            confidence_intervals = [res.confidence_interval 
                                 for res in trend_results.values() 
                                 if res.confidence_interval != (-np.inf, np.inf)]
            
            # Aggregate confidence intervals
            if confidence_intervals:
                conf_lower = np.mean([ci[0] for ci in confidence_intervals])
                conf_upper = np.mean([ci[1] for ci in confidence_intervals])
            else:
                conf_lower = conf_upper = np.nan
            
            trend_strength = np.mean([res.trend_strength 
                                    for res in trend_results.values()])
            
            results[dt1] = {
                't_value': combined_t,
                'conf_lower': conf_lower,
                'conf_upper': conf_upper,
                'trend_strength': trend_strength
            }
        
        if not results:
            return None
        
        max_dt = max(results.items(), key=lambda x: abs(x[1]['t_value']))[0]
        max_result = results[max_dt]
        
        return pd.Series({
            't1': max_dt,
            'tVal': max_result['t_value'],
            'bin': np.sign(max_result['t_value']),
            'confidence_lower': max_result['conf_lower'],
            'confidence_upper': max_result['conf_upper'],
            'trend_strength': max_result['trend_strength']
        })
    
    with ProcessPoolExecutor() as executor:
        results = list(executor.map(process_date, molecule))
    
    for dt0, result in zip(molecule, results):
        if result is not None:
            out.loc[dt0] = result
    
    out['t1'] = pd.to_datetime(out['t1'])
    out['bin'] = out['bin'].astype('int8')
    return out.dropna(subset=['bin'])

def plot_trend_labels(close: pd.Series, labels: pd.DataFrame, title: str):
    """Enhanced visualization with confidence intervals and trend strength."""
    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 12), sharex=True)
    
    # Price and trend direction
    scatter = ax1.scatter(labels.index, close.loc[labels.index],
                         c=labels['bin'], cmap='viridis',
                         s=labels['trend_strength'] * 50)  # Size by strength
    ax1.plot(close.index, close.values, color='gray', alpha=0.5)
    ax1.set_title(f'{title} - Price and Trend Direction')
    
    # T-values with confidence intervals
    ax2.fill_between(labels.index, 
                    labels['confidence_lower'],
                    labels['confidence_upper'],
                    alpha=0.2, color='gray')
    ax2.plot(labels.index, labels['tVal'], color='blue')
    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)
    ax2.set_title('T-Values with Confidence Intervals')
    
    # Trend strength
    ax3.plot(labels.index, labels['trend_strength'], color='red')
    ax3.fill_between(labels.index, 0, labels['trend_strength'], alpha=0.2)
    ax3.set_title('Trend Strength')
    
    plt.tight_layout()
    return fig, (ax1, ax2, ax3)

// ---------------------------------------------------

// distance-metrics.py
// shuangology/deepseek/distance-metrics.py
import numpy as np
import scipy.stats as ss
# from sklearn.metrics import mutual_info_score

def calculate_entropy(x, bins):
    hist, _ = np.histogram(x, bins=bins, density=True)
    hist = hist[hist > 0]  # Remove zero entries to avoid log(0)
    return -np.sum(hist * np.log(hist))

def calculate_mutual_info(x, y, bins, norm=False):
    cXY, _, _ = np.histogram2d(x, y, bins=bins, density=True)
    cXY = cXY[cXY > 0]  # Remove zero entries
    iXY = np.sum(cXY * np.log(cXY / np.outer(cXY.sum(axis=1), cXY.sum(axis=0)).flatten()))
    if norm:
        hX = calculate_entropy(x, bins)
        hY = calculate_entropy(y, bins)
        iXY /= min(hX, hY)  # Normalized mutual information
    return iXY

def calculate_joint_entropy(x, y, bins):
    cXY, _, _ = np.histogram2d(x, y, bins=bins, density=True)
    cXY = cXY[cXY > 0]  # Remove zero entries
    return -np.sum(cXY * np.log(cXY))

def calculate_conditional_entropy(x, y, bins):
    hXY = calculate_joint_entropy(x, y, bins)
    hY = calculate_entropy(y, bins)
    return hXY - hY

def calculate_variation_of_info(x, y, bins, norm=False):
    hX = calculate_entropy(x, bins)
    hY = calculate_entropy(y, bins)
    iXY = calculate_mutual_info(x, y, bins)
    vXY = hX + hY - 2 * iXY  # Variation of information
    if norm:
        hXY = calculate_joint_entropy(x, y, bins)
        vXY /= hXY  # Normalized variation of information
    return vXY

def num_bins(nObs, corr=None):
    if corr is None:  # Univariate case
        z = (8 + 324 * nObs + 12 * (36 * nObs + 729 * nObs**2)**0.5)**(1/3.)
        b = int(np.ceil(z / 6 + 2 / (3 * z) + 1 / 3))
    else:  # Bivariate case
        if np.isclose(1. - corr**2, 0):
            corr = np.sign(corr) * (np.abs(corr) - 1e-5)
        b = int(np.ceil(2**-0.5 * (1 + (1 + 24 * nObs / (1. - corr**2))**0.5)**0.5))
    return max(1, b)

def calculate_mutual_info_opt_bins(x, y, norm=False):
    bXY = num_bins(len(x), corr=np.corrcoef(x, y)[0, 1])
    return calculate_mutual_info(x, y, bXY, norm)

def calculate_variation_of_info_opt_bins(x, y, norm=False):
    bXY = num_bins(len(x), corr=np.corrcoef(x, y)[0, 1])
    return calculate_variation_of_info(x, y, bXY, norm)


// ---------------------------------------------------

// optimal-clustering.py
// shuangology/deepseek/optimal-clustering.py
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples
from scipy.linalg import block_diag
from sklearn.utils import check_random_state
import seaborn as sns

def cluster_kmeans_base(corr, max_num_clusters=10, n_init=10):
    """
    Perform base clustering using KMeans on a correlation matrix.
    
    Parameters:
    - corr: Correlation matrix (pd.DataFrame).
    - max_num_clusters: Maximum number of clusters to consider.
    - n_init: Number of initializations for KMeans.
    
    Returns:
    - corr1: Reordered correlation matrix.
    - clusters: Dictionary of clusters.
    - silh: Silhouette scores.
    """
    x = ((1 - corr.fillna(0)) / 2.) ** 0.5  # Observations matrix
    silh = pd.Series()
    
    for init in range(n_init):
        for i in range(2, max_num_clusters + 1):
            kmeans = KMeans(n_clusters=i, n_init=1, n_jobs=1)
            kmeans.fit(x)
            silh_ = silhouette_samples(x, kmeans.labels_)
            stat = (silh_.mean() / silh_.std(), silh.mean() / silh.std())
            if np.isnan(stat[1]) or stat[0] > stat[1]:
                silh, kmeans = silh_, kmeans
    
    new_idx = np.argsort(kmeans.labels_)
    corr1 = corr.iloc[new_idx, new_idx]  # Reorder rows and columns
    clusters = {i: corr.columns[np.where(kmeans.labels_ == i)[0]].tolist() for i in np.unique(kmeans.labels_)}
    silh = pd.Series(silh, index=x.index)
    
    return corr1, clusters, silh

def cluster_kmeans_top(corr, max_num_clusters=None, n_init=3):
    """
    Perform top-level clustering using KMeans on a correlation matrix.
    
    Parameters:
    - corr: Correlation matrix (pd.DataFrame).
    - max_num_clusters: Maximum number of clusters to consider.
    - n_init: Number of initializations for KMeans.
    
    Returns:
    - corr1: Reordered correlation matrix.
    - clusters: Dictionary of clusters.
    - silh: Silhouette scores.
    """
    if max_num_clusters is None:
        max_num_clusters = corr.shape[1] - 1
    
    corr1, clusters, silh = cluster_kmeans_base(corr, max_num_clusters=max_num_clusters, n_init=n_init)
    cluster_tstats = {i: np.mean(silh[clusters[i]]) / np.std(silh[clusters[i]]) for i in clusters.keys()}
    tstat_mean = np.mean(list(cluster_tstats.values()))
    
    redo_clusters = [i for i in cluster_tstats.keys() if cluster_tstats[i] < tstat_mean]
    if len(redo_clusters) <= 1:
        return corr1, clusters, silh
    
    keys_redo = [j for i in redo_clusters for j in clusters[i]]
    corr_tmp = corr.loc[keys_redo, keys_redo]
    corr2, clusters2, silh2 = cluster_kmeans_top(corr_tmp, max_num_clusters=max_num_clusters, n_init=n_init)
    
    corr_new, clusters_new, silh_new = make_new_outputs(corr, {i: clusters[i] for i in clusters.keys() if i not in redo_clusters}, clusters2)
    new_tstat_mean = np.mean([np.mean(silh_new[clusters_new[i]]) / np.std(silh_new[clusters_new[i]]) for i in clusters_new.keys()])
    
    if new_tstat_mean <= tstat_mean:
        return corr1, clusters, silh
    else:
        return corr_new, clusters_new, silh_new

def make_new_outputs(corr, clusters, clusters2):
    """
    Combine clusters and create new outputs.
    
    Parameters:
    - corr: Original correlation matrix.
    - clusters: Original clusters.
    - clusters2: New clusters.
    
    Returns:
    - corr_new: New reordered correlation matrix.
    - clusters_new: Combined clusters.
    - silh_new: New silhouette scores.
    """
    clusters_new = {}
    for i in clusters.keys():
        clusters_new[len(clusters_new.keys())] = list(clusters[i])
    for i in clusters2.keys():
        clusters_new[len(clusters_new.keys())] = list(clusters2[i])
    
    new_idx = [j for i in clusters_new for j in clusters_new[i]]
    corr_new = corr.loc[new_idx, new_idx]
    
    x = ((1 - corr.fillna(0)) / 2.) ** 0.5
    kmeans_labels = np.zeros(len(x.columns))
    for i in clusters_new.keys():
        idxs = [x.index.get_loc(k) for k in clusters_new[i]]
        kmeans_labels[idxs] = i
    
    silh_new = pd.Series(silhouette_samples(x, kmeans_labels), index=x.index)
    return corr_new, clusters_new, silh_new

def random_block_corr(n_cols, n_blocks, min_block_size=1, random_state=None):
    """
    Generate a random block correlation matrix.
    
    Parameters:
    - n_cols: Number of columns (assets).
    - n_blocks: Number of blocks.
    - min_block_size: Minimum size of each block.
    - random_state: Random seed.
    
    Returns:
    - corr: Random block correlation matrix.
    """
    rng = check_random_state(random_state)
    parts = rng.choice(range(1, n_cols - (min_block_size - 1) * n_blocks), n_blocks - 1, replace=False)
    parts.sort()
    parts = np.append(parts, n_cols - (min_block_size - 1) * n_blocks)
    parts = np.append(parts[0], np.diff(parts)) - 1 + min_block_size
    
    cov = None
    for n_cols_ in parts:
        cov_ = get_cov_sub(int(max(n_cols_ * (n_cols_ + 1) / 2., 100)), n_cols_, sigma=0.5, random_state=rng)
        if cov is None:
            cov = cov_.copy()
        else:
            cov = block_diag(cov, cov_)
    
    cov1 = get_cov_sub(n_cols, n_cols, sigma=1.0, random_state=rng)  # Add noise
    cov += cov1
    corr = cov2corr(cov)
    return pd.DataFrame(corr)

def get_cov_sub(n_obs, n_cols, sigma, random_state=None):
    """
    Generate a sub-correlation matrix.
    
    Parameters:
    - n_obs: Number of observations.
    - n_cols: Number of columns.
    - sigma: Standard deviation of noise.
    - random_state: Random seed.
    
    Returns:
    - cov: Sub-correlation matrix.
    """
    rng = check_random_state(random_state)
    ar0 = rng.normal(size=(n_obs, 1))
    ar0 = np.repeat(ar0, n_cols, axis=1)
    ar0 += rng.normal(scale=sigma, size=ar0.shape)
    return np.cov(ar0, rowvar=False)

def cov2corr(cov):
    """
    Convert covariance matrix to correlation matrix.
    
    Parameters:
    - cov: Covariance matrix.
    
    Returns:
    - corr: Correlation matrix.
    """
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1  # Numerical error correction
    return corr

// ---------------------------------------------------

// denoising-detoning.py
// shuangology/deepseek/denoising-detoning.py
import numpy as np
import pandas as pd
from sklearn.neighbors import KernelDensity
from scipy.optimize import minimize
from sklearn.covariance import LedoitWolf

def mpPDF(var, q, pts=1000):
    eMin, eMax = var * (1 - (1 / q) ** 0.5) ** 2, var * (1 + (1 / q) ** 0.5) ** 2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q / (2 * np.pi * var * eVal) * ((eMax - eVal) * (eVal - eMin)) ** 0.5
    return pd.Series(pdf, index=eVal)

def getPCA(matrix):
    eVal, eVec = np.linalg.eigh(matrix)
    indices = np.argsort(eVal)[::-1]
    eVal, eVec = eVal[indices], eVec[:, indices]
    return np.diag(eVal), eVec

def fitKDE(obs, bWidth=0.25, kernel='gaussian', x=None):
    obs = obs.reshape(-1, 1) if obs.ndim == 1 else obs
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    x = np.linspace(obs.min(), obs.max(), 1000).reshape(-1, 1) if x is None else x
    logProb = kde.score_samples(x)
    return pd.Series(np.exp(logProb), index=x.flatten())

def findMaxEval(eVal, q, bWidth):
    def errPDFs(var, eVal, q, bWidth):
        pdf_theo = mpPDF(var, q)
        pdf_emp = fitKDE(eVal, bWidth, x=pdf_theo.index)
        return np.sum((pdf_emp - pdf_theo) ** 2)
    result = minimize(errPDFs, 0.5, args=(eVal, q, bWidth), bounds=[(1e-5, 1)])
    var = result.x[0] if result.success else 1
    eMax = var * (1 + (1 / q) ** 0.5) ** 2
    return eMax, var

def denoisedCorr(eVal, eVec, nFacts):
    eVal = np.diag(eVal)
    eVal[nFacts:] = eVal[nFacts:].mean()
    corr1 = eVec @ np.diag(eVal) @ eVec.T
    return cov2corr(corr1)

def deNoiseCov(cov, q, bWidth):
    corr0 = cov2corr(cov)
    eVal, eVec = getPCA(corr0)
    eMax, _ = findMaxEval(np.diag(eVal), q, bWidth)
    nFacts = np.sum(np.diag(eVal) > eMax)
    corr1 = denoisedCorr(eVal, eVec, nFacts)
    return corr2cov(corr1, np.sqrt(np.diag(cov)))

def cov2corr(cov):
    std_dev = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std_dev, std_dev)
    np.clip(corr, -1, 1, out=corr)
    return corr

def corr2cov(corr, std):
    return np.outer(std, std) * corr

def optPort(cov, mu=None):
    inv = np.linalg.inv(cov)
    ones = np.ones((inv.shape[0], 1))
    mu = ones if mu is None else mu.reshape(-1, 1)
    w = inv @ mu
    return (w / (ones.T @ w)).flatten()


// ---------------------------------------------------

// optimal-clustering-3.py
// shuangology/deepseek/optimal-clustering-3.py
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples
from scipy.linalg import block_diag
from sklearn.utils import check_random_state

def cluster_kmeans_base(corr, max_num_clusters=10, n_init=10):
    x = ((1 - corr.fillna(0)) / 2.) ** 0.5  # Observations matrix
    silh = pd.Series(dtype=float)
    
    for init in range(n_init):
        for i in range(2, max_num_clusters + 1):
            kmeans = KMeans(n_clusters=i, n_init=1, n_jobs=-1)
            kmeans.fit(x)
            silh_ = silhouette_samples(x, kmeans.labels_)
            stat = (silh_.mean() / silh_.std(), silh.mean() / silh.std() if not silh.empty else 0)
            if np.isnan(stat[1]) or stat[0] > stat[1]:
                silh, kmeans = pd.Series(silh_, index=x.index), kmeans
    
    new_idx = np.argsort(kmeans.labels_)
    corr1 = corr.iloc[new_idx, new_idx]  # Reorder rows and columns
    clusters = {i: corr.columns[np.where(kmeans.labels_ == i)[0]].tolist() for i in np.unique(kmeans.labels_)}
    silh = pd.Series(silh, index=x.index)
    
    return corr1, clusters, silh

def cluster_kmeans_top(corr, max_num_clusters=None, n_init=3):
    if max_num_clusters is None:
        max_num_clusters = corr.shape[1] - 1
    
    corr1, clusters, silh = cluster_kmeans_base(corr, max_num_clusters=max_num_clusters, n_init=n_init)
    cluster_tstats = {i: np.mean(silh[clusters[i]]) / np.std(silh[clusters[i]]) for i in clusters.keys()}
    tstat_mean = np.mean(list(cluster_tstats.values()))
    
    redo_clusters = [i for i in cluster_tstats.keys() if cluster_tstats[i] < tstat_mean]
    if len(redo_clusters) <= 1:
        return corr1, clusters, silh
    
    keys_redo = [j for i in redo_clusters for j in clusters[i]]
    corr_tmp = corr.loc[keys_redo, keys_redo]
    corr2, clusters2, silh2 = cluster_kmeans_top(corr_tmp, max_num_clusters=max_num_clusters, n_init=n_init)
    
    corr_new, clusters_new, silh_new = make_new_outputs(corr, {i: clusters[i] for i in clusters.keys() if i not in redo_clusters}, clusters2)
    new_tstat_mean = np.mean([np.mean(silh_new[clusters_new[i]]) / np.std(silh_new[clusters_new[i]]) for i in clusters_new.keys()])
    
    if new_tstat_mean <= tstat_mean:
        return corr1, clusters, silh
    else:
        return corr_new, clusters_new, silh_new

def make_new_outputs(corr, clusters, clusters2):
    clusters_new = {i: clusters[i] for i in clusters.keys()}
    clusters_new.update({len(clusters_new): clusters2[i] for i in clusters2.keys()})
    
    new_idx = [j for i in clusters_new for j in clusters_new[i]]
    corr_new = corr.loc[new_idx, new_idx]
    
    x = ((1 - corr.fillna(0)) / 2.) ** 0.5
    kmeans_labels = np.zeros(len(x.columns), dtype=int)
    for i in clusters_new.keys():
        idxs = [x.index.get_loc(k) for k in clusters_new[i]]
        kmeans_labels[idxs] = i
    
    silh_new = pd.Series(silhouette_samples(x, kmeans_labels), index=x.index)
    return corr_new, clusters_new, silh_new

def random_block_corr(n_cols, n_blocks, min_block_size=1, random_state=None):
    rng = check_random_state(random_state)
    parts = rng.choice(range(1, n_cols - (min_block_size - 1) * n_blocks), n_blocks - 1, replace=False)
    parts.sort()
    parts = np.append(parts, n_cols - (min_block_size - 1) * n_blocks)
    parts = np.append(parts[0], np.diff(parts)) - 1 + min_block_size
    
    cov = None
    for n_cols_ in parts:
        cov_ = get_cov_sub(int(max(n_cols_ * (n_cols_ + 1) / 2., 100)), n_cols_, sigma=0.5, random_state=rng)
        cov = block_diag(cov, cov_) if cov is not None else cov_.copy()
    
    cov1 = get_cov_sub(n_cols, n_cols, sigma=1.0, random_state=rng)  # Add noise
    cov += cov1
    corr = cov2corr(cov)
    return pd.DataFrame(corr)

def get_cov_sub(n_obs, n_cols, sigma, random_state=None):
    rng = check_random_state(random_state)
    ar0 = rng.normal(size=(n_obs, 1))
    ar0 = np.repeat(ar0, n_cols, axis=1)
    ar0 += rng.normal(scale=sigma, size=ar0.shape)
    return np.cov(ar0, rowvar=False)

def cov2corr(cov):
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1  # Numerical error correction
    return corr


// ---------------------------------------------------

// optimal-clustering-2.py
// shuangology/deepseek/optimal-clustering-2.py
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples
from scipy.linalg import block_diag
from sklearn.utils import check_random_state
from typing import Dict, Tuple
from concurrent.futures import ThreadPoolExecutor

def cluster_kmeans_base(corr: pd.DataFrame, max_num_clusters: int = 10, n_init: int = 10) -> Tuple[pd.DataFrame, Dict, pd.Series]:
    """Optimized base clustering using KMeans."""
    x = np.sqrt((1 - corr.fillna(0)) / 2.)
    best_silh = pd.Series()
    best_kmeans = None
    best_stat = float('-inf')
    
    def run_kmeans(i: int) -> Tuple[pd.Series, KMeans, float]:
        kmeans = KMeans(n_clusters=i, n_init=1, random_state=np.random.randint(0, 1000))
        kmeans.fit(x)
        silh = silhouette_samples(x, kmeans.labels_)
        stat = silh.mean() / silh.std() if silh.std() != 0 else float('-inf')
        return silh, kmeans, stat
    
    with ThreadPoolExecutor() as executor:
        for init in range(n_init):
            futures = [executor.submit(run_kmeans, i) for i in range(2, max_num_clusters + 1)]
            for future in futures:
                silh, kmeans, stat = future.result()
                if stat > best_stat:
                    best_silh, best_kmeans, best_stat = silh, kmeans, stat
    
    new_idx = np.argsort(best_kmeans.labels_)
    corr1 = corr.iloc[new_idx, new_idx]
    clusters = {i: corr.columns[best_kmeans.labels_ == i].tolist() 
               for i in range(best_kmeans.n_clusters)}
    
    return corr1, clusters, pd.Series(best_silh, index=x.index)

def cluster_kmeans_top(corr: pd.DataFrame, max_num_clusters: int = None, n_init: int = 3) -> Tuple[pd.DataFrame, Dict, pd.Series]:
    """Optimized top-level clustering."""
    max_num_clusters = max_num_clusters or corr.shape[1] - 1
    
    corr1, clusters, silh = cluster_kmeans_base(corr, max_num_clusters, n_init)
    
    cluster_tstats = {i: silh[clusters[i]].mean() / silh[clusters[i]].std() 
                     if silh[clusters[i]].std() != 0 else float('-inf')
                     for i in clusters}
    
    tstat_mean = np.mean(list(cluster_tstats.values()))
    redo_clusters = [i for i, stat in cluster_tstats.items() if stat < tstat_mean]
    
    if len(redo_clusters) <= 1:
        return corr1, clusters, silh
        
    keys_redo = [col for i in redo_clusters for col in clusters[i]]
    corr_tmp = corr.loc[keys_redo, keys_redo]
    
    if corr_tmp.shape[0] <= 2:  # Handle edge case
        return corr1, clusters, silh
        
    corr2, clusters2, silh2 = cluster_kmeans_top(corr_tmp, max_num_clusters, n_init)
    
    # Combine results
    clusters_new = {i: clusters[i] for i in clusters if i not in redo_clusters}
    clusters_new.update({len(clusters_new) + i: cluster 
                        for i, cluster in clusters2.items()})
    
    new_idx = [col for cluster in clusters_new.values() for col in cluster]
    corr_new = corr.loc[new_idx, new_idx]
    
    x = np.sqrt((1 - corr.fillna(0)) / 2.)
    kmeans_labels = np.zeros(len(x.columns))
    for i, cluster in clusters_new.items():
        idx = [x.index.get_loc(k) for k in cluster]
        kmeans_labels[idx] = i
    
    silh_new = pd.Series(silhouette_samples(x, kmeans_labels), index=x.index)
    
    new_tstat = np.mean([silh_new[cluster].mean() / silh_new[cluster].std() 
                        if silh_new[cluster].std() != 0 else float('-inf')
                        for cluster in clusters_new.values()])
                        
    return (corr_new, clusters_new, silh_new) if new_tstat > tstat_mean else (corr1, clusters, silh)

def random_block_corr(n_cols: int, n_blocks: int, min_block_size: int = 1, 
                     random_state: int = None) -> pd.DataFrame:
    """Optimized random block correlation matrix generation."""
    rng = check_random_state(random_state)
    max_val = n_cols - (min_block_size - 1) * n_blocks
    
    if max_val <= 1:
        raise ValueError("Invalid parameters: resulting matrix would be too small")
        
    parts = np.sort(rng.choice(range(1, max_val), n_blocks - 1, replace=False))
    parts = np.diff(np.concatenate(([0], parts, [max_val]))) + min_block_size
    
    covs = [get_cov_sub(int(max(n * (n + 1) / 2, 100)), n, 0.5, rng) 
            for n in parts]
    
    cov = block_diag(*covs)
    cov += get_cov_sub(n_cols, n_cols, 1.0, rng)  # Add noise
    
    return pd.DataFrame(cov2corr(cov))

def get_cov_sub(n_obs: int, n_cols: int, sigma: float, 
                random_state: np.random.RandomState = None) -> np.ndarray:
    """Optimized sub-correlation matrix generation."""
    rng = random_state or np.random.RandomState()
    ar0 = rng.normal(size=(n_obs, 1))
    ar0 = np.tile(ar0, (1, n_cols))
    ar0 += rng.normal(scale=sigma, size=ar0.shape)
    return np.cov(ar0, rowvar=False)

def cov2corr(cov: np.ndarray) -> np.ndarray:
    """Optimized covariance to correlation conversion."""
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    np.clip(corr, -1, 1, out=corr)
    return corr

// ---------------------------------------------------

// __init__.py
// machine-learning-for-asset-managers-emoen/__init__.py
from .Machine_Learning_for_Asset_Managers import *

// ---------------------------------------------------

// setup.py
// machine-learning-for-asset-managers-emoen/setup.py
from setuptools import find_packages, setup

setup(
    name='Machine_Learning_for_Asset_Managers',
    packages=find_packages(),
    version='1.0.0',
    description='Implementation of code snippets found in Machine-Learning-for-Asset-Managers by Marcos M. Lòpez de Prado',
    author='Endre Moen',
    license='MIT',
)

// ---------------------------------------------------

// utility_calculations.py
// machine-learning-for-asset-managers-emoen/utility_calculations.py
# -*- coding: utf-8 -*-
import yfinance as yf
import numpy as np
import pandas as pd
# https://pypi.org/project/onc/

def calculate_correlation(S, T=936, N=234):
    M = np.sum(S, axis=1) / T
    de_meaned_S = S - M[:, None]
    covariance = (np.dot(de_meaned_S, de_meaned_S.T)) / (N - 1)
    corr = correlation_from_covariance(covariance)
    eigenvalue, eigenvector = np.linalg.eig(np.corrcoef(S))
    eigenvalue = abs(eigenvalue)
    condition_num = max(eigenvalue) - min(eigenvalue)

def correlation_from_covariance(covariance):
    v = np.sqrt(np.diag(covariance))
    outer_v = np.outer(v, v)
    correlation = covariance / outer_v
    correlation[covariance == 0] = 0
    return correlation

if __name__ == '__main__':
    N = 234 
    T = 936
    stockPrice = np.loadtxt('csv/ol184.csv', delimiter=',')
    ol = pd.read_csv('csv/ol_ticker.csv', sep='\t', header=None)
    portfolio_name = ol[0]
    stockPrice = stockPrice[:, 1:184]
    returns = pd.DataFrame(stockPrice).pct_change().dropna(how="all")
    calculate_correlation(returns.to_numpy(), T=936, N=234)

// ---------------------------------------------------

// covariance_matrix_applied.py
// machine-learning-for-asset-managers-emoen/covariance_matrix_applied.py
import yfinance as yf
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import os
import math
import matplotlib.pylab as plt
import matplotlib

from Machine_Learning_for_Asset_Managers import ch2_fitKDE_find_best_bandwidth as best_bandwidth
from Machine_Learning_for_Asset_Managers import ch2_marcenko_pastur_pdf as mp
from Machine_Learning_for_Asset_Managers import ch2_monte_carlo_experiment as mc
from Machine_Learning_for_Asset_Managers import ch4_optimal_clustering as oc
import onc as onc
from Machine_Learning_for_Asset_Managers import ch5_financial_labels as fl
from Machine_Learning_for_Asset_Managers import ch7_portfolio_construction as pc

def get_OL_tickers_close(T=936, N=234):       
    ol = pd.read_csv('csv/ol_ticker.csv', sep='\t', header=None)
    ticker_names = ol[0]
    closePrice = np.empty([T, N])
    covariance_matrix = np.empty([T, N])
    portfolio_name = [ [ None ] for x in range( N ) ]
    ticker_adder = 0
    for i in range(0, len(ticker_names)):
        ticker = ticker_names[i]
        print(ticker)
        ol_ticker = ticker + '.ol'
        df = yf.Ticker(ol_ticker)
        try:
            ticker_df = df.history(period="7y")
            if ticker_df.shape[0] > T and ticker!='EMAS' and ticker != 'AVM':
                closePrice[:,ticker_adder] = ticker_df['Close'][-T:].values
                portfolio_name[ticker_adder] = ol_ticker
                ticker_adder += 1
            else:
                print("no data for ticker:" + ol_ticker)
        except ValueError:
            print("no history:"+ol_ticker)
    return closePrice, portfolio_name
    
def denoise_OL(S, do_plot=True):
    np.argwhere( np.isnan(S) )
    cor = np.corrcoef(S, rowvar=0) 
    eVal0 , eVec0 = mp.getPCA( cor ) 
    print(np.argwhere(np.isnan(np.diag(eVal0))))
    T = float(S.shape[0])
    N = S.shape[1]
    q = float(S.shape[0])/S.shape[1]
    eMax0, var0 = mp.findMaxEval(np.diag(eVal0), q, bWidth=.01)
    nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)
    if do_plot:
        fig = plt.figure()
        ax  = fig.add_subplot(111)
        ax.hist(np.diag(eVal0), density = True, bins=100)
        pdf0 = mp.mpPDF(var0, q=S.shape[0]/float(S.shape[1]), pts=N)
        plt.plot(pdf0.keys(), pdf0, color='r')
        plt.show()
    corr1 = mp.denoisedCorr(eVal0, eVec0, nFacts0)
    eVal1, eVec1 = mp.getPCA(corr1)
    return eVal0, eVec0, eVal1, eVec1, corr1, var0

def calculate_returns( S, percentageAsProduct=False ):
    ret = np.zeros((S.shape[0]-1, S.shape[1]))
    cum_sums = np.zeros(S.shape[1])
    for j in range(0, S.shape[1]):
        cum_return = 0
        S_ret = np.zeros(S.shape[0]-1)
        for i in range(0,S.shape[0]-1):
            if percentageAsProduct==True:
                S_ret[i] = 1+((S[i+1,j]-S[i,j])/S[i,j])
            else:
                S_ret[i] = ((S[i+1,j]-S[i,j])/S[i,j])
        cum_return = np.prod(S_ret)-1    
        cum_sums[j] = cum_return
        ret[:, j] = S_ret
    return ret, cum_sums

def getVolatility(S):
    return [np.std(S[:,i]) for i in range(0, S.shape[1])]
    
def test_exception_in_plotting_efficient_frontier(S_value):
    mvo = MeanVarianceOptimisation()
    pdPrice = pd.DataFrame(S_value)
    pdPrice.index = pd.RangeIndex(start=0, stop=6, step=1)
    dates = ['2019-01-01','2019-02-01','2019-03-01','2019-04-01','2019-05-01','2019-06-01']
    pdPrice['Datetime'] = pd.to_datetime(dates)
    pdPrice.set_index('Datetime')
    expected_returns = ReturnsEstimators().calculate_mean_historical_returns(asset_prices=pdPrice, resample_by=None)
    covariance = ReturnsEstimators().calculate_returns(asset_prices=pdPrice, resample_by=None).cov()
    plot = mvo.plot_efficient_frontier(covariance=covariance, max_return=1.0, expected_asset_returns=expected_returns)
    assert len(plot._A) == 41
    plot.savefig('books_read.png')
    print("read books")
        
def testNCO():
    N = 5
    T = 5
    S_value = np.array([[1., 2,3,  4,5],
                        [1.1,3,2,  3,5],
                        [1.2,4,1.3,4,5],
                        [1.3,5,1,  3,5],
                        [1.4,6,1,  4,5.5],
                        [1.5,7,1,  3,5.5]])
    S_value[:,1] =1
    S_value[5,1] =1.1
    S, _ = calculate_returns(S_value)
    _, instrument_returns = calculate_returns(S_value, percentageAsProduct=True)
    np.testing.assert_almost_equal(S, pd.DataFrame(S_value).pct_change().dropna(how="all"))
    mu1 = None
    cov1_d = np.cov(S ,rowvar=0, ddof=1)
    corr1 = mp.cov2corr(cov1_d)
    a,b,c = nco.NCO()._cluster_kmeans_base(pd.DataFrame(corr1))
    d,e,f = oc.clusterKMeansBase(pd.DataFrame(corr1))
    min_var_markowitz = mc.optPort(cov1_d, mu1).flatten()
    min_var_NCO = pc.optPort_nco(cov1_d, mu1, max(int(cov1_d.shape[0]/2), 2)).flatten()  
    mlfinlab_NCO= nco.NCO().allocate_nco(cov1_d, mu1, max(int(cov1_d.shape[0]/2), 2)).flatten()
    cov1_d = np.cov(S,rowvar=0, ddof=1)    
    mlfinlab_NCO= nco.NCO().allocate_nco(cov1_d, mu1, int(cov1_d.shape[0]/2)).flatten()
    expected_return_markowitz = [min_var_markowitz[i]*instrument_returns[i] for i in range(0,cov1_d.shape[0])]
    e_m = sum(expected_return_markowitz)
    expected_return_NCO = [min_var_NCO[i]*instrument_returns[i] for i in range(0,cov1_d.shape[0])]
    e_NCO = sum(expected_return_markowitz)
    vol = getVolatility(S_value)
    m_minVol = [min_var_markowitz[i]*vol[i] for i in range(0, cov1_d.shape[0])] 
    NCO_minVol = [mlfinlab_NCO[i]*vol[i] for i in range(0, cov1_d.shape[0])]   
    
if __name__ == '__main__':
    testNCO()
    N = 333
    T = 936
    S_value = np.loadtxt('csv/ol184.csv', delimiter=',')
    if S_value.shape[0] < 1 or not os.path.exists('csv/portfolio_name.csv'):
        S_value, portfolio_name = get_OL_tickers_close(T, N)
        np.savetxt('csv/ol184.csv', S_value, delimiter=',')
        np.savetxt('csv/portfolio_name.csv', portfolio_name, delimiter=',', fmt='%s')
    portfolio_name = pd.read_csv('csv/portfolio_name.csv', sep='\t', header=None).values
    lastIndex = 173
    S_value = S_value[:,0:lastIndex]
    portfolio_name = portfolio_name[0:lastIndex]
    S, instrument_returns = calculate_returns(S_value)
    _, instrument_returns = calculate_returns(S_value, percentageAsProduct=True)
    print(np.asarray(portfolio_name)[np.argsort(instrument_returns)])
    eVal0, eVec0, denoised_eVal, denoised_eVec, denoised_corr, var0 = denoise_OL(S)
    detoned_corr = mp.detoned_corr(denoised_corr, denoised_eVal, denoised_eVec, market_component=1)
    detoned_eVal, detoned_eVec = mp.getPCA(detoned_corr)
    denoised_eigenvalue = np.diag(denoised_eVal)
    eigenvalue_prior = np.diag(eVal0)
    plt.plot(range(0, len(denoised_eigenvalue)), np.log(denoised_eigenvalue), color='r', label="Denoised eigen-function")
    plt.plot(range(0, len(eigenvalue_prior)), np.log(eigenvalue_prior), color='g', label="Original eigen-function")
    plt.xlabel("Eigenvalue number")
    plt.ylabel("Eigenvalue (log-scale)")
    plt.legend(loc="upper right")
    plt.show()
    detoned_cov = mc.corr2cov(detoned_corr, var0)
    w = mc.optPort(detoned_cov)
    print(w)
    minVarPortfolio_var = np.dot(np.dot(w.T, detoned_corr), w)
    nCols, minBlockSize = 183, 2
    print("minBlockSize"+str(minBlockSize))
    corr0 = detoned_corr
    corr1, clstrs, silh = oc.clusterKMeansTop(pd.DataFrame(detoned_corr))
    tStatMeanDepth = np.mean([np.mean(silh[clstrs[i]]) / np.std(silh[clstrs[i]]) for i in clstrs.keys()])
    print("tstat at depth:")
    print(tStatMeanDepth)
    corr1, clstrs, silh = oc.clusterKMeansTop(pd.DataFrame(detoned_corr))
    tStatMeanDepth = np.mean([np.mean(silh[clstrs[i]]) / np.std(silh[clstrs[i]]) for i in clstrs.keys()])
    print("tstat at depth:")
    print(tStatMeanDepth)
    # raise SystemExit

    # ------------------------------------------------------------
    # ------------------------------------------------------------
    
    #corr11, clstrs11, silh11 = onc.get_onc_clusters(pd.DataFrame(detoned_corr)) #test with mlfinlab impl: 1: [18, 24, 57, 81, 86, 99, 112, 120, 134, 165]
    
    matplotlib.pyplot.matshow(corr11) #invert y-axis to get origo at lower left corner
    matplotlib.pyplot.gca().xaxis.tick_bottom()
    matplotlib.pyplot.gca().invert_yaxis()
    matplotlib.pyplot.colorbar()
    matplotlib.pyplot.show()
    
    #Chapter 5 Financial labels
    #Lets try trend-following on PHO
    idxPHO =118
    idxBGBIO = 29
    idxWWI = 169
    pho = S_value[:,idxBGBIO]
    df0 = pd.Series(pho[-50:])
    df1 = fl.getBinsFromTrend(df0.index, df0, [3, 10, 1])  # [3,10,1] = range(3,10)
    tValues = df1['tVal'].values

    lastTValue = []
    for i in range(0, lastIndex):
        pho = S_value[:, i]
        df0 = pd.Series(pho[-50:])
        df1 = fl.getBinsFromTrend(df0.index, df0, [3,10,1]) #[3,10,1] = range(3,10)
        tValues = df1['tVal'].values
        lastTValue.append(tValues[41])

    np.argmax(lastTValue)

    plt.scatter(df1.index, df0.loc[df1.index].values, c=tValues, cmap='viridis') #df1['tVal'].values, cmap='viridis')
    plt.colorbar()
    plt.show()

    bgbio_df = yf.Ticker("BGBIO.ol")
    bg_bio_ticker_df = bgbio_df.history(period="7y")
                
    bgbio = bg_bio_ticker_df['Close']
    df0 = pd.Series(bgbio[-200:])
    df1 = fl.getBinsFromTrend(df0.index, df0, [3,20,1]) #[3,10,1] = range(3,10)
    tValues = df1['tVal'].values
    plt.scatter(df1.index, df0.loc[df1.index].values, c=tValues, cmap='viridis') #df1['tVal'].values, cmap='viridis')
    plt.colorbar()
    plt.show()
    
    S, pnames = get_OL_tickers_close()

    #get t-statistics from all instruments on OL
    S, pnames = get_OL_tickers_close(T=200,N=237)
    
    np.argwhere(np.isnan(S))
    S[182, 110] = S[181,110]

    #implementing from book
    abc = [None for i in range(0,237)]
    for i in range(0, 20):#len(pnames)):
        instrument = S[:,i]
        df0 = pd.Series(instrument)
        print("running bins on:"+pnames[i]+" i:"+str(i))
        abc[i] = fl.getBinsFromTrend(df0.index, df0, [3,10,1])['tVal']
    
    tValLatest =  [abc[i].values[-20] for i in range(0, len(abc))]
    #most significant t-value:
    np.max(tValLatest)
    pnames[np.argmax(tValLatest)]
    #END / implementing from book
    
    #mlfinlab impl
    S[181,110]=S[180,110] #nan
    abc = [None for i in range(0,237)]
    for i in range(0, len(abc)):
        ticker_close = pd.DataFrame(S[:,i], columns={'ticker'})
        print(i)
        t_events = ticker_close.index
        tr_scan_labels = ts.trend_scanning_labels(ticker_close, t_events, 20)
        abc[i] = tr_scan_labels['t_value']
    
    abc = np.asarray(abc)
    tValLatest =  [abc[i,-20] for i in range(0, len(abc))]
    #most significant t-value:
    np.max(tValLatest)
    pnames[np.argmax(tValLatest)]
    
    plt.scatter(ticker_close.index, S[:,78], c=abc[78], cmap='viridis')

    # Chapter 7 - apply the Nested Clustered Optimization (NCO) algorithm
    N = 234 
    T = 936
    S_value = np.loadtxt('csv/ol184.csv', delimiter=',')
    S, instrument_returns = calculate_returns(S_value)
    _, instrument_returns = calculate_returns(S_value, percentageAsProduct=True)
    np.argsort(instrument_returns)
    #26,  84, 167,  35,  76, 169,  31, 137,  28,  64,  36,  37,  92, 116], dtype=int64)
    
    eVal0, eVec0, denoised_eVal, denoised_eVec, denoised_corr, var0 = denoise_OL(S)
    q = float(S.shape[0])/float(S.shape[1])#T/N
    bWidth = best_bandwidth.findOptimalBWidth(np.diag(eVal0))
    cov1_d = mc.deNoiseCov(np.cov(S,rowvar=0, ddof=1), q, bWidth['bandwidth'])
    
    mu1 = None
    min_var_markowitz = mc.optPort(cov1_d, mu1).flatten()
    min_var_NCO = pc.optPort_nco(cov1_d, mu1, int(cov1_d.shape[0]/2)).flatten()
    
    # calulate on time-series not returns
    cov1_d = np.cov(S_value,rowvar=0, ddof=1)   
    min_var_markowitz = mc.optPort(cov1_d, mu1).flatten()
    min_var_NCO = pc.optPort_nco(cov1_d, mu1, int(cov1_d.shape[0]/2)).flatten()
    #note pnames = pnames[1:] - first element is obx

    ########
    T, N = 237, 235
    #x = np.random.normal(0, 1, size = (T, N))
    S, pnames = get_OL_tickers_close(T, N)
    np.argwhere(np.isnan(S))
    S[204, 109]=S[203, 109]

    cov0 = np.cov(S, rowvar=0, ddof=1)
    q = float(S.shape[0])/float(S.shape[1])#T/N
    #eMax0, var0 = mp.findMaxEval(np.diag(eVal0), q, bWidth=.01)

    corr0 = mp.cov2corr(cov0)
    eVal0, eVec0 = mp.getPCA(corr0)
    bWidth = best_bandwidth.findOptimalBWidth(np.diag(eVal0))
    
    min_var_markowitz = mc.optPort(cov1_d, mu1).flatten()
    min_var_NCO = pc.optPort_nco(cov1_d, mu1, int(cov1_d.shape[0]/2)).flatten()
    
    
    ##################
    # Test if bWidth found makes sense
    pdf0 = mp.mpPDF(1., q=T/float(N), pts=N)
    pdf1 = mp.fitKDE(np.diag(eVal0), bWidth=bWidth['bandwidth']) #empirical pdf
    #pdf1 = mp.fitKDE(np.diag(eVal0), bWidth=0.1)

    fig = plt.figure()
    ax  = fig.add_subplot(111)
    ax.hist(np.diag(eVal0), density = True, bins=50) # Histogram the eigenvalues
    plt.plot(pdf0.keys(), pdf0, color='r', label="Marcenko-Pastur pdf")
    plt.plot(pdf1.keys(), pdf1, color='g', label="Empirical:KDE")
    plt.legend(loc="upper right")
    plt.show()
        
    N = 1000
    T = 10000
    x = np.random.normal(0, 1, size = (T, N))
    cor = np.corrcoef(x, rowvar=0)
    eVal0, eVec0 = mp.getPCA(cor)
    bWidth = best_bandwidth.findOptimalBWidth(np.diag(eVal0))
    #{'bandwidth': 4.328761281083057}
    ###############
    
    
    bWidth=0.1
    cov1_d = mc.deNoiseCov(cov0, q, bWidth)
    mu1 = None

    min_var_markowitz = mc.optPort(cov1_d, mu1).flatten()
    min_var_NCO = pc.optPort_nco(cov1_d, mu1, int(cov1_d.shape[0]/2)).flatten()
    


// ---------------------------------------------------

// ch3_metrics.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch3_metrics.py
# -*- coding: utf-8 -*-
import numpy as np
import scipy.stats as ss
from sklearn.metrics import mutual_info_score

def numBins(nObs, corr=None):
    if corr is None:
        z = (8 + 324 * nObs + 12 * (36 * nObs + 729 * nObs ** 2) ** .5) ** (1 / 3.)
        b = round(z / 6. + 2. / (3 * z) + 1. / 3)
    else:
        b = round(2 ** -.5 * (1 + (1 + 24 * nObs / (1. - corr ** 2)) ** .5) ** .5)
    
    return int(b)

def varInfo(x, y, bins, norm=False):
    bXY = numBins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    bins = bXY
    cXY = np.histogram2d(x, y, bins)[0]
    hX = ss.entropy(np.histogram(x, bins)[0])
    hY = ss.entropy(np.histogram(y, bins)[0])
    iXY = mutual_info_score(None, None, contingency=cXY)
    vXY = hX + hY - 2 * iXY
    if norm:
        hXY = hX + hY - iXY
        vXY = vXY / hXY
        
    return vXY

def mutualInfor(x, y, norm=False):
    bXY = numBins(x.shape[0], corr=np.corrcoef(x, y)[0, 1])
    cXY = np.histogram2d(x, y, bXY)[0]
    iXY = mutual_info_score(None, None, contingency=cXY)
    if norm:
        hX = ss.entropy(np.histogram(x, bXY)[0])
        hY = ss.entropy(np.histogram(y, bXY)[0])
        iXY /= min(hX, hY)

    return iXY

if __name__ == '__main__':
    x = np.random.normal(0, 1, 1000)
    y = np.random.normal(0, 1, 1000)
    bins = 10

    cXY = np.histogram2d(x, y, bins)[0]
    hX = ss.entropy(np.histogram(x, bins)[0])
    hY = ss.entropy(np.histogram(y, bins)[0])
    iXY = mutual_info_score(None, None, contingency=cXY)
    iXYn = iXY / min(hX, hY)
    hXY = hX + hY - iXY
    hX_Y = hXY - hY
    hY_X = hXY - hX
    
    size, seed = 5000, 0
    np.random.seed(seed)
    x = np.random.normal(size=size)
    e = np.random.normal(size=size)
    y = 0 * x + e
    nmi = mutualInfor(x, y, True)
    corr = np.corrcoef(x, y)[0, 1]

// ---------------------------------------------------

// ch7_portfolio_construction.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch7_portfolio_construction.py
# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from scipy.linalg import block_diag
import matplotlib.pylab as plt
import seaborn as sns

from Machine_Learning_for_Asset_Managers import ch2_monte_carlo_experiment as mc
from Machine_Learning_for_Asset_Managers import ch2_marcenko_pastur_pdf as mp
from Machine_Learning_for_Asset_Managers import ch4_optimal_clustering as oc

def minVarPort(cov):
    return mc.optPort(cov, mu=None)

def optPort_nco(cov, mu=None, maxNumClusters=None):
    cov = pd.DataFrame(cov)
    if mu is not None:
        mu = pd.Series(mu[:, 0])
    
    corr1 = mp.cov2corr(cov)
    corr1, clstrs, _ = oc.clusterKMeansBase(corr1, maxNumClusters, n_init=10)
    w_intra_clusters = pd.DataFrame(0, index=cov.index, columns=clstrs.keys())
    for i in clstrs:
        cov_cluster = cov.loc[clstrs[i], clstrs[i]].values
        mu_cluster = mu.loc[clstrs[i]].values.reshape(-1, 1) if mu is not None else None
        w_intra_clusters.loc[clstrs[i], i] = allocate_cvo(cov_cluster, mu_cluster).flatten()

    cov_inter_cluster = w_intra_clusters.T.dot(np.dot(cov, w_intra_clusters))
    mu_inter_cluster = (None if mu is None else w_intra_clusters.T.dot(mu))
    w_inter_clusters = pd.Series(allocate_cvo(cov_inter_cluster, mu_inter_cluster).flatten(), index=cov_inter_cluster.index)
    nco = w_intra_clusters.mul(w_inter_clusters, axis=1).sum(axis=1).values.reshape(-1, 1)
    return nco

def allocate_cvo(cov, mu_vec=None):
    inv_cov = np.linalg.inv(cov)
    ones = np.ones(shape=(inv_cov.shape[0], 1))
    
    if mu_vec is None:
        mu_vec = ones
    
    w_cvo = np.dot(inv_cov, mu_vec)
    w_cvo /= np.dot(mu_vec.T, w_cvo)
    
    return w_cvo

if __name__ == '__main__':
    corr0 = mc.formBlockMatrix(2, 2, .5)
    eVal, eVec = np.linalg.eigh(corr0)
    matrix_condition_number = max(eVal) / min(eVal)
    print(matrix_condition_number)

    fig, ax = plt.subplots(figsize=(13, 10))
    sns.heatmap(corr0, cmap='viridis')
    plt.show()

    corr0 = block_diag(mc.formBlockMatrix(1, 2, .5))
    corr1 = mc.formBlockMatrix(1, 2, .0)
    corr0 = block_diag(corr0, corr1)
    eVal, eVec = np.linalg.eigh(corr0)
    matrix_condition_number = max(eVal) / min(eVal)
    print(matrix_condition_number)

    fig, ax = plt.subplots(figsize=(13, 10))
    sns.heatmap(corr1, cmap='viridis')
    plt.show()

    nBlocks, bSize, bCorr = 2, 2, .5
    q = 10.0
    np.random.seed(0)
    mu0, cov0 = mc.formTrueMatrix(nBlocks, bSize, bCorr)
    cols = cov0.columns
    cov1 = mc.deNoiseCov(cov0, q, bWidth=.01)
    cov1 = pd.DataFrame(cov1, index=cols, columns=cols)
    corr1 = mp.cov2corr(cov1)
    corr1, clstrs, silh = oc.clusterKMeansBase(pd.DataFrame(corr0))

    wIntra = pd.DataFrame(0, index=cov0.index, columns=clstrs.keys())
    for i in clstrs:
        wIntra.loc[clstrs[i], i] = minVarPort(cov1.loc[clstrs[i], clstrs[i]]).flatten()
        
    cov2 = wIntra.T.dot(np.dot(cov1, wIntra))

    wInter = pd.Series(minVarPort(cov2).flatten(), index=cov2.index)
    wAll0 = wIntra.mul(wInter, axis=1).sum(axis=1).sort_index()

    nBlocks, bSize, bCorr = 10, 50, .5
    np.random.seed(0)
    mu0, cov0 = mc.formTrueMatrix(nBlocks, bSize, bCorr)
       
    nObs, nSims, shrink, minVarPortf = 1000, 1000, False, True
    np.random.seed(0)
    w1 = pd.DataFrame(0, index=range(0, nSims), columns=range(0, nBlocks * bSize))
    w1_d = pd.DataFrame(0, index=range(0, nSims), columns=range(0, nBlocks * bSize))
    for i in range(0, nSims):
        mu1, cov1 = mc.simCovMu(mu0, cov0, nObs, shrink=shrink)
        if minVarPortf:
            mu1 = None
        w1.loc[i] = mc.optPort(cov1, mu1).flatten()
        w1_d.loc[i] = optPort_nco(cov1, mu1, int(cov1.shape[0] / 2)).flatten()

    w0 = mc.optPort(cov0, None if minVarPortf else mu0)
    w0 = np.repeat(w0.T, w1.shape[0], axis=0)
    rmsd = np.mean((w1 - w0).values.flatten() ** 2) ** .5
    rmsd_d = np.mean((w1_d - w0).values.flatten() ** 2) ** .5

// ---------------------------------------------------

// __init__.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/__init__.py


// ---------------------------------------------------

// ch2_marcenko_pastur_pdf.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch2_marcenko_pastur_pdf.py
# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from sklearn.neighbors import KernelDensity
import matplotlib.pylab as plt
from scipy.optimize import minimize
from scipy.linalg import block_diag
from sklearn.covariance import LedoitWolf

def mpPDF(var, q, pts):
    eMin, eMax = var * (1 - (1. / q) ** .5) ** 2, var * (1 + (1. / q) ** .5) ** 2
    eVal = np.linspace(eMin, eMax, pts)
    pdf = q / (2 * np.pi * var * eVal) * ((eMax - eVal) * (eVal - eMin)) ** .5
    pdf = pd.Series(pdf, index=eVal)
    return pdf

def getPCA(matrix):
    eVal, eVec = np.linalg.eig(matrix)
    indices = eVal.argsort()[::-1]
    eVal, eVec = eVal[indices], eVec[:, indices]
    eVal = np.diagflat(eVal)
    return eVal, eVec

def fitKDE(obs, bWidth=.15, kernel='gaussian', x=None):
    if len(obs.shape) == 1: obs = obs.reshape(-1, 1)
    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)
    if x is None: x = np.unique(obs).reshape(-1, 1)
    if len(x.shape) == 1: x = x.reshape(-1, 1)
    logProb = kde.score_samples(x)
    pdf = pd.Series(np.exp(logProb), index=x.flatten())
    return pdf

def getRndCov(nCols, nFacts):
    w = np.random.normal(size=(nCols, nFacts))
    cov = np.dot(w, w.T)
    cov += np.diag(np.random.uniform(size=nCols))
    return cov

def cov2corr(cov):
    std = np.sqrt(np.diag(cov))
    corr = cov / np.outer(std, std)
    corr[corr < -1], corr[corr > 1] = -1, 1
    return corr

def corr2cov(corr, std):
    cov = corr * np.outer(std, std)
    return cov

def errPDFs(var, eVal, q, bWidth, pts=1000):
    var = var[0]
    pdf0 = mpPDF(var, q, pts)
    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)
    sse = np.sum((pdf1 - pdf0) ** 2)
    return sse

def findMaxEval(eVal, q, bWidth):
    out = minimize(lambda *x: errPDFs(*x), x0=np.array(0.5), args=(eVal, q, bWidth), bounds=((1E-5, 1 - 1E-5),))
    if out['success']: var = out['x'][0]
    else: var = 1
    eMax = var * (1 + (1. / q) ** .5) ** 2
    return eMax, var

def denoisedCorr(eVal, eVec, nFacts):
    eVal_ = np.diag(eVal).copy()
    eVal_[nFacts:] = eVal_[nFacts:].sum() / float(eVal_.shape[0] - nFacts)
    eVal_ = np.diag(eVal_)
    corr1 = np.dot(eVec, eVal_).dot(eVec.T)
    corr1 = cov2corr(corr1)
    return corr1

def detoned_corr(corr, eigenvalues, eigenvectors, market_component=1):
    eigenvalues_mark = eigenvalues[:market_component, :market_component]
    eigenvectors_mark = eigenvectors[:, :market_component]
    corr_mark = np.dot(eigenvectors_mark, eigenvalues_mark).dot(eigenvectors_mark.T)
    corr = corr - corr_mark
    corr = cov2corr(corr)
    return corr

def test_detone():
    cov_matrix = np.array([[0.01, 0.002, -0.001],
                           [0.002, 0.04, -0.006],
                           [-0.001, -0.006, 0.01]])
    cor_test = np.corrcoef(cov_matrix, rowvar=0)
    eVal_test, eVec_test = getPCA(cor_test)
    eMax_test, var_test = findMaxEval(np.diag(eVal_test), q, bWidth=.01)
    nFacts_test = eVal_test.shape[0] - np.diag(eVal_test)[::-1].searchsorted(eMax_test)
    corr1_test = denoisedCorr(eVal_test, eVec_test, nFacts_test)
    eVal_denoised_test, eVec_denoised_test = getPCA(corr1_test)
    corr_detoned_denoised_test = detoned_corr(corr1_test, eVal_denoised_test, eVec_denoised_test)
    eVal_detoned_denoised_test, _ = getPCA(corr_detoned_denoised_test)

if __name__ == '__main__':
    N = 1000
    T = 10000
    x = np.random.normal(0, 1, size=(T, N))
    cor = np.corrcoef(x, rowvar=0)
    eVal0, eVec0 = getPCA(cor)
    pdf0 = mpPDF(1., q=x.shape[0] / float(x.shape[1]), pts=N)
    pdf1 = fitKDE(np.diag(eVal0), bWidth=.005)

    alpha, nCols, nFact, q = .995, 1000, 100, 10
    pdf0 = mpPDF(1., q=x.shape[0] / float(x.shape[1]), pts=N)
    cov = np.cov(np.random.normal(size=(nCols * q, nCols)), rowvar=0)
    cov = alpha * cov + (1 - alpha) * getRndCov(nCols, nFact)
    corr0 = cov2corr(cov)
    eVal01, eVec01 = getPCA(corr0)

    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.hist(np.diag(eVal01), density=True, bins=50)

    eMax0, var0 = findMaxEval(np.diag(eVal01), q, bWidth=.01)
    nFacts0 = eVal01.shape[0] - np.diag(eVal01)[::-1].searchsorted(eMax0)
    
    pdf0 = mpPDF(var0, q=x.shape[0] / float(x.shape[1]), pts=N)
    fig = plt.figure()
    ax = fig.add_subplot(111)
    ax.hist(np.diag(eVal01), density=True, bins=50)

    plt.plot(pdf0.keys(), pdf0, color='r', label="Marcenko-Pastur pdf")
    plt.legend(loc="upper right")
    plt.show()

    corr1 = denoisedCorr(eVal01, eVec01, nFacts0)
    eVal1, eVec1 = getPCA(corr1)

    denoised_eigenvalue = np.diag(eVal1)
    eigenvalue_prior = np.diag(eVal01)
    plt.plot(range(0, len(denoised_eigenvalue)), np.log(denoised_eigenvalue), color='r', label="Denoised eigen-function")
    plt.plot(range(0, len(eigenvalue_prior)), np.log(eigenvalue_prior), color='g', label="Original eigen-function")
    plt.xlabel("Eigenvalue number")
    plt.ylabel("Eigenvalue (log-scale)")
    plt.legend(loc="upper right")
    plt.show()

    corr_detoned_denoised = detoned_corr(corr1, eVal1, eVec1)
    eVal1_detoned, eVec1_detoned = getPCA(corr_detoned_denoised)
    detoned_denoised_eigenvalue = np.diag(eVal1_detoned)
    denoised_eigenvalue = np.diag(eVal1)
    eigenvalue_prior = np.diag(eVal01)

    plt.plot(range(0, len(detoned_denoised_eigenvalue)), np.log(detoned_denoised_eigenvalue), color='b', label="Detoned, denoised eigen-function")
    plt.plot(range(0, len(denoised_eigenvalue)), np.log(denoised_eigenvalue), color='r', label="Denoised eigen-function")
    plt.plot(range(0, len(eigenvalue_prior)), np.log(eigenvalue_prior), color='g', label="Original eigen-function")
    plt.xlabel("Eigenvalue number")
    plt.ylabel("Eigenvalue (log-scale)")
    plt.legend(loc="upper right")
    plt.show()

// ---------------------------------------------------

// ch2_monte_carlo_experiment.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch2_monte_carlo_experiment.py
# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from scipy.linalg import block_diag
from sklearn.covariance import LedoitWolf
from Machine_Learning_for_Asset_Managers import ch2_marcenko_pastur_pdf as mp

def formBlockMatrix(nBlocks, bSize, bCorr):
    block = np.ones((bSize, bSize)) * bCorr
    block[range(bSize), range(bSize)] = 1
    corr = block_diag(*([block] * nBlocks))
    return corr

def formTrueMatrix(nBlocks, bSize, bCorr):
    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)
    corr0 = pd.DataFrame(corr0)
    cols = corr0.columns.tolist()
    np.random.shuffle(cols)
    corr0 = corr0[cols].loc[cols].copy(deep=True)
    std0 = np.random.uniform(.05, .2, corr0.shape[0])
    cov0 = corr2cov(corr0, std0)
    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)
    return mu0, cov0

def corr2cov(corr, std):
    cov = corr * np.outer(std, std)
    return cov

def simCovMu(mu0, cov0, nObs, shrink=False):
    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)
    mu1 = x.mean(axis=0).reshape(-1, 1)
    if shrink:
        cov1 = LedoitWolf().fit(x).covariance_
    else:
        cov1 = np.cov(x, rowvar=0)
    return mu1, cov1

def deNoiseCov(cov0, q, bWidth):
    corr0 = mp.cov2corr(cov0)
    eVal0, eVec0 = mp.getPCA(corr0)
    eMax0, var0 = mp.findMaxEval(np.diag(eVal0), q, bWidth)
    nFacts0 = eVal0.shape[0] - np.diag(eVal0)[::-1].searchsorted(eMax0)
    corr1 = mp.denoisedCorr(eVal0, eVec0, nFacts0)
    cov1 = corr2cov(corr1, np.diag(cov0) ** .5)
    return cov1

def optPort(cov, mu=None):
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = np.dot(inv, mu)
    w /= np.dot(ones.T, w)
    return w

def optPortLongOnly(cov, mu=None):
    inv = np.linalg.inv(cov)
    ones = np.ones(shape=(inv.shape[0], 1))
    if mu is None:
        mu = ones
    w = np.dot(inv, mu)
    w /= np.dot(ones.T, w)
    w = w.flatten()
    threshold = w < 0
    wpluss = w.copy()
    wpluss[threshold] = 0
    wpluss = wpluss / np.sum(wpluss)
    return wpluss

if __name__ == '__main__':
    nBlocks, bSize, bCorr = 2, 2, .5
    np.random.seed(0)
    mu0, cov0 = formTrueMatrix(nBlocks, bSize, bCorr)

    nObs, nTrials, bWidth, shrink, minVarPortf = 5, 5, .01, False, True
    w1 = pd.DataFrame(columns=range(cov0.shape[0]), index=range(nTrials), dtype=float)
    w1_d = w1.copy(deep=True)
    np.random.seed(0)

    for i in range(nTrials):
        mu1, cov1 = simCovMu(mu0, cov0, nObs, shrink=shrink)
        if minVarPortf:
            mu1 = None
        cov1_d = deNoiseCov(cov1, nObs * 1. / cov1.shape[1], bWidth)
        w1.loc[i] = optPort(cov1, mu1).flatten()
        w1_d.loc[i] = optPort(cov1_d, mu1).flatten()

    min_var_port = 1. / nTrials * (np.sum(w1_d, axis=0))
    w0 = optPort(cov0, None if minVarPortf else mu0)
    w0 = np.repeat(w0.T, w1.shape[0], axis=0)
    rmsd = np.mean((w1 - w0).values.flatten() ** 2) ** .5
    rmsd_d = np.mean((w1_d - w0).values.flatten() ** 2) ** .5
    print("RMSE not denoised:" + str(rmsd))
    print("RMSE denoised:" + str(rmsd_d))

// ---------------------------------------------------

// ch2_fitKDE_find_best_bandwidth.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch2_fitKDE_find_best_bandwidth.py
# -*- coding: utf-8 -*-
import numpy as np
from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import LeaveOneOut


def findOptimalBWidth(eigenvalues):
    bandwidths = 10 ** np.linspace(-2, 1, 100)
    grid = GridSearchCV(KernelDensity(kernel='gaussian'),
                        {'bandwidth': bandwidths},
                        cv=LeaveOneOut())
    grid.fit(eigenvalues[:, None])

    return grid.best_params_


// ---------------------------------------------------

// ch5_financial_labels.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch5_financial_labels.py
# -*- coding: utf-8 -*-
import statsmodels.api as sm1
import numpy as np
import pandas as pd
import matplotlib.pylab as plt

def tValLinR(close):
    x = np.ones((close.shape[0], 2))
    x[:, 1] = np.arange(close.shape[0])
    ols = sm1.OLS(close, x).fit()
    return ols.tvalues[1]

def getBinsFromTrend(molecule, close, span):
    out = pd.DataFrame(index=molecule, columns=['t1', 'tVal', 'bin', 'windowSize'])
    hrzns = range(*span)
    maxWindow = span[1] - 1
    for idx in close.index:
        idx += maxWindow
        if idx >= len(close):
            break
        df_tval = pd.Series(dtype='float64')
        iloc0 = close.index.get_loc(idx)
        for hrzn in hrzns:
            dt1 = close.index[iloc0 - hrzn + 1]
            df1 = close.loc[dt1:idx]
            df_tval.loc[dt1] = tValLinR(df1.values)
        dt1 = df_tval.replace([-np.inf, np.inf, np.nan], 0).abs().idxmax()
        out.loc[idx, ['t1', 'tVal', 'bin', 'windowSize']] = df_tval.index[-1], df_tval[dt1], np.sign(df_tval[dt1]), abs(df_tval.values).argmax() + (span[0])
    out['t1'] = pd.to_datetime(out['t1'])
    out['bin'] = pd.to_numeric(out['bin'], downcast='signed')
    tValueVariance = out['tVal'].values.var()
    tMax = 20
    if tValueVariance < tMax:
        tMax = tValueVariance
    out.loc[out['tVal'] > tMax, 'tVal'] = tMax
    out.loc[out['tVal'] < (-1) * tMax, 'tVal'] = (-1) * tMax
    return out.dropna(subset=['bin'])

if __name__ == '__main__':
    idx_range_from = 3
    idx_range_to = 10
    df0 = pd.Series(np.random.normal(0, .1, 100)).cumsum()
    df0 += np.sin(np.linspace(0, 10, df0.shape[0]))
    df1 = getBinsFromTrend(df0.index, df0, [idx_range_from, idx_range_to, 1])
    tValues = df1['tVal'].values
    doNormalize = False
    if doNormalize:
        minusArgs = [i for i in range(len(tValues)) if tValues[i] < 0]
        tValues[minusArgs] = tValues[minusArgs] / (np.min(tValues) * (-1.0))
        plus_one = [i for i in range(len(tValues)) if tValues[i] > 0]
        tValues[plus_one] = tValues[plus_one] / np.max(tValues)
    plt.scatter(df1.index, df0.loc[df1.index].values, c=tValues, cmap='viridis')
    plt.plot(df0.index, df0.values, color='gray')
    plt.colorbar()
    plt.show()
    plt.savefig('fig5.2.png')
    plt.clf()
    plt.close()
    plt.scatter(df1.index, df0.loc[df1.index].values, c=df1['bin'].values, cmap='viridis')

    ols_tvalue = tValLinR(np.array([3.0, 3.5, 4.0]))

// ---------------------------------------------------

// ch6_feature_importance_analysis.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch6_feature_importance_analysis.py
# -*- coding: utf-8 -*-
from sklearn.datasets import make_classification
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection._split import KFold
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm1
import matplotlib.pylab as plt

from ch4_optimal_clustering import clusterKMeansBase 

def getTestData(n_features=100, n_informative=25, n_redundant=25, n_samples=10000, random_state=0, sigmaStd=.0):
    np.random.seed(random_state)
    X, y = make_classification(n_samples=n_samples, n_features=n_features-n_redundant, 
        n_informative=n_informative, n_redundant=0, shuffle=False, random_state=random_state)
    cols = ['I_'+str(i) for i in range(0, n_informative)]
    cols += ['N_'+str(i) for i in range(0, n_features - n_informative - n_redundant)]
    X, y = pd.DataFrame(X, columns=cols), pd.Series(y)
    i = np.random.choice(range(0, n_informative), size=n_redundant)
    for k, j in enumerate(i):
        X['R_'+str(k)] = X['I_' + str(j)] + np.random.normal(size=X.shape[0])*sigmaStd    
    return X, y 

def featImpMDI(fit, featNames):
    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}
    df0 = pd.DataFrame.from_dict(df0, orient='index')
    df0.columns = featNames
    df0 = df0.replace(0, np.nan)
    imp = pd.concat({'mean': df0.mean(), 'std': df0.std() * df0.shape[0]**-.5}, axis=1)
    imp /= imp['mean'].sum()
    return imp
    
def featImpMDA(clf, X, y, n_splits=10):
    cvGen = KFold(n_splits=n_splits)
    scr0, scr1 = pd.Series(dtype='float64'), pd.DataFrame(columns=X.columns)
    for i, (train, test) in enumerate(cvGen.split(X=X)):
        x0, y0 = X.iloc[train, :], y.iloc[train]
        x1, y1 = X.iloc[test, :], y.iloc[test]
        fit = clf.fit(X=x0, y=y0)
        prob = fit.predict_proba(x1)
        scr0.loc[i] = -log_loss(y1, prob, labels=clf.classes_)
        for j in X.columns:
            X1_ = x1.copy(deep=True)
            np.random.shuffle(X1_[j].values)
            prob = fit.predict_proba(X1_)
            scr1.loc[i, j] = -log_loss(y1, prob, labels=clf.classes_)
    imp = (-1 * scr1).add(scr0, axis=0)
    imp = imp / (-1 * scr1)
    imp = pd.concat({'mean': imp.mean(), 'std': imp.std() * imp.shape[0]**-.5}, axis=1)
    return imp
    
def groupMeanStd(df0, clstrs):
    out = pd.DataFrame(columns=['mean', 'std'])
    for i, j in clstrs.items():
        df1 = df0[j].sum(axis=1)
        out.loc['C_'+str(i), 'mean'] = df1.mean()
        out.loc['C_'+str(i), 'std'] = df1.std() * df1.shape[0]**-.5
    return out

def featImpMDI_Clustered(fit, featNames, clstrs):
    df0 = {i: tree.feature_importances_ for i, tree in enumerate(fit.estimators_)}
    df0 = pd.DataFrame.from_dict(df0, orient='index')
    df0.columns = featNames
    df0 = df0.replace(0, np.nan)
    imp = groupMeanStd(df0, clstrs)
    imp /= imp['mean'].sum()
    return imp
    
def featImpMDA_Clustered(clf, X, y, clstrs, n_splits=10):
    cvGen = KFold(n_splits=n_splits)
    scr0, scr1 = pd.Series(dtype='float64'), pd.DataFrame(columns=clstrs.keys())
    for i, (train, test) in enumerate(cvGen.split(X=X)):
        X0, y0 = X.iloc[train, :], y.iloc[train] 
        X1, y1 = X.iloc[test, :], y.iloc[test]
        fit = clf.fit(X=X0, y=y0)
        prob = fit.predict_proba(X1)
        scr0.loc[i] = -log_loss(y1, prob, labels=clf.classes_)
        for j in scr1.columns:
            X1_ = X1.copy(deep=True)
            for k in clstrs[j]:
                np.random.shuffle(X1_[k].values)
            prob = fit.predict_proba(X1_)
            scr1.loc[i, j] = -log_loss(y1, prob, labels=clf.classes_)
    imp = (-1 * scr1).add(scr0, axis=0)
    imp = imp / (-1 * scr1)
    imp = pd.concat({'mean': imp.mean(), 'std': imp.std() * imp.shape[0]**-.5}, axis=1)
    imp.index = ['C_' + str(i) for i in imp.index]
    return imp

if __name__ == '__main__':
    X, y = getTestData(40, 5, 30, 10000, sigmaStd=.1)
    ols = sm1.Logit(y, X).fit()
    plot_data = ols.pvalues.sort_values(ascending=False)
    plot_data.plot(kind='barh', figsize=(20, 10), title="Figure 6.1 p-Values computed on a set of explanatory variables")
    plt.show()
    
    X, y = getTestData(40, 5, 30, 10000, sigmaStd=.1)
    clf = DecisionTreeClassifier(criterion='entropy', 
                                 max_features=1, 
                                 class_weight='balanced', 
                                 min_weight_fraction_leaf=0)
    clf = BaggingClassifier(base_estimator=clf, 
                          n_estimators=1000, 
                          max_features=1., 
                          max_samples=1., 
                          oob_score=False)
    fit = clf.fit(X, y)
    imp = featImpMDI(fit, featNames=X.columns)
    
    imp.sort_values('mean', inplace=True)
    plt.figure(figsize=(10, imp.shape[0] / 5))
    imp['mean'].plot(kind='barh', color='b', alpha=0.25, xerr=imp['std'], error_kw={'ecolor': 'r'})
    plt.title('Figure 6.2 Example of MDI results')
    plt.show()
    
    X, y = getTestData(40, 5, 30, 10000, sigmaStd=.1)
    clf = DecisionTreeClassifier(criterion='entropy', 
                                 max_features=1, 
                                 class_weight='balanced', 
                                 min_weight_fraction_leaf=0)
    clf = BaggingClassifier(base_estimator=clf, 
                          n_estimators=1000, 
                          max_features=1., 
                          max_samples=1., 
                          oob_score=False)
    fit = clf.fit(X, y)
    imp = featImpMDA(clf, X, y, 10)
    
    imp.sort_values('mean', inplace=True)
    plt.figure(figsize=(10, imp.shape[0] / 5))
    imp['mean'].plot(kind='barh', color='b', alpha=0.25, xerr=imp['std'], error_kw={'ecolor': 'r'})
    plt.title('Figure 6.3 Example of MDA results')
    plt.show()
    
    X, y = getTestData(40, 5, 30, 10000, sigmaStd=.1)
    corr0, clstrs, silh = clusterKMeansBase(X.corr(), maxNumClusters=10, n_init=10)
    fig, ax = plt.subplots(figsize=(13, 10))  
    sns.heatmap(corr0, cmap='viridis')
    plt.show()
    
    X, y = getTestData(40, 5, 30, 10000, sigmaStd=.1)
    clf = DecisionTreeClassifier(criterion='entropy', 
                                 max_features=1, 
                                 class_weight='balanced', 
                                 min_weight_fraction_leaf=0)
    clf = BaggingClassifier(base_estimator=clf, 
                          n_estimators=1000, 
                          max_features=1., 
                          max_samples=1., 
                          oob_score=False)
    fit = clf.fit(X, y)
    imp = featImpMDI_Clustered(fit, X.columns, clstrs)

    imp.sort_values('mean', inplace=True)
    plt.figure(figsize=(10, 5))
    imp['mean'].plot(kind='barh', color='b', alpha=0.25, xerr=imp['std'], error_kw={'ecolor': 'r'})
    plt.title('Figure 6.5 Clustered MDI')
    plt.show()
    
    clf = DecisionTreeClassifier(criterion='entropy', 
                                 max_features=1, 
                                 class_weight='balanced', 
                                 min_weight_fraction_leaf=0)
    clf = BaggingClassifier(base_estimator=clf, 
                          n_estimators=1000, 
                          max_features=1., 
                          max_samples=1., 
                          oob_score=False)
    fit = clf.fit(X, y)
    imp = featImpMDA_Clustered(clf, X, y, clstrs, 10)

    imp.sort_values('mean', inplace=True)
    plt.figure(figsize=(10, 5))
    imp['mean'].plot(kind='barh', color='b', alpha=0.25, xerr=imp['std'], error_kw={'ecolor': 'r'})
    plt.title('Figure 6.6 Clustered MDA')
    plt.show()

// ---------------------------------------------------

// ch8_testing_set_overfitting.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch8_testing_set_overfitting.py
import numpy as np
# import cupy
import pandas as pd
from scipy.stats import norm
import scipy.stats as ss
import matplotlib.pylab as plt
import matplotlib as mpl
import itertools

def getExpectedMaxSR(nTrials, meanSR, stdSR):
    emc = 0.477215664901532860606512090082402431042159336
    sr0 = (1 - emc) * norm.ppf(1 - 1. / nTrials) + emc * norm.ppf(1 - (nTrials * np.e) ** -1)
    sr0 = meanSR + stdSR * sr0
    return sr0

def getDistMaxSR(nSims, nTrials, stdSR, meanSR):
    rng = np.random.RandomState()
    out = pd.DataFrame()
    for nTrials_ in nTrials:
        sr = pd.DataFrame(rng.randn(nSims, nTrials_))
        sr = sr.sub(sr.mean(axis=1), axis=0)
        sr = sr.div(sr.std(axis=1), axis=0)
        sr = meanSR + sr * stdSR
        out_ = sr.max(axis=1).to_frame('max{SR}')
        out_['nTrials'] = nTrials_
        out = out.append(out_, ignore_index=True)
    return out

def getMeanStdError(nSims0, nSims1, nTrials, stdSR=1, meanSR=0):
    sr0 = pd.Series({i: getExpectedMaxSR(i, meanSR, stdSR) for i in nTrials})
    sr0 = sr0.to_frame('E[max{SR}]')
    sr0.index.name = 'nTrials'
    err = pd.DataFrame()
    for i in range(0, int(nSims1)):
        sr1 = getDistMaxSR(nSims=1000, nTrials=nTrials, meanSR=0, stdSR=1)
        sr1 = sr1.groupby('nTrials').mean()
        err_ = sr0.join(sr1).reset_index()
        err_['err'] = err_['max{SR}'] / err_['E[max{SR}]'] - 1.
        err = err.append(err_)
    out = {'meanErr': err.groupby('nTrials')['err'].mean()}
    out['stdErr'] = err.groupby('nTrials')['err'].std()
    out = pd.DataFrame.from_dict(out, orient='columns')
    return out

def getZStat(sr, t, sr_=0, skew=0, kurt=3):
    z = (sr - sr_) * (t - 1) ** .5
    z /= (1 - skew * sr + (kurt - 1) / 4. * sr ** 2) ** .5
    return z

def type1Err(z, k=1):
    alpha = ss.norm.cdf(-z)
    alpha_k = 1 - (1 - alpha) ** k
    return alpha_k

def getTheta(sr, t, sr_=0., skew=0., kurt=3):
    theta = sr_ * (t - 1) ** .5
    theta /= (1 - skew * sr + (kurt - 1) / .4 * sr ** 2) ** .5
    return theta

def type2Err(alpha_k, k, theta):
    z = ss.norm.ppf((1 - alpha_k) ** (1. / k))
    beta = ss.norm.cdf(z - theta)
    return beta

if __name__ == '__main__':
    nTrials = list(set(np.logspace(1, 6, 100).astype(int)))
    nTrials.sort()
    sr0 = pd.Series({i: getExpectedMaxSR(i, meanSR=0, stdSR=1) for i in nTrials}, name="E[max{SR}] (prior)")
    sr1 = getDistMaxSR(nSims=100, nTrials=nTrials, meanSR=0, stdSR=1)

    nnSR0 = list(itertools.chain.from_iterable(itertools.repeat(x, 100) for x in sr0.values))
    deviationFromExpectation = abs(sr1['max{SR}'] - nnSR0)

    ax = sr1.plot.scatter(x='nTrials', y='max{SR}', label='Max{SR} (observed)', c=deviationFromExpectation, cmap=mpl.cm.viridis.reversed())
    ax.set_xscale('log')
    ax.plot(nTrials, sr0, linestyle='--', linewidth=1, label='E[max{SR}] (prior)', color='black')
    plt.legend()
    ax.figure.savefig('/gpfs/gpfs0/deep/maxSR_across_uniform_strategies_8_1.png')

    nTrials = list(set(np.logspace(1, 6, 1000).astype(int)))
    nTrials.sort()
    stats = getMeanStdError(nSims0=1000, nSims1=100, nTrials=nTrials, stdSR=1)

    ax = stats.plot()
    ax.set_xscale('log')
    ax.figure.savefig('/gpfs/gpfs0/deep/fig82.png')

    t, skew, kurt, k, freq = 1250, -3, 10, 10, 250
    sr = 1.25 / freq ** .5
    sr_ = 1. / freq ** .5
    z = getZStat(sr, t, 0, skew, kurt)
    alpha_k = type1Err(z, k=k)
    print(alpha_k)

    z = getZStat(sr, t, 0, skew, kurt)
    alpha_k = type1Err(z, k=k)
    theta = getTheta(sr, t, sr_, skew, kurt)
    beta = type2Err(alpha_k, k, theta)
    beta_k = beta ** k
    print(beta_k)

// ---------------------------------------------------

// ch4_optimal_clustering.py
// machine-learning-for-asset-managers-emoen/Machine_Learning_for_Asset_Managers/ch4_optimal_clustering.py
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.utils import check_random_state
from scipy.linalg import block_diag
import matplotlib.pylab as plt

from Machine_Learning_for_Asset_Managers import ch2_marcenko_pastur_pdf as mp

def clusterKMeansBase(corr0, maxNumClusters=10, n_init=10, debug=False):
    corr0[corr0 > 1] = 1
    dist_matrix = ((1 - corr0.fillna(0)) / 2.) ** .5
    silh_coef_optimal = pd.Series(dtype='float64')
    kmeans = None
    maxNumClusters = min(maxNumClusters, int(np.floor(dist_matrix.shape[0] / 2)))
    for init in range(n_init):
        for num_clusters in range(2, maxNumClusters + 1):
            kmeans_ = KMeans(n_clusters=num_clusters, n_init=10)
            kmeans_ = kmeans_.fit(dist_matrix)
            silh_coef = silhouette_samples(dist_matrix, kmeans_.labels_)
            stat = (silh_coef.mean() / silh_coef.std(), silh_coef_optimal.mean() / silh_coef_optimal.std())
            if np.isnan(stat[1]) or stat[0] > stat[1]:
                silh_coef_optimal = silh_coef
                kmeans = kmeans_
                if debug:
                    silhouette_avg = silhouette_score(dist_matrix, kmeans_.labels_)
                    print("For n_clusters =" + str(num_clusters) + " The average silhouette_score is :" + str(silhouette_avg))
    
    newIdx = np.argsort(kmeans.labels_)
    corr1 = corr0.iloc[newIdx]
    corr1 = corr1.iloc[:, newIdx]
    clstrs = {i: corr0.columns[np.where(kmeans.labels_ == i)[0]].tolist() for i in np.unique(kmeans.labels_)}
    silh_coef_optimal = pd.Series(silh_coef_optimal, index=dist_matrix.index)
    
    return corr1, clstrs, silh_coef_optimal

def makeNewOutputs(corr0, clstrs, clstrs2):
    clstrsNew, newIdx = {}, []
    for i in clstrs.keys():
        clstrsNew[len(clstrsNew.keys())] = list(clstrs[i])
    
    for i in clstrs2.keys():
        clstrsNew[len(clstrsNew.keys())] = list(clstrs2[i])
    
    newIdx = [j for i in clstrsNew for j in clstrsNew[i]]
    corrNew = corr0.loc[newIdx, newIdx]
    
    dist = ((1 - corr0.fillna(0)) / 2.) ** .5
    kmeans_labels = np.zeros(len(dist.columns))
    for i in clstrsNew.keys():
        idxs = [dist.index.get_loc(k) for k in clstrsNew[i]]
        kmeans_labels[idxs] = i
    
    silhNew = pd.Series(silhouette_samples(dist, kmeans_labels), index=dist.index)
    
    return corrNew, clstrsNew, silhNew

def clusterKMeansTop(corr0: pd.DataFrame, maxNumClusters=None, n_init=10):
    if maxNumClusters is None:
        maxNumClusters = corr0.shape[1] - 1
        
    corr1, clstrs, silh = clusterKMeansBase(corr0, maxNumClusters=min(maxNumClusters, corr0.shape[1] - 1), n_init=n_init)
    clusterTstats = {i: np.mean(silh[clstrs[i]]) / np.std(silh[clstrs[i]]) for i in clstrs.keys()}
    tStatMean = sum(clusterTstats.values()) / len(clusterTstats)
    redoClusters = [i for i in clusterTstats.keys() if clusterTstats[i] < tStatMean]
    
    if len(redoClusters) <= 2:
        return corr1, clstrs, silh
    else:
        keysRedo = [j for i in redoClusters for j in clstrs[i]]
        corrTmp = corr0.loc[keysRedo, keysRedo]
        _, clstrs2, _ = clusterKMeansTop(corrTmp, maxNumClusters=min(maxNumClusters, corrTmp.shape[1] - 1), n_init=n_init)
        dict_redo_clstrs = {i: clstrs[i] for i in clstrs.keys() if i not in redoClusters}
        corrNew, clstrsNew, silhNew = makeNewOutputs(corr0, dict_redo_clstrs, clstrs2)
        newTstatMean = np.mean([np.mean(silhNew[clstrsNew[i]]) / np.std(silhNew[clstrsNew[i]]) for i in clstrsNew.keys()])
        
        if newTstatMean <= tStatMean:
            return corr1, clstrs, silh
        else:
            return corrNew, clstrsNew, silhNew

def getCovSub(nObs, nCols, sigma, random_state=None):
    rng = check_random_state(random_state)
    if nCols == 1:
        return np.ones((1, 1))
    ar0 = rng.normal(size=(nObs, 1))
    ar0 = np.repeat(ar0, nCols, axis=1)
    ar0 += rng.normal(loc=0, scale=sigma, size=ar0.shape)
    ar0 = np.cov(ar0, rowvar=False)
    return ar0

def getRndBlockCov(nCols, nBlocks, minBlockSize=1, sigma=1., random_state=None):
    rng = check_random_state(random_state)
    parts = rng.choice(range(1, nCols - (minBlockSize - 1) * nBlocks), nBlocks - 1, replace=False)
    parts.sort()
    parts = np.append(parts, nCols - (minBlockSize - 1) * nBlocks)
    parts = np.append(parts[0], np.diff(parts)) - 1 + minBlockSize
    cov = None
    for nCols_ in parts:
        cov_ = getCovSub(int(max(nCols_ * (nCols_ + 1) / 2., 100)), nCols_, sigma, random_state=rng)
        if cov is None:
            cov = cov_.copy()
        else:
            cov = block_diag(cov, cov_)
    
    return cov

def randomBlockCorr(nCols, nBlocks, random_state=None, minBlockSize=1):
    rng = check_random_state(random_state)
    cov0 = getRndBlockCov(nCols, nBlocks, minBlockSize=minBlockSize, sigma=.5, random_state=rng)
    cov1 = getRndBlockCov(nCols, 1, minBlockSize=minBlockSize, sigma=1., random_state=rng)
    cov0 += cov1
    corr0 = mp.cov2corr(cov0)
    corr0 = pd.DataFrame(corr0)
    return corr0

if __name__ == '__main__':
    nCols, nBlocks = 6, 3
    nObs = 8
    sigma = 1.
    corr0 = randomBlockCorr(nCols, nBlocks)
    testGetCovSub = getCovSub(nObs, nCols, sigma, random_state=None)
    
    nCols, nBlocks, minBlockSize = 30, 6, 2
    corr0 = randomBlockCorr(nCols, nBlocks, minBlockSize=minBlockSize)
    corr1 = clusterKMeansTop(corr0)

    plt.matshow(corr0)
    plt.gca().xaxis.tick_bottom()
    plt.gca().invert_yaxis()
    plt.colorbar()
    plt.show()

    corrNew, clstrsNew, silhNew = clusterKMeansTop(corr0)
    plt.matshow(corrNew)
    plt.gca().xaxis.tick_bottom()
    plt.gca().invert_yaxis()
    plt.colorbar()
    plt.show()

// ---------------------------------------------------

