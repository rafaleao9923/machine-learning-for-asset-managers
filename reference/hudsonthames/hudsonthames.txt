// docs_bs4.py
// docs_bs4.py
import requests
from bs4 import BeautifulSoup
import html2text
import os
import time
import random
from requests.exceptions import RequestException

def convert_url_to_markdown(slug, max_retries=3):
    attempt = 0
    while attempt < max_retries:
        try:
            delay = random.uniform(2, 5)
            time.sleep(delay)
            
            base_url = 'https://www.mlfinlab.com/en/latest/'
            url = f'{base_url}{slug}.html'
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Find the specific div
            content_div = soup.find('div', {'itemprop': 'articleBody'})
            if not content_div:
                raise Exception(f"Could not find content div with id '{slug.split('/')[-1].replace('_', '-')}'")
            
            converter = html2text.HTML2Text()
            converter.ignore_links = False
            markdown_content = converter.handle(str(content_div))
            
            output_filename = f"output/{slug.replace('/', '-')}.md"
            os.makedirs('output', exist_ok=True)
            
            with open(output_filename, 'w') as md_file:
                md_file.write(markdown_content)
            
            print(f"Successfully processed {url} (delay: {delay:.2f}s)")
            return output_filename

        except (RequestException, Exception) as e:
            attempt += 1
            if attempt < max_retries:
                retry_delay = random.uniform(5, 10)
                print(f"Attempt {attempt} failed for {slug}. Error: {str(e)}. Retrying in {retry_delay:.2f}s...")
                time.sleep(retry_delay)
            else:
                print(f"Failed to process {slug} after {max_retries} attempts. Error: {str(e)}")
                return None

if __name__ == '__main__':

    # Example usage
    slugs = [
        # 'regression/history_weighted_regression',
        # 'clustering/feature_clusters',
        
        # 'backtest_overfitting/backtest_statistics',

        'data_structures/data_prep',
        'data_structures/standard',
        'data_structures/imbalance',
        'data_structures/runbars',
        'data_structures/futures_roll',
        'labeling/introduction',
        'labeling/labeling_raw_return',
        'labeling/labeling_fixed_time_horizon',
        'labeling/labeling_excess_mean',
        'labeling/labeling_excess_median',
        'labeling/labeling_vs_benchmark',
        'labeling/labeling_bull_bear',
        'labeling/labeling_matrix_flags',
        'labeling/labeling_tail_sets',
        'labeling/labeling_trend_scanning',
        'labeling/tb_meta_labeling',
        'sampling/intro_sampling',
        'sampling/sample_uniqueness',
        'sampling/sequential_boot',
        'sampling/sample_weights',
        'feature_engineering/frac_diff',
        'feature_engineering/structural_breaks',
        'feature_engineering/filters',
        'feature_engineering/volatility_estimators',
        'feature_engineering/noise_reduction',
        'feature_engineering/directional_change',
        'feature_engineering/micro_features_first_gen',
        'feature_engineering/entropy',
        'feature_engineering/micro_features_other',
        'modelling/sb_bagging',
        'clustering/onc',
        'clustering/hierarchical_clustering',
        'cross_validation/purged_embargo',
        'cross_validation/cpcv',
        'feature_importance/afm',
        'feature_importance/clustered',
        'feature_importance/fingerprint',
        'feature_importance/pca',
        'feature_importance/shapley',
        'bet_sizing/bet_sizing',
        'bet_sizing/EF3M',
        'data_generation/introduction',
        'data_generation/corrgan',
        'data_generation/vine_methods',
        'data_generation/correlated_random_walks',
        'data_generation/hcbm',
        'data_generation/bootstrap',
        'data_generation/data_verification',
        'networks/introduction',
        'networks/mst',
        'networks/almst',
        'networks/pmfg',
        'networks/visualisations',
        'networks/dash',
        'codependence/introduction',
        'codependence/correlation_based_metrics',
        'codependence/information_theory_metrics',
        'codependence/codependence_marti',
        'codependence/optimal_transport',
        'codependence/codependence_matrix',
    ]

    for slug in slugs:
        output_file = convert_url_to_markdown(slug)
        print(f"Converted {slug} to {output_file}")

// ---------------------------------------------------

// YFinance_Tutorial.py
// backtest_tutorial/YFinance_Tutorial.py
# Generated from: YFinance_Tutorial.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # YFinance Tutorial: Pulling Free Financial Data
#
# yfinance is a popular Python library that offers a reliable, fast, and easy way to fetch historical market data from Yahoo Finance. Whether you're a seasoned data analyst, a budding quant trader, or a finance enthusiast, yfinance is a tool you'll find indispensable. In this tutorial, we'll walk through the basics of using yfinance to pull financial data.
#
# ## Documentation
# You can find the online documentation for `yfinance` here: https://python-yahoofinance.readthedocs.io/en/latest/
#
# ## Setting Up
#
# Ensure you have `yfinance` installed. If not, install it using pip, inside a notebook cell:
#
# ```python
# !pip install yfinance
# ```
#
# * If you are installing from the terminal, use: `pip install yfinance`.
# * If you followed the getting_started instructions, your python environment will already have it installed.


# ---
# ## Basic Usage of yfinance
# ### Importing the library
#
# After installation, you can import yfinance in your Python script or notebook:


import yfinance as yf


# ### Getting Historical Data
#
# To fetch historical market data for a specific stock, use the yf.Ticker class. For example, let's fetch data for Apple Inc:


apple = yf.Ticker("AAPL")
apple


# * You can get the historical market data using the history method. By default, it retrieves the last five days of stock data.
# * We will now reference the object `apple` when extracting data.


# Get the last 5 days of historical data
hist = apple.history(period="5d")
hist

# Fetching Real-Time Data
# Note: Real-time data might be delayed due to limitations from Yahoo Finance


# * To fetch data for a specific period, you can specify the period or start and end dates:


# Data for a specific period
hist_1m = apple.history(period="1mo")  # 1 month
hist_1year = apple.history(start="2020-01-01", end="2021-01-01")  # Specific date range, for each day

print('History: Monthly')
hist_1m.tail()


print('History: Year - each day')
hist_1year


# ### Adjusting for Stock Splits
# yfinance automatically adjusts historical data for stock splits. If you prefer unadjusted data, set auto_adjust to False:


hist_unadj = apple.history(period="1mo", auto_adjust=False)
hist_unadj


# ### Including Dividends and Stock Splits
# To include dividend and stock split data, set actions to True:


hist_adj = apple.history(period="5y", actions=True)
hist_adj


# ---
# ## Advanced Usage
#
# ### Fetching Data for Multiple Stocks
#
# To fetch data for multiple stocks, pass a list of ticker symbols to yf.download:


data = yf.download("AAPL MSFT GOOG", start="2020-01-01", end="2021-01-01")
data


# ### Fetching Real-Time Data
# For real-time data, you can use the interval parameter with the history method. For example, fetching minute-level data for the past 5 days:


real_time_data = apple.history(period="5d", interval="1m")
real_time_data


# **Note:**
# * Real-time data might be delayed by up to 15 minutes due to Yahoo Finance’s limitations.


# ---
# ## Conclusion
# yfinance is a powerful tool that provides easy access to a wealth of financial data. By integrating yfinance into your analysis, you gain the ability to make more informed decisions based on historical and real-time market data. Happy data hunting.



// ---------------------------------------------------

// Intro_Transaction_Costs.py
// backtest_tutorial/Intro_Transaction_Costs.py
# Generated from: Intro_Transaction_Costs.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Introduction to Transaction Costs in Backtesting
# The realism and effectiveness of backtesting largely hinge on accurately accounting for transaction costs. These costs can significantly impact the profitability and viability of a trading strategy. Note that if you want an ultra realistic backtest - the closest you can get is an event driven engine, a vectorised backtest will only give us a heuristic measure, which is good enough for 90% of professional traders. 
#
# There are three primary types of transaction costs that traders need to consider in a backtest:
#
# 1. **Commissions**
# Commissions are fees levied by brokers for executing trades. They can vary widely based on the broker's pricing structure, the asset class, and the volume of the trade. Commissions can either be a fixed fee per trade, a percentage of the trade value, or based on the number of shares or contracts traded.
#
# * **Best Practices**
#     * Broker-Specific Calculation: Ensure that the commission structure in your backtest mirrors the structure of the broker you plan to use for live trading.
#     * Tiered Structures: If your broker uses a tiered commission structure, make sure your backtest accounts for varying rates at different volume thresholds.
#     * Impact on Strategy: Pay attention to strategies involving frequent trades, such as day trading or high-frequency trading, where commission costs can quickly accumulate and significantly impact net returns.
#
# 2. **Bid-Ask Spread**
# The bid-ask spread is the difference between the highest price a buyer is willing to pay (bid) and the lowest price a seller is willing to accept (ask). This spread is a cost to traders, and it's particularly relevant for assets with lower liquidity, where spreads can be wider.
#
# * **Best Practices**
#
#     * Historical Spreads: Utilize historical spread data to estimate average spreads for the assets in your strategy.
#     * Intraday Trading: For intraday trading strategies, consider using intraday spread data, as spreads can vary significantly throughout the trading day.
#     * Asset Liquidity: Be mindful of asset liquidity, as illiquid assets often have wider spreads, which can erode profits, especially in large trades.
#
# 3. **Market Impact**
# Market impact refers to the effect that a trader's orders have on the market price of an asset. Particularly with large orders, a trader’s activity can shift the market price, leading to slippage – the difference between the expected price of a trade and the price at which the trade is actually executed.
#
# * **Best Practices**
#
#     * Size Matters: The larger the order relative to the market volume, the greater the potential market impact. Factor this into strategies involving large trades.
#     * Modeling Market Impact: Utilize market impact models, like the linear cost model - or the square root model, which proposes that the impact is proportional to the square root of the order size.
#     * Adaptive Strategies: Consider strategies that minimize market impact, such as breaking up large orders into smaller ones or using algorithmic execution strategies that adapt to market conditions.
#
# ## Extra Costs:
# If you want to go even further, you can add (**Note**: I don't know many people who added these in a vectorised setting):
# * Borrowing costs for short selling.
# * Swap Rates and Financing Costs for Forex.
#
# ---
#
# ## Replicate the Momentum Strategy
#
# Below we replicate the volatility targeted momentum strategy that we built in the previous tutorial.


# Import libraries
import numpy as np
import pandas as pd
import yfinance as yf
import quantstats as qs

import matplotlib.pyplot as plt


# ## Fetching SPX Adj Close Price Data from Yahoo Finance


# Downloading S&P 500 index data from Yahoo Finance
spx = yf.download('SPY', start='2000-01-01', end='2023-01-01')

# Extracting the closing prices
close_prices = spx['Close']
volume = spx['Volume']


# ##### Visualizing S&P 500 Closing Prices
# Visual representation of data is vital for a better understanding of market trends. Here, we plot the closing prices of the S&P 500 Index.


# Plotting the S&P 500 closing prices
close_prices.plot(figsize=(10, 7), linewidth=1.5)
plt.title('S&P 500 Close Prices (1991-2023)', fontsize=14)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Price (USD)', fontsize=12)
plt.grid(True)
plt.show()


# ### Moving Average Crossover Strategy from Previous Tutorial


# Calculating Moving Averages (Indicators)
short_window = 50
long_window = 252

short_ma = close_prices.rolling(window=short_window, min_periods=short_window).mean()
long_ma = close_prices.rolling(window=long_window, min_periods=long_window).mean()


# Generating Trading Signals
signals = pd.DataFrame(index=close_prices.index)
signals['signal'] = 0.0  # Set the default to be 0
signals['short_ma'] = short_ma
signals['long_ma'] = long_ma

# When the short is above the long, buy - else go short sell.
# Note that our signal is just a simple buy or sell, -1 or 1.
signals['signal'][signals['short_ma'] > signals['long_ma']] = 1
signals['signal'][signals['short_ma'] < signals['long_ma']] = -1


# Set the target volatility level
tgt_vol = 0.15

# Calculate daily percentage returns of SPX
spx_returns = close_prices.pct_change()

# Calculate the rolling annual standard deviation of SPX returns
signals['stdev'] = spx_returns.rolling(22).std() * np.sqrt(252)  # Convert to annualized standard deviation

# Calculate volatility target weights
signals['vol_tgt'] = tgt_vol / signals['stdev']

# Clip weights to a maximum of 1 to avoid leverage
# Here we use 2x leverage to make sure we can hit our volatility target of 15%
signals['vol_tgt'] = signals['vol_tgt'].clip(0, 2)


# Adjust the signal by the volatility target, lagging both to avoid look-ahead bias
vol_signal = signals['signal'].shift(1) * signals['vol_tgt'].shift(1)

# Compute returns for the volatility-targeted strategy
strategy_voltgt_returns = spx_returns * vol_signal


# Plot the cumulative rets (compounded growth)
((strategy_voltgt_returns + 1).cumprod()-1).plot(title='Cum Rets')
plt.ylabel('Total Returns')
plt.show()


# ---
#
# ## Computing Turnover to Calculate Transaction Costs
#
# Since transaction costs arise primarily through trading activities, it's pivotal to focus on turnover to gauge these expenses effectively. Turnover, in this context, refers to the frequency and volume of trades within a portfolio.
#
# Recall how we implemented volatility targeting to determine the percentage of our portfolio to be allocated at any given time? Turnover takes this a step further by measuring the daily fluctuation in our positions. Essentially, it's the rate at which securities are replaced or "turned over" in the portfolio. This metric is calculated as the absolute difference in our position sizes from one day to the next, providing a clear picture of how frequently and significantly we adjust our holdings. Understanding and calculating turnover is crucial as it directly correlates with the frequency of incurred transaction costs, thereby impacting the overall performance of the trading strategy.
#
# **Note:**
# * Shift the turnover by -1 becuase the vol_signal is the position we held at time t, and we need to know the date when the transaction occurred, which was the day before.


turnover = abs(vol_signal.diff())

# Shift by -1 to get the date at which the transaction occurred, which was at close yesterday.
turnover = turnover.shift(-1)

# Plot Turnover
turnover.plot(title='Turnover', figsize=(14, 5))
plt.show()


# ---
# ## Incorporating Brokerage Fees in Backtesting
#
# Brokerage fees, a crucial factor in the profitability of trading strategies, vary widely among brokers and are typically measured in basis points. A basis point is a unit of measure used in finance to describe the percentage change in the value or rate of a financial instrument. One basis point is equivalent to 0.01% (1/100th of a percent) or 0.0001 in decimal form. For instance, if a broker charges 10 basis points as a commission, it means they are charging 0.10% of the trade value.
#
# During my early career at a hedge fund, we typically encountered brokerage fees ranging from 14 to 20 basis points per trade. Such costs are quite variable and heavily depend on factors like the scale of your operations and the nature of your relationship with your broker. Larger funds often negotiate lower fees due to high-volume trading, while retail traders usually face higher rates.
#
# Interestingly, in some scenarios, such as at one of the funds I worked with, trading could actually yield rebates, effectively paying us to trade. This is a prime example of how transaction costs can be highly specific to the organization you're associated with. 
#
# When applying brokerage fees in backtesting, it's vital to:
#
# 1. **Research and Use Accurate Fees:** Determine the specific brokerage fees applicable to your fund or individual trading scenario. This requires understanding the fee structure of your broker and converting these fees into basis points for accurate calculation.
#
# 2. **Consider Your Fund's Scale:** Remember, the larger your fund and the more substantial your trading volume, the lower your negotiated fees might be. Incorporate these negotiated rates into your backtesting model if applicable.
#
# 3. **Account for Retail vs Institutional Differences:** If you are backtesting a strategy as a retail trader, ensure that you use the fee rates that are relevant to retail trading accounts, which are typically higher than those for institutional traders.
#
# 4. **Factor in Rebates if Applicable:** In cases where your trading strategy or volume qualifies for rebates, this should be factored into the cost calculations in your backtest, as it can significantly alter the cost-benefit analysis of your trades.


# Compute brokerage fees
brokerage_fees = 0.0020  # 20 basis points
brokerage = turnover * brokerage_fees

# Plot Brokerage
brokerage.plot(title='Brokerage', figsize=(14, 5))
plt.show()


# Plot Cum Brokerage
(brokerage.cumsum()*100).plot(title='Brokerage', figsize=(14, 5))
plt.ylabel('Brokerage Fees in %')
plt.show()


# #### Impact!
# Wow! We pay almost 50% away to brokerage fees with only 20 basis points.
#
# ---


# ## Accounting for Bid-Ask Spread
#
# Incorporating the bid-ask spread in backtesting is a nuanced decision, often contingent on the nature of the trading strategy. For vectorized backtesting, I usually ommit the bid-ask spread due to its relatively minor impact in this context. However, this element becomes significantly more relevant for intraday trading strategies, where even small fluctuations like bid-ask spreads can materially impact performance.
#
# To effectively account for the bid-ask spread in such strategies, you need access to detailed market data, specifically the best bid and the best ask prices at the time of each transaction. This granularity allows for a more precise simulation of the costs you would encounter in actual trading scenarios.
#
# In our context, we will use the average spread. Referring to a reliable online source such as ETF.com - reveals that the average bid-ask spread for an ETF like SPY (a popular S&P 500 ETF) is approximately 0.000032, which translates to 0.32 basis points. This figure, while seemingly small, can accumulate over numerous transactions, especially in high-frequency trading environments.


# Compute Spread fee
bid_ask_fee =0.000032  # 0.32 basis points
spread = turnover * bid_ask_fee

# Plot Brokerage
spread.plot(title='Spread', figsize=(14, 5))
plt.show()


# Plot Cum Spread
(spread.cumsum()*100).plot(title='Spread', figsize=(14, 5))
plt.ylabel('Fees in %')
plt.show()


# #### Impact ~
# We pay about 0.8% fees over an almost 30 year period.


# ---
# ## Integrating Slippage (Market Impact)
#
# Incorporating slippage, or market impact, in a vectorized backtest is a nuanced process. For simplicity and practicality, I typically use a fixed slippage value per trade. It's important to note, however, that in more sophisticated backtesting frameworks, such as event-driven systems or those using iterative loops, a dynamic approach to slippage is often more accurate. In these cases, the Linear Model is a common choice for estimating slippage.
#
# ### Understanding Slippage
# Slippage represents the discrepancy between the expected price at which a trade is intended to be executed and the actual price at which it is executed. This difference can be significant, especially in fast-moving markets or when dealing with large orders.
#
# ### Key Points:
# - **Simplified Approach:** In vectorized backtesting, due to its aggregate and simplified nature, using a fixed slippage value per trade can be a practical way to approximate market impact without overly complicating the model.
# - **Event-Driven and Iterative Models:** These more detailed backtesting models allow for a more nuanced approach to slippage, accommodating varying market conditions and order sizes.
#
# ### Advanced Techniques:
# - **Beyond the Scope:** While institutional-grade estimation techniques for slippage are available, they often involve complex mathematics and are beyond the scope of this course. However, we remain open to including such advanced content if there's enough student interest.
# - **A Practical Guideline:** An accessible rule of thumb, as highlighted in the AI in Trading course from Udacity, suggests that "when a trade constitutes 1% of the Average Daily Volume, the price tends to shift by about 10 basis points." The transaction cost can then be estimated as the percent change in price multiplied by the amount traded.
#
# This approach to integrating slippage in vectorized backtesting strikes a balance between realism and computational simplicity. It's important for students to understand the limitations of this method and recognize the potential for more complex, dynamic models in advanced trading scenarios. By incorporating even a simplified version of slippage, traders can achieve a more realistic assessment of their strategy's performance.


# Volume on SPY is massive, we are probably never going to be 1% of ADV
daily_value = volume * close_prices
daily_value.plot(title='Value Traded', figsize=(14, 5))
plt.ylabel('USD Traded')
plt.show()


# Lets assume we are going to have a maximum of 1'000'000 USD traded on any day
# Therefor
adv = 1000000 / daily_value
adv.plot(title='Average Daily Value', figsize=(14, 5))
plt.show()


# Here you can see, we never reach that value. 


# Lets use this to compute slippage costs, linearly (This is not the Linear Model)
# If 1% = 10 basis points then every 10 basis points of ADV can be 1 basis point of slippage.
cost = (adv / 0.01) * 0.0010
slippage = cost * turnover

# Plot total slippage
slippage.cumsum().plot(title='Total Slippage', figsize=(14, 5))
plt.ylabel('Slippage')
plt.show()


# **Notes:**
#
# * **Total Slippage**: Is less than 25 basis points for the entire period, assuming a max $1 Million a day transaction.
# * **Observing Initial High Costs:** You might notice that the transaction costs are considerably higher in the early stages. This is attributed to the Average Daily Volume (ADV) being substantially higher before 2004. As the index gains popularity and trading volume increases, these costs tend to decrease.
# * **Approach to Slippage in Vectorized Backtesting:** While vectorized backtesting offers computational efficiency, it's generally advisable to omit detailed slippage calculations from this method. The reason is that vectorized backtesting lacks the granularity needed for accurately simulating slippage. Instead, slippage is more effectively computed within a for-loop or an event-driven backtesting framework, where a more precise model can be applied to each trade individually.
# * **Impact of Liquidity on Slippage:** Be aware that slippage tends to be more pronounced in stocks with lower liquidity. In such cases, even relatively small orders can significantly impact the market price, leading to a larger gap between the expected and executed prices of trades. This factor is particularly important for strategies involving illiquid assets or large trade volumes relative to the asset's trading activity.


# ---
# ## Total Costs


total_costs = slippage + spread + brokerage

# Plot total costs
total_costs.cumsum().plot(title='Total Costs: Slippage + Spread + Brokerage', figsize=(14, 5))
plt.ylabel('Fees')
plt.show()


# ## Before and After Costs


# Plot the cumulative rets (compounded growth)
# Before costs
((strategy_voltgt_returns + 1).cumprod()-1).plot()

# After Costs
after_cost = strategy_voltgt_returns - total_costs 
((after_cost + 1).cumprod()-1).plot(figsize = (12, 8))

# Plot Cum returns
plt.title('Cumulative Returns')
plt.legend(['Before Costs', 'After Costs'])
plt.ylabel('Total Returns')
plt.show()


sr_before = strategy_voltgt_returns.mean() / strategy_voltgt_returns.std() * np.sqrt(252)
sr_after = after_cost.mean() / after_cost.std() * np.sqrt(252)

print(f'Sharpe Ratio Before: {sr_before.round(2)}')
print(f'Sharpe Ratio After: {sr_after.round(2)}')


# **Notes:**
# * Ouch! As you can see, these costs have lowered the sharpe ratio and hurt the total returns.
# * You can always use QuantStats to plot a tearsheet for the two different performances.
#
#
# ---
# ### Conclusion
# Accurately incorporating these transaction costs into backtesting models is essential for a realistic assessment of a strategy’s performance. Underestimating these costs can lead to an over-optimistic evaluation of a strategy, while overestimating them might unduly discard potentially profitable strategies. The key is to strike a balance, using historical data and realistic assumptions to closely mirror the conditions faced in actual trading.



// ---------------------------------------------------

// Vectorized_Backtest_Tutorial.py
// backtest_tutorial/Vectorized_Backtest_Tutorial.py
# Generated from: Vectorized_Backtest_Tutorial.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Vectorized Backtesting Tutorial
# *By: Jacques Francois Joubert*
#
# This Jupyter Notebook provides a tutorial on conducting a vectorized backtest using a simple moving average crossover strategy on the S&P 500 Index (SPX) close price data. We'll use data from Yahoo Finance and leverage QuantStats for performance analysis.


# Import libraries
import numpy as np
import pandas as pd
import yfinance as yf
import quantstats as qs

import matplotlib.pyplot as plt


# #### Trouble?
# If you need help to install the dependencies, then open the markdown file: getting_started.md


# ---
# ## Fetching SPX Adj Close Price Data from Yahoo Finance
# To access historical data for the S&P 500 Index (^GSPC), we will use the yfinance library, a powerful tool for downloading financial data from Yahoo Finance.
#
# ##### Steps for Data Retrieval:
# 1. **Specify the Ticker Symbol**: The S&P 500 Index is represented by the ticker symbol ^GSPC.
# 2. **Define the Time Period**: We set the start date to '1991-01-01' and the end date to '2023-01-01' to retrieve data over this period.
# 3. **Fetch the Data**: We use yf.download() to download the data and extract the Close prices.


# Downloading S&P 500 index data from Yahoo Finance
spx = yf.download('^GSPC', start='1991-01-01', end='2023-01-01')

# Extracting the closing prices
close_prices = spx['Close']


# ##### Visualizing S&P 500 Closing Prices
# Visual representation of data is vital for a better understanding of market trends. Here, we plot the closing prices of the S&P 500 Index.


# Plotting the S&P 500 closing prices
close_prices.plot(figsize=(10, 7), linewidth=1.5)
plt.title('S&P 500 Close Prices (1991-2023)', fontsize=14)
plt.xlabel('Year', fontsize=12)
plt.ylabel('Price (USD)', fontsize=12)
plt.grid(True)
plt.show()


# ##### Key Features of the Plot:
#
# * Figure Size: figsize=(10, 7) sets an appropriate size for the plot, ensuring clarity and readability.
# * Styling: The color and linewidth enhance the visual appeal and emphasis on the plot line.
# * Title and Labels: Descriptive title and axis labels are added for better comprehension. Font sizes are adjusted for better visibility.
# * Gridlines: Gridlines are enabled to aid in reading off specific values from the chart.


# ---
# ## Step 1: Developing Robust Indicators
#
# ### The Foundation of Trading Signals: Understanding Indicators
# In the architecture of algorithmic trading, the role of an 'indicator' is foundational. Indicators serve as the building blocks or raw inputs from which trading signals are constructed. They provide essential data or insights, which, when processed and analyzed, contribute to the formation of predictive trading signals.
#
# #### Exploring the Diversity of Indicators:
# * **Broad Spectrum of Sources**: Indicators can originate from a wide array of sources. While they often include technical metrics, their scope extends far beyond. For example:
#     * **Technical Indicators**: These are derived from statistical calculations based on historical trading data, such as price and volume. Common examples include the Relative Strength Index (RSI), Moving Averages, or Bollinger Bands. We recommend to limit use of these indicators. There are 2 sources here that are known to work: momentum and short term reversals.
#
#     * **Alternative Indicators**: These encompass a range of data points from various fields. A quintessential example would be weather data used in energy market trading. For instance, extremely cold temperatures might increase the demand for heating, thereby impacting energy prices. Quant traders will construct signals based on indicators created from alternative data sources, such as NLP applied to twitter data, or satellite images of cars in a wallmart parking.
#
# #### Characteristics of Indicators:
# * Descriptive, Not Predictive: It’s important to distinguish that, in isolation, indicators typically lack direct predictive power. They are descriptive metrics that represent current or historical states or trends.
#
# * The Raw Material for Signals: Think of indicators as the raw material that, when refined through analysis and combined with other data, transforms into a signal. This transformation often involves integrating multiple indicators, applying statistical methods, or feeding them into machine learning models to distill actionable insights.
#
# #### Crafting Effective Indicators:
# * **Relevance and Accuracy**: The effectiveness of an indicator lies in its relevance to the market being analyzed and the accuracy of the data it represents. Irrelevant or inaccurate indicators can lead to misleading signals.
# * **Diverse Data for Robustness**: Employing a variety of indicators, ranging from market data to economic factors, enhances the robustness of your trading strategy. This diversity can help capture different aspects of market dynamics, leading to more comprehensive and reliable signals.
#
# Developing indicators is a crucial step in the process of creating reliable trading signals. While they are not predictive in themselves, their role in providing critical information and insights cannot be understated. The art lies in selecting the right indicators, interpreting them correctly, and skillfully integrating them into a cohesive strategy that informs profitable trading decisions.


# ### Our example:
# #### Moving Average Crossover Strategy Logic
# The strategy involves two moving averages (indicators):
# - A short-term moving average (e.g., 50 days)
# - A long-term moving average (e.g., 200 days)
#
# We generate a buy signal when the short-term moving average crosses above the long-term moving average and a sell signal when it crosses below. This is just a toy exampe to illustrate how to run a backtest - don't use this in real life!


# Calculating Moving Averages (Indicators)
short_window = 50
long_window = 252

short_ma = close_prices.rolling(window=short_window, min_periods=short_window).mean()
long_ma = close_prices.rolling(window=long_window, min_periods=long_window).mean()


# ## Step 2: Crafting Effective Trading Signals
# In the realm of algorithmic trading, the concept of a 'signal' is pivotal and distinct from mere 'indicators'. A trading signal is essentially a prognostic tool, possessing predictive power about future market movements. Unlike indicators, which primarily describe current or past market conditions, signals provide actionable insights for future trades.
#
# ### Understanding the Nature of Signals:
# * Predictive Strength: Signals range from -1 to 1, representing the confidence level of a prediction. The closer a signal is to the extremities (-1 or 1), the higher the confidence in the forecast.
#
# Interpreting Signal Values:
# * **Negative Values (-1 to -0.001)**: Indicate a bearish outlook, suggesting a potential decrease in the asset's value. In practical terms, this range implies taking a 'short' position.
# * **Positive Values (0.001 to 1)**: Suggest a bullish outlook, pointing towards an anticipated increase in the asset's value. Here, the strategy would lean towards a 'long' position.
# * **Values Around 0**: Signal a state of uncertainty or neutrality in the model's prediction. A signal close to 0 implies that the model lacks enough conviction to suggest a clear market direction.
#
# ### Practical Tip for Signal Generation:
# * Leveraging the TanH Function: In machine learning models, employing the hyperbolic tangent (TanH) function is a common practice to normalize output signals to the range of [-1, 1]. This normalization helps in standardizing the prediction output, making it interpretable in the context of trading signals.
#
# Additionally, this technique isn't confined to machine learning. It can be equally effective in conventional trading algorithms for transforming raw prediction values into standardized trading signals.
#
# ### The Role of Signals in Trade Decision Making:
# It's crucial to understand that while signals guide the direction of the trade (long or short), they don't dictate the position size or the risk management strategy. The strength of the signal should be used in conjunction with other factors, such as portfolio balance, risk tolerance, and market conditions, to make informed trading decisions.
#
# In essence, a well-constructed signal provides a clear directional bias for trading, but it's just one piece of the puzzle in the broader strategy of trade execution and risk management. Integrating signals with a robust trading system ensures that both market forecasts and risk are optimally balanced.
#
# ## Our example
# Our signal is just a simple buy or sell, -1 or 1. If we have positive momentum then buy, else sell.
#
# * Note that we will use the raw signal as the weight of the position. 


# Generating Trading Signals
signals = pd.DataFrame(index=close_prices.index)
signals['signal'] = 0.0  # Set the default to be 0
signals['short_ma'] = short_ma
signals['long_ma'] = long_ma

# When the short is above the long, buy - else go short sell.
# Note that our signal is just a simple buy or sell, -1 or 1.
signals['signal'][signals['short_ma'] > signals['long_ma']] = 1
signals['signal'][signals['short_ma'] < signals['long_ma']] = -1


# ## Step 3: Calculating Daily Returns for the Strategy
#
# Proper calculation of returns is essential for evaluating the performance of any trading strategy. In our approach, we take special care to account for look-ahead bias, a common pitfall in strategy development.
#
# Addressing Look-Ahead Bias:
# * **Look-Ahead Bias**: This occurs when a strategy uses information that would not have been available at the time of trading. To avoid this, we lag our signals by one day. This adjustment ensures that our strategy only uses information that would have been available at the market close of the previous day.
#
# ### Implementation: Computing Strategy Returns
#
# 1. **Calculating Base Market Returns**: First, we compute the daily returns of the S&P 500 index (or spx). Daily returns are calculated based on the percentage change in closing prices from one day to the next.


# Calculate daily percentage returns of SPX
spx_returns = close_prices.pct_change()


# 2. **Applying the Strategy**: Next, we apply our trading signals to these returns. The signal is shifted by one day (shift(1)) to avoid look-ahead bias. This means that the trading decision for a given day is based on the signal from the previous day.


# Calculate strategy returns by multiplying SPX returns with the lagged signals
strategy_returns = spx_returns * signals['signal'].shift(1)


# #### Explanation:
# * Multiplying Returns by Signals: The `strategy_returns` are the product of the `spx_returns` and the lagged signals. Here, a positive signal indicates a long position, and a negative signal indicates a short position. A zero signal implies no position, resulting in zero returns for that day.
#
# * Lagged Signals: By using `shift(1)`, we ensure that the trading decision for any given day is based solely on information available up to the end of the previous day, thereby realistically simulating a trading scenario and maintaining the integrity of our backtest.
#
# By accurately computing the strategy's daily returns and conscientiously addressing look-ahead bias, we lay a solid foundation for a realistic and reliable evaluation of the strategy's performance. This methodological rigor is crucial for developing strategies that not only perform well historically but also hold up in live trading scenarios.


# ## Step 4: Position Sizing
#
# Position sizing is a critical aspect of trading strategy that can significantly impact the risk and return profile of an investment portfolio.
#
# ### Exploring Position Sizing Techniques
# * **Field of Study**: Position sizing is an extensive field of study under portfolio construction. It involves determining the appropriate amount of capital to allocate to each investment, balancing risk and potential returns.
#
# * **Recommended Learning Paths**:
#
#     * **Mean-Variance Portfolio Construction**: This foundational concept in modern portfolio theory helps in optimizing the allocation of weights to different assets, based on their expected returns, risks (variance), and correlations.
#     * **Meta-Labeling**: An advanced technique that integrates machine learning. Meta-labeling not only decides the direction of trades (long or short) but also helps in determining the size of positions, based on the probability of a successful prediction.
#
# For this course, we show you how to backtest a strategy where you forecast a single asset, for this we recommend learning more about volatility targeting, and the Kelly Criterion. There are lectures and papers covered in this course.
#
# ### Implementing Volatility-Targeted Position Sizing
# ##### Computing Volatility-Targeted Weights
# We aim to implement a strategy where each position is sized according to a target volatility level. This approach helps in standardizing risk across different trades.


# Set the target volatility level
tgt_vol = 0.15

# Calculate the rolling annual standard deviation of SPX returns
signals['stdev'] = spx_returns.rolling(22).std() * np.sqrt(252)  # Convert to annualized standard deviation

# Calculate volatility target weights
signals['vol_tgt'] = tgt_vol / signals['stdev']

# Clip weights to a maximum of 1 to avoid leverage
# Here we use 2x leverage to make sure we can hit our volatility target of 15%
signals['vol_tgt'] = signals['vol_tgt'].clip(0, 2)


# ##### Calculating Strategy Returns with Volatility Targeting
# We then apply these volatility-targeted weights to our strategy, adjusting the signal strength based on the desired level of risk exposure.


# Adjust the signal by the volatility target, lagging both to avoid look-ahead bias
vol_signal = signals['signal'].shift(1) * signals['vol_tgt'].shift(1)

# Compute returns for the volatility-targeted strategy
strategy_voltgt_returns = spx_returns * vol_signal


# Plot the position sizes to see how much leverage is being used.
vol_signal.plot()
plt.title('Position Size: > 1 is using leverage')
plt.ylabel('Position Size')
plt.show()


# #### Key Takeaways
# * **Risk Standardization**: Volatility-targeted position sizing helps in standardizing risk across different positions, aligning each trade with the overall risk tolerance of the portfolio.
# * **Dynamic Adjustment**: This approach dynamically adjusts position sizes based on changing market volatility, contributing to more effective risk management.
# * **Limiting Leverage**: By clipping the maximum weight to 1, the strategy avoids the use of leverage, sticking to a more conservative investment approach.
#
# Mastering position sizing is crucial for the successful implementation of a trading strategy. By aligning each position with a predefined risk parameter, traders can achieve a more balanced and risk-aware portfolio. The techniques of mean-variance optimization and meta-labeling, along with practical implementations like volatility targeting, provide a solid foundation for effective portfolio management.


# ---
# ## Comprehensive Performance Analysis with QuantStats
#
# Next we utilize the powerful capabilities of QuantStats to conduct a thorough performance analysis. Our objective is to compare and contrast the results of two distinct strategies: the baseline strategy (which serves as our benchmark) and the strategy incorporating volatility-targeted weights.
#
# QuantStats is a Python library specifically designed for the performance analysis of financial portfolios and trading strategies. It provides a suite of tools for generating comprehensive reports, known as performance tearsheets, that offer in-depth insights into the strategy's effectiveness.
#
# To execute this comparative analysis, we will utilize QuantStats' full report functionality. This method generates a detailed report that encompasses various performance metrics and visualizations, offering a holistic view of the strategy's performance.
#
# ### Plotting the Results:
# Here's how we implement the QuantStats report in our notebook:
#
# ##### Parameters Explained:
# * `returns`: The strategy_voltgt_returns represents the returns of our strategy that applies volatility-targeted weights.
# * `benchmark`: The strategy_returns acts as the baseline for comparison. It represents the returns of our basic strategy without volatility adjustments.
# * `output`: Specifies the output format of the report. Here, 'report.html' indicates that the report will be generated as an HTML file.
# * `title`: Gives a title to the report. In this case, 'Volatility Targeted Strategy vs. Basic Strategy' succinctly describes the comparative nature of the analysis.


qs.reports.full(returns=strategy_voltgt_returns, 
                benchmark=strategy_returns,
                output='report.html', 
                title='Volatility Targeted Strategy vs. Basic Strategy')


# ## Troubleshooting
#
# From some experiments that I ran, it looks like QuantStats is having problems with the latest version of the plotting libraries. This is a common problem with python libraries. 
#
# I noted that in the monthly active returns plot, those values don't look correct. The rest looks pretty good. 


# # Conclusion
# The integration of QuantStats into our analysis workflow empowers us to conduct a nuanced and comprehensive evaluation of our trading strategies. This step is crucial in understanding the real-world applicability of our theoretical constructs and in making informed decisions to refine and optimize our trading approaches.
#
# * In this tutorial, we successfully implemented and backtested a moving average crossover strategy on SPX close price data. 
# * We added a volatility-Targeted weighting scheme.
# * The QuantStats library provided comprehensive performance metrics for our strategy.



// ---------------------------------------------------

// multivariate_cointegration.py
// arbitrage_research_basic_PCA/Cointegration Approach/multivariate_cointegration.py
# Generated from: multivariate_cointegration.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# References: 
#
# - [Galenko, A., Popova, E., and Popova, I. (2012). **Trading in the presence of cointegration.** *The Journal of Alternative Investments*, 15(1):85–97.](http://www.ntuzov.com/Nik_Site/Niks_files/Research/papers/stat_arb/Galenko_2007.pdf)


# # Multivariate Cointegration Framework


# ## Introduction


# The cointegration relations between time series imply that the time series are bound together. Over time the time series
# might drift apart for a short period of time, but they ought to re-converge. A trading strategy on $N \: (N \geq 3)$ cointegrated assets that have a positive expectation of profit can be designed based on this property. 
#
# In this notebook, the trading strategy will be demonstrated, and an empirical example of applying this strategy to four European stock indices will be given.


# ## Multivariate Cointegration


# Cointegration is defined by the stochastic relationships among the asset log returns.
#
# Let $P_i$, where $i = 1, 2, \ldots, N$ denote the price of $N$ assets. The continuously compounded asset
# returns, i.e. log-returns at time $t > 0$ can be written as:
#
# \begin{equation*}
# r_t^i = \ln{P_t^i} - \ln{P_{t-1}^i}
# \end{equation*}
#
# Now construct a process $Y_t$ as a linear combination of the $N$ asset prices:
#
# \begin{equation*}
# Y_t = \sum_{i=1}^N b^i \ln{P_t^i}
# \end{equation*}
#
# where $b^i$ denotes the $i$-th element for a finite vector $\mathbf{b}$.
#
# The corresponding asset returns series $Z_t$ can be defined as:
#
# \begin{equation*}
# Z_t = Y_t - Y_{t-1} = \sum_{i=1}^N b^i r_t^i
# \end{equation*}
#
# Assume that the memory of the process $Y_t$ does not extend into the infinite past, which can be expressed as the
# following expression in terms of the autocovariance of the process $Y_t$:
#
# \begin{equation*}
# \lim_{p \to \infty} \text{Cov} \lbrack Y_t, Y_{t-p} \rbrack = 0
# \end{equation*} 
#
# Then the **log-price** process $Y_t$ is stationary, if and only if the following three conditions on
# **log-returns** process $Z_t$ are satisfied:
#
# \begin{gather*}
# E[Z_t] = 0 \\
# \text{Var }Z_t = -2 \sum_{p=1}^{\infty} \text{Cov} \lbrack Z_t, Z_{t-p} \rbrack \\
# \sum_{p=1}^{\infty} p \text{ Cov} \lbrack Z_t, Z_{t-p} \rbrack < \infty
# \end{gather*}
#
# When $Y_t$ is stationary, the log-price series of the assets are cointegrated.
#
# For equity markets, the log-returns time series can be assumed as stationary and thus satisfy the above conditions.
# Therefore, when it comes to empirical applications, the Johansen test could be directly applied to the log price series
# to derive the vector $\mathbf{b}$.


# ## Trading Strategy


# The core idea of the strategy is to bet on the spread formed by the cointegrated $N$ assets that have gone apart
# but are expected to mean revert in the future.
#
# The trading strategy, using the notation in the above section, can be described as follows.
#
# 1. Estimate the cointegration vector $\hat{\mathbf{b}}$ with Johansen test using training data.
# 2. Construct the realization $\hat{Y}_t$ of the process $Y_t$ by calculating $\hat{\mathbf{b}}^T \ln P_t$, and calculate $\hat{Z}_t = \hat{Y}_t - \hat{Y}_{t-1}$.
# 3. Compute the finite sum $\sum_{p=1}^P \hat{Z}_{t-p}$, where the lag $P$ is an input argument.
# 4. Partition the assets into two sets $L$ and $S$ according to the sign of the element in the cointegration vector $\hat{\mathbf{b}}$. For each asset $i$,
#
# \begin{eqnarray*}
# i \in L \iff b^i \geq 0 \\
# i \in S \iff b^i < 0
# \end{eqnarray*}
#
# 5. Following the formulae below, calculate the number of assets to trade so that the notional of the positions would equal to $C$.
#
# \begin{eqnarray*}
#     \Bigg \lfloor \frac{-b^i C \text{ sgn} \bigg( \sum_{p=1}^{P} Z_{t-p} \bigg)}{P_t^i \sum_{j \in L} b^j} \Bigg \rfloor, \: i \in L \\
#     \Bigg \lfloor \frac{b^i C \text{ sgn} \bigg( \sum_{p=1}^{P} Z_{t-p} \bigg)}{P_t^i \sum_{j \in L} b^j} \Bigg \rfloor, \: i \in S
# \end{eqnarray*}
#
# 6. Open the positions on time $t$ and close the positions on time $t+1$.
# 7. Update the training data with the closing price.
# 8. The cointegration vector will be re-estimated monthly (22 trading days). If it is time for a re-estimate, go to step 1; otherwise, go to step 2.
#
# The strategy is trading at daily frequency and always in the market.


# ## Usage of the Module


# In this section, the usage of multivariate cointegration trading strategy will be demonstrated with an empirical example of four European stock indices, i.e. DAX (Germany), CAC 40 (France), FTSE 100 (UK), and AEX (Netherlands). Price history from Jan 2nd, 1996 to Dec 28th, 2006 was used. The module allows two missing data imputation methods: forward-fill and polynomial spline. In the following demonstration, missing data due to the difference in working days in different countries was imputed with a forward-fill method in order to avoid the introduction of phantom returns on non-trading days.
#
# Trading for out-of-sample tests starts on Nov 6th, 2001 and ends on Dec 28th, 2006. The cointegration vector $\mathbf{b}$ was re-estimated monthly (every 22 trading days) using the Johansen test. The notional value of the long positions and short positions each day was set to $\$10 \text{M}$, respectively. To be specific, each day $\$10 \text{M}$ were invested in longs and another $\$10 \text{M}$ were invested in shorts, resulting in a $\$20 \text{M}$ portfolio.


%matplotlib inline


# Importing libraries
import pandas as pd
import numpy as np

from arbitragelab.cointegration_approach.multi_coint import MultivariateCointegration


# Loading data
euro_stocks_df = pd.read_csv("multi_coint.csv", parse_dates=['Date'])
euro_stocks_df.set_index("Date", inplace=True)

# Out-of-sample data split time point
split_point = pd.Timestamp(2001, 11, 6)

# Indexing with DateTimeIndex is always inclusive. Removing the last data point in the training data
train_df = euro_stocks_df.loc[:split_point].iloc[:-1]
trade_df = euro_stocks_df.loc[split_point:]


# Checking train data
train_df.tail()


# Checking test data
trade_df.head()


# ### Generate Trading Signal and Equity Curve


# In this section, a few experiments will be done to showcase the following:
#
# - In-sample test of the strategy with a rolling window.
# - In-sample test of the strategy with a cumulative window.
# - Out-of-sample test of the strategy with a rolling window.
# - Out-of-sample test of the strategy with a cumulative window.
#
# A lag of 30 days will be used across four experiments to calculate the signals.
#
# For the in-sample test, the cointegration vector will be estimated with all available data (including training and test data) and will not be updated monthly. 
#
# For the out-of-sample test, since the cointegration vector will be re-estimated every month, the time evolution of the cointegration vector will be shown as well as the number of shares to trade. We gave two examples, one of which displays the raw number of shares, the other the weight of portfolio notional value. Positive values indicate long positions, while negative values indicate short positions.
#
# The equity curve and the statistics of the returns are shown for each experiment.


# Initializing the trading signal generator
multi_coint_signal = MultivariateCointegration(train_df, trade_df)

# Imputing all missing values
multi_coint_signal.fillna_inplace(nan_method='ffill')


# ### 1) In-sample, rolling window of 1500 days


# Generating the signal, recording the cointegration vector time evolution and calculating portfolio returns
signal, _, coint_vec, port_returns = multi_coint_signal.trading_signal(nlags=30, rolling_window_size=1500, insample=True)


# Plotting the equity curve
plot = multi_coint_signal.plot_returns(port_returns, start_date=pd.Timestamp(2001, 11, 6), end_date=pd.Timestamp(2007, 1, 2),
                                       figw=15, figh=10, title="In-sample Test, 1500-day Rolling Window")


# Returns statistics
multi_coint_signal.summary(port_returns)


# ### 2) In-sample, cumulative window


# Generating the signal, recording the cointegration vector time evolution and calculating portfolio returns
signal, _, coint_vec, port_returns = multi_coint_signal.trading_signal(nlags=30, rolling_window_size=None, insample=True)


# Plotting the equity curve
plot = multi_coint_signal.plot_returns(port_returns, start_date=pd.Timestamp(2001, 11, 6), end_date=pd.Timestamp(2007, 1, 2),
                                       figw=15, figh=10, title="In-sample Test, Cumulative Window")


# Returns statistics
multi_coint_signal.summary(port_returns)


# ### 3) Out-of-sample, rolling window of 1500 days


# Generating the signal, recording the cointegration vector time evolution and calculating portfolio returns
signal, signal_ntn,  coint_vec, port_returns = multi_coint_signal.trading_signal(nlags=30, rolling_window_size=1500, 
                                                                                 insample=False)


# An example of trading signals in notional value will look as follows:


signal_ntn.head()


# Display the weight of portfolio notional value to invest for each equity index. The plot function will automatically normalize the weights with respect to the sum of long notional and short notional value.


# Plotting the equity curve, cointegration vectors, and trading signals
plot = multi_coint_signal.plot_all(signal, signal_ntn, coint_vec, port_returns,
                                   start_date=pd.Timestamp(2001, 11, 6), end_date=pd.Timestamp(2007, 1, 2),
                                   title="Out-of-sample Test, 1500-day Rolling Window", use_weights=True)


# Returns statistics
multi_coint_signal.summary(port_returns)


# ### 4) Out-of-sample, cumulative window


# Generating the signal, recording the cointegration vector time evolution and calculate portfolio returns
signal, signal_ntn, coint_vec, port_returns = multi_coint_signal.trading_signal(nlags=30, rolling_window_size=None, 
                                                                                insample=False)


# Display the raw number of shares to trade.


signal.head()


# Plot the equity curve, cointegration vectors, and trading signals
plot = multi_coint_signal.plot_all(signal, signal_ntn, coint_vec, port_returns,
                                   start_date=pd.Timestamp(2001, 11, 6), end_date=pd.Timestamp(2007, 1, 2),
                                   title="Out-of-sample Test, 1500-day Rolling Window")


# Returns statistics
multi_coint_signal.summary(port_returns)


# ## Discussion


# The result of the above experiments has shown that the rolling window setup is better than the cumulative window setup. Also, re-estimating the cointegration vector monthly improves the performance of the strategy, as the out-of-sample test outperformed. Both of these results suggest that it is better to exclude further history when estimating the cointegration vector, as the cointegration relationship between the $N$ assets are time-varying. It also provides circumstantial evidence that the following assumptions of the model are reasonable:
#
# \begin{eqnarray*}
# \lim_{p \to \infty} \text{Cov} \lbrack Y_t, Y_{t-p} \rbrack = 0 \\
# \sum_{p=1}^{\infty} p \text{ Cov} \lbrack Z_t, Z_{t-p} \rbrack < \infty
# \end{eqnarray*}
#
# These two assumptions indicate that long-term memory for the cointegrated assets will be almost non-existent. 
#
# However, this trading strategy also has its limitations. Since the index value of AEX is much smaller than DAX, FTSE, and CAC 40, the number of AEX shares/contracts that need to be traded is much larger than its counterpart. Therefore, when the prices of the assets are different in the order of magnitude, it is better to double-check the position limit before trading the strategy.


# ## Conclusion


# This notebook demonstrated a profitable trading strategy using the properties of cointegration among $N$ assets. An empirical example of trading four European stock indices (AEX, DAX, FTSE, and CAC) has shown that: using a lookback of 30 trading days and re-estimating cointegration vector monthly with a 1500-day rolling window, the trading strategy was able to yield a cumulative return of 42.7%, the returns distribution of the strategy did not have significant skewness and kurtosis, and the strategy achieved a Sharpe ratio of 1.31 and a Sortino ratio of 1.85.
#
# ### Key Takeaways
#
# - The cointegration relation can be defined by the properties of compounded returns rather than asset prices.
# - It is possible to trade a strategy that has positive profit expectancy based on this cointegration relation of $N$ assets.


# ## Reference


# 1. [Galenko, A., Popova, E. and Popova, I., 2012. Trading in the presence of cointegration. The Journal of Alternative Investments, 15(1), pp.85-97.](http://www.ntuzov.com/Nik_Site/Niks_files/Research/papers/stat_arb/Galenko_2007.pdf)



// ---------------------------------------------------

// trading_simulation.py
// arbitrage_research_basic_PCA/Cointegration Approach/trading_simulation.py
# Copyright 2019, Hudson and Thames Quantitative Research
# All rights reserved
# Read more: https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/additional_information/license.html

# pylint: disable=invalid-name
"""
This module simulates trading based on the minimum profit trading signal, reports the trades,
and plots the equity curve.
"""

from typing import Optional, Tuple

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


class TradingSim:
    """
    This class simulates the trades based on an optimized minimum profit trading signal.
    It plots the trading signal on the cointegration error and the equity curves as well.
    """

    def __init__(self, starting_equity: float = np.Inf):
        """
        Setting up a new trading account for simulating the trading strategy.

        :param starting_equity: (float) The amount available to trade in this simulation.
        """

        # Set position and P&L to 0, fund the account with the dollar amount specified
        self._position = np.zeros((2, ))
        self._base_equity_value = starting_equity
        self._total_trades = np.zeros((2,))
        self._report = dict()

        # Record mark-to-market P&L everyday to give a proper view of drawdowns during the trades
        self._pnl = 0.
        self._mtm = None

    def initialize_report(self):
        """
        Initialize the dictionary for trade reports.
        """

        # Dictionary for generating the equity curve and trade report DataFrame
        self._report = {
            "Trade Date": [],
            "Trade Type": [],
            "Leg 1": [],
            "Leg 1 Shares": [],
            "Leg 1 Price": [],
            "Leg 2": [],
            "Leg 2 Shares": [],
            "Leg 2 Price": []}

        self._mtm = {
            "P&L": [],
            "Total Equity": []}

        self._position = np.zeros((2,))
        self._pnl = 0.

    def _trade(self, signals: pd.DataFrame, num_of_shares: np.array):
        """
        Trade the cointegrated pairs based on the optimized signal.

        :param signals: (pd.DataFrame) Dataframe that contains asset prices and trade signals.
        :param num_of_shares: (np.array) Optimized number of shares to trade.
        """

        # Generate report
        self.initialize_report()

        # Trading periods in the trade_df
        period = len(signals)

        # Add a flag to let the simulator know if a U-trade is currently open or a L-trade
        current_trade = 0

        # Start trading
        entry_price = np.zeros((2, ))

        for i in range(period):
            current_price = signals.iloc[i, [0, 1]].values

            # Check mark-to-market P&L
            trade_pnl = np.dot(current_price - entry_price, self._position)

            # Record mark-to-market P&L
            self._mtm['P&L'].append(trade_pnl)
            self._mtm['Total Equity'].append(self._base_equity_value + trade_pnl)

            if current_trade == 0:
                # No position, and the opening trade condition is satisfied
                # Before opening the trade, check if the dollar constraint allows us to open
                capital_req = np.dot(current_price, num_of_shares)

                # Capital requirement satisfied, open the position
                if capital_req <= self._base_equity_value:
                    # Record the entry price.
                    entry_price = current_price

                    # Do we open a U-trade or L-trade?
                    if signals['otc_U'].iloc[i]:
                        # U-trade, short share S1, long share S2
                        self._report['Trade Type'].append("U-trade Open")
                        self._position = num_of_shares * np.array([-1, 1])
                        current_trade = 1

                    elif signals['otc_L'].iloc[i]:
                        # L-trade, long share S1, short share S2
                        self._report['Trade Type'].append("L-trade Open")
                        self._position = num_of_shares * np.array([1, -1])
                        current_trade = -1
                    else:
                        # No opening condition met, forward to next day
                        continue

                    # Bookkeeping
                    self._report['Trade Date'].append(signals.index[i].date())
                    self._report['Leg 1'].append(signals.columns[0])
                    self._report['Leg 2'].append(signals.columns[1])
                    self._report['Leg 1 Price'].append(entry_price[0])
                    self._report['Leg 2 Price'].append(entry_price[1])
                    self._report['Leg 1 Shares'].append(self._position[0])
                    self._report['Leg 2 Shares'].append(self._position[1])

                # Make sure the trade will not be closed on the same day (using elif)

            else:
                # We have a trade on
                if current_trade == 1 and signals['ctc_U'].iloc[i]:
                    # The open trade is a U-trade
                    self._report['Trade Type'].append("U-trade Close")
                    self._total_trades[0] += 1

                elif current_trade == -1 and signals['ctc_L'].iloc[i]:
                    # The open trade is a L-trade
                    self._report['Trade Type'].append("L-trade Close")
                    self._total_trades[1] += 1

                else:
                    # No condition triggered, just forward to next day
                    continue

                # Bookkeeping
                self._report['Trade Date'].append(signals.index[i].date())
                self._report['Leg 1'].append(signals.columns[0])
                self._report['Leg 2'].append(signals.columns[1])
                self._report['Leg 1 Price'].append(current_price[0])
                self._report['Leg 2 Price'].append(current_price[1])
                self._report['Leg 1 Shares'].append(-1 * self._position[0])
                self._report['Leg 2 Shares'].append(-1 * self._position[1])

                # Add the final profit to base equity value.

                self._base_equity_value += trade_pnl

                # Clear the trade book.
                current_trade = 0
                entry_price = np.zeros((2,))
                self._position = np.zeros((2,))

                # Will not close the trade on the same day (using elif)

    def summary(self, signals: pd.DataFrame, num_of_shares: np.array) -> pd.DataFrame:
        """
        Trade the strategy and generate the trade reports.

        :param signals: (pd.DataFrame) Dataframe that contains asset prices and trade signals.
        :param num_of_shares: (np.array) Optimized number of shares to trade.
        :return (pd.DataFrame, np.array): A dataframe that contains each opening/closing trade details, P&L,
            and equity curves; a NumPy array that represents the number of U-trade and L-trade over the period
        """

        self._trade(signals, num_of_shares)
        report_df = pd.DataFrame(self._report)
        return report_df

    def get_pnl(self, signals: pd.DataFrame) -> pd.DataFrame:
        """
        Retrieve the daily mark-to-market P&L of trading the strategy.

        :param signals: (pd.DataFrame) Dataframe that contains the trading signal.
        :return: (pd.DataFrame) Dataframe that contains the mark-to-market P&L every trading day.
        """

        # Build the P&L dataframe
        pnl_df = pd.DataFrame(self._mtm)

        # Set the date index
        pnl_df.index = signals.index
        return pnl_df

    def _plot_signals(self, signals: pd.DataFrame, num_of_shares: np.array, cond_lines: np.array,
                      figw: float = 15, figh: float = 10, start_date: Optional[pd.Timestamp] = None,
                      end_date: Optional[pd.Timestamp] = None) -> plt.Figure:
        """
        Plot the spread and the signals.

        :param signals: (pd.DataFrame) Dataframe that contains the trading signal.
        :param num_of_shares: (np.array) Numpy array that contains the number of shares.
        :param cond_lines: (np.array) Numpy array that contains the trade initiation/close signal line.
        :param figw: (float) Figure width.
        :param figh: (float) Figure height.
        :param start_date: (pd.Timestamp) The starting point of the plot.
        :param end_date: (pd.Timestamp) The end point of the plot.
        :return: (plt.Figure) The object of the plot.
        """

        # Retrieve the report
        report_df = pd.DataFrame(self._report)

        # Define the ticks on the x-axis
        years = mdates.YearLocator()  # every year
        months = mdates.MonthLocator()  # every month
        years_fmt = mdates.DateFormatter('\n%Y')
        months_fmt = mdates.DateFormatter('%b')

        # Plot the price action of each leg as well as the cointegration error
        fig, axes = plt.subplots(2, 1, sharex=True, figsize=(figw, figh), gridspec_kw={'height_ratios': [2.5, 1]})

        # Plot prices
        axes[0].plot(signals.iloc[:, 0], label=signals.columns[0])
        axes[0].plot(signals.iloc[:, 1], label=signals.columns[1])
        axes[0].legend(loc='upper left', fontsize=12)
        axes[0].tick_params(axis='y', labelsize=14)

        # Plot cointegration error
        axes[1].plot(signals.iloc[:, 2], label='spread')
        axes[1].legend(loc='best', fontsize=12)

        # Plot signal lines
        axes[1].axhline(y=cond_lines[1], color='black')  # Closing condition
        axes[1].axhline(y=cond_lines[0], color='red')  # L-trade opens
        axes[1].axhline(y=cond_lines[2], color='green')  # U-trade opens

        # Formatting the tick labels
        axes[1].xaxis.set_major_locator(years)
        axes[1].xaxis.set_major_formatter(years_fmt)
        axes[1].xaxis.set_minor_locator(months)
        axes[1].xaxis.set_minor_formatter(months_fmt)
        axes[1].tick_params(axis='x', labelsize=14)
        axes[1].tick_params(axis='y', labelsize=14)

        # Define the date range of the plot
        if start_date is not None and end_date is not None:
            axes[1].set_xlim((start_date, end_date))

        # Plot arrows for buy and sell signal
        for idx in range(len(report_df)):
            trade_type = report_df.iloc[idx]['Trade Type']
            trade_date = report_df.iloc[idx]['Trade Date']
            arrow_xpos = mdates.date2num(trade_date)
            arrow_ypos = signals.loc[pd.Timestamp(trade_date)]['coint_error']

            # Green arrow for opening U-trade, red arrow for opening L-trade, black arrow for closing the trade.
            if trade_type == "U-trade Open":
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, 15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='green'))
            elif trade_type == "L-trade Open":
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, -15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='red'))
            elif trade_type == "L-trade Close":
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, 15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='black'))
            else:
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, -15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='black'))

        fig.suptitle("Optimal Pre-set Boundaries and Trading Signals", fontsize=20)
        return fig

    def _plot_pnl_curve(self, signal: pd.DataFrame, figw: float = 15., figh: float = 10.,
                        start_date: Optional[pd.Timestamp] = None,
                        end_date: Optional[pd.Timestamp] = None) -> plt.Figure:
        """
        Plot the equity curve (marked to market daily) trading the strategy.

        :param signal: (pd.DataFrame) Dataframe containing the trading signal.
        :param figw: (float) Figure width.
        :param figh: (float) Figure heght.
        :param start_date: (pd.Timestamp) The starting point of the plot.
        :param end_date: (pd.Timestamp) The end point of the plot.
        :return: (plt.Figure) The object of the plot.
        """

        # Build the equity curve dataframe
        equity_curve_df = pd.DataFrame(self._mtm)

        # Set up date index
        equity_curve_df.index = signal.index

        # Define the ticks on the x-axis
        years = mdates.YearLocator()  # every year
        months = mdates.MonthLocator()  # every month
        years_fmt = mdates.DateFormatter('\n%Y')
        months_fmt = mdates.DateFormatter('%b')

        # Plot the equity curve
        fig, ax = plt.subplots(figsize=(figw, figh))
        ax.plot(equity_curve_df['Total Equity'])

        # Formatting the tick labels
        ax.xaxis.set_major_locator(years)
        ax.xaxis.set_major_formatter(years_fmt)
        ax.xaxis.set_minor_locator(months)
        ax.xaxis.set_minor_formatter(months_fmt)
        ax.tick_params(axis='x', labelsize=14)
        ax.tick_params(axis='y', labelsize=14)

        # Define the date range of the plot
        if start_date is not None and end_date is not None:
            ax.set_xlim((start_date, end_date))

        fig.suptitle("P&L Curve of the Trading Strategy", fontsize=20)
        return fig

    def plot_strategy(self, signal: pd.DataFrame, num_of_shares: np.array, cond_lines: np.array,
                      figw: float = 15, figh: float = 10, start_date: Optional[pd.Timestamp] = None,
                      end_date: Optional[pd.Timestamp] = None) -> Tuple[plt.Figure, plt.Figure]:
        """
        Plot the trading signal and the PnL curve of the trading strategy

        :param signal: (pd.DataFrame) Dataframe containing the trading signal.
        :param num_of_shares: (np.array) Numpy array that contains the number of shares.
        :param cond_lines: (np.array) Numpy array that contains the trade initiation/close signal line.
        :param figw: (float) Figure width.
        :param figh: (float) Figure heght.
        :param start_date: (pd.Timestamp) The starting point of the plot.
        :param end_date: (pd.Timestamp) The end point of the plot.
        :return: (plt.Figure, plt.Figure) The object of the trading signal plot; the object of the P&L curve plot.
        """

        # A wrapper function to plot two figures
        fig1 = self._plot_signals(signal, num_of_shares, cond_lines, figw=figw, figh=figh,
                                  start_date=start_date, end_date=end_date)
        fig2 = self._plot_pnl_curve(signal, figw=figw, figh=figh, start_date=start_date, end_date=end_date)

        return fig1, fig2


// ---------------------------------------------------

// sparse_mean-reverting_portfolio_selection.py
// arbitrage_research_basic_PCA/Cointegration Approach/sparse_mean-reverting_portfolio_selection.py
# Generated from: sparse_mean-reverting_portfolio_selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# References:
#
# * [d'Aspremont, A. (2011). Identifying small mean-reverting portfolios. *Quantitative Finance*, 11(3), pp.351-364.](https://arxiv.org/pdf/0708.3048.pdf)
# * [Cuturi, M. and d'Aspremont, A. (2015). Mean-reverting portfolios: Tradeoffs between sparsity and volatility. *arXiv preprint arXiv:1509.05954.*](https://arxiv.org/abs/1509.05954)


# # Sparse Mean-reverting Portfolio Selection


# ## Introduction


# Assets that exhibit significant mean-reversion are difficult to find in efficient markets. As a result, investors focus on creating long-short asset baskets to form a mean-reverting portfolio whose aggregate value shows mean-reversion. Classic solutions including cointegration or canonical correlation analysis can only construct dense mean-reverting portfolios, i.e. they include every asset in the investing universe. These portfolios have shown significant such as higher transaction costs, worse P&L interpretability, and inability to capture meaningful statistical arbitrage opportunities. On the other hand, sparse mean-reverting portfolios, which require trading as few assets as possible, can mitigate these shortcomings.
#
# This research notebook will showcase three approaches to constructing a sparse mean-reverting portfolio using **ArbitrageLab**:
#
# 1. Covariance selection and penalized regression techniques to narrow down the investing universe;
# 2. Greedy search to construct sparse mean-reverting portfolios;
# 3. Semidefinite programming (SDP) approach to construct sparse mean-reverting portfolios above a volatility threshold.
#
# A few key concepts will be introduced first to provide a better understanding of the sparse mean-reverting portfolio selection problem and an explanation to why these three approaches are effective.


# ## Mean-reversion Strength Metrics and Proxies


# ### Ornstein-Uhlenbeck Process and Mean-reversion Speed


# The straightforward solution is to assume the portfolio value follows an Ornstein-Uhlenbeck (OU) process and use the mean-reversion speed parameter $\lambda$ to measure the mean-reversion strength.


# $$
# \begin{gather*}
# dP_t = \lambda (\bar{P} - P_t)dt + \sigma dZ_t \\
# P_t = \mathbf{x}^T S_t \\
# \lambda > 0
# \end{gather*}
# $$
#
# where $P_t$ is the portfolio value at time $t$, $\bar{P}$ is the average portfolio value, $Z_t$ is a standard Brownian motion, $\mathbf{x}$ is the weight vector of each asset in the portfolio, and $S_t$ is the individual asset prices at time $t$. The objective is to maximize $\lambda$ by adjusting $\mathbf{x}$ under the constraints that $\lVert \mathbf{x} \rVert_2 = 1$ and $\lVert \mathbf{x} \rVert_0 = k$, where 
#
# * $\lVert \mathbf{x} \rVert_0$ denotes the number of non-zero elements in $\mathbf{x}$; this constraint is also referred to as *cardinality constraint*;
# * $k > 0$ is an integer which represents the number of assets that should be included in the portfolio.


# If the OU mean-reversion speed $\lambda$ can be expressed as a function of the portfolio weight vector $\mathbf{x}$, maximizing $\lambda$ will give the optimal asset weightings $\mathbf{x}$ during the process. However, this is rather difficult. Instead, three other mean-reversion strength proxies are employed to solve the sparse mean-reverting portfolio selection problem:
#
# 1. Predictability based on Box-Tiao canonical decomposition.
# 2. Portmanteau statistic.
# 3. Crossing statistic.
#
# Meanwhile, the OU mean-reversion speed $\lambda$ is only used to evaluate the sparse portfolios generated by ArbitrageLab.


# ### Predictability and Box-Tiao Canonical Decomposition


# Assume that the asset prices $S_t$ follows a vector autoregressive process of order one - a VAR(1) process.
#
# $$
# S_t = S_{t-1} A + Z_t
# $$
#
#
# where $A$ is a $n \times n$ square matrix and $Z_t$ is a vector of i.i.d. Gaussian noise with $Z_t \sim N(0, \Sigma)$, independent of $S_{t-1}$.


# Assume also that the portfolio value $P_t$ follows the recursion:
#
# $$
# P_t = \hat{P}_{t-1} + \varepsilon_t
# $$
#
# where $\hat{P}_{t-1}$ is a predictor of $x_t$ built upon all past portfolio values recorded up to $t-1$,
# and $\varepsilon_t$ is a vector i.i.d. Gaussian noise, where $\varepsilon_t \sim N(0, \Sigma)$, independent
# of all past portfolio values $P_0, P_1, \ldots, P_{t-1}$.


# Substitute the portfolio value with the linear combination of the asset price, and use the VAR(1) model as the predictor $\hat{P}_{t-1}$:
#
# $$
# \mathbf{x}^T S_t = \mathbf{x}^T {S}_{t-1} A + \mathbf{x}^T \varepsilon_t
# $$


# Now calculate the variance for both sides of the equation. Since the variance would not change if the mean is shifted, it is safe to assume that the mean price of each asset is zero. The variance equation can be written as:
#
# $$
# \mathbf{x}^T \Gamma_0 \mathbf{x} = \mathbf{x}^T A^T \Gamma_0 A \mathbf{x} + \Sigma
# $$
#
# where $\Gamma_0$ are the covariance matrices of and $S_t$.


# Define the predictability of the portfolio value process as:
#
# $$
# \nu = \frac{\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}}{\mathbf{x}^T \Gamma_0 \mathbf{x}}
# $$
#
# Then the above equation can be written as:
#
# $$
# 1 = \nu + \frac{\Sigma}{\mathbf{x}^T \Gamma_0 \mathbf{x}}
# $$
#
# The interpretation of $\nu$ is straightforward. When $\nu$ is small, the variance of the Gaussian noise $\Sigma$ dominates and the portfolio value process will look like noise and is more strongly mean-reverting. Otherwise, the variance of the predicted value $\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}$ dominates and the portfolio value process can be accurately predicted on average. More importantly, the predictability $\nu$ is a function of the portfolio weights $\mathbf{x}$, and thus the optimal portfolio can be obtained during the optimization.


# Without the cardinality constraint $\lVert \mathbf{x} \rVert_0 = k$, optimizing predictability can be summarized as a generalized eigenvalue problem:
#
# $$
# \det (\lambda \Gamma_0 - A^T \Gamma_0 A) = 0
# $$
#
# To find the portfolio weight that forms a portfolio with the most (or the least) predictability, just calculate the eigenvector that corresponds to the maximal (or minimal) eigenvalue of the matrix $\Gamma_0^{-1} A^T \Gamma_0 A$. This process is Box-Tiao canonical decomposition. An example of combining covariance selection and Box-Tiao canonical decomposition to form a sparse mean-reverting portfolio will be shown in the later part of this research notebook.


# ### Portmanteau Statistic


# Portmanteau statistic of order $p$ (Ljung and Box, 1978) tests if a process is a white noise.
# By definition, the portmanteau statistic is 0 if a process is a white noise. Therefore, maximizing mean-reversion strength
# is equivalent to minimizing the portmanteau statistic.
#
# The advantage of the portmanteau statistic over the Box-Tiao predictability is that this statistic requires no modeling
# assumptions. The disadvantage, on the other hand, is higher computational complexity. The estimate of the portmanteau
# statistic of order $p$ is given as follows:
#
# $$
# \hat{\phi}_p(y) = \frac{1}{p} \sum_{k=1}^p \Big( \frac{\mathbf{x}^T \gamma_k \mathbf{x}}{\mathbf{x}^T \gamma_0 \mathbf{x}} \Big)^2
# $$
#
# where $\gamma_k$ is the sample lag-$k$ autocovariance matrix, defined as:
#
# $$
# \begin{align*}
# \gamma_k & \equiv \frac{1}{T-k-1} \sum_{t=1}^{T-k}\tilde{S}_t \tilde{S}_{t+k}^T \\
# \tilde{S}_t & \equiv S_t - \frac{1}{T} \sum_{t=1}^T S_t
# \end{align*}
# $$


# ### Crossing Statistic


# Kedem and Yakowitz (1994) define the crossing statistic of a univariate process $x_t$ as the expected number of
# crosses around 0 per unit of time:
#
# $$
# \xi(x_t) = \mathbf{E} \Bigg[ \frac{\sum_{t=2}^T \mathbf{1}_{\{x_t x_{t-1} \leq 0 \}}}{T-1} \Bigg]
# $$
#
# For a stationary AR(1) process, the crossing statistic can be reformulated with the cosine formula:
#
# $$
# \xi(x_t) = \frac{\arccos (a)}{\pi}
# $$
#
# where $a$ is the first-order autocorrelation, or the AR(1) coefficient of the stationary AR(1) process, where $\lvert a \rvert < 1$. The function $y = \arccos (a)$ is monotonic decreasing with respect to $a$
# when $\lvert a \rvert < 1$. Therefore, stronger mean-reversion strength implies a greater crossing statistic,
# which in turn implies a smaller first-order autocorrelation. To extend this result to the multivariate case,
# Cuturi (2015) proposed to minimize $\mathbf{x}^T \gamma_1 \mathbf{x}$ and ensure that all absolute higher-order
# autocorrelations $\lvert \mathbf{x}^T \gamma_k \mathbf{x} \rvert, \, k > 1$ are small.


# ## Constructing the Sparse Mean-reverting Portfolio


# This section will showcase how ArbitrageLab help construct the sparse mean-reverting portfolios with the three approaches mentioned previously. All examples uses daily price data of 45 international equity ETFs from Jan 1st, 2016 to Jan 27th, 2021.


# ### Covariance Selection via Graphical LASSO and Structured VAR(1) Estimate via Penalized Regression


# The Box-Tiao canonical decomposition relies on estimates of both the covariance matrix $\Gamma_0$ and the VAR(1)
# coefficient matrix $A$ of the asset prices. Using an $\ell_1$-penalty, as shown in d'Aspremont (2011),
# is able to simultaneously obtain numerically stable estimates and isolate key idiosyncratic dependencies
# in the asset prices. The penalized estimates of $\Gamma_0$ and $A$ provides different perspective on the
# conditional dependencies and their graphical representations help cluster the assets into several smaller groups.


# #### Covariance Selection


# Covariance selection is a process where the maximum likelihood estimation of the covariance matrix $\Gamma_0$ is
# penalized by setting a certain number of coefficients in the inverse covariance matrix $\Gamma_0^{-1}$ to zero.
# Zeroes in $\Gamma_0^{-1}$ corresponds to conditionally independent assets in the model, and the penalized, or
# sparse, estimate of $\Gamma_0^{-1}$ is both numerically robust and indicative of the underlying structure of the
# asset price dynamics.
#
# The sparse estimate of $\Gamma_0^{-1}$ is obtained by solving the following optimization problem:
#
# $$
# \max_X \log \det X - \mathbf{Tr} (\Sigma X) - \alpha \lVert X \rVert_1
# $$
#
#
# where $\Sigma = \gamma_0$ is the sample covariance matrix, $\alpha> 0$ is the $\ell_1$-regularization
# parameter, and $\lVert X \rVert_1$ is the sum of the absolute value of all the matrix elements.


# #### Structured VAR(1) Estimate via Penalized Regression


# Recall that under a VAR(1) model assumption, the asset prices $S_t$ follow the following process:
#
# $$
#     S_t = S_{t-1} A + Z_t
# $$
#
# For most financial time series, the noise terms are correlated such that $Z_t \sim N(0, \Sigma)$, where the noise
# covariance is $\Sigma$. In this case, the VAR(1) coefficient matrix $A$ has to be directly estimated from
# the data. A structured (penalized), or sparse, estimate of $A$ can be obtained column-wise via a LASSO regression
# by minimizing the following objective function:
#
# $$
#     a_i = \argmin_x \lVert S_{it} - S_{t-1}x \rVert^2 + \lambda \lVert x \rVert_1
# $$
#
# where $a_i$ is a column of the matrix $A$, and $S_{it}$ is the price of asset $i$.
#
# The sparse estimate of $A$ can be also obtained by applying a more aggressive penalty under a multi-task LASSO
# model. The objective function being minimized is:
#
# $$
#     \argmin_A \lVert S_{t} - S_{t-1}A \rVert^2 + \alpha \sum_i \sqrt{\sum_j a_{ij}^2}
# $$
#
# where $a_{ij}$ is the element of the matrix $A$.
#
# The multi-task LASSO model will suppress the coefficients in an entire column to zero, but its estimate is less robust
# than the column-wise LASSO regression in terms of numerical stability. 


# #### Clustering


# If the Gaussian noise in the VAR(1) model is uncorrelated:
#
# $$
#     S_t = S_{t-1} A + Z_t, \; Z_t \sim N(0, \sigma \mathbf{I}), \, \sigma > 0
# $$
#
# then Gilbert (1994) has shown that the graph of the inverse covariance matrix $\Gamma_0^{-1}$ and the graph of
# $A^T A$ share the same structure, i.e. the graph of $\Gamma_0^{-1}$ and $A^T A$ are disconnected along
# the same clusters of assets.
#
# When the Gaussian noise $Z_t$ is correlated, however, the above relation is no longer valid, but it is still
# possible to find common clusters between the graph of penalized estimate of $\Gamma_0^{-1}$ and penalized estimate
# of $A^T A$. This will help find a much smaller investing universe for sparse mean-reverting portfolios. 


# #### Example


# <div class="alert alert-block alert-warning">
#
# <b>Warning:</b> In order for this module to work correctly on Windows machines, please install the cvxpy package
# from conda:
#
# ``conda install -c conda-forge "cvxpy=1.1.10"``
#
# For more information on the installation process, please visit [ArbitrageLab installation guide](https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/getting_started/installation.html).
#
# </div>


# Import libraries.
import networkx as nx
import numpy as np
import pandas as pd

import matplotlib.dates as mdates
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.sparse_mr_portfolio import SparseMeanReversionPortfolio

%matplotlib inline


# Load data.
etf_df = pd.read_csv("Country_ETF.csv", parse_dates=["Dates"])
etf_df.set_index("Dates", inplace=True)
etf_df.fillna(method='ffill', inplace=True)
etf_df.columns = [x.split()[0] for x in etf_df.columns]

# Construct the sparse mean-reverting portfolio selection class.
etf_sparse_portf = SparseMeanReversionPortfolio(etf_df)


# Check the data.
etf_sparse_portf.assets.head(5)


# Calculate the penalized estimates.
sparse_var_est = etf_sparse_portf.LASSO_VAR_fit(1.4, threshold=7, multi_task_lasso=True)
_, sparse_prec_est = etf_sparse_portf.covar_sparse_fit(0.89)

# Generate the clusters.
multi_LASSO_cluster_graph = etf_sparse_portf.find_clusters(sparse_prec_est, sparse_var_est)
multi_LASSO_clusters = list(sorted(nx.connected_components(multi_LASSO_cluster_graph), key=len, reverse=True))

# Check the first two clusters.
print("1st cluster: {}\nAsset count: {}".format(multi_LASSO_clusters[0], len(multi_LASSO_clusters[0])))
print("2nd cluster: {}\nAsset count: {}".format(multi_LASSO_clusters[1], len(multi_LASSO_clusters[1])))


# <div class="alert alert-block alert-info">
# <b>Tip:</b> Covariance selection determines the size of the clusters. When fitting the graphical LASSO model, apply a larger regularizer parameter to make the covariance matrix as sparse as possible such that smaller clusters are obtained. </div> 


# <div class="alert alert-block alert-info">
# <b>Tip:</b> Both graphical LASSO model and LASSO regression are sensitive to the scale of the data. Use standardized data for this step, otherwise the LASSO models will not be able to yield a reasonable, or even feasible, solution.</div> 


# The second cluster looks quite sparse. Now perform a Box-Tiao canonical decomposition on this small cluster.


# Construct another class on the cluster obtained previously.
small_etf_sparse_portf = SparseMeanReversionPortfolio(etf_df[multi_LASSO_clusters[1]])

# Check the data.
small_etf_sparse_portf.assets.head(5)


# Perform Box-Tiao canonical decomposition.
bt_weights = small_etf_sparse_portf.box_tiao()

# Retrieve the portfolio weights that corresponds to the smallest eigenvalue.
sparse_portf_weight1 = bt_weights[:, -1]

# Check weights and pretty print the results.
sparse_portf_weight_df = pd.DataFrame(sparse_portf_weight1.reshape(-1, 1))
sparse_portf_weight_df.columns = ['Weight']
sparse_portf_weight_df.index = small_etf_sparse_portf.assets.columns

sparse_portf_weight_df


# Retrieve the portfolio weights that yield the least predictability, i.e. strongest mean-reversion. Check the OU mean-reversion coefficient and half-life and plot the portfolio value in Figure 1.


# Form the portfolio.
sparse_portf1 = small_etf_sparse_portf.assets @ sparse_portf_weight1

# Fit an OU-model to see the mean-reversion speed parameter and half-life.
coeff, hl = small_etf_sparse_portf.mean_rev_coeff(sparse_portf_weight1, small_etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


def plot_portf(portf_data, title):
    years = mdates.YearLocator()  # every year
    months = mdates.MonthLocator()  # every month
    years_fmt = mdates.DateFormatter('%Y')

    fig, ax = plt.subplots(1,1, figsize=(15, 5), sharex=True)

    ax.xaxis.set_major_locator(years)
    ax.xaxis.set_major_formatter(years_fmt)
    ax.xaxis.set_minor_locator(months)
    ax.tick_params(axis='y', labelsize=14)
    ax.tick_params(axis='x', labelsize=12)

    ax.set_title(title, fontsize=18)
    ax.plot(portf_data, label='Weak Mean-reversion')
    ax.axhline(portf_data.mean(), linewidth=0.3, linestyle='--', color=(0, 0, 0, 0.85))
    ax.set_xlim(pd.Timestamp(2016, 1, 1), pd.Timestamp(2021, 2,1))

    plt.show()


plot_portf(sparse_portf1, "Sparse Portfolio from Covariance Selection and Structured VAR(1) Estimate")


# Half-life is around 9 days, which gives enough number of trades. Figure 1 also shows that the portfolio is decently mean-reverting. 


# ### Greedy Search


# It has been already shown in the previous section that constructing a mean-reverting portfolio from a set of assets using Box-Tiao canonical decomposition is equivalent to solving the generalized eigenvalue problem,
#
# $$
# \det (\lambda \Gamma_0 - A^T \Gamma_0 A) = 0
# $$
#
# and retrieve the eigenvector corresponding to the smallest eigenvalue. This generalized eigenvalue problem can be also
# written in the variational form as follows:
#
# $$
#     \lambda_{\text{min}}(A^T \Gamma_0 A, \Gamma_0) = \min_{\mathbf{x} \in \mathbb{R}^n} \frac{\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}}{\mathbf{x}^T \Gamma_0 \mathbf{x}}
# $$
#
# However, once the cardinality constraint has been added, the resulting sparse generalized eigenvalue problem is equivalent to subset selection, which is an NP-hard problem. 
#
# $$
# \begin{align*}
# \text{minimize } & \frac{\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}}{\mathbf{x}^T \Gamma_0 \mathbf{x}} \\
# \text{subject to } & \lVert \mathbf{x} \rVert_0 = k \\
# & \lVert \mathbf{x} \rVert_2 = 1
# \end{align*}
# $$
#
# Since no polynomial time solutions are available to get the global optimal solution, greedy search is thus used to get good approximate solutions of the problem, which will have a polynomial time complexity. Greedy search builds the solution recursively, i.e. it always tries to add the asset that results in the least increase in predictability to the current "optimal" solution. Figure 2 demonstrates the working process of the greedy algorithm with a toy example.


from IPython.display import Image
Image(filename='images/greedy_demo.gif')


# #### Examples


# The example in this section uses data of all 45 ETFs and attempts to construct a sparse mean-reverting portfolio with greedy algorithm. The sample covariance matrix and the least-square VAR(1) estimate were used here to demonstrate that the greedy algorithm can work well even without the preprocessing step with covariance selection and penalized regression.


# Calculate least-square VAR(1) estimate and sample covariance.
full_var_est = etf_sparse_portf.least_square_VAR_fit()
full_cov_est = etf_sparse_portf.autocov(0, use_standardized=False)

# Use greedy algorithm to calculate portfolio weights.
greedy_weight = etf_sparse_portf.greedy_search(8, full_var_est, full_cov_est, maximize=False)

# Pretty print the weights.
greedy_weight_df = pd.DataFrame(greedy_weight.reshape(-1, 1))
greedy_weight_df.columns = ['Weight']
greedy_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
greedy_weight_df[greedy_weight_df['Weight'] != 0]


# Build the portfolio and check the OU model fit.
greedy_portf = etf_sparse_portf.assets @ greedy_weight

coeff, hl = etf_sparse_portf.mean_rev_coeff(greedy_weight.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(greedy_portf.squeeze(), "Sparse Portfolio Constructed by Greedy Algorithm")


# The sparse portfolio built by greedy algorithm has similar volatility to the previous one, but its mean-reversion is more stable, especially during the Coronavirus crash in 2020. The OU half-life of this portfolio is longer at 27.5 days, which suggests a lower trading frequency for a possible mean-reversion strategy.


# ### Convex Relaxation


# An alternative to greedy search is to relax the cardinality constraint and reformulate the original non-convex optimization problem
#
# $$
# \begin{align*}
# \text{minimize } & \frac{\mathbf{x}^T \mathbf{A} \mathbf{x}}{\mathbf{x}^T \mathbf{B} \mathbf{x}} \\
# \text{subject to } & \lVert \mathbf{x} \rVert_0 = k \\
# & \lVert \mathbf{x} \rVert_2 = 1
# \end{align*}    
# $$
#
# into a convex one. The concept "convex" means "when an optimal solution is found, then it is guaranteed to be the
# best solution". The convex optimization problem is formed in terms of the symmetric matrix $X = \mathbf{x} \mathbf{x}^T$:


# $$
# \begin{align*}
# \text{minimize } & \frac{\mathbf{Tr} (\mathbf{A} X)}{\mathbf{Tr} (\mathbf{B} X)} \\
# \text{subject to } & \mathbf{1}^T \lvert X \rvert \mathbf{1} \leq k \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# The objective function is the quotient of the traces of two matrices, which is only quasi-convex. Even if this quasi-convex optimization problem can be transformed into a convex optimization problem, or semidefinite programming (SDP) to be exact, with a change of variables, this formulation of SDP still suffers from a few drawbacks:
#
# 1. It has numerical stability issues;
# 2. It cannot proper handle volatility constraints;
# 3. It cannot optimize mean-reversion strength proxies other than predictability.
#
# Therefore, this module followed the regularizer form of the SDP proposed by Cuturi (2015) to mitigate these drawbacks.


# The predictability optimization SDP is as follows:
#
# $$
# \begin{align*}
# \text{minimize } & \mathbf{Tr} (\gamma_1 \gamma_0^{-1} \gamma_1^T X) + \rho \lVert X \rVert_1 \\
# \text{subject to } & \mathbf{Tr} (\gamma_0 X) \geq V \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# The portmanteau statistic optimization SDP is as follows:
# $$
# \begin{align*}
# \text{minimize } & \sum_{i=1}^p \mathbf{Tr} (\gamma_i X)^2 + \rho \lVert X \rVert_1 \\
# \text{subject to } & \mathbf{Tr} (\gamma_0 X) \geq V \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# The crossing statistic optimization SDP is as follows:
# $$
# \begin{align*}
# \text{minimize } & \mathbf{Tr}(\gamma_1 X) + \mu \sum_{i=2}^p \mathbf{Tr} (\gamma_i X)^2 + \rho \lVert X \rVert_1 \\
# \text{subject to } & \mathbf{Tr} (\gamma_0 X) \geq V \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# where $\rho>0$ is the $\ell_1$-regularization parameter, $\mu>0$ is a specific regularization parameter
# for crossing statistic optimization, and $V > 0$ is the portfolio variance lower threshold.


# In some restricted cases, the convex relaxation are tight, which means the optimal solution of the SDP is exactly the
# optimal solution to the original non-convex problem. However, in most cases this correspondence is not guaranteed and the
# optimal solution of these SDPs has to be deflated into a rank one matrix $xx^T$ where $x$ can be considered
# as a good candidate for portfolio weights with the designated cardinality $k$. This module uses Truncated Power
# method (Yuan and Zhang, 2013) as the deflation method to retrieve the leading sparse vector of the optimal solution
# $X^*$ that has $k$ non-zero elements.


# #### Examples: Predictability


# The examples in the following section also uses data of all 45 ETFs to demonstrate the effectiveness of the convex relaxation framework.


# Solve the predictability SDP.
sdp_pred_vol_result = etf_sparse_portf.sdp_predictability_vol(rho=0.001, variance=5, 
                                                              max_iter=5000, use_standardized=False,
                                                              verbose=False)

# Deflate the optimization result into a weight vector.
sdp_pred_vol_weights = etf_sparse_portf.sparse_eigen_deflate(sdp_pred_vol_result, 8, verbose=False)


# <div class="alert alert-block alert-warning">
#
# <b>Note:</b> Running `sparse_eigen_deflate` method after getting the SDP result is a required step. In most cases, the SDP result is a full-rank matrix rather than a rank-one matrix, so it cannot be translated into a feasible portfolio weight vector with the desired cardinality unless `sparse_eigen_deflate` has been run immediately afterwards.
#
# </div>


# Pretty print the weights.
sdp_pred_vol_weight_df = pd.DataFrame(sdp_pred_vol_weights.reshape(-1, 1))
sdp_pred_vol_weight_df.columns = ['Weight']
sdp_pred_vol_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
sdp_pred_vol_weight_df[sdp_pred_vol_weight_df['Weight'] != 0]


# <div class="alert alert-block alert-info">
# <b>Tip:</b> Always check the weights to see if there are significantly smaller weights than the others. If yes, then this means the regularizer parameter was set too large for the desired cardinality. Reduce the regularizer parameter until all the weights are approximately at the same scale. </div> 


# Build the portfolio and check the OU model fit.
sdp_pred_vol_portf = etf_sparse_portf.assets @ sdp_pred_vol_weights

coeff, hl = etf_sparse_portf.mean_rev_coeff(sdp_pred_vol_weights.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(sdp_pred_vol_portf.squeeze(), "Sparse Portfolio Constructed by SDP framework, Minimizing Predictability")


# #### Examples: Portmanteau


# Solve the portmanteau SDP.
sdp_portmanteau_vol_result = etf_sparse_portf.sdp_portmanteau_vol(rho=0.001, variance=5, nlags=3,
                                                                  max_iter=10000, use_standardized=False,
                                                                  verbose=False)

# Deflate the optimization result into a weight vector.
sdp_portmanteau_vol_weights = etf_sparse_portf.sparse_eigen_deflate(sdp_portmanteau_vol_result, 8, verbose=False)


# <div class="alert alert-block alert-info">
#
# <b>Tip:</b> Use `verbose=True` to output `cvxpy` solver info to confirm if an accurate solution has been obtained. 
#
# </div> 


# Pretty print the weights.
sdp_portmanteau_vol_weight_df = pd.DataFrame(sdp_portmanteau_vol_weights.reshape(-1, 1))
sdp_portmanteau_vol_weight_df.columns = ['Weight']
sdp_portmanteau_vol_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
sdp_portmanteau_vol_weight_df[sdp_portmanteau_vol_weight_df['Weight'] != 0]


# Build the portfolio and check the OU model fit.
sdp_portmanteau_vol_portf = etf_sparse_portf.assets @ sdp_portmanteau_vol_weights

coeff, hl = etf_sparse_portf.mean_rev_coeff(sdp_portmanteau_vol_weights.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(sdp_portmanteau_vol_portf.squeeze(), 
           "Sparse Portfolio Constructed by SDP framework, Minimizing Portmanteau Statistic")


# #### Examples: Crossing Statistic


# Solve the crossing statistic SDP.
sdp_crossing_vol_result = etf_sparse_portf.sdp_crossing_vol(rho=0.002, mu=0.1, variance=5, nlags=4,
                                                            max_iter=10000, use_standardized=False,
                                                            verbose=False)

# Deflate the optimization result into a weight vector.
sdp_crossing_vol_weights = etf_sparse_portf.sparse_eigen_deflate(sdp_crossing_vol_result, 8, verbose=False)


# Pretty print the weights.
sdp_crossing_vol_weight_df = pd.DataFrame(sdp_crossing_vol_weights.reshape(-1, 1))
sdp_crossing_vol_weight_df.columns = ['Weight']
sdp_crossing_vol_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
sdp_crossing_vol_weight_df[sdp_crossing_vol_weight_df['Weight'] != 0]


# Build the portfolio and check the OU model fit.
sdp_crossing_vol_portf = etf_sparse_portf.assets @ sdp_crossing_vol_weights

coeff, hl = etf_sparse_portf.mean_rev_coeff(sdp_crossing_vol_weights.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(sdp_crossing_vol_portf.squeeze(), 
           "Sparse Portfolio Constructed by SDP framework, Minimizing Crossing Statistic")


# Minimizing portmanteau statistic and crossing statistic yielded similar mean-reverting portfolios, both of which have a slightly longer OU half-life than the greedy algorithm result. On the other hand, minimizing predictability has generated a noisier portfolio with a shorter OU half-life.


# ## Discussion


# The notebook has demonstrated how to use ArbitrageLab to construct sparse mean-reverting portfolios using covariance selection techniques and structured VAR(1) estimate, greedy algorithm, and convex relaxation framework with SDP. The module allows considerable flexibility and efficiency for sparse mean-reverting portfolio construction.
#
# Among the three approaches, greedy algorithm is the most robust due to its simplicity and the stableness of the portfolio it generates. As shown in the example, the portfolio generated by greedy algorithm was able to stand the test of a market crash. Moreover, greedy algorithm runs fast. Therefore, it is the top choice if given a completely new dataset.
#
# Convex relaxation framework has shown greater flexibility compared to the other two approaches as it allows addition of other constraints. The examples here has shown that a volatility lower bound could be added such that meaningful statistical arbitrage opportunities are available as the deviation from portfolio mean is greater. However, the mean-reversion strengths of the portfolios are slightly weaker than the one generated by the greedy algorithm, which is likely because the convex relaxation is not tight.
#
# Covariance selection and structured VAR(1) estimate approach can narrow down the investing universe effectively. The examples have shown that the 45 international equity ETFs have been readily clustered into a 14-asset and an 8-asset subset from which a sparse mean-reverting portfolio can be constructed using Box-Tiao canonical decomposition. Note that this approach can be used in conjunction with the greedy algorithm and convex relaxation approach. For example, greedy algorithm can be directly applied to the aforementioned 14-asset subset.


# ## Conclusion


# This research notebook has discussed in detail the theoretical background for solving sparse mean-reversion portfolio selection problem and how ArbitrageLab can help build them in three different approaches.
#
# ### Key Takeaways
#
# * Greedy algorithm is the most robust approach among the three. It is recommended to try greedy algorithm first when given a new dataset.
# * Convex relaxation is the most flexible approach among the three. Extra constraints can be added, for example, a volatility lower threshold. However, the quality of the solution may be compromised due to an extra deflation step to yield the sparse weight vector.
# * Covariance selection and structured VAR(1) estimate is more of a preprocessing step among the three. This approach can efficiently narrow down the investing universe. Box-Tiao canonical decomposition, greedy algorithm, and convex relaxation can be all applied afterwards on the resulting asset subsets.
# * This module helps select which assets to trade and allocate the capital for each selected asset. It does **NOT** help decide the mean-reversion trading strategy. The users can choose whichever strategies they may find suitable for the highly mean-reverting portfolio.


# ## References


# 1. [Cuturi, M. and d'Aspremont, A., 2015. Mean-reverting portfolios: Tradeoffs between sparsity and volatility. arXiv preprint arXiv:1509.05954.](https://arxiv.org/pdf/1509.05954.pdf)
# 2. [d'Aspremont, A., 2011. Identifying small mean-reverting portfolios. Quantitative Finance, 11(3), pp.351-364.](https://arxiv.org/pdf/0708.3048.pdf)
# 3. [Fogarasi, N. and Levendovszky, J., 2012. Improved parameter estimation and simple trading algorithm for sparse, mean-reverting portfolios. In Annales Univ. Sci. Budapest., Sect. Comp, 37, pp. 121-144.](http://www.hit.bme.hu/~fogarasi/Fogarasi12Improved.pdf)
# 4. [Gilbert, J.R., 1994. Predicting structure in sparse matrix computations. SIAM Journal on Matrix Analysis and Applications, 15(1), pp.62-79.](https://www.osti.gov/servlets/purl/6987948)
# 5. [Kedem, B. and Yakowitz, S., 1994. Time series analysis by higher order crossings (pp. 115-143). New York: IEEE press.](http://www2.math.umd.edu/~bnk/HOC.Japan02.pdf)
# 6. [Ljung, G.M. and Box, G.E., 1978. On a measure of lack of fit in time series models. Biometrika, 65(2), pp.297-303.](https://apps.dtic.mil/sti/pdfs/ADA049397.pdf)
# 7. [Natarajan, B.K., 1995. Sparse approximate solutions to linear systems. SIAM journal on computing, 24(2), pp.227-234.](https://epubs.siam.org/doi/abs/10.1137/S0097539792240406?journalCode=smjcat)
# 8. [Yuan, X.T. and Zhang, T., 2013. Truncated Power Method for Sparse Eigenvalue Problems. Journal of Machine Learning Research, 14(4).](https://www.jmlr.org/papers/volume14/yuan13a/yuan13a.pdf)



// ---------------------------------------------------

// mean_reversion.py
// arbitrage_research_basic_PCA/Cointegration Approach/mean_reversion.py
# Generated from: mean_reversion.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Algorithmic Trading: Winning Strategies and Their Rationale__ _by_ Ernest P. Chan


# # Mean Reversion


# This description of the mean-reverting processes closely follows the work of _Ernest P. Chan_ __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146). 
#
# Additional information on the Engle-Granger cointegration test can be found in a paper by _Faik Bilgili_ __Stationarity and cointegration tests: Comparison of Engle-Granger and Johansen methodologies__ [available here](https://mpra.ub.uni-muenchen.de/75967/1/MPRA_paper_75967.pdf).


# ## Introduction


# Mean-reverting processes and events often occur in nature. Observations of the processes that have a mean-reverting nature tend to move to their average value over time. However, as mentioned in the work of E.P. Chan, most financial price series are not mean-reverting.
#
# The upside is that we can construct advanced financial instruments from multiple simple ones thus obtaining the desired property. Observation series (stock, commodity prices, etc.) that can be combined to achieve a mean-reverting process are called *cointegrating*. The approach described above allows us to use the properties of mean-reverting processes to generate profit.


# ## Augmented Dickey–Fuller (ADF) test
#
# According to Ernest P. Chan:
# "The mathematical description of a mean-reverting price series is that the change of the price series in the next period is proportional to the difference between the mean price and the current price. This gives rise to the ADF test, which tests whether we can reject the null hypothesis that the proportionality
# constant is zero."
#
# The ADF test is based on the idea that the current price level gives us information about the future price level: if it's lower than the mean, the next move will be upwards, and vice versa.
#
# The ADF test uses the linear model that describes the price changes as follows:
#
# $$\Delta y(t) = \lambda y(t-1) + \mu + \beta t + \alpha_1 \Delta y(t-1) + ... \alpha_k \Delta y(t-k) + \epsilon_t$$
#
# where $\Delta y(t) \equiv y(t) - y(t-1)$, $\Delta y(t) \equiv y(t-1) - y(t-2)$, ...
#
# The hypothesis that is being tested is: $\lambda = 0$. For simplicity we assume the drift term to be zero ($\beta = 0$). If we reject the hypothesis, this means that the next price move depends on the current price level.


# ## The Half-life period
#
# Mean reversion tests, such as ADF usually require at least 90 percent certainty. But in practice, we can create strategies that are profitable even at lower certainty levels. The measure $\lambda$ can be used to calculate the *half-life*, which indicates how long it takes for a price to mean revert:
#
# $$Half-life = -log(2) / \lambda$$
#
# Furthermore, we can see that if the $\lambda$ value is positive, the price series are not mean-reverting. If it's close to zero, the half-life is very long and the strategy won't be profitable due to slow mean reversion.
#
# Half-life period can be helpful to determine some of the parameters to use in the trading strategy. Say, if the half-life period is 20 days, then using 5 days backward-looking window for moving average or volatility calculation may not give the best results.
#
# The most common approach is to use two cointegrated price series to construct a portfolio. This is done by simultaneously going long on one asset and short on the other, with an appropriate capital allocation for each asset. This approach is also called a "pairs trading strategy". However, the approach can be extended to three and more assets.


# ## Johansen cointegration test
#
# This is one of the most widely used cointegration tests, it's upside is that, first it allows multiple price series for stationarity testing, and second it provides hedge ratios for price series used to combine elements into a stationary portfolio.
#
# To understand how to test the cointegration of more than two variables, we can transform the equation used in the ADF test to a vector form. So $y(t)$ would be vectors representing multiple price series, and the $\lambda$ and $\alpha$ are matrices. We also assume that the drift term is zero ($\beta t = 0$). So the equation can be rewritten as follows:
#
# $$\Delta Y(t) = \Lambda Y(t-1) + M + A_1 \Delta Y(t-1) + ... + A_k \Delta Y(t-k) + \epsilon_t$$


# This way we can test the hypothesis of $\Lambda = 0$, in which case, we don't have cointegration present. Denoting the rank of the obtained matrix $\Lambda$ as $r$ and the number of price series as $n$, the number of independent portfolios that can be formed is equal to $r$.
#
# The Johansen test calculates the $r$ and tests the hypotheses of $r = 0$ (cointegrating relationship exists), $r \le 1$, ..., $r \le n - 1$. In case all the above hypotheses are rejected, the result is that $r = n$ and the eigenvectors of the $\Lambda$ can be used as hedge ratios to construct a mean-reverting portfolio.
#
# Note that the Johansen test is independent of the order of the price series, in contrast to the CADF test.


# ## Engle-Granger cointegration test
#
#
# The cointegration testing approach proposed by Engle-Granger allows us to test whether two or more price series are cointegrated of a given order.
#
# The Engle-Granger cointegration test is performed as follows:
#
# - First, we need to determine the order of integration of variables $x$ and $y$
#   (or $y_{1}, y_{2}, ...$ in case of more than two variables). If they are integrated of the same order, we can apply the cointegration test.
# - Next, if the variables are integrated of order one at the previous step, the following regressions can be performed:
#
# $$x_t = a_0 + a_1 y_t + e_{1,t},$$


# $$y_t = b_0 + b_1 x_t + e_{2,t}$$
#
# - Finally we run the following regressions and test for unit root for each equation:
#
# $$\Delta e_{1,t} = a_1 e_{1, t-1} + v_{1, t},$$


# $$\Delta e_{2,t} = a_2 e_{2, t-1} + v_{2, t}$$


# If we cannot reject the null hypotheses that $|a_1| = 0$ and $|a_2| = 0$, we cannot reject the hypothesis that the variables are not cointegrated.
#
# The hedge ratios for constructing a mean-reverting portfolio in the case of the Engle-Granger test are set to $1$ for the $x$ variable and the coefficient $-a_1$ for the $y$ variable (or $-a_1, -a_2, ..$ in case of multiple $y_i$ price series).
#
# The Engle-Granger cointegration test implemented in the ArbitrageLab package assumes that the first step of the algorithm is passed and that the variables are integrated of order one.


# ## Linear trading strategy
#
# Using a mean-reverting portfolio obtained from the Johansen test, a linear mean-reverting trading strategy can be constructed.
#
# The idea is to own a number of units of a portfolio proportional to their negative normalized deviation from its moving average (the Z-score). A unit portfolio is the one with shares of its elements determined by the eigenvector of the $\Lambda$ matrix from the Johansen test.
#
# So the number of portfolio units to hold ($N_t$) is calculated as:
#
# $$N_{t} = \frac{P_{t} - MA(P_{t}, T_{MA})}{std(P_{t}, T_{std})}$$
#
# where
#
# - $P_{t}$ is the price of a portfolio,
#
# - $MA(P_{t}, T_{MA})$ is the moving average of the portfolio price calculated
#   using a backward-looking $T_{MA}$ window,
#
# - $std(P_{t}, T_{std})$ is the rolling standard deviation of the portfolio price
#   calculated using a backward-looking $T_{std}$ window.


# Linear strategy in this case means that the number of invested units is proportional to the Z-Score and not that the market value of our investment is proportional.
#
# For this basic strategy, the maximum required capital is unknown, therefore it's not a very practical one in its simplest version.
#
# The upside however is that this strategy has no parameters to optimize (the windows for moving average and the rolling standard deviation taken equal to the half-life period) and is useful for backtesting the properties of a mean-reverting price series.


# ## Bollinger Bands trading strategy
#
# This is a more practical strategy in comparison to the Linear Trading strategy mentioned in this module, as it deals with the issues present in the linear strategy: infinitesimal portfolio rebalances and no predefined buying power (as the costs needed to be allocated to the strategy are dependant on the Z-score).
#
# By using the Bollinger bands on the Z-scores from the linear mean-reversion trading strategy, we can construct a more practical trading strategy. The idea is to enter a position only when the price deviates by more than *entryZscore* standard deviations from the mean. This parameter can be optimized in a training set.
#
# Also, the look-back windows for calculating the mean and the standard deviation are the parameters that can be optimized. We can later exit the strategy when the price reverts to the *exitZscore* standard deviations from the mean (*exitZscore* $<$ *entryZscore*).
#
# If *exitZscore* $= -$ *entryZscore*, we will exit when the price moves beyond the opposite band, also triggering a trading signal of the opposite sign. At either time we can have either zero or one unit (long or short) invested, so capital allocation is easy.
#
# If the look-back window is short and we set a small *entryZscore* and *exitZscore*, the holding period will be shorter and we get more round trip trades and generally higher profits.


# ## Usage of the Algorithms


# Let's use the above tools on real data. 
#
# First, we will use the cointegration tests on datasets to determine if it's possible to construct a mean-reverting portfolio. Then we will create such a portfolio, find its half-lif, and run trading strategies on it. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# Following the example in the Optimal Mean Reversion module, we will use Gold Shares ETF (GLD), Gold Miners ETF (GDX), and Junior Gold Miners ETF (GDXJ) to construct a portfolio of three elements.


# Loading data
train_data =  yf.download("GLD GDX GDXJ", start="2016-01-01", end="2018-01-01")
test_data =  yf.download("GLD GDX GDXJ", start="2018-01-02", end="2020-01-01")

# Taking close prices for chosen instruments
train_three_elements = train_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

test_three_elements = test_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

# Looking at the downloaded data
train_three_elements.head()


# ### Johansen test


# Now let's run the cointegration tests and analyze the results.


# Initialising an object containing needed methods
j_portfolio = al.cointegration_approach.JohansenPortfolio()

# Fitting the data on a dataset of three elements with constant term
j_portfolio.fit(train_three_elements, det_order=0)

# Getting results of the eigenvalue and trace Johansen tests
j_eigenvalue_statistics = j_portfolio.johansen_eigen_statistic
j_trace_statistics = j_portfolio.johansen_trace_statistic
j_cointegration_vectors = j_portfolio.cointegration_vectors


# First, the eigenvalue statistic test
j_eigenvalue_statistics


# Using the eigenvalue statistic test, we can see that our eigenvalue statistics are above the 95% significance level values for two elements - GLD and GDXJ. And they are all above the 90% significance level values.
#
# So at a 90% significance level, three elements are cointegrated, we can construct three possible mean-reverting portfolios using the coefficients from the 
# *j_cointegration_vectors* variable.


# Next, the alternative trace statistic test
j_trace_statistics


# We have slightly different results for the trace statistic test. Here, all three trace statistics are above the 95% significance level values.
#
# According to this test, at a 95% significance level, three elements are cointegrated, we can construct three possible mean-reverting portfolios using the coefficients from the same *j_cointegration_vectors* variable.


# Coefficients to construct a mean-reverting portfolio 
j_cointegration_vectors


# ### Engle-Granger test


# Initialising an object containing needed methods
eg_portfolio = al.cointegration_approach.EngleGrangerPortfolio()

# Fitting the data on a dataset of three elements with constant term
eg_portfolio.fit(train_three_elements, add_constant=True)

# Getting results of the Engle-Granger test
eg_adf_statistics = eg_portfolio.adf_statistics
eg_cointegration_vectors = eg_portfolio.cointegration_vectors


# Looking at the statistic from the last step of the Engle-Granger test
eg_adf_statistics


# Using the ADF statistic test output, we can see that our statistic is above the 95% significance level value.
#
# So at a 95% significance level, our elements are cointegrated, we can construct a mean-reverting portfolio using the coefficients from the *eg_cointegration_vectors* variable.


eg_cointegration_vectors


# As described in the theoretical part, the coefficient for the first element is $1$, while other two are equal to negative regression coefficients.


# ### Constructing portfolios


# First, let's construct a portfolio using the coefficients obtained from the Johansen cointegration test. Next, using the coefficients from the Engle-Granger tests and compare these portfolio values. 


# Calculating returns of our elements (ETFs)
train_three_elements_returns = (train_three_elements / train_three_elements.shift(1) - 1)[1:]

train_three_elements_returns.head()


# Weights of elements for the Johansen portfolio
j_cointegration_vectors.loc[0]


# Scaling cointegration vectors so they sum up to 1
j_scaled_vectors = j_cointegration_vectors.loc[0] / abs(j_cointegration_vectors.loc[0]).sum()

j_scaled_vectors


# Also adding weights to take initial prices of our ETFs into account
weights  = train_three_elements.iloc[0] / abs(train_three_elements.iloc[0]).sum()


# Calculating portfolio values during the training period
j_portfolio_returns = (train_three_elements_returns * j_scaled_vectors * weights).sum(axis=1)
j_portfolio_price = (j_portfolio_returns + 1).cumprod()


# Plotting Johansen portfolio price
j_portfolio_price.plot(title='Johansen portfolio price', figsize=(10,5));


# Weights of elements for the Engle-Granger portfolio
eg_cointegration_vectors.loc[0]


# Scaling weights so they sum up to 1
eg_scaled_vectors = eg_cointegration_vectors.loc[0] / abs(eg_cointegration_vectors.loc[0]).sum()

eg_scaled_vectors


# Calculating portfolio values during the training period
eg_portfolio_returns = (train_three_elements_returns * eg_scaled_vectors * weights).sum(axis=1)
eg_portfolio_price = (eg_portfolio_returns + 1).cumprod()


# Plotting Engle-Granger portfolio price
eg_portfolio_price.plot(title='Engle-Granger portfolio price', figsize=(10,5));


# We can see that these portfolios perform similarly, as relative weights of elements in them are close. Let's compare them if we upscale the weights of the Johansen portfolio to match the weight of the first weight in the Engle-Granger portfolio.


# Scaling Johansen portfolio weights
weights = pd.concat([j_cointegration_vectors.loc[0]*2.459,
                     eg_cointegration_vectors.loc[0]],
                    axis=1,
                    keys = ['Johansen weights', 'Engle-Granger weights'])

weights


# Another way to construct portfolios is by using the construct_mean_reverting_portfolio function from the JohansenPortfolio or the EngleGrangerPortfolio class. This portfolio is created by summing up price series with given coefficients.


# Alternative way to construct portfolios
alternative_j_portfolio_price = j_portfolio.construct_mean_reverting_portfolio(train_three_elements, j_cointegration_vectors.loc[0])

# Plotting the portfolio price
alternative_j_portfolio_price.plot(title='Alternative Johansen portfolio price', figsize=(10,5));


# ### Calculating the half-life of obtained portfolios 


# Importing the needed function
from arbitragelab.cointegration_approach import get_half_life_of_mean_reversion


j_half_life = get_half_life_of_mean_reversion(j_portfolio_price)

print('Half-life of the Johansen portfolio is', j_half_life, 'days.')


eg_half_life = get_half_life_of_mean_reversion(eg_portfolio_price)

print('Half-life of the Engle-Granger portfolio is', eg_half_life, 'days.')


# Since the half-life of the Johansen portfolio is shorter, it is reverting to its mean value faster, so we will be using it to test the trading strategies.
#
# Applying them to the Engle-Granger portfolio can be done in the same way.


# ### Applying the Linear trading strategy


# First, we want to apply the trading strategies on the test data, so we will construct a portfolio for testing - based on the data not used for determining Johansen portfolio weights and it's half-life. 


# Importing the needed function
from arbitragelab.cointegration_approach import linear_trading_strategy


# Creating a test portfolio based on the Johansen test results
test_three_elements_returns = (test_three_elements / test_three_elements.shift(1) - 1)[1:]

# Scaling weights for them to sum up to 1
j_scaled_vectors = j_cointegration_vectors.loc[0] / abs(j_cointegration_vectors.loc[0]).sum()

# Also adding weights to take initial prices of ETFs into account
weights  = test_three_elements.iloc[0] / abs(test_three_elements.iloc[0]).sum()

test_portfolio_returns = (test_three_elements_returns * j_scaled_vectors * weights).sum(axis=1)

test_portfolio_price = (test_portfolio_returns + 1).cumprod()


# Applying a linear trading strategy
linear_trading_results = linear_trading_strategy(test_portfolio_price,
                                                 sma_window = int(j_half_life),
                                                 std_window = int(j_half_life))

linear_trading_results.tail()


# Plotting the results
fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]}, figsize=(10,7))
fig.suptitle('Linear trading strategy results')

axs[0].plot(linear_trading_results.iloc[:,0])
axs[0].title.set_text('Portfolio price')

axs[1].plot(linear_trading_results.iloc[:,2], '#b11a21')
axs[1].title.set_text('Number of portfolio units to hold')


# The upper graph shows the price of a mean-reverting portfolio - the same price series that were used as an input to the linear_trading_strategy function. 
# The lower graph shows the target amount of portfolio units to hold at any given time. 
#
# We might want to get trading signals from a more advanced strategy.   


# ### Applying Bollinger Bands trading strategy


# This trading strategy requires defining entry and exit Z-scores. Let's choose the entry Z-score - upon which the strategy will enter a short position, going short one unit of a portfolio. This position will be closed once the exit Z-score will be reached and the position of the opposite side will be opened.
#
# In our example, we will be using an entry Z-score of 2 and an exit Z-score of -2.


# Importing the needed function
from arbitragelab.cointegration_approach import bollinger_bands_trading_strategy


# Applying Bollinger Bands trading strategy
bollinger_trading_results = bollinger_bands_trading_strategy(test_portfolio_price,
                                                 sma_window = int(j_half_life),
                                                 std_window = int(j_half_life),
                                                 entry_z_score=2,
                                                 exit_z_score=-2)

bollinger_trading_results.tail()


# Plotting the results
fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 1]}, figsize=(10,7))
fig.suptitle('Bollinger Bands trading strategy results')

axs[0].plot(bollinger_trading_results.iloc[:,0])
axs[0].title.set_text('Portfolio price')

axs[1].plot(bollinger_trading_results.iloc[:,2], '#b11a21')
axs[1].title.set_text('Number of portfolio units to hold')


# Again, the upper graph shows the price of a mean-reverting portfolio - the same price series that were used as an input to the bollinger_bands_trading_strategy function. The lower graph shows the target amount of portfolio units to hold at any given time. Note that the amount invested is limited to 1 portfolio unit - long or short.
#
# From the above example, the Bollinger Bands strategy is generating multiple signals to open long and short positions on our mean-revering portfolio.
#
# Let's calculate the equity curve of our portfolio, if we would use this trading strategy on this data. We should shift the trading signals one observation ahead to avoid the lookahead bias in this example.


# Calculating the price of our investment portfolio
investment_portfolio_retunrs = test_portfolio_returns * bollinger_trading_results.iloc[:,2].shift(1)

investment_portfolio_price = (investment_portfolio_retunrs + 1).cumprod()


# Calculating the equity curve of our investment portfolio
investment_portfolio_equity_curve = investment_portfolio_price - 1

# Plotting the equity curve
investment_portfolio_equity_curve.plot(title='Bollinger Bands investemnt portfolio equity curve', figsize=(10,5));


# Using trading signals from the Bollinger Bands strategy for this particular example resulted in the value of our investment portfolio increasing from 1 in mid-September 2018 to around 1.01 at the beginning of the year 2020.
#
# We can further test this strategy by choosing different entry and exit Z-scores, or adding transaction costs to see if the strategy is robust.


# ## Conclusion


# This notebook describes the Mean Reversion module tools - cointegration tests and trading strategies. Also, it shows how these tools can be used on real data and that they can output profitable trading signals.
#
# The algorithms and the descriptions used in this notebook were described by _Ernest P. Chan_ in the book __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146).
#
# Key takeaways from the notebook:
#
# - Mean-reverting processes tend to move to their average value over time. However, most financial price series are not mean-reverting.
# - Financial instruments can be combined in a portfolio that has mean-reverting properties.
# - ADF test allows testing if the next price move depends on the current price level.
# - Half-life of a mean-reverting process indicates how long it takes for a price to mean revert.
# - Both Johansen and Engle-Granger cointegration tests allow testing if we can construct a mean-reverting portfolio out of multiple price series, and if so, which combination of these elements should we use to construct a portfolio.
# - Constructed portfolios can be used in the Linear and Bollinger Bands trading strategies.
# - The Bollinger Bands  strategy  deals with the issues present in the linear strategy: infinitesimal portfolio rebalances and no predefined buying power.



// ---------------------------------------------------

// minimum_profit_optimization.py
// arbitrage_research_basic_PCA/Cointegration Approach/minimum_profit_optimization.py
# Generated from: minimum_profit_optimization.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# Reference:
# 1. [Lin, Y.-X., McCrae, M., and Gulati, C., 2006. **Loss protection in pairs trading through minimum profit bounds: a cointegration approach**](http://downloads.hindawi.com/archive/2006/073803.pdf)
#
# 2. [Puspaningrum, H., Lin, Y.-X., and Gulati, C. M. 2010. **Finding the optimal pre-set boundaries for pairs trading strategy based on cointegration technique**](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1040&context=cssmwp)


# # Minimum Profit Optimization


# ## Introduction


# A common pairs trading strategy is to "fade the spread", i.e. to open a trade when the spread is sufficiently far away
# from its equilibrium in anticipation of the spread reverting to the mean. Within the context of cointegration, the
# spread refers to cointegration error, and in the remainder of this documentation "spread" and "cointegration error" will
# be used interchangeably.
#
# The concept of "sufficiently far away from the equilibrium of the spread", i.e. a pre-set boundary chosen to open a trade, need to be quantified. This boundary can affect the minimum total profit (MTP) over a specific trading horizon. The higher the pre-set boundary for opening trades, the higher the profit per trade but the lower the trade numbers. The opposite applies to lowering the boundary values. The number of trades over a specified trading horizon is determined jointly by the average trade duration and the average inter-trade interval.
#
# In this notebook, we will demonstrate how to optimize the pre-set boundary that would maximize the MTP for cointegration error
# following an AR(1) process by numerically estimating the average trade duration, average inter-trade interval, and the
# average number of trades based on the mean first-passage time.


# ## Model Assumptions


# Before getting into the nuts and bolts of the optimization process, the assumptions of the cointegration pairs trading strategy need to be articulated.
#
# - The price of two assets ($S_1$ and $S_2$) are cointegrated over the relevant time period, which includes both in-sample and out-of-sample (trading) period.
# - The cointegration error follows a stationary AR(1) process.
# - The cointegration error is symmetrically distributed so that we can apply the optimal boundary on both sides of the mean.
# - Short sales are permitted or possible through a broker and there is no interest charged for the short sales and no cost for trading.
# - The cointegration coefficient $\beta > 0$, where a cointegration relationship is defined as:
#
# \begin{equation}
#     P_{S_1,t} - \beta P_{S_2,t} = \varepsilon_t
# \end{equation}


# ## Minimum Profit per Trade


# Denote a trade opened when the cointegration error $\varepsilon_t$ overshoots the pre-set upper boundary $U$
# as a **U-trade**, and similarly, a trade opened when $\varepsilon_t$ falls through the pre-set lower
# boundary $L$ as an **L-trade**. Without loss of generality, it can be assumed that the mean of $\varepsilon_t$ is 0. 


# ### Minimum Profit per U-trade


# The setup of a U-trade is as follows:
#
# - When $\varepsilon_t \geq U$ at $t_o$, we open a trade by selling $N$ of asset $S_1$ and buying $\beta N$ of asset $S_2$.
# - When $\varepsilon_t \leq 0$ at $t_c$, we close the trade.
#
# The profit per trade would thus be:
#
# \begin{equation}
#     P = N (P_{S_1, t_o} - P_{S_1, t_c}) + \beta N (P_{S_2, t_c} - P_{S_2, t_o})
# \end{equation}
#
# Since the two assets are cointegrated during the trade period, substitute the cointegration relationship into
# the above equation and derive the following:
#
# \begin{align*}
#     P & =  N (P_{S_1, t_o} - P_{S_1, t_c}) + \beta N (P_{S_2, t_c} - P_{S_2, t_o}) \\
#       & =  N (P_{S_1, t_o} - \beta P_{S_2, t_o}) - N (P_{S_1, t_c} - \beta P_{S_2, t_c}) \\
#       & =  N \varepsilon_{t_o} - N \varepsilon_{t_c} \\
#       & \geq N U
# \end{align*}


# ### Minimum Profit per L-trade


# The setup of an L-trade is as follows:
#
# - When $\varepsilon_t \leq -U$ at $t_o$, we open a trade by buying $N$ of asset $S_1$ and selling $\beta N$ of asset $S_2$.
# - When $\varepsilon_t \geq 0$ at $t_c$, we close the trade.
#
# The profit per trade would thus be:
#
# \begin{equation}
#     P = N (P_{S_1, t_c} - P_{S_1, t_o}) + \beta N (P_{S_2, t_o} - P_{S_2, t_c})
# \end{equation}
#
# Since the two assets are cointegrated during the trade period, substitute the cointegration relationship into
# the above equation and derive the following:
#
# \begin{align*}
#     P & =  N (P_{S_1, t_c} - P_{S_1, t_o}) + \beta N (P_{S_2, t_o} - P_{S_2, t_c}) \\
#       & =  N (P_{S_1, t_c} - \beta P_{S_2, t_c}) - N (P_{S_1, t_o} - \beta P_{S_2, t_o}) \\
#       & =  N \varepsilon_{t_c} - N \varepsilon_{t_o} \\
#       & \geq N U
# \end{align*}


# **Both U-trades and L-trades would make a minimum profit per trade of $U$ if one unit of the cointegrated pair ($N=1$) was traded.**


# ## Minimum Total Profit (MTP)


# Based on the assumption that the boundary to open trades would be symmetrically applied on both sides of the spread mean, we will focus only on U-trade in this section. We can simply double the results to take L-trades into account.
#
# After deriving the minimum profit per U-trade, the next question of interest is the MTP over a trading horizon $[0,T]$. We would like to find the optimal boundary $U$ that can allow enough trades within the trading horizon as well as maximize the total profit. Define MTP as a function of the boundary $U$ as follows.
#
# \begin{equation}
#     MTP(U) = \Big( \frac{T}{{TD}_U + I_U} - 1 \Big) U
# \end{equation}
#
# where ${TD}_U$ is the trade duration and $I_U$ is the inter-trade interval. The derivation from the previous section proved that the minimum profit of one trade is $U$. Therefore, the number of U-trades within the trading horizon is given by:
#
# \begin{equation}
#     \frac{T}{{TD}_U + I_U} - 1
# \end{equation}


# According to the model assumptions, the cointegration error $\varepsilon_t$ follows a stationary AR(1) process. 
#
# \begin{equation}
# \varepsilon_t = \phi \varepsilon_{t-1} + a_t \qquad a_t \sim N(0, \sigma_a^2) \text{ i.i.d}
# \end{equation}


# Without loss of generality, we assume $E(\varepsilon_t) = 0$. Then ${TD}_U$ can be estimated by calculating the average time of $\varepsilon_t$ to pass 0 for the first time given the initial value of $\varepsilon_{t} = U$.
#
# \begin{equation}
#     {TD}_U = E(\mathcal{T}_{0, \infty}(U)) = \lim_{b \to \infty} \frac{1}{\sqrt{2 \pi} \sigma_a} \int_0^b E(\mathcal{T}_{0, b}(s)) \text{ exp} \Big( - \frac{(s- \phi U)^2}{2 \sigma_a^2} \Big) ds + 1
# \end{equation}


# Similarly, $I_U$ can be estimated by calculating the average time of $\varepsilon_t$ to pass $U$ for the first time given the initial value of $\varepsilon_t = 0$.
#
# \begin{equation}
#     I_U = E(\mathcal{T}_{- \infty, U}(0)) = \lim_{-b \to - \infty} \frac{1}{\sqrt{2 \pi} \sigma_a} \int_{-b}^U E(\mathcal{T}_{-b, U}(s)) \text{ exp} \Big( - \frac{s^2}{2 \sigma_a^2} \Big) ds + 1
# \end{equation}


# ### Numerically Estimating the Integral in the Mean First-passage Time of an AR(1) Process


# The crux of the optimization algorithm is thus calculating the above two integrals.
#
# Consider a stationary AR(1) process:
#
# \begin{equation}
#     Y_t = \phi Y_{t-1} + \xi_t
# \end{equation}
#
# where $-1 < \phi < 1$, and
# \begin{equation}
#     \xi_t \sim N(0, \sigma_{\xi}^2) \text{ i.i.d}
# \end{equation}
#
# The mean first-passage time over the interval $\lbrack a, b \rbrack$ of $Y_t$, starting at initial state $y_0 \in \lbrack a, b \rbrack$, which is denoted by $E(\mathcal{T}_{a,b}(y_0))$, is given by
#
# \begin{equation}
#     E(\mathcal{T}_{a,b}(y_0)) = \frac{1}{\sqrt{2 \pi}\sigma_{\xi}}\int_a^b E(\mathcal{T}_{a,b}(u)) \text{ exp} \Big( - \frac{(u-\phi y_0)^2}{2 \sigma_{\xi}^2} \Big) du + 1
# \end{equation}


# This integral equation can be solved numerically using the Nystrom method, i.e. by solving the following linear equations:
#
# $$
# \begin{pmatrix}
#     1 - K(u_0, u_0) & -K(u_0, u_1) & \ldots & -K(u_0, u_n) \\
#     -K(u_1, u_0) & 1 - K(u_1, u_1) & \ldots & -K(u_1, u_n) \\
#     \vdots & \vdots & \vdots & \vdots \\
#     -K(u_n, u_0) & -K(u_n, u_1) & \ldots & 1-K(u_n, u_n)
# \end{pmatrix}
# \begin{pmatrix}
#     E_n(\mathcal{T}_{a,b}(u_0)) \\
#     E_n(\mathcal{T}_{a,b}(u_1)) \\
#     \vdots \\
#     E_n(\mathcal{T}_{a,b}(u_n))
# \end{pmatrix}
# =   
# \begin{pmatrix}
#     1 \\
#     1 \\
#     \vdots \\
#     1 \\
# \end{pmatrix} 
# $$
#
# where $E_n(\mathcal{T}_{a,b}(u_0))$ is a discretized estimate of the integral, and the Gaussian kernel function $K(u_i, u_j)$ is defined as:
#
# \begin{equation}
#     K(u_i, u_j) = \frac{h}{2 \sqrt{2 \pi} \sigma_{\xi}} w_j  \text{ exp} \Big( - \frac{(u_j - \phi u_i)^2}{2 \sigma_{\xi}^2} \Big)
# \end{equation}
#
# and the weight $w_j$ is defined by the trapezoid integration rule:
#
# \begin{equation}
#     w_j = \begin{cases}
#     1 & j = 0 \text{ and } j = n \\
#     2 & 0 < j < n, j \in \mathbb{N}
#     \end{cases}
# \end{equation}
#
# The time complexity for solving the above linear equation system is $O(n^3)$ (see [here](https://www.netlib.org/lapack/lug/node71.html) for an introduction of the time complexity of `numpy.linalg.solve` ), which is the most time-consuming part of this procedure.


# Note that the integrals in the definition of ${TD}_U$ and $I_U$ the integral limit goes to infinity. To approximate the infinity limit, the following stylized fact proves to be useful: for a stationary AR(1) process $\{ \varepsilon_t \}$, the probability that the absolute value of the process $\vert \varepsilon_t \vert$ is greater than 5 times the standard deviation of the process $5 \sigma_{\varepsilon}$ is close to 0. Therefore, $5 \sigma_{\varepsilon}$ can be used as an approximation of the infinity limit in the integrals.


# ## Optimization


# Based on the above discussion, the numerical algorithm to optimize the pre-set boundary that maximizes MTP can be readily given.
#
# 1. Perform Engle-Granger or Johansen test to derive the cointegration coefficient $\beta$.<br>
# 2. Fit the cointegration error $\varepsilon_t$ to an AR(1) process and retrieve the AR(1) coefficient and the fitted residual.<br>
# 3. Calculate the standard deviation of cointegration error ($\sigma_{\varepsilon}$) and the fitted residual ($\sigma_a$).<br>
# 4. Generate a sequence of pre-set upper bounds $U_i$, where $U_i = i \times 0.01, \> i = 0, \ldots, b/0.01$, and $b = 5 \sigma_{\varepsilon}$.<br>
# 5. For each $U_i$,<br>
#   a. Calculate ${TD}_{U_i}$.<br>
#   b. Calculate $I_{U_i}$. *Note: this is the main bottleneck of the optimization speed.*<br>
#   c. Calculate $MTP(U_i)$.<br>
# 6. Find $U^{*}$ such that $MTP(U^{*})$ is the maximum.<br>
# 7. Set a desired minimum profit $K \geq U^{*}$ and calculate the number of assets to trade according to the following equations:<br>
# \begin{eqnarray}
# N_{S_2} = \Big \lceil \frac{K \beta}{U^{*}} \Big \rceil \\
# N_{S_1} = \Big \lceil \frac{N_{S_2}}{\beta} \Big \rceil
# \end{eqnarray}


# ## Want to Test with Simulated Data First?


# We also provided the functionality of simulating cointegrated series in case there is trouble finding cointegrated asset pairs from empirical data. 
#
# The simulations are based on the following cointegration model:
#
# \begin{gather*}
#     P_{S_1}(t) + \beta P_{S_2}(t) = \varepsilon_t \\
#     P_{S_2}(t) - P_{S_2}(t-1) = e_t
# \end{gather*}


# where $\varepsilon_t$ and $e_t$ are AR(1) processes.
#
# \begin{eqnarray*}
# \varepsilon_t - \phi_1 \varepsilon_{t-1} = c_1 + \delta_{1,t} \qquad \delta_{1,t} \sim N(0, \sigma_1^2) \\
# e_t - \phi_2 e_{t-1} = c_2 + \delta_{2,t} \qquad \delta_{2,t} \sim N(0, \sigma_2^2)
# \end{eqnarray*}


# The module allows simulation of multiple cointegrated series all at once.


# ## Usage of the Algorithms


# ### Cointegration Simulation
#
# Firstly, we will showcase the usage of the simulation module.
#
# The following codeblocks will simulate a batch of cointegrated series that are defined by the following parameters.


# \begin{eqnarray*}
#             \phi_1 & = & 0.95 \\
#             c_1 & = & 1.5 \\
#             \sigma_1 & = & 0.5 \\
#             \phi2 & = & 0.9 \\
#             c_2 & = & 0.05 \\
#             \sigma_2 & = & 1.0 \\
#             \beta & = & -0.6 \\
# \end{eqnarray*}


# Import libraries
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.coint_sim import CointegrationSimulation
from arbitragelab.cointegration_approach.minimum_profit import MinimumProfit

# Import helper module for trading simulation
from trading_simulation import TradingSim


# Initialize the data simulator
# Generate 20 cointegrated series, each of the series have 250 data points
coint_simulator = CointegrationSimulation(20, 250)


# Set the parameters for e_t
price_params = {
    "ar_coeff": 0.95,
    "white_noise_var": 0.5,
    "constant_trend": 1.5}

# Set the parameters for epsilon_t
coint_params = {
    "ar_coeff": 0.9,
    "white_noise_var": 1.,
    "constant_trend": 0.05,
    "beta": -0.6}

# Load the parameters
coint_simulator.load_params(price_params, target='price')
coint_simulator.load_params(coint_params, target='coint')


# The simulation can be done either by purely following the recurrence relation given by the AR(1) process, or using the `statsmodels` package. Here we use the `statsmodels` package to simulate the cointegrated series according to the above-mentioned parameters.


# Using statsmodels package to simulate the cointegrated series
s1_series, s2_series, coint_errors = coint_simulator.simulate_coint(initial_price=100., use_statsmodels=True)


# Plot the simulated cointegrated series and the cointegration error.


# Plot an example of the simulated cointegrated series and cointegration error
coint_sim_fig = coint_simulator.plot_coint_series(s1_series[:, 0], s2_series[:, 0], coint_errors[:, 0])


# ### Minimum Profit Optimization


# In this section, the usage of the minimum profit optimization and the performance of the resulting trading strategy will be presented with empirical data. The assets involved in this example are two S&P 500 stocks, Ametek Inc. (Ticker: AME) and Dover Corp. (Ticker: DOV). 


# Read price series data, set date as index
data = pd.read_csv('AME-DOV.csv', parse_dates=['Date'])
data.set_index('Date', inplace=True)

# Show how the price data is structured
data.head(5)


# Although the paper suggested the usage of a 12-month training period and a 6-month trading period with daily data, our results found that the choice of 12-month and 6-month is arbitrary and the performance was not affected if a longer training period was used. In the following demonstration, daily data of both stocks from Jan 4th, 2016 to Dec 31st, 2018 were used for training, and the trading period starts from Jan 2nd, 2019 to Nov 23th, 2020.


# Initialize the minimum profit optimizer
optimizer = MinimumProfit(data)

# Split the entire price history into training and trading period
# 2019 Jan 1st was a market holiday so it is a perfect split point
train_df, trade_df = optimizer.train_test_split(date_cutoff=pd.Timestamp(2019, 1, 1))


# The next step is to test if the two stocks stayed cointegrated during the training period. Here we used the Engle-Granger test to test for cointegration and retrieve the cointegration coefficient.


beta_eg, epsilon_t_eg, ar_coeff_eg, ar_resid_eg = optimizer.fit(use_johansen=False, sig_level="95%")


# If the asset pair is not cointegrated at the designated significance level (here we set it to 95\%), then the `fit()` function will output a warning message that the pair was not cointegrated at the 95\% level. Here the warning did not appear, which means the stock pair (AME-DOV) was cointegrated during the entire training period. 
#
# We can now check the properties of the cointegration error and see how well it follows an AR(1) process.


print("The cointegration coefficient is: {}".format(beta_eg))
print("The AR(1) coefficient of the cointegration error is: {}".format(ar_coeff_eg))

sigma_e = epsilon_t_eg.std()
sigma_a = ar_resid_eg.std()

print("The standard deviation of the cointegration error is: {}".format(sigma_e))
print("The standard deviation of the fitted AR(1) process residual is: {}".format(sigma_a))
print("The ratio of the AR(1) residual std-dev to "
      "cointegration error std-dev is: {}".format(sigma_a / sigma_e))
print("Value of sqrt(1 - phi^2) is: {}".format(np.sqrt(1 - ar_coeff_eg ** 2)))


# According to the above results, we can see that the cointegration error indeed follows a stationary AR(1) process with two pieces of proof.
#
# 1. The standard deviation of the cointegration error and the standard deviation of the AR(1) process residual have the following relationship:
#
# \begin{equation}
# \sigma_a = \sqrt{1 - \phi^2} \sigma_{\varepsilon}
# \end{equation}
#
# And the fitting result has shown that $\frac{\sigma_a}{\sigma_{\varepsilon}} = 0.21$ and $\sqrt{1 - \phi^2} = 0.18$, which is relatively close.
#
# 2. The absolute value of the AR(1) coefficient is less than 1 ($\phi = 0.9831 < 1$).
#
# Now we proceed to optimize the upper bound.
#
# **Note:** This process will take a while, so a progress bar has been provided.


# Optimize the pre-set boundaries based on the fitted parameters
optimal_ub, _, _, optimal_mtp, optimal_num_of_trades = optimizer.optimize(ar_coeff_eg, epsilon_t_eg, 
                                                                          ar_resid_eg, len(train_df))


# Let's check the optimal upper bound, the optimal minimal total profit, and the optimal total number of U-trades. We can simply double the minimal total profit and the number of U-trades to take into account L-trades.


print("The optimal upper-bound is: {}".format(optimal_ub))
print("The optimal minimal total profit over the in-sample period with only U-trades: ${:.2f}".format(optimal_mtp))
print("The optimal total number of U-trades is: {}".format(np.floor(optimal_num_of_trades)))
print("The optimal minimal total profit over the in-sample period with both "
      "U-trades and L-trades: ${:.2f}".format(2. * optimal_mtp))
print("The optimal total number of U-trades and L-trades is: {}".format(2. * np.floor(optimal_num_of_trades)))


# The above results correspond to trading one unit of the AME-DOV pair. The optimal upper-bound is $\$2.05$, which is also the minimum profit per trade according to the derivation in the **Minimum Profit Per Trade** section.


# ### Trading the Strategy


# Obtain the trading signals and number of shares to trade on in-sample data
trade_signals_is, num_of_shares_is, cond_values_is = optimizer.trade_signal(optimal_ub, optimal_ub, 
                                                                            beta_eg, epsilon_t_eg, insample=True)

# Show the number of shares to trade
print("The number of AME and DOV shares to trade is", num_of_shares_is)


# The number of shares has been decided by the optimizer. One unit of trade corresponds to trading 2 shares of AME and 1 share of DOV. Now we initialize a trading account to simulate the trading process and we only allow the account to trade one unit of pair for each trade.


# Initialize a trading account with $1,000
trade_sim1 = TradingSim(starting_equity=1000.)

# Simulate the trade for in-sample data 
trade_result_is = trade_sim1.summary(trade_signals_is, num_of_shares_is)

# Number of U-trade and L-trade
num_utrade_is = len(trade_result_is[trade_result_is['Trade Type'] == "U-trade Close"])
num_ltrade_is = len(trade_result_is[trade_result_is['Trade Type'] == "L-trade Close"])

print("Number of U-trade in-sample: {}".format(num_utrade_is))
print("Number of L-trade in-sample: {}".format(num_ltrade_is))

# Show last 3 trades
trade_result_is.tail(7)


# From the above results, we can see that there is still a U-trade open at the end of Dec 18th, 2018. Excluding this trade, we will show the P&L up to Dec 12th, 2018 where the last trade was closed.


# Show P&L
pnl_df = trade_sim1.get_pnl(trade_signals_is)
pnl = pnl_df.loc[pd.Timestamp(2018, 12, 12)]['Total Equity'] - 1000.
print("Total P&L: ${:.2f}".format(pnl))
print("Average P&L per trade: ${:.2f}".format(pnl / (num_utrade_is + num_ltrade_is)))

# Plot the signals and the P&L curve
trade_sim1.plot_strategy(trade_signals_is, num_of_shares_is, cond_values_is,
                         start_date=pd.Timestamp(2016, 1, 1), end_date=pd.Timestamp(2018, 12, 12));


# The pair traded very well during the in-sample period, yielding 5 U-trades and 5 L-trades, and $\$49.04$ profit in total. The average profit per trade is $\$4.90$, which is higher than the optimized average profit per trade $\$2.05$.
#
# Now let's see how well the cointegrated pair trades during the out-of-sample period.


# Obtain the trading signals and number of shares to trade on out-of-sample data
trade_signals_oos, num_of_shares_oos, cond_values_oos = optimizer.trade_signal(optimal_ub, optimal_ub, 
                                                                               beta_eg, epsilon_t_eg, insample=False)

# Initialize a trading account with $1,000
trade_sim2 = TradingSim(starting_equity=1000.)

# Simulate the trade for in-sample data 
trade_result_oos = trade_sim2.summary(trade_signals_oos, num_of_shares_oos)

# Number of U-trade and L-trade
num_utrade_oos = len(trade_result_oos[trade_result_oos['Trade Type'] == "U-trade Close"])
num_ltrade_oos = len(trade_result_oos[trade_result_oos['Trade Type'] == "L-trade Close"])

print("Number of U-trade out-of-sample: {}".format(num_utrade_oos))
print("Number of L-trade out-of-sample: {}".format(num_ltrade_oos))

# Show last 3 trades
trade_result_oos.tail(7)


# The coronavirus shock to the stock market yielded more trades for this cointegrated pair during the turbulent year of 2020. Let's see how the P&L looks like.


# Show P&L
pnl_df = trade_sim2.get_pnl(trade_signals_oos)
pnl = pnl_df.loc[pd.Timestamp(2020, 11, 4)]['Total Equity'] - 1000.
print("Total P&L is: {:.2f}".format(pnl))
print("Average P&L per trade: ${:.2f}".format(pnl / (num_utrade_oos + num_ltrade_oos)))

# Plot the signals and the P&L curve
trade_sim2.plot_strategy(trade_signals_oos, num_of_shares_oos, cond_values_oos,
                         start_date=pd.Timestamp(2019, 1, 1), end_date=pd.Timestamp(2020,11,4));


# From the out-of-sample testing results, we can see the pair maintained its cointegration property and the performance has not deteriorated. The average profit per trade out-of-sample ($\$4.74$) is almost the same as that in-sample ($\$4.90$), but the special market environment led to more trades.
#
# The minimum profit per trade by trading one unit of the cointegrated pair is $\$2.05$. The stability of out-of-sample tests led to a natural proposition: scale up the strategy. Now let us fund a bigger trading account with $\$1\text{M}$ available to trade. We would like to guarantee a minimum profit per trade of $\$5,000$ (which is $0.5\%$ of the account), and see how the strategy fares.


# Get a new optimizer
optimizer_big = MinimumProfit(data)
_, _ = optimizer_big.train_test_split(date_cutoff=pd.Timestamp(2019, 1, 1))
beta_eg, epsilon_t_eg, ar_coeff_eg, ar_resid_eg = optimizer_big.fit(use_johansen=False, sig_level="95%")

# Obtain the trading signals and number of shares to trade on out-of-sample data
minimum_profit = 5000.
trade_signals_oos, num_of_shares_oos, cond_values_oos = optimizer_big.trade_signal(optimal_ub, minimum_profit, 
                                                                                   beta_eg, epsilon_t_eg, insample=False)

# Initialize a trading account with $1,000,000
trade_sim3 = TradingSim(starting_equity=1000000.)

# Simulate the trade for in-sample data 
trade_result_oos = trade_sim3.summary(trade_signals_oos, num_of_shares_oos)

# Show number of shares to trade for each stock
print("The number of AME and DOV shares to trade is", num_of_shares_oos)


# Therefore, we need to trade 2,440 shares of AME and 2,316 shares of DOV to guarantee a minimum profit per trade of $\$5,000$. The P&L curve then looks as follows.


%%capture
signal_plot, pnl_plot = trade_sim3.plot_strategy(trade_signals_oos, num_of_shares_oos, cond_values_oos,
                                                 start_date=pd.Timestamp(2019, 1, 1), end_date=pd.Timestamp(2020,11,4))


pnl_plot


# ## Conclusion


# This notebook described how to design a cointegrated pair trading strategy by optimizing the pre-set boundaries to "fade the spread" to maximize the minimum total profit over a trading period. The assumptions of the cointegration model, the optimization algorithm, and the application of the algorithm to an empirical pair have been demonstrated in detail.
#
# ### Key takeaways
#
# * When trading a cointegrated asset pair, two elements are required to guarantee a minimum profit per trade.
#   - Build the spread by weighting the assets by the cointegration coefficient.
#   - Fade the spread at a pre-set boundary and square the trade when the spread returns to its mean.
# * The spread must maintain cointegrated during the training and the trading period.
# * If the spread follows a stationary AR(1) process, the optimal pre-set boundary can be estimated with mean first-passage time.
# * A numerical algorithm has been given to calculate the mean first-passage time.
# * The optimal boundary maximizes the total minimum profit over a trading period.
# * By assuming a symmetric distribution of the spread, the optimal boundary is applied over both sides of the spread mean, so the spread can be faded both when overvalued and undervalued.
# * The strategy can be scaled up by setting a higher minimum profit per trade.


# ## Reference


# 1. [Lin, Y.-X., McCrae, M., and Gulati, C. (2006). Loss protection in pairs trading through minimum profit bounds: A cointegration approach.Journal of Applied Mathematics and Decision Sciences, 2006(1):1–14.](http://downloads.hindawi.com/archive/2006/073803.pdf)
# 2. [Puspaningrum, H., Lin, Y.-X., and Gulati, C. M. (2010).  Finding the optimal pre-set boundaries for pairs trading strategy based on cointegration technique. Journal of  Statistical  Theory and Practice, 4(3):391–419.](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1040&context=cssmwp)



// ---------------------------------------------------

// Copula_Strategy_Mispricing_Index.py
// arbitrage_research_basic_PCA/Copula Approach/Copula_Strategy_Mispricing_Index.py
# Generated from: Copula_Strategy_Mispricing_Index.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Copula Strategy Using Mispricing Index
# This notebook demonstrates the usage of the `copula_strategy_mpi` module.
# The framework of which was originally proposed in
#
# * Xie, W., Liew, R.Q., Wu, Y. and Zou, X., 2016. Pairs trading with copulas. 
#
# For a fundamental introduction of concepts in copula, please refer to the documentation for this module, or the basic copula strategy notebook.
# The rest of the notebook is written assuming the reader in possession of basic understanding of those concepts.
#
# **Warning:**
# The authors claimed a relatively robust 8-10% returns from this strategy in the formation period (6 mo).
# We are pretty positive that the rules proposed in the paper were implemented correctly in the `CopulaStrategyMPI`
# module with thorough unit testing on every possible case, and thus there is very unlikely to have mistakes.
# However the P&L is very sensitive to the opening and exiting parameters value, input data and copula choice,
# and it cannot lead to the claimed returns, after trying all the possible interpretations of ambiguities.
#
# We still implement this module for people who intend to explore possibilities with copula methods, however the user should be
# aware of the nature of the proposed framework.
# Interested reader may read through the *Possible Issues Discussion* part and see where this strategy can be improved.
# We also marked in the source code where the trading logics are implemented for you to change them quickly as you wish.


# ## Introduction to the Strategy Framework
# For convenience, the **mispricing index** implemented in the strategy will be referred to as **MPI** when no ambiguity arises.


# ### How is the MPI Strategy Constructed?
# At first glance, the MPI strategy documented in [Xie et al. 2016] looks quite bizarre.
# However, it is reasonably consistent when one goes through the logic of its construction:
# In order to use returns to generate trading signals, one needs to be creative about utilizing the information.
# It is one thing to know the dependence structure of a pair of stocks, it is another thing to trade based on it because intrinsically stocks are traded on prices, not returns.
#
# If one takes conditional probabilities as a distance measure on the spread, then it is natural to think about how far the returns can cumulatively drive the prices apart (or together), thereby introducing trading opportunities.
#
# Hence we introduce the following concepts for the strategy framework:
#
# #### Mispricing Index
# **MPI** is defined as the conditional probability (cumulative density) of returns, i.e., 
# $$
# MI_t^{X\mid Y} = P(R_t^X < r_t^X \mid R_t^Y = r_t^Y)
# $$
# $$
# MI_t^{Y\mid X} = P(R_t^Y < r_t^Y \mid R_t^X = r_t^X)
# $$
# for stocks $(X, Y)$ with returns random variables at day $t$: $(R_t^X, R_t^Y)$ and returns value at day $t$: $(r_t^X, r_t^Y)$.
# Those two values determine how mispriced each stock is, based on that day's return.
# Note that so far only one day's return information contributes, and we want to add it up to cumulatively use returns to gauge how mispriced the stocks are.
# Therefore we introduce the **flag** series:
#
# #### Flag and Raw Flag
# A more descriptive name than flag, in my opinion, would be the **cumulative mispricing index**.
# The **raw flag** series (with a star) is the cumulative sum of daily MPIs minus 0.5, i.e.,
# $$
# FlagX^*(t) = FlagX^*(t-1) + (MI_t^{X\mid Y} - 0.5), \quad FlagX^*(0) = 0.
# $$
# $$
# FlagY^*(t) = FlagY^*(t-1) + (MI_t^{Y\mid X} - 0.5), \quad FlagY^*(0) = 0.
# $$
# Or equivalently
# $$
# FlagX^*(t) = \sum_{s=0}^t (MI_s^{X\mid Y} - 0.5)
# $$
# $$
# FlagY^*(t) = \sum_{s=0}^t (MI_s^{Y\mid X} - 0.5)
# $$
# If one plots the raw flags series, they look quite similar to cumulative returns from their price series, which is what they were designed to do:
# Accumulate information from daily returns to reflect information on prices.
# Therefore, you may consider it as a fancy way to represent the returns series.
#
# However, the **real flag** series (without a star, $FlagX(t), FlagY(t)$) **will be reset to $0$** whenever there is an exiting signal, which brings us to the trading logic.


# ### Trading Logic
#
# The authors propose a **dollar-neutral** trade scheme worded as follows:
#
# Suppose stock $X$, $Y$ are associated with $FlagX$, $FlagY$ respectively.
#
# Opening rules: ($D = 0.6$ in the paper)
# * When $FlagX$ reaches $D$, we short-sell stock $X$ and buy stock $Y$ in **equal amounts**. ($-1$ Position)
# * When $FlagX$ reaches $-D$, we short-sell stock $Y$ and buy stock $X$ in **equal amounts**. ($1$ Position)
# * When $FlagY$ reaches $D$, we short-sell stock $Y$ and buy stock $X$ in **equal amounts**. ($1$ Position)
# * When $FlagY$ reaches $-D$, we short-sell stock $X$ and buy stock $Y$ in **equal amounts**. ($-1$ Position)
#
# Exiting rules: ($S = 2$ in the paper)
# * If trades are opened based on $FlagX$, then they are closed if $FlagX$ returns to zero or reaches stop-loss position $S$ or $-S$.
#
# * If trades are opened based on $FlagY$, then they are closed if $FlagY$ returns to zero or reaches stop-loss position $S$ or $-S$.
#
# * After trades are closed, both $FlagX$ and $FlagY$ are reset to $0$.
#
# The rationale behind the dollar-neutral choice might be that (the authors did not mention this), because the signals are generated by returns, it makes sense to "reset" returns when entering into a long/short position.


# #### Ambiguities
# The authors did not specify what will happen if the followings occur:
#
# 1. When $FlagX$ reaches $D$ (or $-D$) and $FlagY$ reaches $D$ (or $-D$) together.
# 2. When in a long(or short) position, receives a short(or long) trigger.
# 3. When receiving an opening and exiting signal together.
# 4. When the position was open based on $FlagX$ (or $FlagY$), $FlagY$ (or $FlagX$) reaches $S$ or $-S$.
#
# Here is our take on the above issues:
#
# 1. Do nothing.
# 2. Change to the trigger position. For example, long position with a short trigger will go short.
# 3. Go for the exiting signal.
# 4. Do nothing.


# #### Choices for Open and Exit Logic
#
# The above default logic is essentially an OR-OR logic for open and exit: When at least one of the 4 open conditions is satisfied, an
# open signal (long or short) is triggered;
# Similarly for the exit logic, to exit only one of them needs to be satisfied.
# The opening trigger is in general too sensitive and leads to too many trades, and [Rad et al. 2016] suggested using AND-OR logic instead.
# Thus, to achieve more flexibility, we allow the user to choose AND, OR for both open and exit logic and hence there are 4 possible combinations.
# Based on our tests we found AND-OR to be the most reasonable choice in general, but in certain situations other choices may have an edge.
#
# The default is OR-OR, as suggested in [Xie et al. 2014], and you can switch to other logic in the `get_positions_and_flags` method
# by setting `open_rule` and `exit_rule` to your own liking.
# For instance `open_rule='and'`, `exit_rule='or'`.


# ## `CopulaStrategyMPI` Functionality ##
# Tools included in the module enables the users to conduct the following workflow:
#
# * Initiate the `CopulaStrategyMPI` class with options to set custom $D$ value `opening_triggers` and $S$ value `stop_loss_positions`, and the option to side-load a fitted `Copula` or `MixedCopula` object.
# * Calculate returns from price series using `to_returns` method.
# * Calculate daily MPIs from returns using `calc_mpi` method.
#   (This step is optional and you do not need the MPI series to do other things.
#   Other methods that uses MPIs will calculate them internally.)
# * Fit returns to different kinds of copulas using `fit_copula` method.
# * Get positions and flags from testing/trading data, using `get_positions_and_flags` method.
# * Translate positions to the number of units to hold for a dollar-neutral approach, using `positions_to_units_dollar_neutral` method.
#
# A few notes:
#
# 1. This module is pandas based, and is built on top of the `BasicCopulaStrategy` class.
# 2. The output is *very sensitive* to the values of `opening_triggers` and `stop_loss_positions`, to the point that it may determine whether this strategy profits or loses.
# 3. If you just want to have the raw flag series, simply toggle `enable_reset_flag` to `False`. But this logically conflicts with the exiting logic generation so do not use the trading signal under this condition.
# 4. If you want to have the raw flag series AND trade on it, you need to set `stop_loss_positions` to `(-np.inf, np.inf)` and toggle `enable_reset_flag` to `False`. Although based on our tests, this strategy is not likely to profit.


# ## Usage ##
# The demonstration part has the following sections using real-world data:
# 1. Importing and Data Pre-processing.
# 2. Fitting Data to Different Copulas with Training Data.
# 3. Generating Trading Positions using Testing Data.
# 4. Possible Issues Discussion.


# ### 1. Importing and Data Preprocessing
#
# I have also included the 2009-2011 data for BKD and ESC. Simply uncomment accordingly for cell 2, 3 and 4.


# Importing libraries and modules
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
import statsmodels.api as sm

from arbitragelab.copula_approach.copula_strategy_mpi import CopulaStrategyMPI
import arbitragelab.copula_approach.copula_calculation as ccalc


# Importing data

# I also included the 2009-2011 data. Uncomment below to use.
# pair_prices = pd.read_csv(r'BKD_ESC_2009_2011.csv', index_col=0, parse_dates=True)

pair_prices = pd.read_csv(r'BKD_ESC_2008_2009_June.csv', index_col=0, parse_dates=True)


# Plotting price series
plt.figure(dpi=120)
plt.plot(pair_prices['BKD'], label='BKD')
plt.plot(pair_prices['ESC'], label='ESC')

# plt.axvline(dt.date(2012, 1, 3), color='red')  # Training testing split date

plt.axvline(dt.date(2008, 12, 31), color='red')  # Training testing split date
plt.legend()
plt.grid()
plt.gcf().autofmt_xdate()
plt.title(r'Original Price Series of BKD and ESC')
plt.show()


# Initiate the analysis module
CSMPI = CopulaStrategyMPI()

# Training and testing split. Note that this module works with returns series.
# training_length = 756# From 01/02/2008 to 12/30/2008 (m/d/y)
training_length = 252 # From 01/02/2008 to 12/30/2008 (m/d/y)

prices_train = pair_prices.iloc[: training_length, :]
returns_train = CSMPI.to_returns(prices_train)

prices_test = pair_prices.iloc[training_length : , :]
returns_test = CSMPI.to_returns(prices_test)

# Empirical CDF for the training set.
cdf1 = ccalc.construct_ecdf_lin(returns_train['BKD'])
cdf2 = ccalc.construct_ecdf_lin(returns_train['ESC'])

cdf1_price = ccalc.construct_ecdf_lin(prices_train['BKD'])
cdf2_price = ccalc.construct_ecdf_lin(prices_train['ESC'])


# Let's at first scatter plot the training data to have a grasp on its dependence structure, and determine which copula it should fit.


# Scatter plot the training data.
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5), dpi=100)
axs[0].scatter(cdf1(returns_train['BKD']), cdf2(returns_train['ESC']), s=2)
axs[0].set_aspect('equal', adjustable='box')
axs[0].set_title(r'From Return Series, Training Data')

axs[1].scatter(cdf1_price(prices_train['BKD']), cdf2_price(prices_train['ESC']), s=2, color='r')
axs[1].set_aspect('equal', adjustable='box')
axs[1].set_title(r'From Price Series, Training Data')
plt.tight_layout()
plt.show()


# **Note: We only use the returns series for the MPI strategy. The price series scatter plot is just for visual comparison.**
#
# The returns plot exhibits some tail dependencies on the lower-left corner.
# There might also be some tail dependencies on the upper-right corner, but it is not directly obvious.
# When compared to the price series scatter plot, the price series shows a much stronger lower dependency and no clear upper dependency.
# In terms of market movement, the two plots indicate that both returns and prices tend to move downward together, but not upward together, at least the interdependence is not as strong as downward movements.
# Considering the training data is in year 2008, those scatter plots make sense.
#
# ### 2. Fitting Data to Different Copulas with Training Data
#
# Regarding which copulas best describe the structure, we propose using N14 and Clayton-Frank-Gumbel mixed copula  (CFG) log-likelihood score, and their ability to capture tail dependencies at both corners.
# One may fit to as much the copulas as they wish in the `copula_approach` package.


# Fit to N13, CFG and Student-t, Gumbel, Frank, N14 copulas respectively, and print the scores.
# You may see warnings coming from fitting a mixed copula. In this case, adjust values for gamma_scad, a_scad.
result_dict_n13,     copula_n13, _, _     = CSMPI.fit_copula(returns_train, copula_name='N13')
result_dict_cfg,     copula_cfg, _, _     = CSMPI.fit_copula(returns_train, copula_name='CFGMixCop', gamma_scad=0.5)
result_dict_t,       copula_t, _, _       = CSMPI.fit_copula(returns_train, copula_name='Student')
result_dict_gumbel,  copula_gumbel, _, _  = CSMPI.fit_copula(returns_train, copula_name='Gumbel')
result_dict_frank,   copula_frank, _, _   = CSMPI.fit_copula(returns_train, copula_name='Frank')
result_dict_n14,     copula_n14, _, _     = CSMPI.fit_copula(returns_train, copula_name='N14')

# Print fit scores
print(result_dict_n13)
print(result_dict_cfg)
print(result_dict_t)
print(result_dict_gumbel)
print(result_dict_frank)
print(result_dict_n14)


# Note that we are working with the returns series, which in general is much noisier than price series, and has a lower level of correlation.


# Scatter plot the training data vs the fitted copulas
fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(9,14), dpi=100)

# axs[0, 0]: Empirical.
axs[0, 0].scatter(cdf1(returns_train['BKD']), cdf2(returns_train['ESC']), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'BKD and ESC Returns Series, Training Data')
axs[0, 0].set_xlim([-0.05, 1.05])
axs[0, 0].set_ylim([-0.05, 1.05])
plt.tight_layout()

# axs[0, 1]: N14.
copula_n14.plot(ax=axs[0, 1], s=1, num=training_length)

# axs[1, 0]: Student-t.
copula_t.plot(ax=axs[1, 0], s=1, num=training_length)

# axs[1, 1]: CFG.
copula_cfg.plot(ax=axs[1, 1], s=1, num=training_length)

# axs[2, 0]: Frank.
copula_frank.plot(ax=axs[2, 0], s=1, num=training_length)

# axs[2, 1]: Gumbel.
copula_gumbel.plot(ax=axs[2, 1], s=1, num=training_length)

plt.show()


# Now we choose Student-t and N14 copula to generate trading signals.
#
# ### 3. Generating Trading Positions using Testing Data
#
# We are using a dollar neutral strategy, and the buy/sell/hold is based on "flag" series, calculated from accumulations of mispricing indices.
# For more detail, please refer to the beginning of the notebook.
#
# Here we use the default OR-OR open-exit logic with the Student-t copula, and AND-OR open-exit logic with the
# N14 copula.


# Make two strategy classes with the fitted Student and N14 copula.
CSMPI_t = CopulaStrategyMPI(copula=copula_t, opening_triggers=(-0.6, 0.6), stop_loss_positions=(-2, 2))
CSMPI_n14 = CopulaStrategyMPI(copula=copula_n14)

# Get positions and flag series
positions_t, flags_t = CSMPI_t.get_positions_and_flags(
    returns=returns_test, cdf1=cdf1, cdf2=cdf2,
    enable_reset_flag=True)
positions_n14, flags_n14 = CSMPI_n14.get_positions_and_flags(
    returns=returns_test, cdf1=cdf1, cdf2=cdf2,
    enable_reset_flag=True, open_rule='and', exit_rule='or',
    opening_triggers=(-0.5, 0.5), stop_loss_positions=(-2.1, 2.1))

# Shift the positions by 1 day
positions_t = positions_t.shift(1)
positions_n14 = positions_n14.shift(1)
positions_t[0] = 0
positions_n14[0] = 0


# As a referece we also plot the normalized price series from the two stocks.
plt.figure(figsize=(10, 3), dpi=150)
plt.plot(prices_test['BKD'] / prices_test['BKD'][0], label='BKD prices')
plt.plot(prices_test['ESC'] / prices_test['ESC'][0], label='ESC prices')
plt.title('Normalized Prices in the Trading Period')
plt.legend()
plt.grid()
plt.show()

# Plot positions and flags
fig, axs = plt.subplots(4, 1, gridspec_kw={'height_ratios': [1, 0.4, 1, 0.4]}, figsize=(10,10), dpi=150)
fig.suptitle('Flags and Corresponding Positions')

# Plotting flags for Student-t copula
axs[0].plot(flags_t['BKD'], label='BKD')
axs[0].plot(flags_t['ESC'], label='ESC')
axs[0].title.set_text('Flags, Student-t Copula')
axs[0].legend()
axs[0].grid()
axs[0].set_yticks([-2, -1, -0.6, 0, 0.6, 1, 2])

# Plotting positions for Student-t copula
axs[1].plot(positions_t, label='Positions', color='brown')
axs[1].title.set_text('Positions, Student-t Copula')
axs[1].grid()
axs[1].set_yticks([-1,0,1])

# Plotting flags for N14 copula
axs[2].plot(flags_n14['BKD'], label='BKD')
axs[2].plot(flags_n14['ESC'], label='ESC')
axs[2].title.set_text('Flags, N14 Copula')
axs[2].legend()
axs[2].grid()
axs[2].set_yticks([-2, -1, -0.6, 0, 0.6, 1, 2])

# Plotting positions for N14 copula
axs[3].plot(positions_n14, label='Positions', color='brown')
axs[3].title.set_text('Positions, N14 Copula')
axs[3].grid()
axs[3].set_yticks([-1,0,1])

fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Avoid title overlap
plt.show()


# #### Forming Equity Curves
# The authors propose a dollar-neutral trading strategy and therefore, we can represent the P&L of the strategy in returns.


# 1. Calculate the unit holding series
units_df_t = CSMPI.positions_to_units_dollar_neutral(prices_df=prices_test, positions=positions_t)
units_df_n14 = CSMPI.positions_to_units_dollar_neutral(prices_df=prices_test, positions=positions_n14)

# Calculate returns from the strategy, i.e., assuming 1$ initial investment and calculate P&L.
# 2. Calculate Daily P&L of the strategy based on the units holding for each security
portfolio_pnl_t = returns_test['BKD'] * units_df_t['BKD'] + returns_test['ESC'] * units_df_t['ESC']
portfolio_pnl_n14 = returns_test['BKD'] * units_df_n14['BKD'] + returns_test['ESC'] * units_df_n14['ESC']

# 3. Calculate and plot the equity curve
equity_t = portfolio_pnl_t.cumsum()
equity_n14 = portfolio_pnl_n14.cumsum()

# Plotting
fig, axs = plt.subplots(4, 1, gridspec_kw={'height_ratios': [0.5, 0.2, 0.2, 0.5]}, figsize=(10,10), dpi=150)

# As a referece we also plot the normalized price series from the two stocks.
axs[0].plot(prices_test['BKD'] / prices_test['BKD'][0], label='BKD prices')
axs[0].plot(prices_test['ESC'] / prices_test['ESC'][0], label='ESC prices')
axs[0].set_title('Normalized Prices in the Trading Period')
axs[0].legend()
axs[0].grid()

# Plot the units series
axs[1].plot(units_df_t['BKD'], label='BKD units')
axs[1].plot(units_df_t['ESC'], label='ESC units')
axs[1].set_title('Amount of Shares to Hold According to Student-t Copula')
axs[1].legend()
axs[1].grid()

axs[2].plot(units_df_n14['BKD'], label='BKD units')
axs[2].plot(units_df_n14['ESC'], label='ESC units')
axs[2].set_title('Amount of Shares to Hold According to N14 Copula')
axs[2].legend()
axs[2].grid()

# Plot the daily P&L
axs[3].plot(equity_t, label='Student-t', color='firebrick')
axs[3].plot(equity_n14, label='N14', color='forestgreen')
axs[3].set_title('Equity Curves, Plotted in Returns')
axs[3].legend()
axs[3].grid()

fig.tight_layout()
plt.show()


# ### 4. Possible Issues Discussion
# The following are critiques for the default strategy.
# For a thorough comparison in large amounts of stocks across several decades, read For the AND-OR strategy, read more 
# in [Rad et al. 2016] on comparisons with other common strategies, using the AND-OR logic.
#
# 1. The strategy's outcome is quite sensitive to the values of opening and exiting triggers to the point that a well-fitted copula with a not good sets of parameters can actually lose money.
#
# 2. The trading signal is generated from the flags series, and the flags series will be calculated from the copula that we use to model.
#    Therefore the explainability suffers.
#    Also, it is based on the model in second order, and therefore the flag series and the suggested positions will be quite different across different copulas, making it not stable and not directly comparable mutually.
#
# 3. The way the flags series are defined does not handle well when both stocks are underpriced/overpriced concurrently.
#
# 4. Because of flags will be reset to 0 once there is an exiting signal, it implicitly models the returns as martingales that do not depend on the current price level of the stock itself and the other stock.
#    Such an assumption may be situational, and the user should be aware. (White noise returns do not imply that the prices are well cointegrated.)
#
# 5. The strategy is betting the flags series having dominating mean-reversion behaviors, for a pair of cointegrated stocks.
#    It is not mathematically clear what justifies the rationale.
#
# 6. If accumulating mispricing index is basically using returns to reflect prices, and the raw flags look basically the same as normalized prices, why not just directly use normalized prices?


# ## Conclusion
#
# (This section follows from [Xie et al. 2016])
#
# * Copulas are able to capture both linear and non-linear relations between random variables, it is possible that the proposed method captures strong non-linear associations that the traditional methods such as distance measure tend to neglect.
#
# * Using returns overcomes the criticism that the prices series not being stationary, as used by [Liew et al. 2013].


# ## References ##
# - [Xie, W., Liew, R.Q., Wu, Y. and Zou, X., 2016. Pairs trading with copulas. The Journal of Trading, 11(3), pp.41-52.](https://efmaefm.org/0efmameetings/EFMA%20ANNUAL%20MEETINGS/2014-Rome/papers/EFMA2014_0222_FullPaper.pdf)
# - [Liew, R.Q. and Wu, Y., 2013. Pairs trading: A copula approach. Journal of Derivatives & Hedge Funds, 19(1), pp.12-30.](https://dr.ntu.edu.sg/bitstream/10220/17826/1/jdhf20131a.pdf)
# - [Rad, H., Low, R.K.Y. and Faff, R., 2016. The profitability of pairs trading strategies: distance, cointegration and copula methods. Quantitative Finance, 16(10), pp.1541-1558.](https://www.tandfonline.com/doi/pdf/10.1080/14697688.2016.1164337?casa_token=X0-FdUqDsv4AAAAA:ZFothfEHF-dO2-uDtFo2ZuFH0uzF6qijsweHD888yfPx3OZGXW6Szon1jvA2BB_AsgC5-kGYreA4fw)



// ---------------------------------------------------

// Copula_Notebook_Liew_etal(legacy).py
// arbitrage_research_basic_PCA/Copula Approach/Copula_Notebook_Liew_etal(legacy).py
# Generated from: Copula_Notebook_Liew_etal(legacy).ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Copula Pair's Trading Basic Strategy
#
# This notebook demonstrates the usage of the `copula_strategy` module.
# The framework is originally proposed in 
# - Liew, Rong Qi, and Yuan Wu. "Pairs trading: A copula approach."
# - Stander, Yolanda, Daniël Marais, and Ilse Botha. "Trading strategies with copulas."
#
# ## A Bare Minimum Intro to Copula
#
# ### What Merit Does It Have
# Consider having a pair of cointegrated stocks. 
# By analyzing their time series, one can calculate their standardized price gap as part of a distance approach, or project their long-run mean as in a cointegrated system as part of a cointegration approach. 
# However, none of the two methods are built with the assumptions on distributions from the stocks' time series, which may lead to unused information.
# Further, just using a single parameter (i.e., Euclidean distance) to quantify two coupled time series might have fundamentally simplified the problem too much.
# The copula model naturally incorporates their marginal distributions, together with other interesting properties from each copula inherited from their own structures, e.g., tail dependency for capturing rare and/or extreme moments like large, cointegrated swings in the market.
#
# Briefly speaking, a copula is a tool to capture details of how two random variables are “correlated”.
# By having a more detailed modeling framework, we expect the pairs trading strategy followed to be more realistic and robust, and possibly to bring more trading opportunities.
#
# ### Definitions for Bivariate Copula
# (**Definition using Sklar's Theorem**) For two random variables $S_1$, $S_2 \in [-\infty, \infty]$.
# $S_1$ and $S_2$ have their own fixed, continuous CDFs $F_1, F_2$.
# Consider their (cumulative) joint distribution $H(s_1, s_2) := P(S_1 \le s_1, S_2 \le s_2)$.
# Now take the uniformly distributed quantile random variable $U_1(S_1)$, $U_2(S_2)$, for every pair
# $(u_1, u_2)$ drawn from the pair's quantile we define the **bivariate copula**
# $C: [0, 1] \times [0, 1] \rightarrow [0, 1]$ as:
#
# $$
#     \begin{align}
#     C(u_1, u_2) &= P(U_1 \le u_1, U_2 \le u_2) \\
#     &= P(S_1 \le F_1^{-1}(u_1), S_2 \le F_2^{-1}(u_2)) \\
#     &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))
#     \end{align}
# $$
# where $F_1^{-1}$ and $F_2^{-1}$ are quasi-inverse of the marginal CDFs $F_1$ and $F_2$.
# The definition, although mathematically more fundamental, is not used as much in trading.
# Instead, cumulative conditional probabilities and copula density are used more often, which we will define as below:
#
# ### Conditional Probabilities
# $$
#     \begin{align}
#     P(U_1\le u_1 | U_2 = u_2) &:= \frac{\partial C(u_1, u_2)}{\partial u_2}, \\
#     P(U_2\le u_2 | U_1 = u_1) &:= \frac{\partial C(u_1, u_2)}{\partial u_1}.
#     \end{align}
# $$
# ### Copula Density
#
# $$
#     c(u_1 , u_2) := \frac{\partial^2 C(u_1, u_2)}{\partial u_1 \partial u_2},
# $$
# which by definition is the probability density.
#
# For interested readers, Nelsen's book [An Introduction to Copulas](https://www.springer.com/gp/book/9780387286594) and note [Properties and applications of copulas: A brief survey](http://w4.stern.nyu.edu/ioms/docs/sg/seminars/nelsen.pdf) presents more rigorously and thoroughly on the subject.


# ## How to Use Copula for Trading
#
# ### Use Conditional Probabilities
# This was originally proposed in [Liew et al., 2013] and [Stander et al., 2013].
# We start with a pair of stocks of interest $S_1$ and $S_2$, which can be selected by various methods.
# For example, using the Engle-Granger test for cointegration.
# By consensus, we define the spread as $S_1$ in relation to $S_2$.
# e.g. Short the spread means buying $S_1$ and/or selling $S_2$.
#
# Use **cumulative log return** data of the stocks during the training/formation period, we proceed with a **pseudo-MLE** fit to establish a copula that reflects the relation of the two stocks during the training/formation period.
#
# **Note**: <br>
# the type of processed data fed in need to be **approximately stationary**.
# i.e., $\mathbb{E}[X(t_1)] \approx \mathbb{E}[X(t_2)]$ for time series $X$, for all $t_1, t_2$ in
# the scope of interest.
# For example, if we model each stock's price to have a log-Normal distribution, then the price itself cannot be stationary after some time.
# Using cumulative log return has the same issue if the time span is sufficiently long.
# One can consider just using the daily return or its logarithm instead, given that the stock's price has a log-Normal distribution. i.e., $\frac{X(t+1)}{X(t)}$ or $\ln \left( \frac{X(t+1)}{X(t)} \right)$.
#
# - $u_i \in [0, 1]$ is the quantile of trading period data mapped by a CDF formed in the training period.
# - When $P(U_1\le u_1 | U_2 = u_2) < 0.5$, then stock 1 is considered under-valued.
# - When $P(U_1\le u_1 | U_2 = u_2) > 0.5$, then stock 1 is considered over-valued.
#
# Now we define an upper threshold $b_{up}$ (e.g. $0.95$) and a lower threshold $b_{lo}$ (e.g. $0.05$),
# then the logic goes as follows:
#
# - If $P(U_1\le u_1 | U_2 = u_2) \le b_{lo}$ and $P(U_2\le u_2 | U_1 = u_1) \ge b_{up}$, then stock 1 is
#   undervalued, and stock 2 is overvalued. Hence we long the spread.
# - If $P(U_2\le u_2 | U_1 = u_1) \le b_{lo}$ and $P(U_1\le u_1 | U_2 = u_2) \ge b_{up}$, then stock 2 is
#   undervalued, and stock 1 is overvalued. Hence we short the spread.
# - If one of the conditional probabilities cross the boundary of $0.5$ in relation to its previous time step, then we exit the position, as we consider the position is no longer valid.


# ### `CopulaStrategy` Class Functionalities
# Tools presented in this module enable the user to:
# * Transform and fit pair's price data to a given type of copula;
# * Sample and plot from a given copula;
# * Generate trading positions given the pair's data using a copula:
#     - Feed in training lists (i.e., data from 2016-2019) and thus generate a position list.
#     - Feed in a single pair's data point (i.e., EOD data from just today) and thus generate a single position.
#
# There are 8 commonly used ones that are now available: `Gumbel`, `Frank`, `Clayton`, `Joe`, `N13`, `N14`,
# `Gaussian`, and `Student` (Student-t).
# They are all subclasses of the class `Copula`, and they share some common repertoire of methods and attributes.
# However, most of the time, the user is not expected to directly use the copulas.
# All trading related functionalities are stated above, and included in the `CopulaStrategy` class.
#
# The user may choose to fit the pair's data to all provided copulas, then compare the information criterion scores (AIC,
# SIC, HQIC) to decide the best copula. One can further use the fitted copula to generate trading positions by giving
# thresholds from data.
#
# **Very Important Note**:<br>
# For `Student` copula, the user can choose to provide the degrees of freedom parameter $\nu$, as an extra input argument `nu`.
# Use $\nu = \text{sample size} - 1$ is strongly discouraged, as data from time series are clearly not mutually independent instances. For $\nu > 12$, consider using the `Gaussian` copula instead.
#
# ### `copula_calculation` Module
# To be able to calculate marginal CDFs (quantile function) for plotting purposes or processing other data to be able to feed to some copula, one may use its `find_marginal_cdf` function,
# although `ECDF` from `statsmodels.distributions.empirical_distribution` suffices for a general plotting purpose.
# This function finds an empirical CDF based on given training data, and does not generate $0$ or $1$, instead gives numbers sufficiently close to $0$ and $1$.
# This is done because when some copulas take values in $0$ or $1$, the density and/or marginal conditional probability may become $\infty$.
#
# ## Usage
# The demonstratration part has the following sections using real world data:
# 1. Importing and Data Pre-processing.
# 2. Fitting Data to Different Copulas with Training Data.
# 3. Generating Trading Positions using Testing Data.
# 4. Possible Issues Discussion.


# ### 1. Importing and Data Pre-processing


# Importing libraries and modules
import pandas as pd
import numpy as np
import datetime as dt  # For plotting x-axis as dates
import matplotlib.pyplot as plt
import statsmodels.api as sm
from arbitragelab.copula_approach.copula_strategy import CopulaStrategy
import arbitragelab.copula_approach.copula_calculation as ccalc


# For the copula approach to work, the stocks pair need to be cointegrated to begin with.
# Here we choose BKD and ESC, the same as the author, and use their daily closing price from the start of 2009 to the end of 2012.
# Note that the original price series is not what the copula will use. Instead, we use cumulative return data implied from the price series.


# Importing data
pair_prices = pd.read_csv(r'BKD_ESC_2009_2011.csv', index_col=0)
# Formatting dates
formatted_dates = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in pair_prices.index]
pair_prices.index = formatted_dates

BKD_series = pair_prices['BKD'].to_numpy()
ESC_series = pair_prices['ESC'].to_numpy()


# Now we take a look at the price series for the whole period.
# They indeed have cointegrated behavior at first glance.
# We then do a training and testing split to fit and simulate trading.


plt.figure(dpi=120)
plt.plot(formatted_dates, BKD_series, label='BKD')
plt.plot(formatted_dates, ESC_series, label='ESC')
plt.axvline(dt.date(2012, 1, 3), color='red')  # Training testing split date
plt.legend()
plt.grid()
plt.gcf().autofmt_xdate()
plt.title(r'Original Price Series of BKD and ESC')
plt.show()


# Initiate the analysis module
CS = CopulaStrategy()

# Calculate cumulative log return
BKD_clr = CS.cum_log_return(BKD_series)
ESC_clr = CS.cum_log_return(ESC_series)

# Training and testing split
training_length = 756# From 01/02/2009 to 12/30/2011 (m/d/y)

BKD_train = BKD_clr[ : training_length]
ESC_train = ESC_clr[ : training_length]
BKD_train_prices = BKD_series[ : training_length]
ESC_train_prices = ESC_series[ : training_length]
train_dates = formatted_dates[ : training_length]

BKD_test = BKD_clr[training_length : ]
ESC_test = ESC_clr[training_length : ]
BKD_test_prices = BKD_series[training_length : ]
ESC_test_prices = ESC_series[training_length : ]
test_dates = formatted_dates[training_length : ]

# Empirical CDF for the training set.
# This step is necessary for plotting.
cdf1 = ccalc.find_marginal_cdf(BKD_train)
cdf2 = ccalc.find_marginal_cdf(ESC_train)


# ### 2. Fitting Data to Different Copulas with Training Data.
# Here we fit to every copula type available in the module, and print out the scores in terms of SIC, AIC, and HQIC. The lower the score (Note they are all negative), the better the fit.
#
# **Note**:<br>
# 1. `fit_copula` returns the fit result, the fitted copula, and marginal CDFs for the two input lists. Here the `cdf1`, `cdf2` is exactly the same as directly using `find_marginal_cdf` in `copula_calculation` module.
# 2. For `Student` copula, the MLE of $\nu$ is slow.


# Fit different copulas, store the results in dictionaries
fit_result_gumbel, copula_gumbel, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Gumbel')

fit_result_frank, copula_frank, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Frank')

fit_result_clayton, copula_clayton, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Clayton')

fit_result_joe, copula_joe, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Joe')

fit_result_n13, copula_n13, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='N13')

fit_result_n14, copula_n14, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='N14')

fit_result_gauss, copula_gauss, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Gaussian')

fit_result_t, copula_t, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Student')


# print all the fit scores
print(fit_result_gumbel)
print(fit_result_frank)
print(fit_result_clayton)
print(fit_result_joe)
print(fit_result_n13)
print(fit_result_n14)
print(fit_result_gauss)
print(fit_result_t)


# It seems by the score, N13 and Student-t are reasonable choices of copula that fit our training data the best.
# Clayton also look somewhat reasonable, although the tail dependency on the upper right corner is not taken into account of.
# So at first, we take a look at their information.


print(copula_n13.describe(), '\n')
print(copula_t.describe(), '\n')
print(copula_clayton.describe(), '\n')


# Now we plot the training data (as an empirical copula), together with $5$ other copulas fitted.


fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9,18), dpi=100)
# axs[0, 0]: Empirical.
axs[0, 0].scatter(cdf1(BKD_train), cdf2(ESC_train), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'Empirical from BKD and ESC, Training Data')
axs[0, 0].set_xlim([0,1])
axs[0, 0].set_ylim([0,1])
plt.tight_layout()

# axs[0, 1]: N13.
CS.graph_copula(copula_name='N13', theta=copula_n13.theta,
                ax=axs[0, 1], s=1, num=len(ESC_train))

# axs[1, 0]: Student-t.
CS.graph_copula(copula_name='Student', cov=copula_t.cov, nu=copula_t.nu,
                ax=axs[1, 0], s=1, num=len(ESC_train))
# axs[1, 1]: Joe.
CS.graph_copula(copula_name='Joe', theta=copula_joe.theta,
                ax=axs[1, 1], s=1, num=len(ESC_train))
# axs[2, 0]: Frank.
CS.graph_copula(copula_name='Frank', theta=copula_frank.theta,
                ax=axs[2, 0], s=1, num=len(ESC_train))
# axs[2, 1]: Gumbel.
CS.graph_copula(copula_name='Gumbel', theta=copula_gumbel.theta,
                ax=axs[2, 1], s=1, num=len(ESC_train))
# axs[3, 0]: Clayton.
CS.graph_copula(copula_name='Clayton', theta=copula_clayton.theta,
                ax=axs[3, 0], s=1, num=len(ESC_train))
# axs[3, 1]: Gaussian.
CS.graph_copula(copula_name='Gaussian', cov=copula_gauss.cov,
                ax=axs[3, 1], s=1, num=len(ESC_train))
plt.show()


# ### 3. Generate Trading Positions Using Test Data
# At first we plot the testing data (as an empirical copula) and the two chosen copulas, together with three other copulas fitted.


fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9,18), dpi=100)
# axs[0, 0]: Empirical.
axs[0, 0].scatter(cdf1(BKD_test), cdf2(ESC_test), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'Empirical from BKD and ESC, Testing Data')
axs[0, 0].set_xlim([0,1])
axs[0, 0].set_ylim([0,1])

# axs[0, 1]: N13.
CS.graph_copula(copula_name='N13', theta=copula_n13.theta,
                ax=axs[0, 1], s=1, num=len(ESC_test))

# axs[1, 0]: Student-t.
CS.graph_copula(copula_name='Student', cov=copula_t.cov, nu=copula_t.nu,
                ax=axs[1, 0], s=1, num=len(ESC_test))
# axs[1, 1]: Joe.
CS.graph_copula(copula_name='Joe', theta=copula_joe.theta,
                ax=axs[1, 1], s=1, num=len(ESC_test))
# axs[2, 0]: Frank.
CS.graph_copula(copula_name='Frank', theta=copula_frank.theta,
                ax=axs[2, 0], s=1, num=len(ESC_test))
# axs[2, 1]: Gumbel.
CS.graph_copula(copula_name='Gumbel', theta=copula_gumbel.theta,
                ax=axs[2, 1], s=1, num=len(ESC_test))
# axs[3, 0]: Clayton.
CS.graph_copula(copula_name='Clayton', theta=copula_clayton.theta,
                ax=axs[3, 0], s=1, num=len(ESC_test))
# axs[3, 1]: Gaussian.
CS.graph_copula(copula_name='Gaussian', cov=copula_gauss.cov,
                ax=axs[3, 1], s=1, num=len(ESC_test))

plt.tight_layout()
plt.show()


# Also, one can run an information test with the fitted copulas using test data.
# This is a necessary step to see how consistent or robust the chosen copula model is.
# If the answer changes greatly from the training data, there are several possibilities, for example:
# * Testing data differ significantly from training data in terms of distributions, although they are still cointegrated.
# * The two stocks should be modeled by another copula that is not included in this module.
# * The market has fundamentally changed and as a result, the two stocks have decoupled.
#
# As a rule of thumb, it is always a good practice to look at multiple models instead of using only one, especially when the outcome is quite sensitive to inputs.
#
# As can be seen from the outputs below, N13 is no longer the best copula to describe the test data.


# Running information tests for the test data with the fitted copula
test_ic_n13 = CS.ic_test(BKD_test, ESC_test, cdf1, cdf2, copula=copula_n13)
test_ic_t = CS.ic_test(BKD_test, ESC_test, cdf1, cdf2, copula=copula_t)
test_ic_clayton = CS.ic_test(BKD_test, ESC_test, cdf1, cdf2, copula=copula_clayton)
print(test_ic_n13)
print(test_ic_t)
print(test_ic_clayton)


# Then we get trading positions from the two chosen copulas.
#
# **Note**:<br>
# Although theoretically, one can change the threshold to potentially bring more trading opportunities, it needs to proceed with caution, as the recommended positions are very sensitive to the choice of copula and training data if the thresholds are not as strict.


# #### Plot Trading Positions
# Plot trading positions with recalculated cumulative log return (CLR) from test data, with the starting reference day as day 1 in the test data.
#
# Here, 0 means no position, 1 means long the spread, -1 means short the spread.
#
# **Note**:<br>
# This is not the same as cumulative log returns used in the test data.
# Suppose the training/testing split happens on day $755$, then the CLR is still calculated as $\ln(S(t))-\ln(S(0))$ for $t>755$.
# However, we are plotting $\ln(S(t))-\ln(S(756))$, so that it is easier to see the spread and evaluate our recommended trading positions.
# It is thus can be understood as a *repositioned log price*, or *reset cumulative log return*.


# Generate Trading Positions
# Use N13 copula.
# Instantiate the CopulaStrategy with the fitted N13 copula.
CS_n13 = CopulaStrategy(copula=copula_n13)
positions_n13 = CS_n13.analyze_time_series(BKD_test, ESC_test, cdf1, cdf2,
                                           upper_threshold=0.95,
                                           lower_threshold=0.05,
                                           start_position=0)  # Author used 0.95 and 0.05
# Use Student-t Copula.
# Instantiate the CopulaStrategy with the fitted Student-t copula.
CS_t = CopulaStrategy(copula=copula_t)
positions_t = CS_t.analyze_time_series(BKD_test, ESC_test, cdf1, cdf2,
                                         upper_threshold=0.95,
                                         lower_threshold=0.05,
                                         start_position=0)
# Use Clayton copula.
# Instantiate the CopulaStrategy with the fitted Clayton copula.
CS_clayton = CopulaStrategy(copula=copula_clayton)
positions_clayton = CS_t.analyze_time_series(BKD_test, ESC_test, cdf1, cdf2,
                                             upper_threshold=0.95,
                                             lower_threshold=0.05,
                                             start_position=0)

# Roll all positions forward by 1 day
positions_n13 = np.roll(positions_n13, 1)
positions_t = np.roll(positions_t, 1)
positions_clayton = np.roll(positions_clayton, 1)

# Reset position at day 0
positions_n13[0] = 0
positions_t[0] = 0
positions_clayton[0] = 0


fig, axs = plt.subplots(4, 1, gridspec_kw={'height_ratios': [3, 0.7, 0.7, 0.7]}, figsize=(10,10), dpi=150)
fig.suptitle('Copula Trading Strategy Results')
# Plotting repositioned log prices
axs[0].plot(test_dates, BKD_test-BKD_test[0], label='BKD', color='cornflowerblue')
axs[0].plot(test_dates, ESC_test-ESC_test[0], label='ESC', color='seagreen')
axs[0].title.set_text('(Repositioned) Log Prices')
axs[0].legend()
axs[0].grid()
# Plotting positions from N13
axs[1].plot(positions_n13, label='Positions', color='darkorange')
axs[1].title.set_text('Positions from N13 Copula')
axs[1].set_yticks([-1,0,1])
# Plotting positions from Student-t
axs[2].plot(positions_t, label='Positions', color='darkorange')
axs[2].title.set_text(r'Positions from Student-t Copula')
axs[2].set_yticks([-1,0,1])
# Plotting positions from Clayton
axs[3].plot(positions_clayton, label='Positions', color='darkorange')
axs[3].title.set_text(r'Positions from Clayton Copula')
axs[3].set_yticks([-1,0,1])

fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Avoid title overlap
plt.show()


# #### Equity Curve for the Portfolio
# 1. Defining weights (hedging ratio)
# 2. Calculating returns
# 3. Portfolio prices
# 4. Equity curve for copula strategy
#
# There are multiple methods to define weights.
# For example, using OLS or Johansen test on training data.
# Here we define weights as below to reflect the (repositioned) series mentioned above:
# At first, we define the unnormalized weight $w_{1}^* = w_{BKD}$ and $w_2^* = w_{ESC}$ for $S_1 = S_{BKD}$ and $S_2 = S_{ESC}$ as follows:
# $$
# w_{1}^* S_{1}[0] - w_{2}^* S_{2}[0] = 0
# $$
# with another arbitrary linear constraint
# $$w_1^* - w_2^* = 1.$$
# So that it has value $0$ initially.
# Hence
# $$
# w_1^* = -\frac{S_2[0]}{S_1[0]-S_2[0]}, \quad
# w_2^* = -\frac{S_1[0]}{S_1[0]-S_2[0]}.
# $$
# Then we normalize them so that
# $$
# w_1 = \frac{w_1^*}{w_1^* + w_2^*}, \quad
# w_2 = \frac{w_2^*}{w_1^* + w_2^*}
# $$


# 1. Calculating weights
w1_star = -ESC_test_prices[0] / (BKD_test_prices[0] - ESC_test_prices[0])
w2_star = w1_star - 1

w1 = w1_star / (w1_star + w2_star)
w2 = w2_star / (w1_star + w2_star)
print('Unnormalized weight: \n\
w1_star={}, \nw2_star={},\n\
Normalized weight:\n\
w1={} \nw2={}'.format(w1_star, w2_star, w1, w2))


# 2. Calculating Portfolio Series and daily P&L
portfolio_prices = w1 * BKD_test_prices - w2 * ESC_test_prices
portfolio_pnl = np.diff(portfolio_prices, prepend=0)

# 3. Plotting portfolio prices
fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(test_dates, portfolio_prices)
ax.title.set_text('Unit Portfolio Value for Pair ("BKD", "ESC") Calcualted From Price Series')
ax.grid()
fig.autofmt_xdate()
plt.show()

# 4. Calculating strategy daily P&L
pnl_n13 = portfolio_pnl * positions_n13
pnl_t = portfolio_pnl * positions_t
pnl_clayton = portfolio_pnl * positions_clayton
equity_n13 = pnl_n13.cumsum()
equity_t = pnl_t.cumsum()
equity_clayton = pnl_clayton.cumsum()

fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(test_dates, equity_n13, label='N13')
ax.plot(test_dates, equity_t, '--', label=r'Student-t')
ax.plot(test_dates, equity_clayton, ':', label=r'Clayton')
ax.title.set_text('Strategy Performance on Unit Portfolio Calcualted From Daily P&L')
ax.grid()
fig.autofmt_xdate()
ax.legend()
plt.show()


# ### 4. Possible Issues Discussion
#
# #### Data Processing
# Log prices or CLR are not fundamentally stationary, and especially in real data, the training period may differ significantly from the testing period, rendering testing period data out of range.
# One may consider other alternatives.
#
# #### Copula Fitting
# The fitting results are very sensitive to the inputs.
# And more often than not, the fit score among copulas are not very different from each other.
# In this case, one should look at the few best-suited copulas as candidates, instead of using a single copula.
#
# #### Copula Modeling
# Modeling and trading a pair of stocks' movements using copula, as implemented in the module, treats each stocks time series data as a random variable with stationary distribution.
# Though mathematically valid, this approach does not take into account each random variable as a time series, and re-shuffle the training data will yield the exact same result in terms of  fitted copula parameters and thus recommended positions.
# However, when compared to a vanilla Euclidean distance approach, this is one step further.
#
# Moreover, all the commonly used copulas are either Archimedean (Gumbel, Frank, etc.) or Elliptic (Student-t, Gaussian), which share some nice properties such as symmetry.
# However, the best-suited copula behind a pair of stocks may not necessarily be symmetric.


# ## Conclusion
# (This section follows from [Liew et al. 2013])<br>
# Copula introduces delicate assumptions on the exact coupled structure, along with other nice properties, of two random variables.
# In the pairs trading context, two stocks time series.
#
# * When compared to the Euclidean distance or cointegration approach, copula does not rely on assumptions of linear association or correlation coefficients as a measure of dependency.
#
# * Copula-based approach results in a far richer set of information, such as the shape and nature of the dependency between the stock pairs, thereby leading to potentially more robust modeling of the pair.
#
# * Some copula choices measure well with upper and lower tail dependencies of different extent, in an environment that considers both linear and non-linear relationship. 
#
# * Copulas possess an attractive property of being invariant under strictly monotone transformations of random variables. In other words, the same copula will be obtained regardless of whether the analyst is using price series or log price series.


# ## References ##
# - [Liew, R.Q. and Wu, Y., 2013. Pairs trading: A copula approach. Journal of Derivatives & Hedge Funds, 19(1), pp.12-30.](https://link.springer.com/article/10.1057/jdhf.2013.1)
# - [Stander, Y., Marais, D. and Botha, I., 2013. Trading strategies with copulas. Journal of Economic and Financial Sciences, 6(1), pp.83-107.](https://www.researchgate.net/publication/318054326_Trading_strategies_with_copulas)



// ---------------------------------------------------

// Copula_Strategy_Basic.py
// arbitrage_research_basic_PCA/Copula Approach/Copula_Strategy_Basic.py
# Generated from: Copula_Strategy_Basic.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Copula Pair's Trading Basic Strategy
#
# This notebook demonstrates the usage of the `copula_strategy_basic` module.
# The framework is originally proposed in 
# - Liew, Rong Qi, and Yuan Wu. "Pairs trading: A copula approach."
# - Stander, Yolanda, Daniël Marais, and Ilse Botha. "Trading strategies with copulas."
#
# ## A Bare Minimum Intro to Copula
#
# ### What Merit Does It Have
# Consider having a pair of cointegrated stocks. 
# By analyzing their time series, one can calculate their standardized price gap as part of a distance approach, or project their long-run mean as in a cointegrated system as part of a cointegration approach. 
# However, none of the two methods are built with the assumptions on distributions from the stocks' time series, which may lead to unused information.
# Further, just using a single parameter (i.e., Euclidean distance) to quantify two coupled time series might have fundamentally simplified the problem too much.
# The copula model naturally incorporates their marginal distributions, together with other interesting properties from each copula inherited from their own structures, e.g., tail dependency for capturing rare and/or extreme moments like large, cointegrated swings in the market.
#
# Briefly speaking, a copula is a tool to capture details of how two random variables are “correlated”.
# By having a more detailed modeling framework, we expect the pairs trading strategy followed to be more realistic and robust, and possibly to bring more trading opportunities.


# ### Definitions for Bivariate Copula
# (**Definition using Sklar's Theorem**) For two random variables $S_1$, $S_2 \in [-\infty, \infty]$.
# $S_1$ and $S_2$ have their own fixed, continuous CDFs $F_1, F_2$.
# Consider their (cumulative) joint distribution $H(s_1, s_2) := P(S_1 \le s_1, S_2 \le s_2)$.
# Now take the uniformly distributed quantile random variable $U_1(S_1)$, $U_2(S_2)$, for every pair
# $(u_1, u_2)$ drawn from the pair's quantile we define the **bivariate copula**
# $C: [0, 1] \times [0, 1] \rightarrow [0, 1]$ as:
#
# $$
#     \begin{align}
#     C(u_1, u_2) &= P(U_1 \le u_1, U_2 \le u_2) \\
#     &= P(S_1 \le F_1^{-1}(u_1), S_2 \le F_2^{-1}(u_2)) \\
#     &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))
#     \end{align}
# $$
# where $F_1^{-1}$ and $F_2^{-1}$ are quasi-inverse of the marginal CDFs $F_1$ and $F_2$.
# The definition, although mathematically more fundamental, is not used as much in trading.
# Instead, cumulative conditional probabilities and copula density are used more often, which we will define as below:
#
# ### Conditional Probabilities
# $$
#     \begin{align}
#     P(U_1\le u_1 | U_2 = u_2) &:= \frac{\partial C(u_1, u_2)}{\partial u_2}, \\
#     P(U_2\le u_2 | U_1 = u_1) &:= \frac{\partial C(u_1, u_2)}{\partial u_1}.
#     \end{align}
# $$
# ### Copula Density
#
# $$
#     c(u_1 , u_2) := \frac{\partial^2 C(u_1, u_2)}{\partial u_1 \partial u_2},
# $$
# which by definition is the probability density.
#
# For interested readers, Nelsen's book [An Introduction to Copulas](https://www.springer.com/gp/book/9780387286594) and note [Properties and applications of copulas: A brief survey](http://w4.stern.nyu.edu/ioms/docs/sg/seminars/nelsen.pdf) presents more rigorously and thoroughly on the subject.


# ## Basic Strategy (Conditional Probabilities Threshold)
#
# ### Logic
# This was originally proposed in [Liew et al., 2013] and [Stander et al., 2013].
# We start with a pair of stocks of interest $S_1$ and $S_2$, which can be selected by various methods.
# For example, using the Engle-Granger test for cointegration.
# By consensus, we define the spread as $S_1$ in relation to $S_2$.
# e.g. Short the spread means buying $S_1$ and/or selling $S_2$.
#
# Use **cumulative log return** data of the stocks during the training/formation period, we proceed with a **pseudo-MLE** fit to establish a copula that reflects the relation of the two stocks during the training/formation period.
#
# **Note**: <br>
# the type of processed data fed in need to be **approximately stationary**.
# i.e., $\mathbb{E}[X(t_1)] \approx \mathbb{E}[X(t_2)]$ for time series $X$, for all $t_1, t_2$ in
# the scope of interest.
# For example, if we model each stock's price to have a log-Normal distribution, then the price itself cannot be stationary after some time.
# Using cumulative log return has the same issue if the time span is sufficiently long.
# One can consider just using the daily return or its logarithm instead, given that the stock's price has a log-Normal distribution. i.e., $\frac{X(t+1)}{X(t)}$ or $\ln \left( \frac{X(t+1)}{X(t)} \right)$.
#
# - $u_i \in [0, 1]$ is the quantile of trading period data mapped by a CDF formed in the training period.
# - When $P(U_1\le u_1 | U_2 = u_2) < 0.5$, then stock 1 is considered under-valued.
# - When $P(U_1\le u_1 | U_2 = u_2) > 0.5$, then stock 1 is considered over-valued.
#
# Now we define an upper threshold $b_{up}$ (e.g. $0.95$) and a lower threshold $b_{lo}$ (e.g. $0.05$),
# then the logic goes as follows:
#
# - If $P(U_1\le u_1 | U_2 = u_2) \le b_{lo}$ and $P(U_2\le u_2 | U_1 = u_1) \ge b_{up}$, then stock 1 is
#   undervalued, and stock 2 is overvalued. Hence we long the spread.
# - If $P(U_2\le u_2 | U_1 = u_1) \le b_{lo}$ and $P(U_1\le u_1 | U_2 = u_2) \ge b_{up}$, then stock 2 is
#   undervalued, and stock 1 is overvalued. Hence we short the spread.
# - If **both/either** conditional probabilities cross the boundary of $0.5$, then we exit the position, as we consider the position no longer valid.


# ### Ambiguities and Comments
# The authors did not specify what will happen if the followings occur:
#
# 1. When there is an open signal and an exit signal.
# 2. When there is an open signal and currently there is a position.
# 3. When there is a long and short signal together.
#
# Here is our take:
#
# 1. Exit signal overrides open signal.
# 2. Flip the position to the signal's suggestion. For example, originally have a short position, and receives
#    a long signal, then the position becomes long.
# 3. Technically this should never happen with the default trading logic. However, if it did happen for whatever 
#    reason, long + short signal will lead to no opening signal and the positions will not change, unless there
#    is an exit signal and that resets the position to 0.
#
# For exiting a position, the authors proposed using **'and'** logic: Both conditional probabilities need to cross $0.5$.
# However, we found this too strict and sometimes fails to exit a position when it should.
# Therefore we also provide the **'or'** logic: At least one of the conditional probabilities cross $0.5$.


# ### `BasicCopulaStrategy` Class Functionalities
# Tools presented in this module enable the user to:
# * Transform and fit pair's price data to a given type of copula;
# * Generate trading positions given the pair's data using a copula:
#     - Feed in training lists (i.e., data from 2016-2019) and thus generate a position list.
#     - Feed in a single pair's data point (i.e., EOD data from just today) and thus generate a single position.
#
# There are 8 commonly used pure copulas that are now available: `Gumbel`, `Frank`, `Clayton`, `Joe`, `N13`, 
# `N14`, `Gaussian`, and `Student` (Student-t), and 2 mixed copulas `CFGMixCop` (Clayton-Frank-Gumbel) and 
# `CTGMixCop` (Clayton-Student-Gumbel).
# They are all subclasses of the class `Copula` and `MixedCopula`, and they share some common repertoire of methods and attributes.
# However, most of the time for trading purposes, the user is not expected to directly use the copulas.
# All trading related functionalities are stated above, and included in the `BasicCopulaStrategy` class.
#
# The user may choose to fit the pair's data to all provided copulas, then compare the information criterion scores (AIC,
# SIC, HQIC, Log-likelihood) to decide the best copula. One can further use the fitted copula to generate trading positions by giving
# thresholds from data.
#
# **Very Important Note**:<br>
# For `Student` copula, the user can choose to provide the degrees of freedom parameter $\nu$, as an extra input argument `nu`.
# Use $\nu = \text{sample size} - 1$ is strongly discouraged, as data from time series are clearly not mutually independent instances. For $\nu > 12$, consider using the `Gaussian` copula instead.
#
# Mixed copulas especially `CTGMixCop` is relatively slow to fit. However, it generally provides the best log-likelihood score.


# ### `copula_calculation` Module
# To be able to calculate marginal CDFs (quantile function) for plotting purposes or processing other data to be able to feed to some copula, one may use its `construct_ecdf_lin` function.
# It is a wrapper around `ECDF` function from `statsmodels`, and allows linear interpolation between points instead of using a step function as `ECDF`.
#
# This function finds an empirical CDF based on given training data, and does not generate $0$ or $1$, instead gives numbers sufficiently close to $0$ and $1$.
# This is done because when some copulas take values in $0$ or $1$, the calculation of density and/or marginal conditional probability may suffer numerical issues.
#
# ## Usage
# The demonstration part has the following sections using real-world data:
# 1. Importing and Data Pre-processing.
# 2. Fitting Data to Different Copulas with Training Data.
# 3. Generating Trading Positions using Testing Data.
# 4. Possible Issues Discussion.


# ### 1. Importing and Data Pre-processing


# Importing libraries and modules
import pandas as pd
import numpy as np
import datetime as dt  # For plotting x-axis as dates
import matplotlib.pyplot as plt
import statsmodels.api as sm

from arbitragelab.copula_approach.copula_strategy_basic import BasicCopulaStrategy
import arbitragelab.copula_approach.copula_calculation as ccalc


# For the copula approach to work, the stocks pair need to be cointegrated to begin with.
# Here we choose BKD and ESC, the same as the author, and use their daily closing price from the start of 2009 to the end of 2012.
# Note that the original price series is not what the copula will use. Instead, we use cumulative return data implied from the price series.


# Importing data
pair_prices = pd.read_csv(r'BKD_ESC_2009_2011.csv', index_col=0, parse_dates=True)


# Now we take a look at the price series for the whole period.
# They indeed have cointegrated behavior at first glance.
# We then do a training and testing split to fit and simulate trading.


# Plotting data
plt.figure(dpi=120)
plt.plot(pair_prices['BKD'], label='BKD')
plt.plot(pair_prices['ESC'], label='ESC')
plt.axvline(dt.date(2012, 1, 3), color='red')  # Training testing split date
plt.legend()
plt.grid()
plt.gcf().autofmt_xdate()
plt.title(r'Original Price Series of BKD and ESC')
plt.show()


# Initiate the analysis module
BCS = BasicCopulaStrategy()

# Training and testing split
training_length = 756 # From 01/02/2009 to 12/30/2011 (m/d/y)

prices_train = pair_prices.iloc[: training_length]
prices_test = pair_prices.iloc[training_length :]

# Empirical CDF for the training set.
# This step is only necessary for plotting.
cdf1 = ccalc.construct_ecdf_lin(prices_train['BKD'])
cdf2 = ccalc.construct_ecdf_lin(prices_train['ESC'])


# ### 2. Fitting Data to Different Copulas with Training Data.
# Here we fit to every copula type available in the module, and print out the scores in terms of SIC, AIC, and HQIC. The lower the score (Note they are all negative), the better the fit.
#
# **Note**:<br>
# 1. `fit_copula` returns the fit result, the fitted copula, and marginal CDFs for the two input lists. Here the `cdf1`, `cdf2` is exactly the same as directly using `construct_ecdf_lin` function in `copula_calculation` module.
# 2. For `Student` copula, the MLE of $\nu$ is slow.
# 3. For mixed copulas, the fit is slow however they generally gives the best score.
# 4. You may need to tune the fitting parameters for mixed copulas: gamma_scad, a_scad to avoid warnings outputs.


# Fit different copulas, store the results in dictionaries
fit_result_gumbel, copula_gumbel, cdf1, cdf2 =\
    BCS.fit_copula(data=prices_train, copula_name='Gumbel')

fit_result_frank, copula_frank, cdf1, cdf2 =\
    BCS.fit_copula(data=prices_train, copula_name='Frank')

fit_result_clayton, copula_clayton, cdf1, cdf2 =\
    BCS.fit_copula(data=prices_train, copula_name='Clayton')

fit_result_joe, copula_joe, cdf1, cdf2=\
    BCS.fit_copula(data=prices_train, copula_name='Joe')

fit_result_cfg, copula_cfg, cdf1, cdf2=\
    BCS.fit_copula(data=prices_train, copula_name='CFGMixCop', gamma_scad=0.45)

fit_result_n14, copula_n14, cdf1, cdf2=\
    BCS.fit_copula(data=prices_train, copula_name='N14')

fit_result_gauss, copula_gauss, cdf1, cdf2 =\
    BCS.fit_copula(data=prices_train, copula_name='Gaussian')

fit_result_t, copula_t, cdf1, cdf2=\
    BCS.fit_copula(data=prices_train, copula_name='Student')


# **Note**: The warnings generated above is due to the optimization package by scipy not strictly following the prescribed bounds in the mixed copula fitting algo. And based on our experiments it does not influence the fit result with statistical significance. To completely get rid of the warnings, you will need to choose specific values `gamma_scad` and `a_scad` of the fitting parameter, and this heavily depends on the data."


# Print all the fit scores
print(fit_result_gumbel)
print(fit_result_frank)
print(fit_result_clayton)
print(fit_result_joe)
print(fit_result_cfg)
print(fit_result_n14)
print(fit_result_gauss)
print(fit_result_t)


# It seems by the score, CFG and Student-t are reasonable choices of copula that fit our training data the best.
# Clayton also looks somewhat reasonable, although the tail dependency on the upper right corner is not taken into account.
# So at first, we take a look at their information.


# Print copula descriptions
print(copula_cfg.describe(), '\n')
print(copula_t.describe(), '\n')
print(copula_clayton.describe(), '\n')


# Now we plot the training data (as an empirical copula), together with $5$ other copulas fitted.


# Plotting copulas
fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9,18), dpi=100)

# axs[0, 0]: Empirical.
axs[0, 0].scatter(prices_train['BKD'].apply(cdf1), prices_train['ESC'].apply(cdf2), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'Empirical from BKD and ESC, Training Data')
axs[0, 0].set_xlim([-0.02, 1.02])
axs[0, 0].set_ylim([-0.02, 1.02])
plt.tight_layout()

# axs[0, 1]: N13.
copula_cfg.plot(ax=axs[0, 1], s=1, num=training_length)

# axs[1, 0]: Student-t.
copula_t.plot(ax=axs[1, 0], s=1, num=training_length)

# axs[1, 1]: Joe.
copula_joe.plot(ax=axs[1, 1], s=1, num=training_length)
                
# axs[2, 0]: Frank.
copula_frank.plot(ax=axs[2, 0], s=1, num=training_length)
                
# axs[2, 1]: Gumbel.
copula_gumbel.plot(ax=axs[2, 1], s=1, num=training_length)

# axs[3, 0]: Clayton.
copula_clayton.plot(ax=axs[3, 0], s=1, num=training_length)

# axs[3, 1]: Gaussian.
copula_gauss.plot(ax=axs[3, 1], s=1, num=training_length)

plt.show()


# ### 3. Generate Trading Positions Using Test Data
# At first, we plot the testing data (as an empirical copula) and the two chosen copulas, together with three other copulas fitted.


# Plotting copulas
fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9,18), dpi=100)

# axs[0, 0]: Empirical.
axs[0, 0].scatter(prices_test['BKD'].map(cdf1), prices_test['ESC'].map(cdf2), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'Empirical from BKD and ESC, Test Data')
axs[0, 0].set_xlim([-0.02, 1.02])
axs[0, 0].set_ylim([-0.02, 1.02])
plt.tight_layout()

test_length = len(prices_test)

# axs[0, 1]: N13.
copula_cfg.plot(ax=axs[0, 1], s=1, num=test_length)

# axs[1, 0]: Student-t.
copula_t.plot(ax=axs[1, 0], s=1, num=test_length)

# axs[1, 1]: Joe.
copula_joe.plot(ax=axs[1, 1], s=1, num=test_length)
                
# axs[2, 0]: Frank.
copula_frank.plot(ax=axs[2, 0], s=1, num=test_length)
                
# axs[2, 1]: Gumbel.
copula_gumbel.plot(ax=axs[2, 1], s=1, num=test_length)

# axs[3, 0]: Clayton.
copula_clayton.plot(ax=axs[3, 0], s=1, num=test_length)

# axs[3, 1]: Gaussian.
copula_gauss.plot(ax=axs[3, 1], s=1, num=test_length)

plt.show()


# Then we get trading positions from the two chosen copulas.
#
# **Note**:<br>
# Although theoretically, one can change the threshold to potentially bring more trading opportunities, it needs to proceed with caution, as the recommended positions are very sensitive to the choice of copula and training data if the thresholds are not as strict.


# #### Plot Trading Positions
# Plot trading positions with recalculated cumulative log return (CLR) from test data, with the starting reference day as day 1 in the test data.
#
# Here, 0 means no position, 1 means long the spread, -1 means short the spread.
#
# **Note**:<br>
# This is not the same as cumulative log returns used in the test data.
# Suppose the training/testing split happens on day $755$, then the CLR is still calculated as $\ln(S(t))-\ln(S(0))$ for $t>755$.
# However, we are plotting $\ln(S(t))-\ln(S(756))$, so that it is easier to see the spread and evaluate our recommended trading positions.
# It is thus can be understood as a *repositioned log price*, or *reset cumulative log return*.


# Generate Trading Positions
# Use CFG copula.
# Those uses 'and' logic by default.
# Instantiate the CopulaStrategy with the fitted N13 copula.
BCS_cfg = BasicCopulaStrategy(copula=copula_cfg)
positions_cfg = BCS_cfg.get_positions(prices_test, cdf1, cdf2, open_thresholds=(0.05, 0.95),
                                      init_pos=0)  # Author used 0.05 and 0.95, it is thus optional.
# Use Student-t Copula.
# Instantiate the CopulaStrategy with the fitted Student-t copula.
BCS_t = BasicCopulaStrategy(copula=copula_t)
positions_t = BCS_t.get_positions(prices_test, cdf1, cdf2, open_thresholds=(0.05, 0.95),
                                  init_pos=0)
# Use Clayton copula.
# Instantiate the CopulaStrategy with the fitted Clayton copula.
BCS_clayton = BasicCopulaStrategy(copula=copula_clayton)
positions_clayton = BCS_clayton.get_positions(prices_test, cdf1, cdf2, open_thresholds=(0.05, 0.95),
                                              init_pos=0)

# Instantiate the CopulaStrategy with the fitted N13 copula.
# Use 'or' logic.
BCS_cfg = BasicCopulaStrategy(copula=copula_cfg)
positions_cfg_or = BCS_cfg.get_positions(prices_test, cdf1, cdf2, open_thresholds=(0.05, 0.95),
                                      init_pos=0, exit_rule='or')

# Roll all positions forward by 1 day
positions_cfg = positions_cfg.shift(1)
positions_t = positions_t.shift(1)
positions_clayton = positions_clayton.shift(1)
positions_cfg_or =positions_cfg_or.shift(1)

# Reset position at day 0
positions_cfg[0] = 0
positions_t[0] = 0
positions_clayton[0] = 0
positions_cfg_or[0] = 0


# Plotting generated positions
fig, axs = plt.subplots(5, 1, gridspec_kw={'height_ratios': [3, 0.7, 0.7, 0.7, 0.7]}, figsize=(10,11), dpi=150)
fig.suptitle('Copula Trading Strategy Results')

# Plotting repositioned log prices
axs[0].plot((prices_test['BKD'] / prices_test['BKD'][0]).map(np.log), label='BKD', color='cornflowerblue')
axs[0].plot((prices_test['ESC'] / prices_test['ESC'][0]).map(np.log), label='ESC', color='seagreen')
axs[0].title.set_text('Repositioned Log Prices')
axs[0].legend()
axs[0].grid()

# Plotting positions from CFG
axs[1].plot(positions_cfg, label='Positions', color='darkorange')
axs[1].title.set_text('Positions from Clayton-Frank-Gumbel Mixed Copula, AND logic')
axs[1].set_yticks([-1,0,1])

# Plotting positions from Student-t
axs[2].plot(positions_t, label='Positions', color='darkorange')
axs[2].title.set_text(r'Positions from Student-t Copula, AND logic')
axs[2].set_yticks([-1,0,1])

# Plotting positions from Clayton
axs[3].plot(positions_clayton, label='Positions', color='darkorange')
axs[3].title.set_text(r'Positions from Clayton Copula, AND logic')
axs[3].set_yticks([-1,0,1])

# Plotting positions from CFG
axs[4].plot(positions_cfg_or, label='Positions', color='darkorange')
axs[4].title.set_text(r'Positions from Clayton-Frank-Gumbel Mixed Copula, OR logic')
axs[4].set_yticks([-1,0,1])

fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Avoid title overlap
plt.show()


# #### Equity Curve for the Portfolio
# 1. Defining weights (hedging ratio)
# 2. Calculating returns
# 3. Portfolio prices
# 4. Equity curve for copula strategy
#
# There are multiple methods to define weights.
# For example, using OLS or Johansen test on training data.
# Here we define weights as below to reflect the (repositioned) series mentioned above:
# At first, we define the unnormalized weight $w_{1}^* = w_{BKD}$ and $w_2^* = w_{ESC}$ for $S_1 = S_{BKD}$ and $S_2 = S_{ESC}$ as follows:
# $$
# w_{1}^* S_{1}[0] - w_{2}^* S_{2}[0] = 0
# $$
# with another arbitrary linear constraint
# $$w_1^* - w_2^* = 1.$$
# So that it has value $0$ initially.
# Hence
# $$
# w_1^* = -\frac{S_2[0]}{S_1[0]-S_2[0]}, \quad
# w_2^* = -\frac{S_1[0]}{S_1[0]-S_2[0]}.
# $$
# Then we normalize them so that
# $$
# w_1 = \frac{w_1^*}{w_1^* + w_2^*}, \quad
# w_2 = \frac{w_2^*}{w_1^* + w_2^*}
# $$


# 1. Calculating weights
w1_star = -prices_test['ESC'][0] / (prices_test['BKD'][0] - prices_test['ESC'][0])
w2_star = w1_star - 1

w1 = w1_star / (w1_star + w2_star)
w2 = w2_star / (w1_star + w2_star)

print('Unnormalized weight: \n\
w1_star={}, \nw2_star={},\n\
Normalized weight:\n\
w1={} \nw2={}'.format(w1_star, w2_star, w1, w2))


# 2. Calculating Portfolio Series and daily P&L
portfolio_prices = w1 * prices_test['BKD'] - w2 * prices_test['ESC']
portfolio_pnl = np.diff(portfolio_prices, prepend=0)

# 3. Plotting portfolio prices
fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(portfolio_prices)
ax.title.set_text('Unit Portfolio Value for Pair ("BKD", "ESC") Calcualted From Price Series')
ax.grid()
fig.autofmt_xdate()
plt.show()

# 4. Calculating strategy daily P&L
pnl_cfg = portfolio_pnl * positions_cfg
pnl_t = portfolio_pnl * positions_t
pnl_clayton = portfolio_pnl * positions_clayton
pnl_cfg_or = portfolio_pnl * positions_cfg_or
equity_cfg = pnl_cfg.cumsum()
equity_t = pnl_t.cumsum()
equity_clayton = pnl_clayton.cumsum()
equity_cfg_or = pnl_cfg_or.cumsum()

fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(equity_cfg, label='CFG AND')
ax.plot(equity_t, '--', label=r'Student-t AND')
ax.plot(equity_clayton, ':', label=r'Clayton AND')
ax.plot(equity_cfg_or, '-.', label='CFG OR')
ax.title.set_text('Strategy Performance on Unit Portfolio Calcualted From Daily P&L')
ax.grid()
fig.autofmt_xdate()
ax.legend()
plt.show()


# ### 4. Possible Issues Discussion
#
# #### Data Processing
# Log prices or CLR are not fundamentally stationary, and especially in real data, the training period may differ significantly from the testing period, rendering testing period data out of range.
# One may consider other alternatives.
#
# #### Copula Fitting
# The fitting results are very sensitive to the inputs.
# And more often than not, the fit score among copulas are not very different from each other.
# In this case, one should look at the few best-suited copulas as candidates, instead of using a single copula.
#
# #### Copula Modeling
# Modeling and trading a pair of stocks' movements using copula, as implemented in the module, treats each stocks time series data as a random variable with stationary distribution.
# Though mathematically valid, this approach does not take into account each random variable as a time series, and re-shuffle the training data will yield the exact same result in terms of  fitted copula parameters and thus recommended positions.
# However, when compared to a vanilla Euclidean distance approach, this is one step further.
#
# Moreover, all the commonly used copulas are either Archimedean (Gumbel, Frank, etc.) or Elliptic (Student-t, Gaussian), which share some nice properties such as symmetry.
# However, the best-suited copula behind a pair of stocks may not necessarily be symmetric.


# ## Conclusion
# (This section follows from [Liew et al. 2013])<br>
# Copula introduces delicate assumptions on the exact coupled structure, along with other nice properties, of two random variables.
# In the pairs trading context, two stocks time series.
#
# * When compared to the Euclidean distance or cointegration approach, copula does not rely on assumptions of linear association or correlation coefficients as a measure of dependency.
#
# * Copula-based approach results in a far richer set of information, such as the shape and nature of the dependency between the stock pairs, thereby leading to potentially more robust modeling of the pair.
#
# * Some copula choices measure well with upper and lower tail dependencies of different extent, in an environment that considers both linear and non-linear relationship. 
#
# * Copulas possess an attractive property of being invariant under strictly monotone transformations of random variables. In other words, the same copula will be obtained regardless of whether the analyst is using price series or log price series.


# ## References ##
# - [Liew, R.Q. and Wu, Y., 2013. Pairs trading: A copula approach. Journal of Derivatives & Hedge Funds, 19(1), pp.12-30.](https://link.springer.com/article/10.1057/jdhf.2013.1)
# - [Stander, Y., Marais, D. and Botha, I., 2013. Trading strategies with copulas. Journal of Economic and Financial Sciences, 6(1), pp.83-107.](https://www.researchgate.net/publication/318054326_Trading_strategies_with_copulas)



// ---------------------------------------------------

// basic_distance_approach.py
// arbitrage_research_basic_PCA/Distance Approach/basic_distance_approach.py
# Generated from: basic_distance_approach.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Pairs Trading: Performance of a Relative Value Arbitrage Rule__ _by_ Gatev et al.


# # Distance Approach


# This description of the distance approach closely follows the paper by _Gatev, E., Goetzmann, W. N.,_ and _Rouwenhorst, K. G._ __Pairs Trading: Performance of a Relative Value Arbitrage Rule__  [available here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=141615). 


# ## Introduction


# The distance approach works as follows:
# - First, a historical period is defined, cumulative returns for assets in this period are normalized.
# - Second, using the Euclidean squared distance on the normalized price time series, $n$ closest pairs of assets are picked.
# - After the pairs are formed, the trading period starts, and the trading signals are generated. The mechanism
#   behind this process if the following:
#   - If the difference between the price of elements in a pair diverged by
#     more than 2 standard deviations (calculated for each pair during the training period), the positions are
#     opened - long for the element with a lower price in a portfolio and short for an element with a higher price
#     in a portfolio.
#   - These positions are closed when the normalized prices cross or when the trading period ends.
#
# Using this standard description, the distance approach is a parameter-free strategy.
#
# **Note:** No cointegration tests (as opposed to the mean reversion approach) are being performed in the distance
# approach. As spotted in the work by Krauss (2015), dependencies found using this approach can be spurious.
# This also leads to higher divergence risks, and as shown in the work by Do and Faff (2010), up to 32% of
# pairs identified by this method are not converging.
#
# There are, however, possible adjustments to this strategy, like choosing distances other from the Euclidean
# square distance, adjusting the threshold to enter a trade for each pair, etc. 


# ## Pairs formation step
#
# This stage of the DistanceStrategy consists of the following steps:
#
# 1. **Normalization of the input data.**
#
# To use the Euclidean square distance, the training price time series are being normalized using the following
# formula:
#
# $$P_{normalized} = \frac{P - min(P)}{max(P) - min(P)}$$
#
# where $P$ is the training price series of an asset, $min(P)$ and $max(P)$ are the minimum and maximum values from the price series.
#
# 2. **Finding pairs.**
#
# Using the normalized price series, the distances between each pair of assets are calculated. These
# distances are then sorted in the ascending order and the :math:`n` closest pairs are picked (our
# function also allows skipping a number of first pairs, so one can choose pairs 10-15 to study).
#
# The distances between elements (Euclidean square distance - SSD) are calculated as:
#
# $$SSD = \sum^{N}_{t=1} (P^1_t - P^2_t)^{2}$$
#
# where $P^1_t$ and $P^2_t$ are normalized prices at time $t$ for the first and
# the second elements in a pair.
#
# Using the prices of elements in a pair a portfolio is being constructed - the difference between
# their normalized prices.
#
# 3. **Calculating historical volatility.**
#
# For $n$ portfolios (differences between normalized price series of elements) calculated in the
# previous step, their volatility is being calculated. Historical standard deviations of these portfolios
# will later be used to generate trading signals.


# ## Trading signals generation
#
#
# After pairs were formed, we can proceed to the second stage of the DistanceStrategy - trading
# signals generation. The input to this stage is a dataframe with testing price series for assets - not
# used in the pairs formation stage.
#
# This stage of the DistanceStrategy consists of the following steps:
#
# 1. **Normalization of the input data.**
#
# Using the same approach as in the pairs formation stage, we normalize the input trading dataset using
# the same maximum and minimum historical values from the training price series.
#
# 2. **Portfolios creation.**
#
# In this step, the portfolios are being constructed based on the asset pairs chosen in the pairs
# formation step. Portfolio values series are differences between normalized price series of elements
# in a pair - as we're opening a long position for the first element in a pair and a short position for
# the second element in a pair. A buy signal generated by the strategy means going long on the first
# element and short on the second. A sell signal means the opposite - going short on the first element
# and long on the second element.
#
# 3. **Generating signals.**
#
# If the portfolio value exceeds two historical deviations, a sell signal is generated - we expect
# the price of the first element to decrease and the price of the second element to increase. And if
# the value of the portfolio is below minus two historical deviations, a buy signal is generated.
#
# An open position is closed when the portfolio value crosses the zero mark - or when the prices of
# elements in a pair cross. So at any given time, we have one (buy or sell) or none active positions
# opened. This makes cost allocation for the strategy easier. Resulting trading signals are target
# quantities of portfolios to hold for each pair (with values -1, 0, or +1).


# ## Results output and plotting
#
# The DistanceStrategy class contains multiple methods to get results in the desired form.
#
# Functions that can be used to get data:
#
# - **get_signals()** outputs generated trading signals for each pair.
#
# - **get_portfolios()** outputs values series of each pair portfolios.
#
# - **get_scaling_parameters()** outputs scaling parameters from the training dataset used to normalize data.
#
# - **get_pairs()** outputs a list of tuples, containing chosen top pairs in the pairs formation step.
#
# Functions that can be used to plot data:
#
# - **plot_pair()** plots normalized price series for elements in a given pair and the corresponding
#   trading signals for portfolio of these elements.
#
# - **plot_portfolio()** plots portfolio value for a given pair and the corresponding trading signals.


# ## Usage of the Algorithms


# Let's use the above strategy on real data. 
#
# First, we will choose a training period of 12 months to form pairs. Second, we'll create trading signals for the following 6 months window. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# As a dataset we'll download price time series for 176 stocks over a period from 01.2018 to 07.2019. First 12 months of data will be used for training and the following 6 months for trading signal generation and analysis of results. 


# List of tickers to use in the analysis
tickers = ['MMM', 'ABT', 'ANF', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A', 'APD',
           'AKAM', 'AA', 'ALXN', 'ATI', 'ALL', 'MO', 'AMZN', 'AEE',
           'AEP', 'AXP', 'AIG', 'AMT', 'AMP', 'ABC', 'AMGN', 'APH', 'ADI', 'AON',
           'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'AIZ', 'T', 'ADSK', 'ADP', 'AN',
           'AZO', 'AVB', 'AVY', 'BLL', 'BAC', 'BK', 'BAX', 'BDX', 'BBBY', 'BIG',
           'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'CHRW', 'COG',
           'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CNP', 'CERN', 'CF', 'SCHW',
           'CVX', 'CMG', 'CB', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CTXS', 'CLF',
           'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',
           'CNX', 'ED', 'STZ', 'GLW', 'COST', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI',
           'DHR', 'DRI', 'DVA', 'DE', 'XRAY', 'DVN', 'DFS', 'DISCA',
           'DLTR', 'D', 'RRD', 'DOV', 'DTE', 'DD', 'DUK', 'ETFC', 'EMN',
           'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EOG', 'EQT',
           'EFX', 'EQR', 'EL', 'EXC', 'EXPE', 'EXPD', 'XOM', 'FFIV', 'FAST', 'FDX',
           'FIS', 'FITB', 'FHN', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC',
           'FTI', 'F', 'FOSL', 'BEN', 'FCX', 'GME', 'GPS', 'GD', 'GE', 'GIS',
           'GPC', 'GNW', 'GILD', 'GS', 'GT', 'GOOG', 'GWW', 'HAL', 'HOG', 'HIG',
           'HAS', 'HP', 'HES', 'HPQ', 'HD', 'HON', 'HRL', 'HST', 'HUM', 'HBAN',
           'ITW']

# Loading data
train_data =  yf.download(tickers, start="2018-01-03", end="2019-01-01")
test_data =  yf.download(tickers, start="2019-01-02", end="2019-07-01")

# Taking close prices for chosen instruments
train_data = train_data["Adj Close"]
test_data = test_data["Adj Close"]

# Looking at the downloaded data
train_data.head()


# ### Forming pairs


# Now let's form pairs and calculate historical volatilities for chosen portfolio pairs based on training data.


# Initialising an object containing needed methods
strategy = al.distance_approach.DistanceStrategy()

# Performing the pairs formation step and picking top 20 pairs
strategy.form_pairs(train_data, num_top=20)

# Getting scaling values used to normalize data, a list of created pairs and historical volatility for each chosen pair portfolio
scaling_parameters = strategy.get_scaling_parameters()
pairs = strategy.get_pairs()
historical_std = strategy.train_std


# Looking at the scaling parameters 
scaling_parameters


# These scaling parameters can be used to calculate weights for elements when creating a portfolio.
#
# For example, if we have a pair portfolio of ('A', 'AA'), we can construct series of their normalized prices as:


# Normalizing the price series on our own (already done inside the DistanceStrategy class)
A_series_scaled = (train_data['A'] - scaling_parameters['min_value']['A']) / \
                  (scaling_parameters['max_value']['A'] - scaling_parameters['min_value']['A'])

AA_series_scaled = (train_data['AA'] - scaling_parameters['min_value']['AA']) / \
                   (scaling_parameters['max_value']['AA'] - scaling_parameters['min_value']['AA'])


# Plotting the results
plt.figure(figsize=(12,5))

ax1 = A_series_scaled.plot(color='blue', label='A normalized series')
ax2 = AA_series_scaled.plot(color='red', label='AA normalized series')

plt.legend()
plt.show()


# Looking at top closest pairs 
pairs


# These pairs will be used during the trading signal generation stage.


# Looking at historical standard deviations of pair portfolios
historical_std


# Generally, we can observe that with the increase of Euclidean distance between pairs the volatility is also rising.


# ### Generating trading signals


# Now let's generate trading signals for the testing dataset.


# Performing the signal generation stage using (2 * st. variation) as a threshold
strategy.trade_pairs(test_data, divergence=2)

# Getting series of portfolio values, trading signals, and normalized price series of elements for each chosen pair
portfolio_series = strategy.get_portfolios()
trading_signals = strategy.get_signals()
normalized_prices = strategy.normalized_data


# Looking at calculated portfolio value series
portfolio_series.head()


# Looking at generated trading signals - target quantities of portfolios to hold
trading_signals.head()


# Also normalized price series for each asset
normalized_prices.head()


# The DistanceStrategy class also allows plotting data for a chosen pair. Looking again at the list of chosen pairs to pick a pair to plot.


# Looking at top closest pairs 
pairs


# Let's look at normalized prices, portfolio values and generated trading signals for the pair ('BAC', 'C') - with number 7 (counting from zero).


# Plotting normalized price series of elements in a pair
figure_pair = strategy.plot_pair(7)


# Plotting portfolio value series
figure_portfolio = strategy.plot_portfolio(7)


# This pair of stocks is moving similarly over the testing period. A signal to open a sell position on a portfolio is generated in the middle of January 2019 and a signal to close this position is generated around the end of May 2019.
#
# As the long asset in a portfolio is 'BAC' and the short asset is 'C', the signal to sell a portfolio means we should sell 'BAC' and buy 'C'.
#
# We can either buy and sell one share for each asset in a pair or calculate weights for 'BAC' and 'C' based on starting prices and scaling parameters.
#
# First, we should scale by starting prices of each stock and next by scaling parameters ($max(P) - min(P)$) (returns are proportional to initial prices of stocks and negatively proportional to the scaling parameter):
#
# - $Scale_{1} = \frac{P_{1}^{0}}{P_{1}^{0} + P_{2}^{0}} * \frac{(max(P_{2}) - min(P_{2})}{(max(P_{1}) - min(P_{1})) + (max(P_{2}) - min(P_{2}))}$
#
# - $Scale_{2} = \frac{P_{2}^{0}}{P_{1}^{0} + P_{2}^{0}} * \frac{(max(P_{1}) - min(P_{1})}{(max(P_{1}) - min(P_{1})) + (max(P_{2}) - min(P_{2}))}$


# Looking at the scaling parameters (min and max values) used for elements in a portfolio
pair_scales = scaling_parameters.loc[['BAC', 'C']]

pair_scales


# So the scaling parameters for 'BAC' and 'C' are
maxmin_BAC = pair_scales.loc['BAC'][1] - pair_scales.loc['BAC'][0]
maxmin_C = pair_scales.loc['C'][1] - pair_scales.loc['C'][0]

scale_BAC = (test_data['BAC'][0] / (test_data['BAC'][0] + test_data['C'][0])) * (maxmin_C / (maxmin_BAC + maxmin_C))
scale_C = (test_data['C'][0] / (test_data['BAC'][0] + test_data['C'][0])) * (maxmin_BAC / (maxmin_BAC + maxmin_C))

print('Scaling parameter for BAC is ', scale_BAC)
print('Scaling parameter for C is ', scale_C)


# Now, let's check how much profit would this distance strategy generate on a given (BAC', 'C') pair.


# Returns of elemrnts in a test dataset
test_data_returns = (test_data / test_data.shift(1) - 1)[1:]

test_data_returns.head()


# For unscaled portfolio we'll invest 50% into the 'BAC' asset and 50% in the 'C' asset. 
#
# For scaled portfolio we should calculate the weights - make scales for 'BAC' and 'C' sum up to 1.


weight_BAC = scale_BAC / (scale_BAC + scale_C)
weight_C = 1 - weight_BAC

print("For scaled portfolio we'll invest ", round(weight_BAC, 3), "% into the BAC asset.")
print("And ", round(weight_C, 3), "% into the C asset.")


# Let's test that weight parameters are calculated right


# Pair portfolio price from returns using weight parameters
pair_portfolio_returns = test_data_returns['BAC'] * weight_BAC - test_data_returns['C'] * weight_C
pair_portfolio_price = (pair_portfolio_returns + 1).cumprod()
pair_portfolio_price.plot(title="Portfolio values for pair ('BAC', 'C') calcualted from returns based on weights", figsize=(10,5));


# As we can see, this price performance matches the pair portfolio price performance from the DistanceStrategy class plot_portfolio() function. 


# Invested portfolio prices for scaled and unscaled weights
portfolio_returns_unscaled = test_data_returns['BAC'] * 0.5 - test_data_returns['C'] * 0.5
portfolio_returns_unscaled = portfolio_returns_unscaled * (trading_signals["('BAC', 'C')"].shift(1))
portfolio_price_unscaled = (portfolio_returns_unscaled + 1).cumprod()

portfolio_returns_scaled = test_data_returns['BAC'] * weight_BAC - test_data_returns['C'] * weight_C
portfolio_returns_scaled = portfolio_returns_scaled * (trading_signals["('BAC', 'C')"].shift(1))
portfolio_price_scaled = (portfolio_returns_scaled + 1).cumprod()


# Equity curve of our unscaled portfolio price
equity_curve_unscaled = portfolio_price_unscaled - 1

equity_curve_unscaled.plot(title='Distance Strategy investemnt portfolio equity curve - unscaled weights', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_price_unscaled[-1])


# Equity curve of our scaled portfolio price
equity_curve_scaled = portfolio_price_scaled - 1

equity_curve_scaled.plot(title='Distance Strategy investemnt portfolio equity curve - scaled weights', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_price_scaled[-1])


# So using trading signals from the Distance Strategy for this particular example resulted in the equity curve of our investment portfolio increasing from 1 in mid-January 2019 to around 1.0475 in late May 2019 for the scaled portfolio and 1.0466 for the unscaled one.


# ## Conclusion


# This notebook describes the Distance Strategy class and its functionality. Also, it shows how the stages of the method (pairs formation and trading signals generation) can be used on real data and that this method can output profitable trading signals.
#
# The algorithms and the descriptions used in this notebook were described by _Gatev, E., Goetzmann, W. N.,_ and _Rouwenhorst, K. G._ in the paper __Pairs Trading: Performance of a Relative Value Arbitrage Rule__  [available here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=141615).
#
# Key takeaways from the notebook:
# - The distance approach can be divided into two stages - pairs formation and trading signals generation.
# - The distance approach works as follows:
#   - First, a historical period is defined, cumulative returns for assets in this period are normalized.
#   - Second, using the Euclidean squared distance on the normalized price time series, $n$ closest pairs of assets are picked.
#   - During the treading period, the trading signals are generated. The mechanism behind this process is the following:
#   - If the difference between the price of elements in a pair diverged by
#     more than 2 standard deviations (calculated for each pair during the training period), the positions are
#     opened - long for the element with a lower price in a portfolio and short for an element with a higher price
#     in a portfolio.
#   - These positions are closed when the normalized prices cross or when the trading period ends.
# - No cointegration tests are being performed in the distance approach, so dependencies found using this approach can be spurious.



// ---------------------------------------------------

// quantile_time_series.py
// arbitrage_research_basic_PCA/Time Series Approach/quantile_time_series.py
# Generated from: quantile_time_series.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __A Machine Learning based Pairs Trading Investment Strategy__ by _Simão Moraes Sarmento_ and _Nuno_ _Horta_


# # Quantile Time Series Strategy


# ## Introduction


# Usually, when a mean-reverting spread is constructed, a researcher will use the Z-score Bollinger Band strategy to trade a portfolio. However, the time series approach may be applied to model the spread dynamics and enter long/short spread positions.


# ## Modeling the spread difference
#
# - Firstly, let's define a spread in 2-dimensional space as $$S_t = Y_t - \beta X_t$$ Parameter $\beta$ can be defined by either using the Engle-Granger or Johansen cointegration approach.
# - Secondly, we need to come up with a spread prediction model - it can be ARIMA, ANN, RNN, or other time series prediction models. Let's define spread prediction at time $t$ as $\hat S_t$.
# - On the third step, we split spread differences $S_t - S_{t-1}$ into positive ($f_{+}$) and negative ($f_{-}$). We define bottom negative differences quantile as $Q_{f_{-}}$ and top positive differences quantile as $Q_{f_{+}}$.


from IPython.display import Image
Image(filename='Quantile_time_series/quantile_thresholds.png')


# ## Quantile-based trading
# In the time series approach, our trading rule can be described as
#
# $$(\hat S_{t+1} - S_{t}) \geq Q_{f_{+}} \Rightarrow OPEN LONG $$ $$ (\hat S_{t+1} - S_{t}) \leq Q_{f_{-}} \Rightarrow OPEN SHORT $$ $$0 \leq \hat S_{t+1} - S_{t} \leq Q_{f_{+}} \Rightarrow HOLDLONG$$ $$Q_{f_{-}} \leq \hat S_{t+1} - S_{t} \leq 0 \Rightarrow HOLDSHORT$$
#
# - OPENLONG/OPENSHORT means opening a new long/short position if none was opened before.
# - HOLDLONG/HOLDSHORT means holding long/short positions if one was opened before.
# - We should exit a long position if HOLDLONG condition was not satisfied, we should exit a short position if HOLDSHORT condition was not satisfied.
#
# **Note**: we use __90%__ and __5%__ quantiles for top positive and bottom negative quantiles thresholds respectively and 0 value as exit threshold, however, the researcher may decide what are the most optimal parameters based on a specific strategy.


Image(filename='Quantile_time_series/trading_example.png')


# ## Spread prediction model
#
# In the notebook, we use __Auto ARIMA(p, d, q)__ approach to generate spread predictions. 
# Non-seasonal ARIMA models are generally denoted ARIMA(p,d,q) where parameters p, d, and q are non-negative integers, __p__ is the order (number of time lags) of the autoregressive model, __d__ is the degree of differencing (the number of times the data have had past values subtracted), and __q__ is the order of the moving-average model.
# In order to choose, best fit ARIMA model parameters we minimize the **Akaike information criterion (AIC)** value: $$AIC = 2k - 2ln(L)$$ where $k$ - number of model parameters and $L$ - likelihood function.
#
# As a part of arbitragelab, we have an Auto ARIMA fit and prediction module which utilizes the **pmdarima** package to find the best model fit.


# ## Usage of the Algorithms


# Let's use the algorithm described above and its performance to Quantile Time Series Strategy. 
#
# - Firstly, we will use the Engle-Granger test to construct a mean-reverting portfolio. 
# - Secondly, we will fit the Auto ARIMA model and generate predictions.
# - In the third step, we will model spread differences and find trading thresholds.
# - Finally, we will generate trading signals using Auto ARIMA and fit thresholds.


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# Following the example in the Optimal Mean Reversion module, we will use Gold Shares ETF (GLD), Gold Miners ETF (GDX), and Junior Gold Miners ETF (GDXJ) to construct a portfolio of three elements.


# Loading data
train_data =  yf.download("GLD GDX GDXJ", start="2016-01-01", end="2018-01-01")
test_data =  yf.download("GLD GDX GDXJ", start="2018-01-02", end="2019-08-01")

# Taking close prices for chosen instruments
train_three_elements = train_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

test_three_elements = test_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

# Looking at the downloaded data
train_three_elements.head()


# ### Engle-Granger test


# Initialising an object containing needed methods
eg_portfolio = al.cointegration_approach.EngleGrangerPortfolio()

# Fitting the data on a dataset of three elements with constant term
eg_portfolio.fit(train_three_elements, add_constant=True)

# Getting results of the Engle-Granger test
eg_adf_statistics = eg_portfolio.adf_statistics
eg_cointegration_vectors = eg_portfolio.cointegration_vectors


# Looking at the statistic from the last step of the Engle-Granger test
eg_adf_statistics


# Using the ADF statistic test output, we can see that our statistic is above the 95% significance level value.
#
# So at a 95% significance level, our elements are cointegrated, we can construct a mean-reverting portfolio using the coefficients from the *eg_cointegration_vectors* variable.


eg_cointegration_vectors


# As described in the theoretical part, the coefficient for the first element is $1$, while other two are equal to negative regression coefficients.


# ### Constructing portfolios


# Calculating returns of our elements (ETFs)
train_three_elements_returns = (train_three_elements / train_three_elements.shift(1) - 1)[1:]
test_three_elements_returns = (test_three_elements / test_three_elements.shift(1) - 1)[1:]

train_three_elements_returns.head()


# Also adding weights to take initial prices of our ETFs into account
weights  = train_three_elements.iloc[0] / abs(train_three_elements.iloc[0]).sum()


# Weights of elements for the Engle-Granger portfolio
eg_cointegration_vectors.loc[0]


# Scaling weights so they sum up to 1
eg_scaled_vectors = eg_cointegration_vectors.loc[0] / abs(eg_cointegration_vectors.loc[0]).sum()

eg_scaled_vectors


# Calculating portfolio values during the training period
eg_portfolio_returns = (train_three_elements_returns * eg_scaled_vectors * weights).sum(axis=1)
eg_portfolio_price = (eg_portfolio_returns + 1).cumprod()


# Plotting Engle-Granger portfolio price
eg_portfolio_price.plot(title='Engle-Granger portfolio price', figsize=(10,5));


# ### Fit Auto ARIMA model


# Initializing the Auto ARIMA model
arima_model = al.time_series_approach.AutoARIMAForecast(start_p=1, start_q=1, max_p=10, max_q=10)


# Finding the best fitting model without silencing the warnings
arima_model.get_best_arima_model(y_train=eg_portfolio_price, verbose=True, silence_warnings = False)


# We can see that we are getting quite a few ConvergenceWarning, so we can assume that the Auto ARIMA method doesn't quite fit to this input data.
#
# There is an option to silence the warning for this particular function by using the _silence_warnings_ flag.


# Finding the best fitting model and silencing the warnings
arima_model.get_best_arima_model(y_train=eg_portfolio_price, verbose=True, silence_warnings = True)


# ### Get trading thresholds (quantiles)


# Plotting portfolio differences
eg_portfolio_price.diff().plot(title='Portfolio differences', figsize=(10,5));


# Init strategy class
time_series_trading = al.time_series_approach.QuantileTimeSeriesTradingStrategy(long_quantile=0.8, short_quantile=0.2)

# Fit portfilio to find Q_{-} and Q_{+}
time_series_trading.fit_thresholds(eg_portfolio_price)


# Plot thresholds used for trading
time_series_trading.plot_thresholds();


# ### Generate out-of-sample ARIMA predictions


# Creating a test portfolio based on the EG test results
test_three_elements_returns = (test_three_elements / test_three_elements.shift(1) - 1)[1:]

# Also adding weights to take initial prices of ETFs into account
weights  = test_three_elements.iloc[0] / abs(test_three_elements.iloc[0]).sum()

test_portfolio_returns = (test_three_elements_returns * eg_scaled_vectors * weights).sum(axis=1)

test_portfolio_price = (test_portfolio_returns + 1).cumprod()


# The progress bar was added to the prediction generation process to check the execution state of the method.


# Generate out-of-sample ARIMA prediction
oos_prediction = arima_model.predict(y=test_portfolio_price, silence_warnings = True)


# Compare results
plt.figure(figsize=(12,5))

ax1 = test_portfolio_price[2:].plot(label='Real price', figsize=(10,5))
ax2 = oos_prediction[2:].plot(label ='Auto ARIMA predictions', figsize=(15,9), title='Out-of-sample Auto ARIMA predictions vs Real values')

plt.legend(loc=2)
plt.show()


# ### Applying Quantile Time Series trading strategy


# Use the difference between prediction and actual value to trade the spread
for prediction, actual in zip(oos_prediction, test_portfolio_price):
    time_series_trading.get_allocation(predicted_difference=prediction-actual, exit_threshold=0)


# Plot positions created using a quantile time series strategy
positions = pd.Series(index=test_portfolio_price.index, data=time_series_trading.positions)

# Getting only short and only long positions
long_positions = positions[positions == 1]
short_positions = positions[positions == -1]

# Plottign the positions
test_portfolio_price.plot(title='Engle-Granger OOS portfolio price', figsize=(10,5), label='portfolio')
plt.scatter(long_positions.index, test_portfolio_price.loc[long_positions.index], color='green', label='long positions')
plt.scatter(short_positions.index, test_portfolio_price.loc[short_positions.index], color='red', label='short positions')
plt.legend(loc='best');


# Generate equity curve
equity_curve = (positions.shift(1) * test_portfolio_price.diff()).cumsum()
equity_curve.plot(title='Quantile Time Series Strategy equity curve', figsize=(10, 5));


# As we can see, the strategy generates negative returns due to the bad quality of the ARIMA model prediction. Let's see how the model works if we have a "perfect prediction" - the spread value for the next day.


# ### Perfect prediction results


# Init strategy class
perfect_series_trading = al.time_series_approach.QuantileTimeSeriesTradingStrategy(long_quantile=0.8, short_quantile=0.2)

# Getting the threshold values
perfect_series_trading.fit_thresholds(eg_portfolio_price)


# Use difference between prediction and actual value to trade the spread
for prediction, actual in zip(test_portfolio_price.shift(-1), test_portfolio_price):
    perfect_series_trading.get_allocation(predicted_difference=prediction-actual, exit_threshold=0)


positions = pd.Series(index=test_portfolio_price.index, data=perfect_series_trading.positions)


# Generate equity curve for "perfect" prediction.
perfect_equity_curve = (positions.shift(1) * test_portfolio_price.diff()).cumsum()
perfect_equity_curve.plot(title='Quantile Time Series Strategy with perfect predictions equity curve', figsize=(10, 5));


# ## Conclusion


# This notebook describes a new strategy from the Time Series Approach - the Quantile Time Series Strategy. The key part of the strategy is to generate time series prediction and trade when the predicted value deviates from the current spread value. In the notebook, we used the Auto ARIMA as a tool to generate spread value prediction.
#
# The algorithms and the descriptions used in this notebook were described in the book by _Simão Moraes Sarmento_ and _Nuno_ _Horta_ __A Machine Learning based Pairs Trading Investment Strategy__ [available here](https://www.springer.com/gp/book/9783030472504).
#
# Key takeaways from the notebook:
# - The strategy is based on the expectation that the investor can benefit from an abrupt movement of the spread value.
# - This strategy needs a forecasting algorithm to work, in this case, the Auto ARIMA tool was used.
# - The thresholds to enter the positions are defined based on the quantiles of the percentage change distribution during a formation period. 
# - For the spread forecasting, non-parametric methods can be used, such as Artificial Neural Networks.



// ---------------------------------------------------

// optimal_transport.py
// arbitrage_research_basic_PCA/Codependence Module/Optimal Transport/optimal_transport.py
# Generated from: optimal_transport.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.__ _by_ Marti et al.


# ## Abstract


# Optimal Copula Transport dependence is a unique measure between two random variables that allows measuring the codependence with respect to similarity to the target codependence type.


# ## Optimal Copula Transport dependence


# This description is based on the paper by _Marti et al._ __“Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.”__  [available here](https://arxiv.org/pdf/1610.09659.pdf).


# ### Optimal Transport and copulas


# As described by Gautier Marti:
#
# “The basic idea of the optimal copula transport dependence measure is rather simple. It relies on leveraging:
#
# - Copulas, which are distributions encoding fully the dependence between random variables.
#
# - A geometrical point of view: Where does the empirical copula stand in the space of copulas? In particular, how far is it from reference copulas such as the Fréchet–Hoeffding copula bounds (copulas associated to comonotone, countermonotone, independent random variables)?”


# ![image](images/optimal_transport_distance.png)
#
# _Dependence is measured as the relative distance from independence to the nearest target-dependence: comonotonicity or countermonotonicity. ([Blog post by Gautier Marti](https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html))_


# “With this geometric view:
#
# - It is rather easy to extend this novel dependence measure to alternative use cases (e.g. by changing the reference copulas).
#
# - It can also allow to look for specific patterns in the dependence structure (generalization of conditional correlation).
#
# With this optimal copula transport tool, one can look for answers to, for example:
#
#  A) “Which pair of assets having ρ=0.7 correlation has the nearest copula to the Gaussian one?”
#
#  B) “Which pairs of assets are both positively and negatively correlated?”
#
#  C) “Which assets occur extreme variations while those of others are relatively small, and conversely?”
#
#  D) “Which pairs of assets are positively correlated for small variations but uncorrelated otherwise?”


# ![image](images/target_copulas.png)
#
#
# _Target copulas (simulated or handcrafted) and their respective nearest copulas which answer questions A,B,C,D. ([Marti et al. 2016](https://arxiv.org/pdf/1610.09659.pdf))_


# ---


# According to the description of the method from [the original paper by Marti et al.](https://arxiv.org/pdf/1610.09659.pdf):


# The idea of the approach is to "target specific dependence patterns and ignore others. We want to target dependence
# which are relevant to such or such problem, and forget about the dependence which are not in the scope of the problems
# at hand, or even worse which may be spurious associations (pure chance or artifacts in the data)."
#
# The proposed codependence coefficient "can be parameterized by a set of target-dependences, and a set
# of forget-dependences. Sets of target and forget dependences can be built using expert hypotheses,
# or by leveraging the centers of clusters resulting from an exploratory clustering of the pairwise dependences.
# To achieve this goal, we will leverage three tools: copulas, optimal transportation, and clustering."
#
# "Optimal transport is based on the underlying theory of the Earth Mover’s Distance. Until very recently,
# optimal transportation distances between distributions were not deemed relevant for machine learning applications
# since the best computational cost known was super-cubic to the number of bins used for discretizing the
# distribution supports which grows itself exponentially with the dimension. A mere distance evaluation
# could take several seconds!"
#
# "Copulas and optimal transportation are not yet mainstream tools, but they have recently gained attention
# in machine learning, and several copula-based dependence measures have been proposed for improving
# feature selection methods".
#
# "Copulas are functions that couple multivariate distribution functions to their univariate marginal distribution functions".
#
# In this implementation, only bivariate copulas are considered, as higher dimensions would cost a high computational burden.
# But most of the results and the methodology presented hold in the multivariate setting.


#  **Theorem 1 (Sklar’s Theorem)** Let $X = (X_i, X_j)$ be  a random  vector with  a joint
#     cumulative distribution function $F$ , and having continuous marginal cumulative
#     distribution functions $F_i, F_j$ respectively. Then, there exists a unique
#     distribution $C$ such that $F(X_i, X_j) = C(F_i(X_i), F_j(X_j))$ .
#     $C$ , the copula of $X$ , is the bivariate distribution of uniform
#     marginals $U_i, U_j := F_i(X_i), F_j(X_j)$


# "Copulas are central for studying the dependence between random variables: their uniform marginals
# jointly encode all the dependence. They allow to study scale-free measures of dependence and are invariant
# to monotonous transformations of the variables. Some copulas play a major role in the measure of dependence,
# namely $\mathcal{W}$ and $\mathcal{M}$ the Frechet-Hoeffding copula bounds, and the independence
# copula $\Pi (u_i,u_j) = u_i u_j$ ".
#
# ![image](images/dependence_copulas.png)
#
# _Copulas measure (left column) and cumulative distribution function (right column) heatmaps for negative dependence (first row), independence (second row), i.e. the uniform distribution over $[0, 1]^2$, and positive dependence (third row) ([Marti et al. 2016](https://arxiv.org/pdf/1610.09659.pdf))_


# **Proposition 1 (Frechet-Hoeffding copula bounds)** For any copula $C: [0, 1]^2 \rightarrow [0, 1]$ and any $(u_i, u_j) \in [0, 1]^2$ the following bounds hold:
#
# $$\mathcal{W} (u_i, u_j) \le C(u_i, u_j) \le \mathcal{M} (u_i, u_j)$$
#
# where $\mathcal{W} (u_i, u_j) = max \{u_i + u_j − 1, 0 \}$ is the copula for countermonotonic random variables and $\mathcal{M} (u_i, u_j) = min \{ u_i, u_j \}$ is the copula for comonotonic random variables.


# "Notice that when working with empirical data, we do not know a priori the margins
# $F_i$ for applying the probability integral transform $U_i := F_i(X_i)$ . Deheuvels has introduced a
# practical estimator for the uniform margins and the underlying copula, the empirical copula transform".


# **Definition 1 (Empirical Copula Transform)** Let $(X^t_i, X^t_j), t = 1, ..., T$ , be $T$ observations
#     from a random vector $(X_i, X_j)$ with continuous margins. Since one cannot directly obtain the corresponding
#     copula observations $(U^t_i, U^t_j) := (F_i(X^t_i), F_j(X^t_j))$ , where $t = 1, ..., T$ , without
#     knowing a priori $F_i$ , one can instead estimate the empirical
#     margins $F^T_i(x) = \frac{1}{T} \sum^T_{t=1} I(X^t_i \le x)$ , to obtain the $T$ empirical
#     observations $(\widetilde{U}^t_i, \widetilde{U}^t_j) := (F^T_i(X^t_i), F^T_j(X^t_j))$ . Equivalently,
#     since $U^t_i = R^t_i / T, R^t_i$ being the rank of observation $X^t_i$ , the empirical copula
#     transform can be considered as the normalized rank transform.


# "The idea of optimal transport is intuitive. It was first formulated by Gaspard Monge in 1781 as a problem to
# efficiently level the ground: Given that work is measured by the distance multiplied by the amount of dirt
# displaced, what is the minimum amount of work required to level the ground? Optimal transport plans and distances
# give the answer to this problem. In practice, empirical distributions can be represented by histograms.
#
# Let $r, c$ be two histograms in the probability simplex $\sum_m = \{x \in R^m_+ : x^T 1_m = 1\}$ .
# Let $U(r, c) = \{ P \in R^{m \times m}_+ | P1_m = r, P^T 1_m = c\}$ be the transportation polytope
# of $r$ and $c$ , that is the set containing all possible transport plans between $r$ and $c$ ".


# **Definition 2 (Optimal Transport)** Given a $m \times m$ cost matrix $M$, the cost of mapping $r$ to
#     $c$ using a transportation matrix $P$ can be quantified as $\langle P, M \rangle _F$ , where $\langle \cdot, \cdot \rangle _F$ is
#     the Frobenius dot-product. The optimal transport between $r$ and $c$ given transportation cost
#     $M$ is thus:
#
# $$d_M(r, c) := min_{P \in U (r, c)} \langle P, M \rangle _F$$


# "Whenever $M$ belongs to the cone of distance matrices, the optimum of the transportation problem
# $d_M(r, c)$ is itself a distance.
#
# Using the optimal transport distance between copulas, we now propose a dependence coefficient which is parameterized
# by two sets of copulas: target copulas and forget copulas".


# **Definition 3 (Target/Forget Dependence Coefficient)** Let ${C^-_l}_l$ be the set of forget-dependence copulas.
#     Let ${C^+_k}_k$ be the set of target-dependence copulas. Let $C$ be the copula of $(X_i, X_j)$ .
#     Let $d_M$ be an optimal transport distance parameterized by a ground metric $M$ . We define
#     the Target/Forget Dependence Coefficient as:
#
# $$TFDC(X_i, X_j; {C^+_k}_k, {C^-_l}_l) := \frac{min_l d_M(C^-_l, C)}{min_l d_M(C^-_l, C) + min_k d_M(C, C^+_k)} \in [0, 1]$$


# "Using this definition, we obtain:
#
# $$TFDC (X_i, X_j; {C:+_k}_k, {C:-_l}_l) = 0 \Leftrightarrow C \in {C^-_l}_l$$
#
# $$TFDC(X_i ,X_j; {C^+_k}_k, {C^-_l}_l) = 1 \Leftrightarrow C \in {C^+_k}_k$$
#
# It is known by risk managers how dangerous it can be to rely solely on a correlation coefficient
# to measure dependence. That is why we have proposed a novel approach to explore, summarize and measure the
# pairwise correlations which exist between variables in a dataset. The experiments show the benefits of the
# proposed method: It allows to highlight the various dependence patterns that can be found between financial
# time series, which strongly depart from the Gaussian copula widely used in financial engineering.
# Though answering dependence queries as briefly outlined is still an art, we plan to develop a rich language so
# that a user can formulate complex questions about dependence, which will be automatically translated into
# copulas in order to let the methodology provide these questions accurate answers".


# ---


# ## Usage of the algorithms


# ##### **Warning!** Optimal Copula Transport dependence is computationally heavy, so calculating the codependence matrix may take some time.


# This part shows how the Optimal Copula Transport dependence can be used to measure codependence between a set of stocks


import arbitragelab as al
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Loading the dataset of stocks
stocks = pd.read_csv('../../Sample-Data/stock_prices.csv')
stocks.set_index('Date', inplace=True)
stocks.index = pd.to_datetime(stocks.index)

# Taking first 10 stocks for better output visualization
stocks = stocks.iloc[:,0:10]
stocks.head()


# Calculating returns of a given dataset of stocks
stocks_returns = stocks.pct_change()[1:]
stocks_returns.iloc[:,0:10].head()


# Calculating Optimal Copula Transport dependence using a comonotone target copula
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='comonotonicity')
print('Optimal Copula Transport dependence between EEM and EWG using a comonotone target copula is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a countermonotone target copula
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='countermonotonicity')
print('Optimal Copula Transport dependence between EEM and EWG using a countermonotone target copula is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a Gaussian target copula with ρ=0.5
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='gaussian', gaussian_corr=0.5)
print('Optimal Copula Transport dependence between EEM and EWG using a Gaussian target copula with ρ=0.5 is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a positive and negative correlation target copula
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='positive_negative')
print('Optimal Copula Transport dependence between EEM and EWG using a positive and negative correlation target copula is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a target copula wheree element has extreme variations and the second has small variations
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='different_variations')
print('Optimal Copula Transport dependence between EEM and EWG using a arget copula with one element has extreme variations and the second has small variations is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='small_variations', var_threshold=0.2)
print('Optimal Copula Transport dependence between EEM and EWG using elements being positively correlated for small variations but uncorrelated otherwise: ', ot_dist)


# These codependence measures can also be used on the whole dataframes, the results will be codependence matrices.


# Calculating Optimal Copula Transport dependence between all stocks using a comonotone target copula
ot_matrix_comonotone = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='comonotonicity')

print('Optimal Copula Transport dependence matrix using a comonotone target copula:')
ot_matrix_comonotone


# Calculating Optimal Copula Transport dependence between all stocks using a Gaussian target copula with ρ=0.5
ot_matrix_gaussian = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='gaussian', gaussian_corr=0.5)

# Calculating Optimal Copula Transport dependence between all stocks using a positive and negative correlation target copula
ot_matrix_positive_negative = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='positive_negative')

# Calculating Optimal Copula Transport dependencee between all stocks using a target copula where one element has extreme variations and the second has small variations
ot_matrix_diffvar = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='different_variations')

# Calculating Optimal Copula Transport dependence between all stocks using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise
ot_matrix_diffvar = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='small_variations', var_threshold=0.2)


# ### Heatmap of Optimal Copula Transport dependence using a comonotone target copula


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(ot_matrix_comonotone, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a comonotone target copula')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with ρ=0.5


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_gaussian, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with ρ=0.5')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a positive and negative correlation target copula


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_positive_negative, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a positive and negative correlation target copula')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a target copula with one element has extreme variations and the second has small variations


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_diffvar, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a target copula where one element has extreme variations and the second has small variations')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_diffvar, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise')
plt.show()


# As seen from the heat maps of OT dependence matrices, various target copulas used in measures are showing specific characteristics of the assets. These measures can help uncover and better analyze relationships between assets.


# ---


# ## Conclusion


# This notebook describes the Optimal Copula Transport dependence measure and how it may be used in real-life applications.  
#
# This dependence measure was described by _Marti et al._ in the work __“Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.”__  [available here](https://arxiv.org/pdf/1610.09659.pdf).
#
# Key takeaways from the notebook:
# - Optimal Copula Transport dependence allows to measure distance between random elements in relation to different target copulas.
# - Optimal Copula Transport dependence is computationally heavy, so it may take some time to calculate the whole codependence matrix.
# - Supported target copulas allow to answer the following questions:
#   - “Which pair of assets having ρ=0.7 correlation has the nearest copula to the Gaussian one?”
#   - “Which pairs of assets are both positively and negatively correlated?”
#   - “Which assets occur extreme variations while those of others are relatively small, and conversely?”
#   - “Which pairs of assets are positively correlated for small variations but uncorrelated otherwise?”
# - Results show that this dependence measure can help uncover dependency types of a given set of elements.


# ## Reference


# 1. Marti, Gautier & Andler, Sébastien & Nielsen, Frank & Donnat, Philippe. (2016). Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.      Available at: https://arxiv.org/pdf/1610.09659.pdf
#
# 2. Gautier Marti. (2020) Blog post: Measuring non-linear dependence with Optimal Transport. Available at: https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html



// ---------------------------------------------------

// codependence_by_marti.py
// arbitrage_research_basic_PCA/Codependence Module/Codependence by Marti/codependence_by_marti.py
# Generated from: codependence_by_marti.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Some contributions to the clustering of financial time series and applications to credit default swaps__ _by_ Gautier Marti


# ## Abstract


# GPR and GNPR distances are a part of a novel technique for measuring the distance between two random variables that allows to separate the measurement of distribution information and the dependence information. A mix of both types of information can be used in a chosen proportion.


# ## GPR and GNPR distances


# This description is based on the paper by _Gautier Marti_ __“Some contributions to the clustering of financial time series and applications to credit default swaps”__  [available here](https://www.researchgate.net/publication/322714557).


# ### Classification of distances


# According to _Gautier Marti_:
#
# "Many statistical distances exist to measure the dissimilarity of two random variables, and therefore two i.i.d. random processes. Such distances can be roughly classified in two
# families:
#
# 1. __distributional distances__, ... which focus on dissimilarity between probability distributions and quantify divergences in marginal behaviours,
#
# 2. __dependence distances__, such as the distance correlation or copula-based kernel dependency measures ..., which focus on the joint behaviours of random variables, generally ignoring their distribution properties.
#
# However, we may want to be able to discriminate random variables both on distribution and dependence. This can be motivated, for instance, from the study of financial assets returns: are two perfectly correlated random variables (assets returns), but one being normally distributed and the other one following a heavy-tailed distribution, similar?
#
# From risk perspective, the answer is no ..., hence the propounded distance of this article".


# ### GPR distance


# From __“Some contributions to the clustering of financial time series and applications to credit default swaps”__ :
#
# __Definition:__ (Distance $d_{\Theta}$ between two random variables). Let $\theta \in [0, 1]$. Let $(X, Y) \in \nu^{2}$ , where $\nu$ is the space of all continuous
# real-valued random variables. Let $G = (G_{X}, G_{Y})$, where $G_{X}$ and $G_{Y}$ are respectively $X$ and $Y$ marginal cdfs. We define the following distance
#
# $$d_{\Theta}^{2}(X, Y) = \Theta d_{1}^{2}(G_{X}(X), G_{Y}(Y)) + (1 - \Theta) d_{0}^{2}(G_{X}, G_{Y})$$
#
# where
#
# $$d_{1}^{2}(G_{X}(X), G_{Y}(Y)) = 3 \mathbb{E}[|G_{X}(X) - G_{Y}(Y)|^{2}]$$
#
# and
#
# $$d_{0}^{2}(G_{X}, G_{Y}) = \frac{1}{2} \int_{R} (\sqrt{\frac{d G_{X}}{d \lambda}} - \sqrt{\frac{d G_{Y}}{d \lambda}})^{2} d \lambda$$


# __Example:__ (Distance $d_{\Theta}$ between two Gaussians). Let $(X, Y)$ be a bivariate Gaussian vector, with $X \sim \mathcal{N}(\mu_{X}, \sigma_{X}^{2})$,
# $Y \sim \mathcal{N}(\mu_{Y}, \sigma_{Y}^{2})$ and $\rho (X,Y)$. We obtain,
#
# $$d_{\Theta}^{2}(X, Y) = \Theta \frac{1 - \rho_{S}}{2} + (1 - \Theta) (1 - \sqrt{\frac{2 \sigma_{X} \sigma_{Y}}{\sigma_{X}^{2} + \sigma_{Y}^{2}}} e^{ - \frac{1}{4} \frac{(\mu_{X} - \mu_{Y})^{2}}{\sigma_{X}^{2} + \sigma_{Y}^{2}}})$$
#
# The use of this distance is referenced as the generic parametric representation (GPR) approach.
#
# From the paper:
#
# "GPR distance is a fast and good proxy for distance $d_{\Theta}$ when the first two moments $\mu$ and ${\sigma}$ predominate. Nonetheless, for datasets which contain heavy-tailed distributions, GPR fails to capture this information".


# __Property:__ Let $\Theta \in [0,1]$. The distance $d_{\Theta}$verifies $0 \le d_{\Theta} \le 1$.
#
# __Property:__ For $0 < \Theta < 1$, $d_{\Theta}$ is a metric.
#
# __Property:__ [Diffeomorphism](https://en.wikipedia.org/wiki/Diffeomorphism) invariance. Let $h: \nu \rightarrow \nu$ be a diffeomorphism. Let $(X, Y) \in \nu^{2}$. Distance $d_{\Theta}$ is invariant under diffeomorphism, i.e.
#
# $$d_{\Theta}(h(X), h(Y)) = d_{\Theta}(X, Y)$$


# ### GNPR distance


# According to _Marti_:
#
# "To apply the propounded distance $d_{\Theta}$ on sampled data without parametric assumptions, we have to define its statistical estimate $\tilde{d}_{\Theta}$ working on realizations of the i.i.d. random variables.
#
# Distance $d_{1}$ working with continuous uniform distributions can be approximated by normalized rank statistics yielding to discrete uniform distributions.
#
# Distance $d_{0}$ working with densities can be approximated by using its discrete form working on histogram density estimates".


# __Definition:__ (Empirical distance) Let $(X^{t})_{t=1}^{T}$ and $(Y^{t})_{t=1}^{T}$ be $T$ realizations of real-valued random variables $X, Y \in \nu$ respectively. An empirical distance between realizations of random variables can be defined by
#
# $$\tilde{d}_{\Theta}^{2}((X^{t})_{t=1}^{T}, (Y^{t})_{t=1}^{T}) \stackrel{\text{a.s.}}{=} \Theta \tilde{d}_{1}^{2} + (1 - \Theta) \tilde{d}_{0}^{2}$$
#
# where
#
# $$\tilde{d}_{1}^{2} = \frac{3}{T(T^{2} - 1)} \sum_{t = 1}^{T} (X^{(t)} - Y^{(t)}) ^ {2}$$
#
# and
#
# $$\tilde{d}_{0}^{2} = \frac{1}{2} \sum_{k = - \infty}^{+ \infty} (\sqrt{g_{X}^{h}(hk)} - \sqrt{g_{Y}^{h}(hk)})^{2}$$
#
# $h$ being here a suitable bandwidth, and $g_{X}^{h}(x) = \frac{1}{T} \sum_{t = 1}^{T} \mathbf{1}(\lfloor \frac{x}{h} \rfloor h \le X^{t} < (\lfloor \frac{x}{h} \rfloor + 1)h)$ being a density histogram estimating dpf $g_{X}$ from
# $(X^{t})_{t=1}^{T}$ , $T$ realization of a random variable $X \in \nu$.


# The use of this distance is referenced as the generic non-parametric representation (GNPR) approach.
#
# From the paper:
#
# "To use effectively $d_{\Theta}$ and its statistical estimate, it boils down to select a particular value for $\Theta$. We suggest here an exploratory approach where one can test 
#
# - (i) distribution information ($\Theta = 0$),
# - (ii) dependence information ($\Theta = 1$), and
# - (iii) a mix of both information ($\Theta = 0.5$).
#
# Ideally, $\Theta$ should reflect the balance of dependence and distribution information in the data.
#
# In a supervised setting, one could select an estimate $\hat{\Theta}$ of the right balance $\Theta^{*}$ optimizing some loss function by techniques such as cross-validation. Yet, the lack of a clear loss function makes the estimation of $\Theta^{*}$ difficult in an unsupervised setting".


# **Note:** The implementation of GNPR in the ArbitrageLab package was adjusted so that $\tilde{d}_{0}^{2}$
# (dependence information distance) is being calculated using the 1D Optimal Transport Distance following the example in the
# [POT package documentation](https://pythonot.github.io/auto_examples/plot_OT_1D.html#sphx-glr-auto-examples-plot-ot-1d-py).
# This solution was proposed by Marti.
#
# Distributions of random variables are approximated using histograms with a given number of bins as input.
#
# Optimal Transport Distance is then obtained from the Optimal Transportation Matrix (OTM) using
# the Loss Matrix (M) as shown in 
# [Optimal Transport blog post by Marti](https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html):
#
# $$\tilde{d}_{0}^{2} = tr (OT^{T} * M)$$
#
# where $tr( \cdot )$ is trace of a matrix and $\cdot^{T}$ is a transposed matrix.
#
# This approach solves the issue of defining support for underlying distributions and choosing a number of bins.


# ---


# ## Usage of the algorithms


# This part shows how the GPR and the GNPR distances can be used to measure codependence between a set of stocks


import arbitragelab as al
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Loading the dataset of stocks
stocks = pd.read_csv('../../Sample-Data/stock_prices.csv')
stocks.set_index('Date', inplace=True)
stocks.index = pd.to_datetime(stocks.index)

# Taking first 10 stocks for better output visualization
stocks = stocks.iloc[:,0:10]
stocks.head()


# Calculating returns of a given dataset of stocks
stocks_returns = stocks.pct_change()[1:]
stocks_returns.iloc[:,0:10].head()


# Calculating GPR distance between two tickers - EEM and EWG
gpr_dist = al.codependence.gpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=0.5)
print('GPR distance between EEM and EWG measuring a mix of distribution and dependence information is: ', gpr_dist)


# Calculating GPR distance between two tickers - EEM and EWG using only dependence information
gpr_dist = al.codependence.gpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=1)
print('GPR distance between EEM and EWG measuring dependence information is: ', gpr_dist)


# Calculating GNPR distance between two tickers - EEM and EWG
gpr_dist = al.codependence.gnpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=0.5)
print('GNPR distance between EEM and EWG measuring a mix of distribution and dependence information is: ', gpr_dist)


# Calculating GNPR distance between two tickers - EEM and EWG using only distribution information and 100 bins
gpr_dist = al.codependence.gnpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=0, n_bins=100)
print('GNPR distance between EEM and EWG measuring distribution information is: ', gpr_dist)


# These codependence measures can also be used on the whole dataframes, the results will be codependence matrices.


# Calculating GPR distance between all stocks with Θ = 0.5
gpr_matrix = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gpr_distance',theta=0.5)

print('GPR distance matrix measuring a mix of distribution and dependence information:')
gpr_matrix


# Calculating GNPR distance between all stocks with Θ = 1
gnpr_matrix_dep = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gnpr_distance',theta=1)

# Calculating GNPR distance between all stocks with Θ = 0.5
gnpr_matrix_mix = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gnpr_distance',theta=0.5)

# Calculating GNPR distance between all stocks with Θ = 0
gnpr_matrix_dist = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gnpr_distance',theta=0)

print('GNPR distance matrix measuring distribution information:')
gnpr_matrix_dist


# ### Heatmap of GNPR distribution information distance


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(gnpr_matrix_dist, ax = ax, cbar_kws={'label': 'GNPR distribution distance'})

ax.set_title('Heatmap of GNPR distribution information distance (Θ = 0)')
plt.show()


# ### Heatmap of GNPR mix of distribution and dependence information distance


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(gnpr_matrix_mix, ax = ax, cbar_kws={'label': 'GNPR mixed distance'})

ax.set_title('Heatmap of GNPR mix of distribution and dependence information distance (Θ = 0.5)')
plt.show()


# ### Heatmap of GNPR dependence information distance


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(gnpr_matrix_dep, ax = ax, cbar_kws={'label': 'GNPR dependence distance'})

ax.set_title('Heatmap of GNPR dependence information distance (Θ = 1)')
plt.show()


# As seen from the heat maps of GNPR distance matrices, distribution and information distances show different types of codependency. However, when using a mixed approach with $\Theta = 0.5$ the distribution information part is too high, which makes the dependence information adjustment hardly visible, so for a balanced output one may want to increase $\Theta$ in this particular example. 


# ---


# ## Conclusion


# This notebook describes the GPR and the GNPR distances how they may be used in real-life applications.  
#
# These novel distances were originally presented by the _Gautier Marti_ in the work __“Some contributions to the clustering of financial time series and applications to credit default swaps”__  [available here](https://www.researchgate.net/publication/322714557).
#
# Key takeaways from the notebook:
# - Distances can be roughly classified in two families:
#   - Distributional distances, which focus on dissimilarity between probability distributions and quantify divergences in marginal behaviours.
#   - Dependence distances, which focus on the joint behaviours of random variables, generally ignoring their distribution properties.
# - Distance $d_{\Theta}$ between two random variables allows to discriminate random variables both on distribution and dependence.
# - Distance $d_{\Theta}$ is a metric that falls in range $[0, 1]$.
# - GPR distance is a fast and good proxy for distance $d_{\Theta}$ between two Gaussians.
# - GNPR distance is a proxy for distance $d_{\Theta}$ that works on i.i.d. random variables, it requires a declared width of bins for values discretization.
# - $d_{\Theta}$ should be chosen to reflect the balance of dependence and distribution information in the data.
# - The ArbitrageLab implementation has $\tilde{d}_{0}^{2}$ in GNPR is adjusted to using 1D Optimal Transport Distance to solve the issue of defining support for underlying distributions and choosing a number of bins.


# ## References


# 1. Marti, Gautier. (2017). Some contributions to the clustering of financial time series and applications to credit default swaps. Available at: https://www.researchgate.net/publication/322714557
#
# 2. Marti, Gautier. (2020). Measuring non-linear dependence with Optimal Transport. Available at: https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html



// ---------------------------------------------------

// futures_rollover.py
// arbitrage_research_basic_PCA/ML Approach/futures_rollover.py
# Generated from: futures_rollover.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Futures Rolling
#
# ## Introduction to Futures
# Futures are a form of a contract drawn up between two parties to purchase or sell a quantity of an underlying asset at a specified date in the future. This date is known as the delivery or expiration. When this date is reached, the buyer must deliver the physical underlying (or cash equivalent) to the seller for the price agreed at the contract formation date.
#
# In practice, futures are traded on exchanges for standardised quantities and qualities of the underlying. The prices are marked to market every day. Futures are incredibly liquid and are used heavily for speculative purposes. While futures were often utilised to hedge the prices of agricultural or industrial goods, a futures contract can be formed on any tangible or intangible underlying such as stock indices, interest rates of foreign exchange values.
#
# The main difference between a futures contract and equity ownership is the fact that a futures contract has a limited window of availability by virtue of the expiration date. At any one instant, there will be a variety of futures contracts on the same underlying all with varying dates of expiry. The contract with the nearest date of expiry is known as the near contract.
#
# ## Outline
#
# - Contract Rollers
#     - [Crude Oil - WTI](#wti) 
#     - [UK Gas - NBP](#ukgas)
#     - [Gasoline - RBOB](#rbob)
#     - [Soybean - S](#soyb)
#     - [Soy Oil - B0](#soyo)
#     - [Corn - C](#corn)
#     - [Ethanol - EH](#eth)


import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

from arbitragelab.util.rollers import *

import warnings
warnings.filterwarnings('ignore')


# # Crude Oil WTI  <a class="anchor" id="wti"></a>
#
# NYMEX WTI Crude Oil futures (CL) is the world’s most liquid crude oil contract. When traders need the current oil price, they check the WTI Crude Oil price. WTI (West Texas Intermediate, a US light sweet crude oil blend) futures provide direct crude oil exposure and are the most efficient way to trade oil after a sharp rise in US crude oil production. They can also be used to hedge against adverse oil price moves or speculate on whether WTI oil prices will rise or fall.
#
# https://www.cmegroup.com/trading/energy/crude-oil/light-sweet-crude_contract_specifications.html
#
# ### Termination of Trading
#
# Trading terminates 3 business day prior to the 25th calendar day of the month prior to the contract month. If the 25th calendar day is not a business day, trading terminates 4 business days prior to the 25th calendar day of the month prior to the contract month.


# Load contract price data.
cl_df = pd.read_csv('./data/futures_price_data/CL1.csv')
cl_df['Dates'] = pd.to_datetime(cl_df['Dates'])
cl_df.dropna(inplace=True)
cl_df.index = cl_df['Dates']
cl_df = cl_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
wti_roller = CrudeOilFutureRoller().fit(cl_df)
wti_gaps = wti_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(cl_df['PX_LAST'] - wti_gaps).plot(figsize=(15,10))
cl_df['PX_LAST'].plot()
wti_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("WTI future rolling plot");


# Sometimes rolled contracts dip into negative territory. This
# can cause problems when used for ml models, thus there is the
# ability of using the parameter 'handle_negative_roll', which
# will process the price data into positive returns data.
non_negative_cl = wti_roller.transform(handle_negative_roll=True) 
non_negative_cl.to_csv('./data/nonneg_forward_rolled_futures/NonNegative_CL_forward_roll.csv')
non_negative_cl.plot(figsize=(15,10))
plt.title("WTI Non Negative Forward Roll");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
wti_diag_frame = wti_roller.diagnostic_summary()
wti_diag_frame.head(10)


# # NBP UK Natural Gas Futures  <a class="anchor" id="nbp"></a>
#
# Natural gas is the third most important source of energy after oil and coal. The use of natural gas is growing quickly and is expected to overtake coal in the second spot by 2030.
#
# The world’s largest producers of natural gas are currently the United States, Russia, Iran, Qatar, Canada, China and Norway. These countries have excess natural gas that can be exported to other countries around the world, which is either transported through pipelines or as liquefied natural gas (LNG).
#
# In western Europe, gas is the dominant fuel for electricity production. Prices are set at several trading hubs around the region. The two most important hubs in the region are the National Balancing Point or NBP in the UK and the Title Transfer Facility or TTF in the Netherlands.
#
# https://www.theice.com/products/910/UK-Natural-Gas-Futures
#
# ## Termination of Trading
# Trading will cease at the close of business two Business Days prior to the first calendar day of the delivery month, quarter, season, or calendar.


# Load contract price data.
nbp_df = pd.read_csv('./data/futures_price_data/NBP1.csv')
nbp_df['Dates'] = pd.to_datetime(nbp_df['Dates'])
nbp_df.set_index('Dates', inplace=True)


# Fit corresponding roller and retrieve gaps.
nbp_roller = NBPFutureRoller().fit(nbp_df)
nbp_gaps = nbp_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(nbp_df['PX_LAST'] - nbp_gaps).plot(figsize=(15,10))
nbp_df['PX_LAST'].plot()
nbp_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("NBP future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
nbp_diag_frame = nbp_roller.diagnostic_summary()
nbp_diag_frame.head(10)


# # RBOB  <a class="anchor" id="rbob"></a>
#
# RBOB products offer a way for investors to express views on crude oil, weather, consumer behavior and regulatory action in terms of current and future energy consumption. As the primary fuel for most automobiles on the road, gasoline is an integral commodity to the lives of most consumers. 
#
# https://www.cmegroup.com/trading/energy/refined-products/rbob-gasoline_contract_specifications.html
#
# ### Termination of Trading
# Trading terminates on the last business day of the month prior to the contract month.


# Load contract price data.
rb_df = pd.read_csv('./data/futures_price_data/RB1.csv').dropna()
rb_df['Dates'] = pd.to_datetime(rb_df['Dates'])
rb_df.set_index('Dates', inplace=True)
rb_df = rb_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
rbob_roller = RBFutureRoller().fit(rb_df)
rbob_gaps = rbob_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(rb_df['PX_LAST'] - rbob_gaps).plot(figsize=(15,10))
rb_df['PX_LAST'].plot()
rbob_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("RBOB future rolling plot");


# In this case the rolled contract dips into negative territory.
# Thus the 'handle_negative_roll' parameter is used to post process
# the rolled future data.
non_negative_rbob = rbob_roller.transform(handle_negative_roll=True)
non_negative_rbob.to_csv('./data/nonneg_forward_rolled_futures/NonNegative_RB_forward_roll.csv')
non_negative_rbob.plot(figsize=(15,10))
plt.title("RBOB Non Negative Forward Roll");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
rb_diag_frame = rbob_roller.diagnostic_summary()
rb_diag_frame.head(10)


# # Soybeans S1  <a class="anchor" id="soyb"></a>
#
#
#
# ### Termination of Trading
# Trading terminates on the business day prior to the 15th day of the contract month.


# Load contract price data.
soybean_df = pd.read_csv('./data/futures_price_data/S1.csv', index_col='Date', parse_dates=True).dropna()
soybean_df = soybean_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
soy_roller = GrainFutureRoller().fit(soybean_df)
soy_gaps = soy_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(soybean_df['PX_LAST'] - soy_gaps).plot(figsize=(15,10))
soybean_df['PX_LAST'].plot()
soy_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("S future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
soy_diag_frame = soy_roller.diagnostic_summary()
soy_diag_frame.head(10)


# Example on how to analyze and verify gaps using the backwardation/contango plot.

soybean_m1_df = pd.read_csv('./data/futures_price_data/S1.csv', index_col='Date', parse_dates=True).dropna()
soybean_m1_df = soybean_m1_df['2006-01': '2019-12']

soybean_m2_df = pd.read_csv('./data/futures_price_data/S2.csv', index_col='Date', parse_dates=True).dropna()
soybean_m2_df = soybean_m2_df['2006-01': '2019-12']

soy_gaps.plot(figsize=(15,5))
plt.title("Soybean Future Gaps");
plt.show()

plot_historical_future_slope_state(soybean_m1_df['PX_LAST'], soybean_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Soyoil B01  <a class="anchor" id="soyo"></a>
#
# RBOB products offer a way for investors to express views on crude oil, weather, consumer behavior and regulatory action in terms of current and future energy consumption. As the primary fuel for most automobiles on the road, gasoline is an integral commodity to the lives of most consumers. 
#
# https://www.cmegroup.com/trading/energy/refined-products/rbob-gasoline_contract_specifications.html
#
# ### Termination of Trading
# Trading terminates on the business day prior to the 15th day of the contract month.


# Load contract price data.
soyoil_df = pd.read_csv('./data/futures_price_data/B01.csv', index_col='Date', parse_dates=True).dropna()
soyoil_df = soyoil_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
soyo_roller = GrainFutureRoller().fit(soyoil_df*11)
soyo_gaps = soyo_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(soyoil_df['PX_LAST'] - soyo_gaps).plot(figsize=(15,10))
soyoil_df['PX_LAST'].plot()
soyo_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("B0 future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
soyo_diag_frame = soyo_roller.diagnostic_summary()
soyo_diag_frame.head(10)


# Example on how to analyze and verify gaps using the backwardation/contango plot.

soyo_m1_df = pd.read_csv('./data/futures_price_data/B01.csv', index_col='Date', parse_dates=True).dropna()
soyo_m1_df = soyo_m1_df['2006-01': '2019-12']

soyo_m2_df = pd.read_csv('./data/futures_price_data/B02.csv', index_col='Date', parse_dates=True).dropna()
soyo_m2_df = soyo_m2_df['2006-01': '2019-12']

soyo_gaps.plot(figsize=(15,5))
plt.title("Soyoil Future Gaps");
plt.show()

plot_historical_future_slope_state(soyo_m1_df['PX_LAST'], soyo_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Corn C1  <a class="anchor" id="corn"></a>
#
#
# ### Termination of Trading
# Trading terminates on the business day prior to the 15th day of the contract month.


# Load contract price data.
corn_df = pd.read_csv('./data/futures_price_data/C1.csv', index_col='Date', parse_dates=True).dropna()
corn_df = corn_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
corn_roller = GrainFutureRoller().fit(corn_df)
corn_gaps = corn_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(corn_df['PX_LAST'] - corn_gaps).plot(figsize=(15,10))
corn_df['PX_LAST'].plot()
corn_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("Corn future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
corn_diag_frame = corn_roller.diagnostic_summary()
corn_diag_frame.head(10)


# Example on how to analyze and verify gaps using the backwardation/contango plot.

corn_m1_df = pd.read_csv('./data/futures_price_data/C1.csv', index_col='Date', parse_dates=True).dropna()
corn_m1_df = corn_m1_df['2006-01': '2019-12']

corn_m2_df = pd.read_csv('./data/futures_price_data/C2.csv', index_col='Date', parse_dates=True).dropna()
corn_m2_df = corn_m2_df['2006-01': '2019-12']

corn_gaps.plot(figsize=(15,5))
plt.title("Corn Future Gaps");
plt.show()

plot_historical_future_slope_state(corn_m1_df['PX_LAST'], corn_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Ethanol EH1 <a class="anchor" id="corn"></a>
#
#
# ### Termination of Trading
# Trading terminates on 3rd business day of the contract month.


# Load contract price data.
ethanol_df = pd.read_csv('./data/futures_price_data/EH1.csv', index_col='Date', parse_dates=True).dropna()
ethanol_df = ethanol_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
ethanol_roller = EthanolFutureRoller().fit(ethanol_df)
ethanol_gaps = ethanol_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(ethanol_df['PX_LAST'] - ethanol_gaps).plot(figsize=(15,10))
ethanol_df['PX_LAST'].plot()
ethanol_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("Ethanol future rolling plot");


# Example on how to analyze and verify gaps using the backwardation/contango plot.

ethanol_m1_df = pd.read_csv('./data/futures_price_data/EH1.csv', index_col='Date', parse_dates=True).dropna()
ethanol_m1_df = ethanol_m1_df['2006-01': '2019-12']

ethanol_m2_df = pd.read_csv('./data/futures_price_data/EH2.csv', index_col='Date', parse_dates=True).dropna()
ethanol_m2_df = ethanol_m2_df['2006-01': '2019-12']

ethanol_gaps.plot(figsize=(15,5))
plt.title("Corn future rolling plot");
plt.show()

plot_historical_future_slope_state(ethanol_m1_df['PX_LAST'], ethanol_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Conclusion
#
# This notebook describes the methods used to roll futures for different assets. This is done to get a continuous price series for a given set of futures contracts.
#
# Types of contracts covered in this notebooks are:
# * Crude Oil - WTI
# * UK Gas - NBP
# * Gasoline - RBOB
# * Soybean - S
# * Soy Oil - B0
# * Corn - C
# * Ethanol - EH



// ---------------------------------------------------

// fair_value_modeling.py
// arbitrage_research_basic_PCA/ML Approach/fair_value_modeling.py
# Generated from: fair_value_modeling.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Fair Value Modeling
#
# ## Abstract
# In [(Dunis et al. 2005)](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.568.7460&rep=rep1&type=pdf) the case is made that the crack spread exhibits asymmetry at the \\$5 dollar mark, with seemingly larger moves occurring on the upside of the long-term 'fair value' than on the downside.
#
#
# The gasoline crack spread can be interpreted as the profit margin gained by processing crude oil into unleaded gasoline. It is simply the monetary difference between West Texas Intermediate crude oil and Unleaded Gasoline, both of which are traded on the New York Mercantile Exchange (NYMEX).
#
# $$ S_{t} = GAS_t - WTI_t $$
#
# $S_{t}$ is the price of the spread at time $t$ (in \\$ per barrel), $GAS_t$ is the price of unleaded gasoline at time $t$ (in \\$ per barrel), and $WTI_t$ is the price of West Texas Intermediate crude oil at time $t$ (in \\$ per
# barrel).
#
# ## Non-linear cointegration
#
# Cointegration was first introduced by [(Engle and Granger. 1987)](https://doi.org/10.2307/1913236). The technique is to test the null hypothesis that any combination of two series contains a unit root. If the null hypothesis is refuted and the conclusion is that a unit root does not exist, the combination of the two series is cointegrated. 
#
# As explained in the previous section, the crack spread may exhibit larger moves in one direction than in the other, this is known as asymmetry. Since the traditional unit root test has only one parameter for the autoregressive estimate, it assumes upside and downside moves to be identical or symmetric.
#
# Non-linear cointegration was first introduced by [(Enders and Granger. 1998)](https://doi.org/10.2307/1392506), who extended the unit root test by considering upside and downside moves separately, thus allowing for the possibility of asymmetric adjustment. 
#
# Enders and Granger extend the Dickey-Fuller test to allow for the unit root hypothesis to be tested against an
# alternative of asymmetric adjustment. Here, this is developed from its simplest form; consider the standard Dickey–Fuller test
#
# $$ \Delta \mu_{t} = p \mu_{t-1} + \epsilon_t $$ 
#
# where $\epsilon_t$ is a white noise process. The null hypothesis of $p=0$ is tested against the alternative of $p \neq 0 $. $p=0$ indicates that there is no unit root, and therefore $\mu_i$ is a stationary series. If the series $\mu_i$ are the residuals of a long-run cointegration relationship as indicated by Johansen, this
# simply results in a test of the validity of the cointegrating vector (the residuals of the cointegration equation should form a stationary series).
#
# The extension provided by Enders and Granger is to consider the upside and downside moves separately, thus allowing for the possibility of asymmetric adjustment. Following this approach;
#
# $$ \Delta \mu_{t} = I_t p_1 \mu_{i-1} + (1 - I_t) p_2 \mu_{i-1} + \epsilon_t  $$
#
# where $I_t$ is the zero-one ‘heaviside’ indicator function. The paper uses the following specification;
#
# $$ I_t = \left \{ {{1, if \mu_{t-1} \geq 0} \over {0, if \mu_{t-1} < 0}} \right. $$
#
# Enders and Granger refer to the model defined above as __threshold autoregressive (TAR)__. The null hypothesis of symmetric adjustment is $(H_0: p_1 = p_2)$, which can be tested using the standard F-test (in this case
# the Wald test), with an additional requirement that both $p_1$ and $p_2$ do not equal zero. If $p_1 \neq p_2$, cointegration between the underlying assets is non-linear.


from IPython.display import Image

import statsmodels.api as sm
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.johansen import JohansenPortfolio
from arbitragelab.ml_approach.tar import TAR


# Load Non Negative Crude and Gasoline futures data.
wti_contract_df = pd.read_csv('./data/NonNegative_CL_forward_roll.csv').set_index('Dates')
rbob_contract_df = pd.read_csv('./data/NonNegative_nRB_forward_roll.csv').set_index('Dates')

working_df = pd.concat([wti_contract_df, rbob_contract_df], axis=1)
working_df.index = pd.to_datetime(working_df.index) 
working_df.columns = ['wti', 'gasoline']

working_df.dropna(inplace=True)
working_df


# Calculate naive spread between gasoline and wti.
sprd = (working_df['gasoline'] - working_df['wti'])

# Plot spread.
plt.figure(figsize=(15,10))
sprd.plot();


# The TAR model expects a Zero mean series.
demeaned_spread = (sprd - sprd.mean())

# Initialize and fit TAR model.
model = TAR(demeaned_spread, False)
tar_results = model.fit()
tar_results.summary()


# # Plotted Residuals of the TAR Model


plt.figure(figsize=(15,10))
plt.plot(tar_results.fittedvalues.values);


# # Results
#
# The results from the original paper (Dunis et al. 2005) which uses the crack spread series from (1995 - 2003), finds statistical significant evidence of the presence of non linearity.  


Image(filename='images/paper_results.png')


# In our current time frame we are using (2005 - 2019) we fail to reject the null in the hypothesis of $p_1 = p_2$.


model.summary()


# # Difference between the Linear and Non Linear Model Residuals


# Initialize the linear fair value model.
jp_df = JohansenPortfolio()
jp_df.fit(working_df[['gasoline', 'wti']])

jp_spread = jp_df.construct_mean_reverting_portfolio(working_df[['gasoline', 'wti']])

# Demean the linear fair value model spread.
demeaned_jp_spread = jp_spread-jp_spread.mean()

plt.figure(figsize=(15,10))

# Plot Johansen Cointegration residuals.
plt.plot(demeaned_jp_spread.values)

# The Tar Fitted Results are multiplied by 1000, to provide 
# parity in measurements between models.
plt.plot(tar_results.fittedvalues.values*1000)
plt.legend(["Linear Fair Value Model", "Non Linear Fair Value Model"])


pd.Series(data=tar_results.fittedvalues.values, index=working_df.index[1:]).to_csv('./data/tar_residual_gas_wti.csv')


# # Conclusion
#
# A major consideration to be taken into account when analyzing these results is that the future contracts data used in the paper went out of service a few months after the paper was released. The data that has been used in this notebook was the new contracts that started back then. 
#
# The results clearly show that at least in the period observed the relationship between WTI and RBOB doesn't fit the Threshold Auto Regressive Model. This is evidenced by the results of the Wald test. However we can deduce, since $p_1$ is smaller in absolute terms than $p_2$, movements below fair value tend, on average, to be larger than movements above fair value. 


# # References
#
# * Dunis et al. (2005) 'Modelling and trading the gasoline crack spread: A non-linear story', Available at <https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.568.7460&rep=rep1&type=pdf>
#
# * Engle, R. F. and Granger, C. W. J. (1987) 'Cointegration and Error Correction: Representation, Estimation and Testing', Available at https://doi.org/10.2307/1913236
#
# * Enders, W. and Granger, C. (1998) 'Unit-root Tests and Asymmetric Adjustment with an Example Using the Term Structure of Interest Rates', Available at https://doi.org/10.2307/1392506



// ---------------------------------------------------

// ml_based_pairs_selection.py
// arbitrage_research_basic_PCA/ML Approach/ml_based_pairs_selection.py
# Generated from: ml_based_pairs_selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Enhancing a Pairs Trading strategy with the application of Machine Learning__ _by_ Simão Moraes Sarmento and Nuno Horta


from IPython.display import Image
import pandas as pd
import arbitragelab as al

import warnings
warnings.filterwarnings('ignore')


# # Implementation of a Machine Learning based Pairs Selection Framework
#
# ## Abstract
#
# In this paper[1], Horta and Sarmento propose a two-stage solution to applying machine learning to the problem of pairs trading. The first stage involves the application of a clustering algorithm to infer any meaningful clusters and using these groups to generate pairs that will be run through a selection process that will supply a list of pairs that comply with the conditions set. 
#
# The second stage should start by training forecasting algorithms to predict the spreads of the selected pairs. Furthermore, decile-based and quintile-based thresholds should be collected to integrate the trading model. Having fitted the forecasting algorithms and obtained the two combinations for the thresholds, the model can be applied to the validation set. From the validation performance, the best threshold combination is selected. At this point, the model is finally ready to be applied on unseen data, from which the performance may be inferred.
#
# ## Introduction
#
# This notebook will focus on the first stage, which refers to the pairs selection methodology. It will involve the application of PCA to distill the returns universe into a lower dimensional form. Then the OPTICS algorithm will be applied, on the expectation that it infers meaningful clusters of assets from which to select the pairs. The motivation is to let the data explicitly manifest itself, rather than manually defining the groups each security should belong to. The proposed methodology encompasses the following steps:
#
# - Dimensionality reduction - find a compact representation for each security;
# - Unsupervised Learning - apply an appropriate clustering algorithm;
# - Select pairs - define a set of rules to select pairs for trading.


Image(filename='images/prposed_framework_diagram.png') 


# ---


# # Usage of Implementation
#
# To start using this module we first need to set up our asset universe, in this case, the dataset used is the daily price data of every asset in the S&P 500.


prices_df = pd.read_csv('./data/data.csv').set_index('Date').dropna()
prices_df.index = pd.to_datetime(prices_df.index)

prices_df = prices_df.last('10Y')

prices_df.sample(10)


# ## Step 1 - Dimensionality Reduction
#
#
#
# ### Using PCA to find a compact representation for each security
#
# - Extracts common underlying risk factors from securities’ returns;
# - Produces a compact representation for each security;
#
# Before applying PCA, the pricing data needs to be converted to returns and then normalized by subtracting the mean and dividing by the standard deviation, as follows:
#
# Returns 
#
# $$ R_{i, t} = \frac{P_{i,t} - P_{i,t-1}} {P_{i,t-1}} $$
#
#
# Data Normalization
#
# $$ Y_{i} =  \frac{R_{i} - \bar{R_{i}}} {\sigma_{i}} $$
#
# Decomposition
#
# By applying PCA, $A$ is decomposed into the resulting eigenvectors and eigenvalues. An $n$ number of eigenvectors is selected where $n$ represents the number of features to describe the transformed data. The matrix containing the eigenvalues is set as the feature vector. The final dataset is obtained by multiplying the original matrix A by the feature vector.


ps = al.ml_approach.PairsSelector(prices_df)

# Here the first parameter is the number of features to reduce to.
ps.dimensionality_reduction_by_components(5)

# The following will plot the feature vector from the previous method call.
ps.plot_pca_matrix();


# A quick visual inspection of the feature vector shows a good amount of densely packed groups/clusters. If the points are too sparse, this likely suggests that you don't have enough datapoints.


# ## Step 2 - Unsupervised Learning
# ### Applying OPTICS clustering algorithm
#
# - No need to specify the number of clusters in advance;
# - Robust to outliers;
# - Suitable for clusters with varying density
#
#
# The first method is to use the OPTICS clustering algorithm and letting the built-in automatic 
# procedure to select the most suitable $\epsilon$ for each cluster. 


%matplotlib notebook 

ps.cluster_using_optics(min_samples=3)
ps.plot_clustering_info(method='OPTICS', n_dimensions=3);


# ### Applying DBSCAN clustering algorithm
#
# The second method is to use the DBSCAN clustering algorithm. This is to be used when the user 
# has domain-specific knowledge that can enhance the results given the algorithm's parameter 
# sensitivity. A possible approach to finding $\epsilon$ described in [2] is to inspect the knee plot and fix a 
# suitable $\epsilon$ by observing the global curve turning point.


ps.plot_knee_plot();


# The following are example results of DBSCAN clustering using different $\epsilon$ values, showing the efficacy of the method at different _'k-distance'_ values from the knee plot. 


ps.cluster_using_dbscan(eps=0.1, min_samples=3, metric='euclidean')
ps.plot_clustering_info(method='DBSCAN @ eps: 0.1', figsize=(8,8));

print('-' * 100)

ps.cluster_using_dbscan(eps=0.06, min_samples=3, metric='euclidean')
ps.plot_clustering_info(method='DBSCAN @ eps: 0.06', figsize=(8,8));

print('-' * 100)

ps.cluster_using_dbscan(eps=0.04, min_samples=3, metric='euclidean')
ps.plot_clustering_info(method='DBSCAN @ eps: 0.04', figsize=(8,8));


# The first plot shows the results with the upper bound value of 0.1, which was not sensitive enough to detect any groups. The second plot was set with the optimal value from the knee plot of 0.06 which detected a good amount of structure. The final plot was set with lower bound value of 0.04, which only managed to detect very densely packed clusters. 


# ## Step 3 - Select Pairs
# ### Finding resulting pairs that pass the following set of rules
#
# Sarmento and Horta suggest four criteria to further filter the potential pairs to increase the probability of selecting pairs of securities whose prices will continue to mean revert in the future. 
# - Cointegration using the Engle-Granger Test. 
# - Hurst Exponent $H$: Keep the pairs with (spread) $H<0.5$ for mean-reversion. 
# - Halflife: Keep the pairs with (spread) halflife in between $1$ day and $1$ year.
# - Minimum number of crossing mean in a year: Keep the pairs with (spread) crossing its mean $12$ times a year.
#
#
# These four criteria indicate attractive characteristics for potential tradable pairs of securities. The Engle-Granger tests the pair for cointegration. A Hurst exponent below 0.5 indicates that the pair of prices regresses strongly to the mean. Pairs with extreme half-life values, below 1 or above 356, are excluded from the selected pairs. Extreme half-life values indicate a price series that either reverts too quickly or too slowly to be traded. Finally, the price series must cross the long-term spread mean on average at least 12 times a year.


# <div class="alert alert-warning">
#
# **Warning:** The following pairs selection function is computationally heavy, so execution is going to be long and might slow down your system.
#
# </div>


ps.unsupervised_candidate_pair_selector()


# Helper function to plot a single select pair.
ps.plot_single_pair(('EOG', 'MRO'));


# Helper function to plot all pairs that have passed the final step.
ps.plot_selected_pairs();


# The following method will output detailed pair statistics.
ps.describe_extra()


# The following method will output statistics of each step
# done in the framework.
ps.describe()


# It should be expected that the majority of pairs are eliminated in the first step. Having assets paired up in a cluster doesn't necessarily mean that they are in a long term cointegration.


# ## Analysis of selected pairs sector/industry classifications


sectoral_info = pd.read_csv('./data/sp500_ticker_sector_info.csv').drop('Unnamed: 0', axis=1)

final_pairs_info = ps.describe_extra()

ps.describe_pairs_sectoral_info(final_pairs_info['leg_1'], final_pairs_info['leg_2'], sectoral_info)


# ---


# # Conclusion
#
# This notebook describes the proposed Pairs Selection Framework also shows example usage of the implemented framework to efficiently reduce the search space and select quality trading pairs. 
#
# - Ten years of daily stock price data for 400 securities were reduced to 5 dimensions through PCA. 
# - Over 600 potential trading pairs were identified through OPTICS clustering. 
# - Twenty-three pairs from the clusters met the four selection criteria. 
#
# Key takeaways:
# - The number of pairs left for a trader to handle is much less compared to the number of pairs generated through an open combinatorial search of the whole asset universe.
# - Most of the final pairs selected follow expected economic sectoral clusters even though there was no implied industry/sectoral grouping anywhere in the framework.
#
# Solutions to common pitfalls:
# - Dimensionality reduction techniques need a certain amount of data to work reliably, so if instability is encountered at this junction, it is suggested to increase the amount of data.
# - The number of PCA components needs to balance the amount of variance represented with density in euclidean distance. The rule of thumb is a number less than 15 components.
# - When in doubt use OPTICS.
# - For the clustering methods, the _'minimum samples'_ argument needs to be large enough so that the generated clusters are homogeneous. The rule of thumb is a number larger than 3.


# # References
# 1. Sarmento, Simão. & Horta, Nuno. (2020). Enhancing a Pairs Trading strategy with the application of Machine Learning. Available at: http://premio-vidigal.inesc.pt/pdf/SimaoSarmentoMSc-resumo.pdf
#
# 2. Rahmah N, Sitanggang S (2016). Determination of Optimal Epsilon (Eps) Value on DBSCAN Algorithm to Clustering Data on Peatland Hotspots in Sumatra. Available at: https://doi.org/10.1088/1755-1315/31/1/012012



// ---------------------------------------------------

// crack_spread_modeling.py
// arbitrage_research_basic_PCA/ML Approach/crack_spread_modeling.py
# Generated from: crack_spread_modeling.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Spread Modeling
#
# * Reference: __Modelling and trading the gasoline crack spread: A non-linear story__ *by* Christian L. Dunis, Jason Laws and Ben Evans
#
# ## Introduction
#
# This notebook follows the works of Dunis et al. in the exploration of various Machine Learning models, with application to fundamental commodity-based spreads. 
#
# A fair value model is developed based on the works of (Enders and Granger 1998). This is used as a benchmark for other non-linear models such as multi-layer perceptron  (MLP), recurrent neural networks (RNN) and higher-order neural networks (HONN). The models are used to forecast $\Delta S_t$, the daily change in the spread. 
#
# Finally, the unfiltered spread is benchmarked against a correlation filter, a time-varying leverage-based volatility filter and, the more traditional threshold filter and, if the cointegration exhibits asymmetry, an asymmetric threshold filter.
#
# ## Outline
#
# - Spreads 
#     - Crack - (Gasoline - WTI) (RB - CL)
#
# - Rollover Method
#     - By Same Expiration Date
#
# - Ensemble
#     - RegressorCommittee
#
# - Models
#     - Fair value model
#         - Johansen Portfolio
#         - [TAR Model](#tar) 
#     - [Multi Layer Perceptron](#mlp) 
#     - [Long Short Term Memory Network](#rnn)
#     - Higher-Order Neural Network
#         - Single Layer
#             - [Feature Expander](#flnn)
#         - Multi layer
#             - [Pi Sigma Neural Network](#pisigma)
#
# - Filters
#     - Unfiltered
#     - Threshold
#     - Correlation
#     - Volatility


from IPython.display import Image

import random
import numpy as np
import pandas as pd 
import tensorflow as tf
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.johansen import JohansenPortfolio
from arbitragelab.ml_approach.regressor_committee import RegressorCommittee
from arbitragelab.util.spread_modeling_helper import SpreadModelingHelper
from arbitragelab.ml_approach.neural_networks import (MultiLayerPerceptron, RecurrentNeuralNetwork,
                                                      PiSigmaNeuralNetwork)

import warnings
warnings.filterwarnings('ignore')


# Seed value.
seed_value = 0

# Set the built-in pseudo-random generator at a fixed value.
random.seed(seed_value)

# Set the `numpy` pseudo-random generator at a fixed value.
np.random.seed(seed_value)

# Set the `tensorflow` pseudo-random generator at a fixed value.
tf.random.set_seed(seed_value)

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)


# # Working with Future Contracts
#
# In this notebook, the spread being modelled is the difference between the Crude oil Future contract and the RBOB Gasoline Future contract. 
#
# The size of the Crude oil future is 1,000 barrels, and the size of the RBOB gasoline future is 42,000 gallons. 
#
# Futures prices need to be handled in a special way, because of their tendency to expire in either of two states; contango or backwardation. This leads to gaps in the price series, that makes it unusable for backtesting purposes. 
#
# The processing of these prices is done through the technique called futures rolling. It involves calculating the gap between the day before expiration and the corresponding day of the new contract, and cumulatively summing these gaps in the direction of the user's choice.
#
# Also, note the resulting series from the rolling procedure will be in absolute terms (price differences) instead of relative terms (percentage difference).
#
# ### __Below is an example of the accumulated differences between a rolled and unrolled crude oil contract__


# Load unrolled futures data.
cl_df = pd.read_csv('./data/futures_price_data/CL1.csv', index_col='Dates', parse_dates=True)
cl_df.dropna(inplace=True)

# Load rolled futures data.
cl_forward_df = pd.read_csv('./data/forward_rolled_futures/CL_rolled_forward.csv', index_col='Dates', parse_dates=True)
cl_forward_df.dropna(inplace=True)

# Plot year of differences for intuition.
cl_forward_df['PX_LAST']['1997-01': '1997-12'].diff().cumsum().plot(figsize=(15,10))
cl_df['PX_LAST']['1997-01': '1997-12'].diff().cumsum().plot(figsize=(15,10))

for month in range(1, 12):
    plt.axvline(pd.datetime(1997, month, 21), color='r')

plt.legend(["Rolled", "Non-Rolled", "Estimated Expiry Dates"]);
plt.title("WTI future rolling plot");


# Now in this notebook, the expiration dates of the contracts are different. This is amended using the methodology described in (Dunis et al. 2006) which entails using the same expiration date for both contracts.
#
# At the same time, the RBOB contract exhibited price negativity after the rolling procedure. In general, we want to use non-negative series, thus a postprocessing method described in (De Prado 2018) was used to convert both contracts to non negative series.
#
# ## Rolling Implementation Details: 
#
# Link to [Notebook](futures_rollover.ipynb)


# Load Non Negative Prices.
wti_contract_df = pd.read_csv('./data/nonneg_forward_rolled_futures/NonNegative_CL_forward_roll.csv', index_col='Dates', parse_dates=True)
rbob_contract_df = pd.read_csv('./data/nonneg_forward_rolled_futures/NonNegative_nRB_forward_roll.csv', index_col='Dates', parse_dates=True)

# Concatenate both price series.
working_df = pd.concat([wti_contract_df, rbob_contract_df], axis=1)
working_df.columns = ['wti', 'gasoline']

working_df.dropna(inplace=True)
working_df


# Using arbitragelab implementation to setup the spread based on the Johansen cointegration method.
johansen_portfolio = JohansenPortfolio()
johansen_portfolio.fit(working_df)
sprd = johansen_portfolio.construct_mean_reverting_portfolio(working_df).diff()


# # Threshold Auto Regressive Model  <a class="anchor" id="tar"></a>
#
# Link to [Notebook](fair_value_modeling.ipynb)


# # Multi Layer Perceptron  <a class="anchor" id="mlp"></a>
#
# The MLP network has three layers; they are the input layer (explanatory variables), the output layer (the model estimation of the time series) and the hidden layer. The number of nodes in the hidden layer defines the amount of complexity that the model can fit.


# Initializing the Helper Class with the linear fair value spread as input
# with unique sampling enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=False,
                              unique_sampling=True)

_, frame_size = helper.input_train.shape

# Here the parameters for a standard Regressor Neural Network are set.
mlp_params = {'frame_size': frame_size, 'hidden_size': 8, 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "sigmoid",
                    'output_layer_act_func': "linear"}

# The RegressorCommittee is initialized with the sample network parameters using the 
# MLP class and a committee of size 10.
committee = RegressorCommittee(mlp_params, regressor_class='MultiLayerPerceptron',
                               num_committee=10, epochs=1000, patience=20, verbose=False)

committee.fit(helper.input_train, helper.target_train, helper.input_test, helper.target_test)

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Functional Link Neural Network <a class="anchor" id="flnn"></a>
#
# Functional Link NN use joint activation functions; this technique reduces the need to establish the relationships between inputs when training. Furthermore, this reduces the number of free weights and means that network can be faster to train than even MLPs. Because the number of inputs can be very large for higher-order architectures,
# however, orders of 4 and over are rarely used. Another advantage of reducing free weights is that the problems of overfitting and local optima affecting the results can be largely avoided.


# Initializing the Helper Class with the linear fair value spread as input
# with both unique sampling and feature expansion enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=True,
                              unique_sampling=True)

_, frame_size = helper.input_train.shape

# Here the parameters for a standard Regressor Neural Network are set.
honn_params = {'frame_size': frame_size, 'hidden_size': 2, 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "relu",
                    'output_layer_act_func': "linear"}

# The RegressorCommittee is initialized with the sample network parameters using the 
# MLP class and a committee of size 10.
committee = RegressorCommittee(honn_params, regressor_class='MultiLayerPerceptron',
                               num_committee=10, epochs=1000, patience=20, verbose=False)

committee.fit(helper.input_train, helper.target_train, helper.input_test, helper.target_test)

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Pi Sigma Neural Network <a class="anchor" id="pisigma"></a>
# Pi Sigma network can be considered as a class of feedforward fully connected HONNs. First introduced by (Shin and Ghosh 1991), the Pi Sigma network utilizes product cells as the output units to indirectly incorporate the capabilities of higher-order networks while using a fewer number of weights and processing units. Their creation
# was motivated by the need to create a network combining the fast learning property of single-layer networks with the powerful mapping capability of HONNs while avoiding the combinatorial increase in the required number of weights. While the order of the more traditional HONN architectures is expressed by the complexity of the inputs, in the context of Pi Sigma, it is represented by the number of hidden nodes.


# Initializing the Helper Class with the linear fair value spread as input
# with unique sampling enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=False,
                              unique_sampling=True)

_, frame_size = helper.input_train.shape

# Here the parameters for the PiSigma Neural Network are set.
ps_params = {'frame_size': frame_size, 'hidden_size': 6, 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "linear",
                    'output_layer_act_func': "tanh"}

# Here the committee class is initialized with PiSigmaNeuralNetwork as the member class
# and a committee size of 10.
committee = RegressorCommittee(ps_params, regressor_class='PiSigmaNeuralNetwork',
                               num_committee=10, epochs=1000, patience=200, verbose=False)

committee.fit(helper.input_train, helper.target_train, helper.input_test, helper.target_test)

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Recurrent Neural Network <a class="anchor" id="rnn"></a>
#
# Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. The main driving factors for the development of LSTM's were to overcome two major technical problems in the classical SimpleRNN. The two technical problems are vanishing gradients and exploding gradients, both related to how the network is trained.
#
# An LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. Each one contains one or more recurrently connected memory cells and three multiplicative units – the input, output and forget gates – that provide continuous analogues of write, read and reset operations for the cells.


# Initializing the Helper Class with the linear fair value spread as input
# with unique sampling enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=False,
                              unique_sampling=True)

# The training variables are reshaped into [Samples, Time Steps, Features]
X_rnn_train = helper.input_train.values.reshape((helper.input_train.shape[0], helper.input_train.shape[1], 1))
X_rnn_test = helper.input_test.values.reshape((helper.input_test.shape[0], helper.input_test.shape[1], 1))
X_rnn_oos = helper.input_oos.values.reshape((helper.input_oos.shape[0], helper.input_oos.shape[1], 1))

_, frame_size, no_features = X_rnn_train.shape

# Here the parameters for the Recurrent Neural Network are set.
rnn_params = {'input_shape': (frame_size, no_features), 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "relu",
                    'output_layer_act_func': "linear"}

committee = RegressorCommittee(rnn_params, regressor_class='RecurrentNeuralNetwork',
                               num_committee=10, epochs=500, patience=20, verbose=False)

committee.fit(X_rnn_train, helper.target_train, X_rnn_test, helper.target_test)

helper.input_train = X_rnn_train
helper.input_test = X_rnn_test
helper.input_oos = X_rnn_oos

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Conclusion
#
# Considerations to be taken into account when analyzing these results are;
# - The future contracts data used in the paper went out of service a few months after the paper was released. The data that has been used in this notebook are the new contracts that started back then. 
# - The network training parameters suggested weren't followed. With a modest patience value EarlyStopping detected overfitting at one tenth of the number of epochs that were suggested in the papers.
# - As per the point above, there is quite a high probability that the models used in the paper were overfit.
# - Transaction costs weren't included.
#
# Results produced from each of the models were for the most part mixed with a few outliers. In comparison to earlier work carried out by (Dunis, Laws, and Middleton 2011), it can be concluded that;
#
# - Standard Filters over the model prediction consistently had positive annual returns.   
# - The Correlation Filters showed sporadic performance with periods of massive losses.   
# - The Volatility Filter substantially improved the annual returns when used on top of a Standard Filter.
#
# On the whole, the application of nonlinear methodologies and time-varying volatility leverage filters has proven to be profitable; however, their application will vary depending on market participants. 


# # References
#
# * Dunis, C.L., Laws, J. and Evans, B., 2006. Modelling and trading the gasoline crack spread: A non-linear story. Derivatives Use, Trading & Regulation, 12(1-2), pp.126-145.
# * De Prado, M.L., 2018. Advances in financial machine learning. John Wiley & Sons.
# * Shin, Y. and Ghosh, J. (1991) ‘The Pi-Sigma Network: An Efficient Higher-Order Neural Network for Pattern Classification and Function Approximation’, Proceedings IJCNN, Seattle, July, 13-18.
# * Enders, W. and Granger, C.W.J., 1998. Unit-root tests and asymmetric adjustment with an example using the term structure of interest rates. Journal of Business & Economic Statistics, 16(3), pp.304-311.



// ---------------------------------------------------

// kalman_filter.py
// arbitrage_research_basic_PCA/Other Approaches/kalman_filter.py
# Generated from: kalman_filter.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Algorithmic Trading: Winning Strategies and Their Rationale__ _by_ Ernest P. Chan


# # Kalman Filter


# This description of the Kalman filter closely follows the work of _Ernest P. Chan_ __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146). 


# ## Introduction


# While for truly cointegrating price series we can use the tools described in the Cointegration module
# (Johansen test, etc.), for other price series we should use other tools to estimate the
# hedge ratio. For real price series, the hedge ratio can be changing in time. Using a look-back period
# to estimate the parameters of a model has its disadvantages, as a short period can cut a part of the information.
#
# This module describes a scheme that allows using the Kalman filter for hedge ratio updating, as presented in the
# book by Ernest P. Chan **"Algorithmic Trading: Winning Strategies and Their Rationale"**. One of the advantages
# of this approach is that we don't have to pick a weighting scheme for observations in the look-back period.


# ## Kalman Filter
#
# Following the descriptions by Ernest P. Chan:
#
# "Kalman filter is an optimal linear algorithm that updates the expected value of a hidden
# variable based on the latest value of an observable variable.
#
# It is linear because it assumes that the observable variable is a linear function of the hidden
# variable with noise. It also assumes the hidden variable at time $t$ is a linear function
# of itself at time $t - 1$ with noise, and that the noises present in these functions
# have Gaussian distributions (and hence can be specified with an evolving covariance
# matrix, assuming their means to be zero.) Because of all these linear relations, the expected
# value of the hidden variable at time $t$ is also a linear function of its expected value
# prior to the observation at $t$, as well as a linear function of the value of the observed
# variable at $t$.
#
# The Kalman filter is optimal in the sense that it is the best estimator
# available if we assume that the noises are Gaussian, and it minimizes the mean square error of
# the estimated variables."
#
# As we're searching for the hedge ratio, we're using the following linear function:
#
# $$y(t) = x(t) \beta(t) + \epsilon(t)$$
#
# where $y$ and $x$ are price series of the first and the second asset, $\beta$ is the
# hedge ratio that we are searching and $\epsilon$ is the Gaussian noise with variance $V_{\epsilon}$.
#
# Allowing the spread between the $x$ and $y$ to have a nonzero mean, $\beta$
# will be a vector of size $(2, 1)$ denoting both the intercept and the slope of
# the linear relation between $x$ and $y$. For this needs, the $x(t)$
# is augmented with a vector of ones to create an array of size $(N, 2)$.
#
# Next, an assumption is made that the regression coefficient changes in the following way:
#
# $$\beta(t) = \beta(t-1) + \omega(t-1)$$
#
# where $\omega$ is a Gaussian noise with covariance $V_{\omega}$. So the regression
# coefficient at time $t$ is equal to the regression coefficient at time $t-1$ plus
# noise.
#
# With this specification, the Kalman filter can generate the expected value of the hedge ratio
# $\beta$ at each observation $t$.
#
# Kalman filter also generates an estimate of the standard deviation of the forecast error
# of the observable variable. It can be used as the moving standard deviation of a Bollinger band.


# ## Kalman Filter Strategy
#
# Quantities that were computed using the Kalman filter can be utilized to generate trading
# signals.
#
# The forecast error $e(t)$ can be interpreted as the deviation of a pair spread from
# the predicted value. This spread can be bought when it has high negative values and sold
# when it has high positive values.
#
# As a threshold for the $e(t)$, its standard deviation $\sqrt{Q(t)}$ is used:
#
# - If $e(t) < - entry\_std\_score * \sqrt{Q(t)}$, a long position on the spread should be taken: Long $N$
#   units of the $y$ asset and short $N*\beta$ units of the $x$ asset.
#
# - If $e(t) \ge - exit\_std\_score * \sqrt{Q(t)}$, a long position on the spread should be closed.
#
# - If $e(t) > entry\_std\_score * \sqrt{Q(t)}$, a short position on the spread should be taken: Short $N$
#   units of the $y$ asset and long $N*\beta$ units of the $x$ asset.
#
# - If $e(t) \le exit\_std\_score * \sqrt{Q(t)}$, a short position on the spread should be closed.
#
# So it's the same logic as in the Bollinger Band Strategy from the Mean Reversion section of the Cointegration Approach module.


# ## Usage of the Algorithms


# Let's use the above tools on real data. 
#
# First, we will choose a pair of assets to apply the Kalman filter to. Then, from the filter output, we will generate trading signals based on the rules presented in the Kalman Filter Strategy section. Finally, we will analyze the obtained results. 


from IPython.display import Image

import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# As a pair to test the Kalman Filter approach we will use **EWC**(iShares MSCI Canada ETF) - **EWA**(iShares MSCI Australia ETF). This exact pair was used as an example in the book by Ernest P. Chan. We can also check if our results match the original ones for the same dataset.


# List of tickers to use in the analysis
tickers = ['EWC', 'EWA']

# Loading data
data =  yf.download(tickers, start="2006-04-26", end="2012-04-09")

# Taking close prices for chosen instruments
data = data["Adj Close"]

# Looking at the downloaded data
data.head()


# ### Applying Kalman Filter


# Now we'll apply the Kalman filter to our pair. As an output, we'll get the hedge ratios, intercepts, forecast errors, and standard deviations of forecast errors.
#
# Note that when feeding observations to the Kalman Filter _update()_ function, the first parameter is the x (in our case EWA), and y is the second parameter (in our case EWC).


# Covariance parameters used in the example
observation_covariance = 0.001
delta = 0.0001
transition_covariance = delta / (1 - delta)

# Initialising an object containing needed methods
strategy = al.other_approaches.KalmanFilterStrategy(observation_covariance=observation_covariance,
                                                    transition_covariance=transition_covariance)

# Feeding our price series element by element to the strategy
for observations in data.values:
      strategy.update(observations[0], observations[1])

# Getting lists of hedge ratios, intercepts, forecast errors, and their standard deviations
hedge_ratios = strategy.hedge_ratios
intercepts = strategy.intercepts
forecast_errors = strategy.spread_series
error_st_dev = strategy.spread_std_series


# **Note:** We're feeding price observations to the strategy one by one, so when generating internal parameters it has no access to future data.


# Transforming our variables into pandas formats for plotting
hedge_ratios = pd.DataFrame(hedge_ratios[10:],
                            index = data[10:].index,
                            columns =['Hedge Ratios'])

intercepts = pd.DataFrame(intercepts[10:],
                          index = data[10:].index,
                          columns =['Intercepts'])

forecast_errors = pd.DataFrame(forecast_errors[10:],
                               index = data[10:].index,
                               columns =['Forecast Errors'])

error_st_dev = pd.DataFrame(error_st_dev[10:],
                            index = data[10:].index,
                            columns =['St.D. of Forecast Errors'])


# Comparing generated hedge ratios with the ones from the book
hedge_ratios.plot(ylim=(0,1.8),
                  figsize = (9,7),
                  title='Kalman Filter Estimate of the Slope between EWC and EWA');


Image(filename='Kalman/kalman_slope.png')


# _Slope estimated between EWC(y) and EWA(x) using the Kalman Filter. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# Our calculated hedge ratios are pretty similar to the ones shown in the book, however all slightly higher. This can be explained with small differences in datasets that were used.


# Comparing generated hedge ratios with the ones from the book
intercepts[10:].plot(ylim=(0,6), 
                     figsize = (9,7),
                     title='Kalman Filter Estimate of the Intercept between EWC and EWA');


Image(filename='Kalman/kalman_intercept.png')


# _Intercept estimated between EWC(y) and EWA(x) using the Kalman Filter. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# Our calculated intercept values are also very similar to those shown in the book, but are slightly lower. Again, we can assume it's due to differences in datasets.


# ### Kalman Filter Strategy


# First, let's compare the values of forecast errors and their standard deviations that we got using the functions from the ArbitrageLab package with the values from the book.


ax = forecast_errors.plot(figsize = (9,7))

error_st_dev.plot(ax = ax,
                  figsize = (9,7),
                  title='Measurement Prediction Error - e(t) and its Standard Deviation sqrt(Q(t))');


Image(filename='Kalman/kalman_forecast_errors.png')


# _Measurement Prediction Error $e(t)$ and Standard Deviation of $e(t)$. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# Both values of forecast errors and the standard deviation are similar to the values from the book.
#
# We can say that our implementation works on test data as expected.


# Now let's run a function to generate trading signals for our dataset. As for the parameters of the Kalman Filter Strategy ($entry\_std\_score$ and $exit\_std\_score$) we can use the same values as in the book, so $entry\_std\_score = 1$ and $entry\_std\_score = 1$.


# Generating trading signals
trading_signals = strategy.trading_signals(entry_std_score=1, exit_std_score=1)[10:]

# Setting index for the Dataframe with trading signals
trading_signals = trading_signals.set_index(data[10:].index)


# Plotting obtained target quantities of portfolio to hold
trading_signals['target_quantity'].plot(figsize = (20,7),
                                        title='Kalman Filter Strategy portfolio target quantity values');


# As seen, trading signals change frequently as values of errors often surpass the 3 standard deviation mark, especially during the 2008-2009 period. This should be taken into account as we are able to obtain trading signals (target quantities) only after the observations for that period is known. So using them to trade the same day results in a lookahead bias.
#
# We can build an equity curve for this strategy on this dataset to see if it's profitable.


# Returns of elemrnts in our dataset
data_returns = (data / data.shift(1) - 1)[10:]

data_returns.head()


# Now calculating weights for X and Y in a portfolio.
#
# In a long position on spread we long $N$ units of the $y$ asset and short $N*\beta$ units of the $x$ asset.
# As we want our weights to sum up to 1, we long $\frac{N}{N+N*\beta}$ of the $y$ asset and short $\frac{N*\beta}{N+N*\beta}$ of the $x$ asset.
#
# When we short the spread, we short $N$ units of the $y$ asset and long $N*\beta$ units of the $x$ asset.
# So we short $\frac{N}{N+N*\beta}$ of the $y$ asset and long $\frac{N*\beta}{N+N*\beta}$ of the $x$ asset.


# Weights to use when opening trades
weights_x = pd.Series(strategy.hedge_ratios[10:],
                      index = data[10:].index,
                      name = 'weight_x')

weights_x = weights_x / (weights_x + 1)
weights_y = 1 - weights_x


# Looking at weights series - in the case of a Kalman Filter, they are dynamic
weights_x.head()


# Now constructing the equity curve of a portfolio.
#
# Our asset $x$ is **EWA** and asset $y$ is **EWC**.


# Portfolio returns - in our case it's spread returns
portfolio_returns = data_returns['EWC'] * weights_x - data_returns['EWA'] * weights_y

# Returns of our investment portfolio - using the generated signals on portfolio returns
# Shifting our trading signals one observation ahead to avoid the lookahead bias
investment_portfolio_returns = portfolio_returns * trading_signals['target_quantity'].shift(1)

# Price of our investment portfolio
portfolio_price = (investment_portfolio_returns + 1).cumprod()


# Calculating the equity curve of our investemnt portfolio
equity_curve = portfolio_price - 1


# And plotting it
equity_curve.plot(figsize = (9,7),
                  title='Kalman Filter Strategy investment portfolio equity curve');


Image(filename='Kalman/kalman_cumulative_returns.png')


# _Cumulative Returns of Kalman Filter Strategy on EWA-EWC. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# We are getting results very close to those shows in the book. The equity curve being flattened along the Y axis can be explained with
# that we are normalizing our weights for them to be summed up to one.
#
# These results look good, over the 6 year period equity curve shows an increase in the investment portfolio value from 1 to around 2,38. 
#
# We can further test this strategy by choosing different $entry\_std\_score$ and $exit\_std\_score$ values, or adding transaction costs to see if the strategy is robust.


# ## Conclusion


# This notebook describes the Kalman Filter Strategy class and its functionality. Also, it shows how the tools can be used on real data.
#
# The algorithms and the descriptions used in this notebook were described by _Ernest P. Chan_ in the book __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146).
#
# Key takeaways from the notebook:
# - Kalman Filter Strategy is applicable when we don't have a truly cointegrating pair of price series.
# - No need to pick the look-backward window for the means and the standard deviation estimation, as in strategies from the Mean Reversion approach.
# - Kalman Filter approach allows estimation of changing hedge ratio between elements, whereas tools from the Cointegration Approach module offered a fixed hedge ratio.
# - Kalman Filter Strategy generates trading signals based on forecast errors and their standard deviations. The logic is similar to the one from the Bollinger Bands Strategy from the Mean Reversion section of the Cointegration Approach module.
# - Parameters available for optimization in the Kalman Filter Strategy are the ender and exit standard deviation scores.



// ---------------------------------------------------

// pca_approach.py
// arbitrage_research_basic_PCA/Other Approaches/pca_approach.py
# Generated from: pca_approach.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Statistical Arbitrage in the U.S. Equities Market__ _by_ Marco Avellaneda and Jeong-Hyun Lee


# # PCA Approach


# This description of the PCA approach closely follows the work of _Marco Avellaneda_ and _Jeong-Hyun Lee_ __Statistical Arbitrage in the U.S. Equities Market__  [available here](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf). 


# ## Introduction


# This research notebook shows how the Principal Component Analysis can be used to create mean-reverting portfolios
# and generate trading signals. It's done by considering residuals or idiosyncratic components
# of returns and modeling them as mean-reverting processes.
#
# The original paper presents the following description:
#
# The returns for different stocks are denoted as $\{ R_{i} \}^{N}_{i=1}$. The $F$ represents
# the return of a "market portfolio" over the same period. For each stock in the universe:
#
# $$R_{i} = \beta_{i} F + \tilde{R_{i}}$$
#
# which is a regression, decomposing stock returns into a systematic component $\beta_{i} F$ and
# an (uncorrelated) idiosyncratic component $\tilde{R_{i}}$.
#
# This can also be extended to a multi-factor model with $m$ systematic factors:
#
# $$R_{i} = \sum^{m}_{j=1} \beta_{ij} F_{j} + \tilde{R_{i}}$$
#
# A trading portfolio is a market-neutral one if the amounts $\{ Q_{i} \}^{N}_{i=1}$ invested in
# each of the stocks are such that:
#
# $$\bar{\beta}_{j} = \sum^{N}_{i=1} \beta_{ij} Q_{i} = 0, j = 1, 2,, ..., m.$$
#
# where $\bar{\beta}_{j}$ correspond to the portfolio betas - projections of the
# portfolio returns on different factors.
#
# As derived in the original paper,
#
# $$\sum^{N}_{i=1} Q_{i} R_{i} = \sum^{N}_{i=1} Q_{i} \tilde{R_{i}}$$
#
# So, a market-neutral portfolio is only affected by idiosyncratic returns.


# ## PCA Approach
#
#
# This approach was originally proposed by Jolliffe (2002). It is using a historical share price data
# on a cross-section of $N$ stocks going back $M$ days in history. The stocks return data
# on a date $t_{0}$ going back $M + 1$ days can be represented as a matrix:
#
# $$R_{ik} = \frac{S_{i(t_{0} - (k - 1) \Delta t)} - S_{i(t_{0} - k \Delta t)}}{S_{i(t_{0} - k \Delta t)}}; k = 1, ..., M; i = 1, ..., N.$$
#
# where $S_{it}$ is the price of stock $i$ at time $t$ adjusted for dividends. For
# daily observations $\Delta t = 1 / 252$.
#
# Returns are standardized, as some assets may have greater volatility than others:
#
# $$Y_{ik} = \frac{R_{ik} - \bar{R_{i}}}{\bar{\sigma_{i}}}$$
#
# where
#
# $$\bar{R_{i}} = \frac{1}{M} \sum^{M}_{k=1}R_{ik}$$
#
# and
#
# $$\bar{\sigma_{i}}^{2} = \frac{1}{M-1} \sum^{M}_{k=1} (R_{ik} - \bar{R_{i}})^{2}$$
#
# And the empirical correlation matrix is defined by
#
# $$\rho_{ij} = \frac{1}{M-1} \sum^{M}_{k=1} Y_{ik} Y_{jk}$$
#
# **Note:** It's important to standardize data before inputting it to PCA, as the PCA seeks to maximize the
# variance of each component. Using unstandardized input data will result in worse results.
# The *get_signals()* function in this module automatically standardizes input returns before
# feeding them to PCA.
#
# The original paper mentions that picking long estimation windows for the correlation matrix
# ($M \gg N$, $M$ is the estimation window, $N$ is the number of assets in a portfolio)
# don't make sense because they take into account the distant past which is economically irrelevant.
# The estimation windows used by the authors is fixed at 1 year (252 trading days) prior to the trading date.
#
# The eigenvalues of the correlation matrix are ranked in the decreasing order:
#
# $$N \ge \lambda_{1} \ge \lambda_{2} \ge \lambda_{3} \ge ... \ge \lambda_{N} \ge 0.$$
#
# And the corresponding eigenvectors:
#
# $$v^{(j)} = ( v^{(j)}_{1}, ..., v^{(j)}_{N} ); j = 1, ..., N.$$
#
# Now, for each index $j$ we consider a corresponding "eigenportfolio", in which we
# invest the respective amounts invested in each of the stocks as:
#
# $$Q^{(j)}_{i} = \frac{v^{(j)}_{i}}{\bar{\sigma_{i}}}$$
#
# And the eigenportfolio returns are:
#
# $$F_{jk} = \sum^{N}_{i=1} \frac{v^{(j)}_{i}}{\bar{\sigma_{i}}} R_{ik}; j = 1, 2, ..., m.$$


from IPython.display import Image
Image(filename='PCA/pca_approach_portfolio.png')


# *Performance of a portfolio composed using the PCA approach in comparison to the market cap portfolio.
# An example from ["Statistical Arbitrage in the U.S. Equities Market"](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf)
# by Marco Avellaneda and Jeong-Hyun Lee.*


# In a multi-factor model we assume that stock returns satisfy the system of stochastic
# differential equations:
#
# $$\frac{dS_{i}(t)}{S_{i}(t)} = \alpha_{i} dt + \sum^{N}_{j=1} \beta_{ij} \frac{dI_{j}(t)}{I_{j}(t)} + dX_{i}(t),$$
#
# where $\beta_{ij}$ are the factor loadings.
#
# The idiosyncratic component of the return with drift $\alpha_{i}$ is:
#
# $$d \widetilde{X_{i}}(t) = \alpha_{i} dt + d X_{i} (t).$$
#
# Based on the previous descriptions, a model for $X_{i}(t)$ is estimated as the Ornstein-Uhlenbeck
# process:
#
# $$dX_{i}(t) = \kappa_{i} (m_{i} - X_{i}(t))dt + \sigma_{i} dW_{i}(t), \kappa_{i} > 0.$$
#
# which is stationary and auto-regressive with lag 1.
#
# The parameters $\alpha_{i}, \kappa_{i}, m_{i}, \sigma_{i}$ are specific for each stock.
# They are assumed to *de facto* vary slowly in relation to Brownian motion increments $dW_{i}(t)$,
# in the chosen time-window. The authors of the paper were using a 60-day window to estimate the residual
# processes for each stock and assumed that these parameters were constant over the window.
#
# However, the hypothesis of parameters being constant over the time-window is being accepted
# for stocks which mean reversion (the estimate of $\kappa$) is sufficiently high and is
# rejected for stocks with a slow speed of mean-reversion.
#
# An investment in a market long-short portfolio is being constructed by going long 1 dollar on the stock and
# short $\beta_{ij}$ dollars on the $j$ -th factor. Expected 1-day return of such portfolio
# is:
#
# $$\alpha_{i} dt + \kappa_{i} (m_{i} - X_{i}(t))dt$$
#
# The parameter $\kappa_{i}$ is called the speed of mean-reversion. If $\kappa \gg 1$ the
# stock reverts quickly to its means and the effect of drift is negligible. As we are assuming that
# the parameters of our model are constant, we are interested in stocks with fast mean-reversion,
# such that:
#
# $$\frac{1}{\kappa_{i}} \ll T_{1}$$
#
# where $T_{1}$ is the estimation window to estimate residuals in years.


# ## PCA Trading Strategy
#
# The strategy implemented in the ArbitrageLab module sets a default estimation window for the correlation
# matrix as 252 days, a window for residuals estimation of 60 days ($T_{1} = 60/252$) and the
# threshold for the mean reversion speed of an eigenportfolio for it to be traded so that the reversion time
# is less than $1/2$ period ($\kappa > 252/30 = 8.4$).
#
# For the process $X_{i}(t)$ the equilibrium variance is defined as:
#
# $$\sigma_{eq,i} = \frac{\sigma_{i}}{\sqrt{2 \kappa_{i}}}$$
#
# And the following variable is defined:
#
# $$s_{i} = \frac{X_{i}(t)-m_{i}}{\sigma_{eq,i}}$$
#
# This variable is called the S-score. The S-score measures the distance to the equilibrium of the cointegrated
# residual in units standard deviations, i.e. how far away a given asset eigenportfolio is from the theoretical
# equilibrium value associated with the model.


Image(filename='PCA/pca_approach_s_score.png')


# *Evolution of the S-score of JPM ( vs. XLF ) from January 2006 to December 2007.
# An example from  ["Statistical Arbitrage in the U.S. Equities Market"](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf)
# by Marco Avellaneda and Jeong-Hyun Lee.*


# If the eigenportfolio shows a mean reversion speed above the set threshold ($\kappa$), the
# S-score based on the values from the residual estimation window is being calculated.
#
# The trading signals are generated from the S-scores using the following rules:
#
# - Open a long position if $s_{i} < - \bar{s_{bo}}$
#
# - Close a long position if $s_{i} < + \bar{s_{bc}}$
#
# - Open a short position if $s_{i} > + \bar{s_{so}}$
#
# - Close a short position if $s_{i} > - \bar{s_{sc}}$
#
# Opening a long position means buying 1 dollar of the corresponding stock (of the asset eigenportfolio)
# and selling $\beta_{i1}$ dollars of assets from the first scaled eigenvector ($Q^{(1)}_{i}$),
# $\beta_{i2}$ from the second scaled eigenvector ($Q^{(2)}_{i}$) and so on.
#
# Opening a short position, on the other hand, means selling 1 dollar of the corresponding stock and buying
# respective beta values of stocks from scaled eigenvectors.
#
# Authors of the paper, based on empirical analysis chose the following cutoffs. They were selected
# based on simulating strategies from 2000 to 2004 in the case of ETF factors:
#
# - $\bar{s_{bo}} = \bar{s_{so}} = 1.25$
#
# - $\bar{s_{bc}} = 0.75$, $\bar{s_{sc}} = 0.50$
#
# The rationale behind this strategy is that we open trades when the eigenportfolio shows good mean
# reversion speed and its S-score is far from the equilibrium, as we think that we detected an anomalous
# excursion of the co-integration residual. We expect most of the assets in our portfolio to be near
# equilibrium most of the time, so we are closing trades at values close to zero.
#
# The signal generating function implemented in the ArbitrageLab package outputs target weights for each
# asset in our portfolio for each observation time - target weights here are the sum of weights of all
# eigenportfolios that show high mean reversion speed and have needed S-score value at a given time.


# ## Usage of the Algorithms


# Let's use the above tools on real data. 
#
# First, we will choose a set of stocks to apply the PCA approach to. Then we will go through the steps of the PCA approach and we'll generate trading signals using the PCA Strategy. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# We picked a set containing 176 stocks to apply the PCA Approach to. We'll be looking at a period of years 2018-2019. We'll eventually get trading signals for the year 2019, as the observations from the year 2018 will be needed to estimate the correlation matrix to get the PCA components.  


# List of tickers to use in the analysis
tickers = ['MMM', 'ABT', 'ANF', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A', 'APD',
           'AKAM', 'AA', 'ALXN', 'ATI', 'ALL', 'MO', 'AMZN', 'AEE',
           'AEP', 'AXP', 'AIG', 'AMT', 'AMP', 'ABC', 'AMGN', 'APH', 'ADI', 'AON',
           'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'AIZ', 'T', 'ADSK', 'ADP', 'AN',
           'AZO', 'AVB', 'AVY', 'BLL', 'BAC', 'BK', 'BAX', 'BDX', 'BBBY', 'BIG',
           'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'CHRW', 'COG',
           'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CNP', 'CERN', 'CF', 'SCHW',
           'CVX', 'CMG', 'CB', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CTXS', 'CLF',
           'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',
           'CNX', 'ED', 'STZ', 'GLW', 'COST', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI',
           'DHR', 'DRI', 'DVA', 'DE', 'XRAY', 'DVN', 'DFS', 'DISCA',
           'DLTR', 'D', 'RRD', 'DOV', 'DTE', 'DD', 'DUK', 'ETFC', 'EMN',
           'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EOG', 'EQT',
           'EFX', 'EQR', 'EL', 'EXC', 'EXPE', 'EXPD', 'XOM', 'FFIV', 'FAST', 'FDX',
           'FIS', 'FITB', 'FHN', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC',
           'FTI', 'F', 'FOSL', 'BEN', 'FCX', 'GME', 'GPS', 'GD', 'GE', 'GIS',
           'GPC', 'GNW', 'GILD', 'GS', 'GT', 'GOOG', 'GWW', 'HAL', 'HOG', 'HIG',
           'HAS', 'HP', 'HES', 'HPQ', 'HD', 'HON', 'HRL', 'HST', 'HUM', 'HBAN',
           'ITW']

# Loading data
data =  yf.download(tickers, start="2018-01-03", end="2019-12-31")

# Taking close prices for chosen instruments
data = data["Adj Close"]

# Looking at the downloaded data
data.head()


# As our method takes in returns, we'll calculate them from our dataframe of prices
returns = data.pct_change()[1:]

# Looking at the obtrined returns series
returns.head()


# We will go now through the steps of calculating S-scores for a window of observations to explain how the PCA approach works and will then generate treading signals.


# Starting with setting a PCAStrategy class with 15 principal components
pca_strategy = al.other_approaches.PCAStrategy(n_components=15)


# To get the factor weights from PCA we'll be using a window with 252 observations as in the original paper
data_252days = returns[:252]

# We can standardize our data
standardized_252days = pca_strategy.standardize_data(data_252days)

# But the function for calculation of the factor weights using PCA will do it by itself
factorweights = pca_strategy.get_factorweights(data_252days)

# Looking at the factor weights
factorweights


# Our factor weights are 15 first components from the PCA divided by the standard deviations of returns of our assets.
#
# Now we can get a 60-day window of observations to calculate residuals and coefficients. The calculation is done by fitting a linear regression on the returns from this window and factor returns from this window.


# Getting a 60-day window of observations to calculate factor returns
data_60days = returns[(252-60):252]

# Last day in our window
data_60days.index[-1]


# Calculating factor returns from our returns - multiplying them by factor weights
factorret = pd.DataFrame(np.dot(data_60days, factorweights.transpose()), index=data_60days.index)

# Looking at the obtained factor returns
factorret.head()


# So for each component from PCA and for each observation we get a return value. 


# Now fitting the linear regression to get residuals and coefficients of the regression
residual, coefficient = pca_strategy.get_residuals(data_60days, factorret)

# Residuals dataframe
residual.head()


# Coefficients dataframe
coefficient


# Using each column in the residuals dataframe we'll decide whether to trade an eigenportfolio related to that asset.
# The eigenportfolio will be used if:
#
#     a) the mean reversion speed of the OU process composed from the residuals is high enough,
#
#     b) the OU process deviates enough from its mean value.
#
# So, in our example, we can have up to 176 eigenportfolios. After the checks for the mean reversion speed and the S-scores are
# made we might end up with about 10 eigenportfolios that are suitable to be traded. 
#
# Using the betas - values from the coefficients dataframe the eigenportfolio will be constructed. 
#
# For example, if we have a signal to go long on the **AA** asset eigenportfolio and we use a scaling parameter equal to one, we
# will:
# - go long one dollar of the **AA** asset;
# - go short the **A** stock with the sum of betas from the **A** column (*coefficient* dataframe);
# - go short the **AA** stock with the sum of betas from the **AA** column (*coefficient* dataframe);
# - go short the **AAPL** stock with the sum of betas from the **AAPL** column (*coefficient* dataframe);
# - and so on..
#
# for each stock in a portfolio.


# Now, calculating the S-scores
s_scores = pca_strategy.get_sscores(residual, k=8.4)

# Picking parameters to trade based on the S-score
sbo = 1.25
sso = 1.25
ssc = 0.5
sbc = 0.75

# Stock eigenportfolios that we should long
s_scores[s_scores < -sbo]


# Stock eigenportfolios that we should short
s_scores[s_scores > sso]


# So these are the S-scores for each eigenportfolio. We've printed those of them that are either above or below critical values
# to enter a trade.
#
# We should go long on 19 eigneportfolios and short on 15 eigenportfolios for this observation.
#
# To calculate these S-scores we estimated the PCA components on the 252 trading days of the year 2018 and
# calculated the residuals on the last 60 days of the year 2018. The last observation used for the estimation was
# 2019.01.02. So these trading signals can be used on 2019.01.03.
#
# By moving the 60-day window one observation ahead and recalculating the residuals and the regression coefficients
# we would get new S-scores and based on them we would have a trading signal for 2019.01.04.
#
# As the estimated correlation matrix used to get the PCA factors doesn't change much, it's calculated again only once
# we generated the whole residual window of signals. So in our example, it would be recalculated every 60 days using the
# last 252 observations.


# Now we can simply use the PCA Strategy with a single function and given input parameters to generate trading signals
#
# **Note:** This function can be used on the raw returns dataframe. The previous steps are presented in the notebook
# to explain the idea behind the PCA Strategy.
#
# This function might take a long time to generate output, especially if given a dataframe with a big number of observations, as
# to generate trading signals for a single day we have to go through the all steps mentioned above.


# Simply applying the PCAStrategy with standard parameters
target_weights = pca_strategy.get_signals(returns, k=8.4, corr_window=252,
                                          residual_window=60, sbo=1.25,
                                          sso=1.25, ssc=0.5, sbc=0.75,
                                          size=1)



# Looking at generated trading signals
target_weights.head()


# Now, let's normalize these weights - so that on each day we'd be long or short a sum of 1 asset units. 


# Normalizing weights
norm_weights = target_weights.divide(abs(target_weights).sum(axis=1), axis=0)

# Checking if the sum of absolute weights for each date is 1
abs(norm_weights).head().sum(axis=1)


# Returns dataframe
returns_test = returns[(252 - 1):-1]

# Checking that our returns dataframe have the same index as the trading signals dataframe
returns_test.head()


# Shifting the trading signal dataframe one observation further,
# as we would be able to use those signals on the following day
investment_portfolio_returns = (returns_test * norm_weights.shift(1)).sum(axis=1)

# Calculating the portfolio price of our investment portfolio
investment_portfolio_price = (investment_portfolio_returns + 1).cumprod()

# And calculating the equity curve of our investment portfolio
equity_curve = investment_portfolio_price  - 1


# Plotting the equity curve
equity_curve.plot(figsize = (20,7),
                  title='PCA Strategy investment portfolio equity curve');


# These results look good, over the year 2019 equity curve shows an increase in the investment portfolio value from 1 to around 1,1. 
#
# We can further test this strategy by choosing different critical values for the S-score, increasing the mean reversion speed threshold, or adding transaction costs to see if the strategy is robust.


# ## Conclusion


# This notebook describes the PCA Strategy class and its functionality. Also, it shows how the tools can be used on real data.
#
# The algorithms used in this notebook were described by _Marco Avellaneda_ and _Jeong-Hyun Lee_ in the paper __Statistical Arbitrage in the U.S. Equities Market__  [available here](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf).
#
# Key takeaways from the notebook:
# - Principal Component Analysis can be used to create mean-reverting portfolios and generate trading signals.
# - First, a window of 252 observations is used to get an empirical correlation matrix and use the PCA to get N top components.
# - It’s important to standardize data before inputting it to PCA, as the PCA seeks to maximize the variance of each component.
# - A separate market-neutral eigenportfolio can be calculated for each stock in our portfolio.
# - Next, we pick a window to calculate residuals (60 days in the example).
# - By using linear regression on the second window (60-days) and factor returns for this window we get residuals and the coefficients of regression.
# - These residuals are used to construct an OU process for each eigenportfolio.
# - If the OU process shows a high (above the $\kappa$ threshold) speed of mean reversion, we calculate the s-score for it.
# - S-score is measuring how far away a given asset eigenportfolio is from the theoretical equilibrium value associated with the model.
# - If the S-score is too high or too low we generate a signal to sell or buy this eigenportfolio.
# - Resulting trading signal is the sum of all eigenportfolio weights that satisfy the requirements to be traded.
# - Parameters to optimize in this strategy are the mean reversion speed threshold, the windows for PCA and residual calculation, and the S-score thresholds to enter or exit positions. 



// ---------------------------------------------------

// xou_model.py
// arbitrage_research_basic_PCA/Optimal Mean Reversion/xou_model.py
# Generated from: xou_model.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * References:  **Tim Leung and Xin Li** - *Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications (2015)*


# ## Abstract
#
# An Exponential Ornstein-Uhlenbeck process is a modification of the geometric Ornstein–Uhlenbeck process
# developed previously by Dixit and Pyndick in a paper titled:
# _"The stochastic behavior of commodity prices: Implications for valuation and hedging."_
#
# Tim Leung, Xin Li in _"Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications" (2015)_ present the solution to the optimal timing problems for one-time entering and liquidating the position and also provide the 
# ability to find optimal levels for infinite amount of trades based on Exponential Ornstein-Uhlenbeck process.


# The following implementations and descriptions closely follow the work of Tim Leung: [Tim Leung and Xin Li Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919). Data used to showcase the module was chosen based on the example in the publication.


# ## Exponential Ornstein-Uhlenbeck process
# The Exponential Ornstein-Uhlenbeck (XOU) process is defined the following way:
#
#
#    $$ \xi_t = e^{X_t}, t \geq 0$$
#
# where $X$ is the Ornstein-Uhlenbeck process.


# >The definition of the OU process and the fitting procedure details are presented in **OU model notebook** (ou_model.ipynb)
#
# In other words, $X$ is a *log-price* of a positive XOU
# process $\xi$.


from IPython.display import Image
Image(filename='XOU_model/xou_vs_ou.png')


# _Simulated OU ($\theta$ = 1, $\mu$ = 0.6, $\sigma$ = 0.2) and XOU ($\theta$ = 0, $\mu$ = 0.6, $\sigma$ = 0.2) processes paths._


#
#
# The main parameters of the XOU model coincide with the parameters of the OU model:
#
# * $\theta$ − long term mean level, all future trajectories of 𝑋 will evolve around a mean level 𝜃 in the long run.
#
# * $\mu$ - the speed of reversion, characterizes the velocity at which such trajectories will regroup around $\theta$ in time.
#
# * $\sigma$ - instantaneous volatility, measures the amplitude of randomness entering the system. Higher values imply more randomness.
#
#
#
# To fit the XOU process to our data and find the optimal ratio between the two assets we
# are using the same approach as we utilized for the OU process:
# firstly, we are maximizing the average log-likelihood function with respect to model parameters, and secondly
# choosing the $\beta^*$ that provides the maximum value of the said max log-likelihood function.


# ## Optimal stopping problem
#
# > This approach presumes that the investor wants to commit only two trades: entering the position, and
#     liquidating it.
#
# First of all, let's assume that the investor already has a position the value of which follows the XOU process. When
# the investor closes his position at the time $\tau$ he receives the value $\xi_{\tau}=e^{X_{\tau}}$ and pays a
# constant transaction cost $c_s > 0$. To maximize the expected discounted value we need to solve
# the optimal stopping problem:
#


# $$V^{\xi}(x) = \underset{\tau \in T}{\sup} \mathbb{E}({e^{-r \tau} (e^{X_{\tau}} - c_s)| X_0 = x}),$$


# where $T$ denotes the set of all possible stopping times and $r > 0$ is our subjective constant
# discount rate. $V^{\xi}(x)$ represents the expected liquidation value accounted with $\xi$.
#
# Current price plus transaction cost constitute the cost of entering the trade. After subtracting the found cost from
# the expected optimal value of liquidation - $V(x)$ we can formalize the optimal entry problem:


# $$ J^{\xi}(x) = \underset{\nu \in T}{\sup} \mathbb{E}({e^{-\hat{r} \tau} (V^{\xi}(X_{\nu}) - e^{X_{\nu}} - c_b)| X_0 = x})$$


# $$\hat{r}>0, c_b > 0$$


# To sum up this problem, we, as an investor, want to maximize the expected discounted difference between the current price
# of the position - $e^{x_{\nu}}$ and its expected liquidation value $V^{\xi}(X_{\nu})$ minus transaction cost
# $c_b$.


# ### The solutions:


# Theorem 3.2 (p.54):
#
# **The optimal liquidation problem admits the solution:**
#
# $$V^{\xi}(x) = \begin{cases} (e^{b^{\xi*}} - c_s) \frac{F(x)}{F(b^{\xi*})} , & \mbox{if } x \in (-\infty,b^{\xi*})\\
#   \\ e^x - c_s, &  \mbox{ otherwise}  \end{cases}$$
#
# The optimal liquidation level $b^*$ is found from the equation:
#
# $$e^b F(b) - (e^b - c_s)F'(b) = 0$$


# Corresponding optimal liquidation time is given by
#
# $$\tau^{\xi*} = \inf [t\geq0:X_t \geq b^{\xi*}] = \inf [t\geq0:\xi \geq e^{b^{\xi*}}]$$
#
#
# Theorem 3.4 (p.54):
#
# **The optimal entry timing problem admits the solution:**
#
#
# $$J^{\xi}(x) = \begin{cases} P^{\xi}{F}(x),  & \mbox{if } x \in (-\infty,a^{\xi*})\\
#     \\ V^{\xi}(x) - e^x - c_b, & \mbox{if } x \in [a^{\xi*}, d^{\xi*}]\\
#     \\ Q^{\xi}{G}(x), & \mbox{if } x \in (d^{\xi*}, \infty)\end{cases}$$
#
# The optimal entry interval $(a^{\xi*},d^{\xi*})$ is found using the respective equations:
#
# $$G(d)(V^{\xi'}(d) - e^d) - G'(d)(V^{\xi}(d) - e^d - c_b) = 0$$
# $$F(a)(V^{\xi'}(a) - e^a) - F'(a)(V^{\xi}(a) - e^a - c_b) = 0$$
#
#
# Corresponding optimal entry time is given by
#
#
# $$\nu_{a^{\xi*}, d^{\xi*}} = \inf [t\geq0:X_t \in [a^{\xi*}, d^{\xi*}]]$$
#
#
# To summarize: the investor should enter the market when the price enters the interval
# $[e^{a^{\xi*}}, e^{d^{\xi*}}]$ for the first time, and exit as soon as it reaches the price level
# $e^{b{\xi*}}$.


# ## Optimal switching problem
#
# > This approach presumes that the investor can commit an infinite number of trades.
#
#
# If there is no limit on the number of times the investor will open or close the position, the sequential trading times
# are modelled by the stopping times $\nu_1,\tau_1,\nu_2,\tau_2,... \in T$ such that
#
# $$0\leq\nu_1\leq\tau_1\leq\nu_2\leq\tau_2\leq...$$
#
# Where $\nu_i$ are times when the share of a risky asset was bought and $\tau_i$ - when it was sold. In the case
# of pairs trading, we consider our spread as such an asset.


# ### The solutions:
#
# In case of optimal switching the formulation of the problem will depend on the position we start with, as an investor, and subsequently - our first trading decision. 
#
# **Zero position - Buy**
#
# If we start from the zero position, our first action would be to decide when to buy the share of our asset. In our calculation we would
# like to account for all our possible market re-entries and exits , so the problem is formulated in the following way:


# $$\tilde{J}^{\xi}(x) = \underset{\Lambda_0}{\sup} \{ \mathbb{E}_x \sum^{\infty}_{n=1}[e^{-r\tau_n}h^{\xi}_s(X_{\tau_n}) - e^{-r\nu_n}h^{\xi}_b(X_{\nu_n})]\}$$


# Where $\Lambda_0$ is the set of admissible times, and helper functions denoted as such:
#
# $$h^{\xi}_s=e^x-c_s$$


# $$and$$


# $$h^{\xi}_s=e^x+c_b$$


# **Existing position - Sell**
#
# Vice-versa, if we start with pre-existing position in said risky asset our firt decision would be to decide on liquidation timing, therefore we have to account our first optimal "sell" value and then all of the possible market re-entries and exits:


# $$\tilde{V}^{\xi}(x) = \underset{\Lambda_1}{\sup} \{ \mathbb{E}_x{e^{-r\tau_1}h^{\xi}_s(X_{\tau_1})+\sum^{\infty}_{n=2}[e^{-r\tau_n}h^{\xi}_s(X_{\tau_n}) - e^{-r\nu_n}h^{\xi}_b(X_{\nu_n})]} \}$$


# With $\Lambda_1$ as the set of admissible times.


# ### The solution:
#
# To find both optimal entry and liquidation switching levels we need to ensure that re-entering the market(or entering at all) is optimal. 
#
# For that we need to establish the helper fumctions correlated with the entry and liquidation processes:
#
# $$f_s(x):=(\mu\theta+\frac{1}{2}\sigma^2-r) - \mu x + r c_s e^{-x}$$
# $$f_b(x):=(\mu\theta+\frac{1}{2}\sigma^2-r) - \mu x - r c_b e^{-x}$$


# Theorem 3.7 (p.56):
#
# **Under optimal switching approach it is optimal to re-enter the market if and only if all of the following conditions hold true:**
#
# a) There are two distinct roots to $f_b:\ x_{b1},x_{b2}$
#
# b) $\exists \tilde{a}^* \in (x_{b1},x_{b2})$ satisfying $F(\tilde{a}^*)e^{\tilde{a}^*}=F'(\tilde{a}^*)(e^{\tilde{a}^*}+c_b)$
#
# c) The following inequality must hold true:
#
# $$\frac{e^{\tilde{a}^*}+c_b}{F(\tilde{a}^*)}\geq\frac{b^{\xi*}-c_s}{F(b^{\xi*})}$$
#
# In case any of the conditions are not met - re-entering the market is deemed not optimal it would be advised to exit
# at the optimal liquidation price without re-entering in the case when the investor had already entered the market beforehand,
# or don't enter the market at all in the case when he or she starts with a zero position.


# ## How to use the XOU module
#
# For this module the most suitable input would be a logarithmized pre-built mean-reverting portfolio prices.
#
# Both optimal stopping and optimal switching levels are used alike in determining the rules of our trading strategy:
#
#
# * If not already entered, enter when the price reaches optimal entry level.
#
# * If not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# However, the differenece lies in the applications of the two approaches. The optimal stopping usually 
# has a much longer delay between the buy and sell levels are reached and offers a bigger gain than a one-time entry and liquidation using the optimal switching levels. The optimal switching levels, on the other hand, may provide a bigger cumulative gain by performing the trades multiple times during the same time period.


import arbitragelab.optimal_mean_reversion as omr
import numpy as np
import matplotlib.pyplot as plt


# Creating a class instance
example = omr.ExponentialOrnsteinUhlenbeck()


# We are able to create the training data sample using the module itself:


# We establish our training sample
delta_t = 1/252
np.random.seed(31)
xou_example =  example.ou_model_simulation(n=1000, theta_given=1, mu_given=0.6,
                                           sigma_given=0.2, delta_t_given=delta_t)


# Model fitting uses the same function structure as the OU module:


# Model fitting
example.fit(xou_example, data_frequency="D", discount_rate=0.05,
            transaction_cost=[0.02, 0.02])


# Optimal stopping levels can be found separately via respective functions.


# Solving the optimal stopping problem
b = example.xou_optimal_liquidation_level()

a,d = example.xou_optimal_entry_interval()


print("Optimal liquidation level:", round(b,5),
      "\nOptimal entry interval:[",round(a, 5),",",round(d, 5),"]")



# Both optimal levels can be found via the `optimal_switching_levels` function.


# Solving the optimal switching problem
d_switch, b_switch = example.optimal_switching_levels()


print ("Optimal switching liquidation level:", round(b_switch,5),
       "\nOptimal switching entry interval:[", round(np.exp(example.a_tilde), 5),",",round(d_switch, 5),"]")



# To test the obtained results let's simulate the XOU process using the respective function.


np.random.seed(31)
xou_plot_data =  example.xou_model_simulation(n=1000, theta_given=1, mu_given=0.6,
                                              sigma_given=0.2, delta_t_given=delta_t)


# To visualize the results we use the `xou_plot_levels` function. 


# Showcasing the results on the training data (pd.DataFrame)
fig = example.xou_plot_levels(xou_plot_data, switching=True)

# Adjusting the size of the plot
fig.set_figheight(7)
fig.set_figwidth(12)


# Or you can view the model statistics
example.xou_description(switching=True)


# ## Conclusion


# This notebook describes the Exponential Ornstein-Uhlenbeck (XOU) model and how it is applied to mean reverting portfolios. The main goal of the notebook is to show the usage of the optimal stopping and optimal switching problems.
#
# Both techniques were introduced by *Tim Leung and Xin Li* in [**Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications**](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919)
#
# Key takeaways from the notebook:
#
# * Exponential Ornstein-Uhlenbeck model is a great tool used to model the behavior of mean-reverting assets.
#
# * Main idea behind the use of the  optimal levels is:
#
#     * If position is not already entered, enter when the price reaches optimal entry level.
#
#     * If position is not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# * An optimal stopping problem formulated for the XOU process allows us to maximize the expected discounted value of one-time entering or liquidating the position by finding the optimal price levels at which trades should be committed.
#
# * An optimal switching problem, on the other hand, allows us to maximize the expected discounted value of infinite amount of trades(entering or liquidating the position). By finding the optimal price levels at which the repeated trades should be committed for the maximum overall gain. 
#
# * The model should be considered as a building block for the problem with any finite number of sequential trades.
#


# ## Reference


# 1. Leung, Tim, and Xin Li. Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications. World Scientific Publishing Company, 2451. 



// ---------------------------------------------------

// heat_potentials.py
// arbitrage_research_basic_PCA/Optimal Mean Reversion/heat_potentials.py
# Generated from: heat_potentials.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# References:
#
# **Alexandr Lipton and Marcos Lopez de Prado:** [ _"A closed-form solution for optimal mean-reverting trading strategies"_](https://ssrn.com/abstract=3534445)
#
# **Marcos Lopez de Prado:**  [_"Advances in Financial Machine Learning"_](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089)


# ## Abstract


# An Ornstein-Uhlenbeck process is a great tool for modeling the behavior of mean-reverting portfolio prices. Alexandr Lipton and Marcos Lopez de Prado present a methodology that allows to obtain a closed-form solution for the optimal mean-reverting trading strategies based on the OU model characteristics and the heat potential approach. The optimal trading rule, namely, optimal stop-loss and optimal profit-taking level are found by maximizing the approximated value of the Sharpe ratio.


# ## Data scaling


# **NOTE:**
# >In this approach we use the volume clock metric instead of the time-based metric. More on that in the paper
# [**"The Volume Clock: Insights into the High Frequency Paradigm"**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2034858) by David Easley, Marcos Lopez de Prado, and Maureen O'Hara


# Let's presume an investment strategy S is a long investment strategy with p/l driven by the OU process:
#
# $$dx' = \mu'(\theta'-x')dt'+\sigma'dW_{t'}, x'(0) = 0$$


# and a trading rule $R = \{ \bar{\pi}',\underline{\pi}',T' \}$. To make the application of the method of heat potentials possible we transform it to use its steady-state by performing scaling to remove superfluous parameters.
#
# $$t = \mu't',\ T = \mu'T',\ x = \frac{\sqrt{\mu'}}{\sigma'} x',\ \theta = \frac{\sqrt{\mu'}}{\sigma'} \theta',\ \bar{\pi} = \frac{\sqrt{\mu'}}{\sigma'} \bar{\pi}',
# \ \underline{\pi} = \frac{\sqrt{\mu'}}{\sigma'} \underline{\pi}'$$


# And get:
# $$dx = (\theta-x)dt + dW_t, \ \bar{\pi}' \leq x \leq \underline{\pi},\ 0 \leq t \leq T$$


# **NOTE:**
#
# >Short strategy reverses the roles of ${\bar{\pi}',\underline{\pi}'}$:
# >
# >$-\underline{\pi}$ equals the profit taken when the price hits $\underline{\pi}$ and
# >
# >$-\bar{\pi}$ losses are incurred while price hits $-\bar{\pi}$
#
#


# Hence, we can restrict ourself to case with $\theta \geq 0$.


# ## Sharpe ratio calculation
#
# The calculation of the Sharpe ratio follows the four-step algorithm:
# ***
# **Step 1: Define a calculation grid**
#
# First of all we define the grid $\upsilon$ based on which we will perform our numerical calculation:
#
# $$ 0=\upsilon_0<\upsilon_1<...<\upsilon_n=\Upsilon,\  \upsilon(t) = \frac{1 - e^{-2(T-t)}}{2}$$
#
# **Step 2: Numerically calculate helper functions** $\bar{\epsilon}, \underline{\epsilon}, \bar{\phi}, \underline{\phi}$
#
# We are going to use the classical method of heat potentials to calculate the SR.
# As a preparation, in this step, we solve the two sets of Volterra equations by using the trapezoidal rule of integral calculation.
#
# **Step 3: Calculate the values of** $\hat{E}(\Upsilon,\bar{\omega})$ **and** $\hat{F}(\Upsilon,\bar{\omega})$
#
# We need to compute these functions at one point, which can be done by approximation of the integrals using the
# trapezoidal rule:
#
# $$\hat{E}(\Upsilon,\bar{\omega}) = \frac{1}{2} \sum_{i=1}^k(\underline{w}_{n,i}\underline{\epsilon}_i + \underline{w}_{n,i-1}\underline{\epsilon}_{i-1} + \bar{w}_{n,i}\bar{\epsilon}_i + \bar{w}_{n,i-1}\bar{\epsilon}_{i-1})(\upsilon_i - \upsilon_{i-1})$$
#
# $$ \hat{F}(\Upsilon,\bar{\omega}) = \frac{1}{2} \sum_{i=1}^k(\underline{w}_{n,i}\underline{\phi}_i + \underline{w}_{n,i-1}\underline{\phi}_{i-1} + \bar{w}_{n,i}\bar{\phi}_i + \bar{w}_{n,i-1}\bar{\phi}_{i-1})(\upsilon_i - \upsilon_{i-1})$$
#
# Where *w* are the weights.
#
# **Step 4: calculate the SR using the obtained values**
#
# The previously computed functions $\hat{E}(\Upsilon,\bar{\omega})$ and $\hat{F}(\Upsilon,\bar{\omega})$
# are substituted into the following formula to calculate the Sharpe ratio.
#
# $$SR = \frac{\hat{E}(\Upsilon,\bar{\omega}) - \frac{2(\bar{\omega}-\theta)}{ln(1-2\Upsilon)}}{\sqrt{\hat{F}(\Upsilon,\bar{\omega}) - (\hat{E}(\Upsilon,\bar{\omega}))^2 + \frac{4(\Upsilon + ln(1-2\Upsilon)(\bar{\omega}-\theta)\hat{E}(\Upsilon,\bar{\omega})}{(ln(1-2\Upsilon))^2}}}$$
#
#
# ***


# To find the optimal thresholds for the data provided by the user we maximize the calculated SR with respect to
# $\bar{\pi}\geq0,\underline{\pi}\leq0$**


# ## How to use the heat potentials module
#
# This module gives the ability to calculate optimal values for stop-loss and profit-taking level to construct the trading rule.


# Imports


from arbitragelab.optimal_mean_reversion import OrnsteinUhlenbeck
from arbitragelab.optimal_mean_reversion import HeatPotentials
import matplotlib.pyplot as plt
import numpy as np


# Data preparation:
#
# Let's generate the OU data sample to example the whole model usage cycle.


# Generating the sample OU data
ou_data = OrnsteinUhlenbeck()

data = ou_data.ou_model_simulation(n=1000, theta_given=0.03711, mu_given=65.3333,
                            sigma_given=0.3, delta_t_given=1/255)


# Let's plot our data example
plt.figure(figsize=(12, 7))
plt.plot(data);


# The next step would be to fit the OU model and obtain its parameters:


# To get the model parameters we need to fit the OU model to the data

# Assign the delta value
ou_data.delta_t = 1/252

# Model fitting
ou_data.fit_to_portfolio(data)

# Now we obtained the parameters to use for our optimization procedure
theta,mu,sigma = ou_data.theta,ou_data.mu,np.sqrt(ou_data.sigma_square)


# Printing out fitted parameters
print('theta:', round(theta,5), '\nmu:', round(mu,5), '\nsigma:', round(sigma,5))


# Now we can perform the optimization process:


# To fit the model and calculate optimal thresholds we need to provide:
#
# * OU-model parameters that represent the data
# * The grid density
# * Maximum duration of the trade


# Establish the instance of the class
example = HeatPotentials()

# Fit the model and establish the maximum duration of the trade
example.fit(ou_params=(theta, mu, sigma), delta_grid=0.1, max_trade_duration=0.03)


# Let's calculate the optimal levels and SR separately. 
#
# To do so we can use the respective functions: 


# Calculate the initial optimal levels
levels = example.optimal_levels()


print('profit-taking threshold:', levels[0],
      '\nstop-loss threshold:', levels[1],
      '\nSharpe ratio:', levels[2])


# While calculating the Sharpe ratio separately we will use the rounded values of the optimal threshold to showcase the sensitivity of the approach to small differences in values.


# We can also calculate the Sharpe ratio for given scaled parameters
sr = example.sharpe_calculation(max_trade_duration=1.9599, optimal_profit=5.07525, optimal_stop_loss=-3.41002)

print(sr,'\ndelta:', levels[2]-sr)


# To get scaled back results that correspond to our model parameters we use the description function.


# To get the results scaled back to our initial model we call the description function
example.description()


# ## Conclusion
#
# This notebook describes the heat-potential approach to finding optimal trading rules for mean-reverting strategies.
#
# Key takeaways:
# * Ornstein-Uhlenbeck model is used to model the behavior of mean-reverting assets.
#
# * It is possible to use the heat potential approach on the financial data represented by an OU model.
#
# * We formulate the problem for the long strategy, short strategy results can be obtained by reflection.
#
# * The closed-form solution allows to formulate the optimization problem for calculating the optimal stop-loss and profit-taking thresholds.
#
# * The model is sensitive to small changes in the given data.


# ## Reference
# 1.  Lipton, Alex and López de Prado, Marcos, A Closed-Form Solution for Optimal Mean-Reverting Trading Strategies (February 8, 2020). Available at SSRN: https://ssrn.com/abstract=3534445 or http://dx.doi.org/10.2139/ssrn.3534445 
# 2. Prado, Marcos Lopez de. Advances in Financial Machine Learning. Wiley, 2018.



// ---------------------------------------------------

// ou_model.py
// arbitrage_research_basic_PCA/Optimal Mean Reversion/ou_model.py
# Generated from: ou_model.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * References:  **Tim Leung and Xin Li** - *Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications (2015)*


# ## Abstract
#
# An Ornstein-Uhlenbeck process is a great tool for modeling the behavior of mean-reverting portfolio prices. Tim Leung, Xin Li in "Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications" (2015) present the solution to the optimal timing problems for entering and liquidating the position and the method of creating an optimal mean-reverting portfolio of two assets based on the Ornstein-Uhlenbeck model. Their findings also provide optimal solutions with respect to the stop-loss level if they are provided as an extension of a base problem.


# The following implementations and descriptions closely follow the work of Tim Leung: [Tim Leung and Xin Li Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919). Assets used to showcase the module were chosen based on the example in the publication.


# ## Mean-reverting portfolio
#
# To create a mean-reverting portfolio we *hold* $\alpha = \frac{A}{S_0^{(1)}}$ of a risky asset $S^{(1)}$ and *short* $\beta = \frac{B}{S_0^{(2)}}$, yielding a portfolio value:
# $$X_t^{\alpha,\beta} = \alpha S^{(1)} - \beta S^{(2)}, t \geq 0$$
# Both assets chosen should be correlated or co-moving. Since in terms of mean-reversion we care only about the ratio between $\alpha$ and $\beta$, without the loss of generality we can set $\alpha=const$ and A =  1 (that represents the amount of investment), while varying $\beta$ to find the optimal strategy $(\alpha,\beta^*)$
#


from IPython.display import Image


Image(filename='OU_model/GLD_GDX.png')


# _Historical price paths of gold (GLD) and VanEck Vectors Gold Miners ETF (GDX). An example of assets that can be used for mean-reverting portfolio creation._


# ## Ornstein-Uhlenbeck process
#
# We establish Ornstein-Uhlenbeck process driven by the SDE:
# $$dX_t = \mu(\theta - X_t)dt + \sigma dB_t,$$
# $$\mu, \sigma > 0,$$
# $$\theta \in \mathbb{R},$$
# $$B\ -\text{a standard Brownian motion}$$
#


# Where:
# * $\theta$ − long term mean level, all future trajectories of 𝑋 will evolve around a mean level 𝜃 in the long run.
#
# * $\mu$ - speed of reversion, characterizes the velocity at which such trajectories will regroup around $\theta$ in time.
#
# * $\sigma$ - instantaneous volatility, measures instant by instant the amplitude of randomness entering the system. Higher values imply more randomness.


# ## Model fitting
#
# To fit the OU model to the portfolio and also find the optimal $\beta^*$ we also have to use the probability density function of $X_t$ with increment  $\delta t = t_i - t_{i-1}$ :
#
#
# $$f^{OU} (x_i|x_{i-1};\theta,\mu,\sigma) = \frac{1}{\sqrt{2\pi\tilde{\sigma}^2}}exp(-\frac{(x_i - x_{i-1} e^{-\mu\Delta t} - \theta (1 - e^{-\mu \Delta t})^2)} {2 \tilde{\sigma}^2})$$


#
# $$\text{with the constant } \tilde{\sigma}^2 = \sigma^2 \frac{1 - e^{-2\mu\Delta t}}{2\mu}$$
#
# We observe the resulting portfolio values $(x_i^\beta)_{i = 0,1,\cdots,n}$ for every strategy $\beta$
# realized over an *n*-day period. To fit the model to our data and find optimal parameters we define the average log-likelihood function:
#
# $$\ell (\theta,\mu,\sigma|x_0^{\alpha\beta},x_1^{\alpha\beta},\cdots,x_n^{\alpha\beta}) := \frac{1}{n}\sum_{i=1}^{n} ln f^{OU}(x_i|x_{i-1};\theta,\mu,\sigma)$$


# $$= -\frac{1}{2} ln(2 \pi) - ln(\tilde{\sigma}) - \frac{1}{2\pi\tilde{\sigma}^2}\sum_{i=1}^{n} [x_i^{\alpha\beta} - x_{i-1}^{\alpha\beta} e^{-\mu \Delta t} - \theta (1 - e^{-\mu \Delta t})]^2$$
#
# Maximizing the log-likelihood function by applying maximum likelihood estimation(MLE) we are able to determine
# the parameters of the model and fit the observed portfolio prices to an OU process. Let's denote the maximized average
# log-likelihood by $\hat{\ell}(\theta^*,\mu^*,\sigma^*)$. Then for every $\alpha$ we choose
# $\beta^*$, where:
#
# $$\beta^* = \underset{\beta}{\arg\max}\ \hat{\ell}(\theta^*,\mu^*,\sigma^*|x_0^{\alpha\beta},x_1^{\alpha\beta},\cdots,x_n^{\alpha\beta})$$


Image(filename='OU_model/Optimal_mean_reverting_portfolio.png')


# _An optimal mean-reverting portfolio constructed with GLD and GDX assets using the average log-likelihood method._


# ## Optimal stopping problems
#
# Let's establish an optimal stopping problem. Suppose the investor already has a position with a **value process** $(X_t)_{t>0}$ that follows the OU process. When the investor closes his position at the time $\tau$ he receives the value $(X_{\tau})$ and pays a **constant transaction cost** $c_s \in \mathbb{R}$ Our goal is to maximize the expected discounted value, where $r > 0$ is the **subjective discount rate of liquidation**. To achieve that we need to solve the optimal stopping problem:
#
# $$V(x) = \underset{\tau \in T}{\sup} \mathbb{E}_x{e^{-r \tau} (X_{\tau} - c_s)| X_0 = x}$$
#
# $V(x)$ represents the expected liquidation value accounted with X.
#
# Current price plus transaction cost constitute the cost of entering the trade and in combination with $V(x)$ we can formalize the optimal entry problem:
#
# $$J(x) = \underset{\nu \in T}{\sup} \mathbb{E}_x{e^{-\hat{r} \tau} (V(X_{\nu}) - X_{\nu} - c_b)| X_0 = x}$$


# $$r,\hat{r}>0 \text{ - discount rates}$$


# $$c_s,c_b \in \mathbb{R} \text{ - transaction costs}$$


#
# ### The analytical solution to the optimal stopping problems:
#
# We denote the OU process infinitesimal generator:
#
# $$L = \frac{\sigma^2}{2} \frac{d^2}{dx^2} + \mu(\theta - x) \frac{d}{dx}$$
#
# and recall the classical solution of the differential equation
#
# $$L u(x) = ru(x)$$


# $$F(x) = \int_{0}^{\infty} u^{ \frac{r}{\mu} - 1} e^{\sqrt{\frac{2\mu}{\sigma^2}}(x - \theta)u - \frac{u^2}{2}}du$$


# $$G(x) = \int_{0}^{\infty} u^{\frac{r}{\mu} - 1} e^{\sqrt{\frac{2\mu}{\sigma^2}} (\theta - x)u - \frac{u^2}{2}}du$$


# #### The analytical solutions can be divided into two parts:
#
# * The default solutions
#
#     **Theorem 2.6 (p.23).**
#     The *optimal liquidation problem* admits the solution:
#
#     $$ V(x) = \begin{cases} (b^* - c_s) \frac{F(x)}{F(b^*)} , & \mbox{if } x \in (-\infty,b^*) \\ x - c_s, &  \mbox{ otherwise}  \end{cases}$$
#
#     The optimal liquidation level $b^*$ is found from the equation:
#
#     $$F(b) - (b - c_s)F'(b) = 0$$
#
#     **Theorem 2.10 (p.27).**
#     The *optimal entry timing problem* admits the solution:
#
#     $$ J(x) = \begin{cases} V(x) - x - c_b, & \mbox{if } x \in (-\infty,d^*)\\ \frac{V(d^*) - d^* - c_b}{\hat{G}(d^*)}, & \mbox{if } x \in (d^*, \infty)  \end{cases}$$
#
#     The optimal entry level $d^*$ is found from the equation:
#
#     $$\hat{G}(d)(V'(d) - 1) - \hat{G}'(d)(V(d) - d - c_b) = 0$$
#
#


# * The solutions with the inclusion of stop-loss
#
#     **Theorem 2.13 (p.31):**
#     The *optimal liquidation problem with respect to stop-loss level* admits the solution:
#
#     $$V(x) = \begin{cases} C F(x)+D G(x) , & \mbox{if } x \in (-\infty,b^*)\\ x - c_s, & \mbox{ otherwise}  \end{cases}$$
#
#     The **optimal liquidation level** $b_L^*$ is found from the equation:
#
#     $$F'(b) [(L - c_s) G(b) - (b - c_s) G(L)] + G'(b) [(b - c_s) F(L) - (L - c_s) F(b)] - G(b) F(L) - G(L)F(b) = 0$$
#
#     Helper functions C and D defined as following:
#
#     $$C = \frac{(b_L^* - c_s) G(L) - ( L - c_s) G(b^*)}{F(b_L^*)G(L) - F(L)G(b_L^*)}$$
#
#     $$D = \frac{(L - c_s) F(L) - ( b_L^* - c_s) F(b^*)}{F(b_L^*)G(L) - F(L)G(b_L^*)}$$
#
#     **Theorem 2.42 (p.35):**
#     The *optimal entry timing problem with respect to stop-loss level* admits the solution:
#
#     $$J_L(x) = \begin{cases} P\hat{F}(x),  & \mbox{if } x \in (-\infty,a_L^*)\\ V_L(x) - x - c_b, & \mbox{if } x \in (a_L^*, d_L^*)\\  Q\hat{G}(x), & \mbox{if } x \in (d_L^*, \infty)\end{cases}$$
#
#     The **optimal entry interval** $(a_L^*,d_L^*)$ is found using the respective equations:
#
#     $$\hat{G}(d)(V_L'(d) - 1) - \hat{G}'(d)(V_L(d) - d - c_b) = 0$$
#
#     $$\hat{F}(a)(V_L'(a) - 1) - \hat{F}'(a)(V_L(a) - a - c_b) = 0$$
#


# ## How to use the OU module
#
# This module gives the ability to calculate optimal values for entering and liquidating the position for your portfolio. 
#
# **Note:** It is important that the model returns the *single* best pair of one-time entry/liquidation values. 


# Our main incentive in using this model is fairly simple:
#
# * If not already entered, enter when the price reaches optimal entry level.
#
# * If not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# In the case of the optimal timing solutions with the inclusion of the stop-loss level, the level of entry becomes the first bound of the entry interval to be reached by the portfolio price.
#
# This module also can be used on already constructed mean-reverting portfolios by providing the one-dimensional array or dataframe as input data.


import arbitragelab.optimal_mean_reversion as omr
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt


# ### Data preparation:


# To showcase the module functionality we will use GLD and GDX data from Yahoo Finance:


# Import data from Yahoo finance
data1 =  yf.download("GLD GDX", start="2012-03-25", end="2013-12-09")
data2 =  yf.download("GLD GDX", start="2015-12-10", end="2016-02-20")
data3 =  yf.download("GLD GDX", start="2016-02-21", end="2020-08-20")

data1.head()


# To build the optimal portfolio, first we create a *pd.DataFrame* or a *np.array* of two asset prices that we are using.  In the following cell we separate the data into 3 types of arrays for *initial training*, *testing and retraining the model* and *testing the retrained model*.


# You can use the pd.DataFrame of two asset prices
data_train_dataframe = data1["Adj Close"][["GLD", "GDX"]]

# And also we can create training dataset as an array of two asset prices
data_train = np.array(data1["Adj Close"][["GLD", "GDX"]])

# Create an out-of-sample dataset
data_test_and_retrain = data2["Adj Close"][["GLD", "GDX"]]

data_test_the_retrained = np.array(data3["Adj Close"][["GLD", "GDX"]])

data_train.shape


# **NOTE:** It is important which one of your assets you decide to long and which you decide to short since the outcome of the model and existence of the solution depend on that. Therefore, we have to be mindful of the order of asset prices in our array or dataframe used for training. The one that you intend to long has to be put first, and the one you are shorting - second.


# We are longing the GLD and shorting the GDX
data_train_dataframe.head()


# Logically we can divide the module usage proces into four steps: 
# * model fitting
# * optimal levels calculation
# * showcasing the result
# * model retraining


# ### Step 1: Data training


# First of all, we use a `fit` function to find the optimal ratio between the assets for our portfolio to achieve maximum mean reversion and then fit the OU model to our optimal portfolio. During this step, we also set the discount ratios, transaction costs for entering/exiting the position and data frequency. Adding the stop loss level is optional and can be added or changed along the way.


# Set up a class object
example = omr.OrnsteinUhlenbeck()


# After that we fit the model to the training data and allocate data frequency,
# transaction costs, discount rates and stop-loss level


# You can use the *np.array* as an input data: 


# You can input the np.array as data 
example.fit(data_train, data_frequency="D", discount_rate=[0.05, 0.05],
            transaction_cost=[0.02, 0.02], stop_loss=0.2)

# The parameters can be allocated in an alternative way
example.fit(data_train_dataframe, data_frequency="D", discount_rate=0.05,
            transaction_cost=0.02, stop_loss=0.2)


# You also can use the *pd.DataFrame* as an input data: 


# Chosen data type can be pd.DataFrame
example.fit(data_train_dataframe, data_frequency="D", discount_rate=[0.05, 0.05],
            transaction_cost=[0.02, 0.02], stop_loss=0.2)

# In this case we can also specify the interval we want to use for training
example.fit(data_train_dataframe, data_frequency="D", discount_rate=[0.05, 0.05],
            start="2012-03-27", end="2013-12-08",
            transaction_cost=[0.02, 0.02], stop_loss=0.2)


# Stop-loss level, transaction costs and discount rates
# can be changed along the way


example.L = 0.3


# The crucial point is to understand how well is your input data represented by the fitted model. Is it suitable to fit an OU process to it?
#
# To check we are using the `check_fit` function. The main incentive is to have simulated *max log-likelihood* (mll) function close to a fitted one. If the simulated mll is much greater it means that data provided to the model is not good enough to be modeled by an OU process. 
#
# The check can occasionally return numbers significantly different from the fitted mll due to the random nature of the simulated process and the possibility of outliers. We advise to perform the check multiple times and if it's consistently showing very different values of *mll* you might suspect the unsuitability of your data.


example.check_fit()


# Our data passes the fitness check since the difference between mll's is relatively small.


# ### Step 2: Optimal levels calculation


# If you need to calculate the optimal levels separately to use for your trading strategy you should call the function for the respective optimal level.


# To calculate the optimal entry of liquidation levels separately
# you need to use following functions


# Calculate the optimal liquidation level
b = example.optimal_liquidation_level()

# Calculate the optimal entry level
d = example.optimal_entry_level()

# Calculate the optimal liquidation level accounting for stop-loss
b_L = example.optimal_liquidation_level_stop_loss()

# Calculate the optimal entry interval accounting for stop-loss
interval_L = example.optimal_entry_interval_stop_loss()


print("b*=",np.round(b, 4),"\nd*=",np.round(d, 4),"\nb_L*=",np.round(b_L, 4),"\n[a_L*,d_L*]=",np.round(interval_L, 4))


# ### Step 3: Showcasing the results


# To showcase all the parameters of the fitted model and found optimal levels we use the `description` function.


# If the stop-loss level is not set all the functions that are using it will not be calculated.


# Setting the stop-loss level to "None"
example.L = None

# Call the description function to see all the model's parameters and optimal levels
example.description()


# We deleted our stop-loss level in the previous cell, lets set it back to showcase the how the description function works wth the stop-loss.


# Setting the stop-loss level back to previous value
example.L = 0.3

# Call the description function to see all the model's parameters and optimal levels
example.description()


# We can also showcase our results on any data we choose by calling the `plot_levels` function. When provided an np.array or pd.DataFrame for two asset prices it uses the previously found optimal coefficient to create a portfolio out of them and plots it and the found optimal levels.


# Showcasing the results on the training data (pd.DataFrame)
fig = example.plot_levels(data=data_train_dataframe, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# Showcasing the results on the test data (np.array)
fig = example.plot_levels(data=data_test_and_retrain, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# During the first test period none of the optimal levels are reached yet.


# ### Step 4: Retraining the model


# The next possiple step to take is to retrain the model.For that we are using `fit_to_assets` or `fit_to_data` depending on the input data.  
# As input, you can either use new data or if you used pd.DataFrame during the fitting process, you can retrain the model on a different time interval of your already provided data.


# Retrain the model on the different training interval from already provided data
example.fit_to_assets(start="2012-05-27", end="2013-05-08")

# Showcase the results of retraining on the different training period of already provided data:
example.L = None
example.description()


# Lets use a new training data:


# Retrain the model on new data
example.fit_to_assets(data=data_test_and_retrain)

# Showcase the results of retraining on the new data:
example.L = 0.5
example.description()


# And now plotting our results on the data from the `data_test_retrained` dataset.


fig = example.plot_levels(data_test_the_retrained, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# Here we can see that both optimal entry and exit levels are reached at some point of time during this observation period. After that moment it is advised to retrain the model and continue the process.


# ### Additional functionality


# **Half-life**
#
# Since the half-life parameter of the OU model is widely used in researches we also implemented the function that calculates it. 
#
# *Half-life represents the average time that it takes for the process to revert to it's long term mean on a half of its initial deviation.*


example.half_life()


# **Generating synthetic mean-reverting data**


# We can also use `ou_model_simulation` to generate synthetic data to test our model on or for other purposes. The function uses the parameter values of previously fitted model by default but it can be used with your own OU model paramenters. 


# Syntetic data generated with fitted model parameters
ou_fitted = example.ou_model_simulation(n=400)

plt.plot(ou_fitted)


# Presuming we are using daily data
delta_t = 1/252
# Syntetic data generated with given model parameters
ou_given = example.ou_model_simulation(n=400, theta_given=0.7, mu_given=21,
                                       sigma_given=0.3, delta_t_given=delta_t)

plt.plot(ou_given)


# You can plot the previously found optimal levels on generated data

fig = example.plot_levels(ou_given, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# As an another application you can train and test the model on the simulated data.


# Creating the dataset
ou_train_given = example.ou_model_simulation(n=400, theta_given=0.7, mu_given=12,
                                       sigma_given=0.1, delta_t_given=delta_t)

# Training our model on simulated data
example.fit(ou_train_given, data_frequency="D", discount_rate=[0.05, 0.05],
            transaction_cost=[0.02, 0.02], stop_loss=0.55)

# Showcasing the model's details
example.description()


# Creating a testing dataset
ou_test_given = example.ou_model_simulation(n=1000, theta_given=0.7, mu_given=12,
                                            sigma_given=0.15, delta_t_given=delta_t)

# Plotting found optimal levels on a testing dataset
fig = example.plot_levels(ou_test_given, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# *“Our model can be considered as the building block for the problem with any finite number of sequential trades”* – Professor Tim Leung and Xin Li, 2015.


# ## Conclusion


# This notebook describes the Ornstein-Uhlenbeck (OU) model and how it is applied to mean reverting portfolios. The main focus was portfolio optimization for pairs trading and the optimal timing of trades.
#
# Both techniques were introduced by *Tim Leung and Xin Li* in [**Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications**](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919)
#
# Key takeaways from the notebook:
#
# * Ornstein-Uhlenbeck model is used to model the behavior of mean-reverting assets.
#
# * Using the OU model we can create an optimal mean-reverting portfolio with a maximum level of mean reversion and the best fit. We achieve both goals using the average loglikelihood function.
#
# * For the OU process, we can formulate an optimal stopping problem that allows us to maximize the expected discounted value of entering or liquidating the position.
#
# * The problem can be solved for the case with a defined stop-loss level or without it.
#
# * The model should be considered as a building block for the problem with any finite number of sequential trades.
#
# * Main idea behind the use of the model is:
#
#     * If position is not already entered, enter when the price reaches optimal entry level.
#
#     * If position is not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# * The model is built on a premise of one entry and one exit point during the observation period.
#
#
#


# ## Reference


# 1. Leung, Tim, and Xin Li. Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications. World Scientific Publishing Company, 2451. 



// ---------------------------------------------------

// cir_model.py
// arbitrage_research_basic_PCA/Optimal Mean Reversion/cir_model.py
# Generated from: cir_model.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * References:  **Tim Leung and Xin Li** - *Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications (2015)*


# ## Abstract
#
# The Cox-Ingersoll-Ross model or CIR model is a process mostly used in mathematical finance to describe the evolution of interest rates. It was first introduced in 1985 by John C. Cox, Jonathan E. Ingersoll and Stephen A. Ross as an extension of the Vasicek model. 
#
# Tim Leung, Xin Li in "Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications" (2015) present the solution to the optimal timing problems for one-time entering and liquidating the position and also provide the ability to find optimal levels for an infinite amount of trades based on CIR process.
#
# The following implementations and descriptions closely follow the work of Tim Leung: Tim Leung and Xin Li Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications. Data used to showcase the module was chosen based on the example in the publication.


# ## Cox-Ingersoll-Ross model


# A CIR process satisfies the following stochastic differential equation:
# $$dY_t = \mu(\theta - Y_t)dt + \sigma \sqrt{Y_t} dB_t,$$
#
# $$\theta, \mu, \sigma > 0,$$
# $$B\ -\text{a standard Brownian motion}$$


# * $\theta$ − long term mean level, all future trajectories of Y will evolve around a mean level 𝜃 in the long run.
#
# * $\mu$ - speed of reversion, characterizes the velocity at which such trajectories will regroup around $\theta$ in time.
#
# * $\sigma$ - instantaneous volatility, measures instant by instant the amplitude of randomness entering the system. Higher values imply more randomness.


# The standard deviation factor, $\sigma \sqrt {Y_{t}}$, avoids the possibility of negative $Y_{t}$ values for all positive values of a $\theta$ and $\mu$.


# To fit the model to given data we define the log-likelihood function as in Borodin and Salminen (2002):
#
# $$ \ell (\theta,\mu,\sigma|y_0,y_1,\cdots,y_n) := \frac{1}{n}\sum_{i=1}^{n} ln
#     f^{CIR}(y_i|y_{i-1};\theta,\mu,\sigma)$$


# $$= -ln(\tilde{\sigma}) - \frac{1}{n\tilde{\sigma}}\sum_{i=1}^{n} [y_i +y_{i-1}e^{-\mu \Delta t}] - \frac{1}{n} \sum_{i=1}^{n} [\frac{q}{2}ln(\frac{y_i}{y_{i-1}e^{-\mu\Delta t}}) - ln I_q(\frac{2}{\tilde{\sigma}^2}\sqrt{y_i y_{i-1}e^{-\mu\Delta t}})$$


# By applying the maximum likelihood estimation(MLE) method we are able to determine
# the parameters of the model and fit the observed portfolio prices to a CIR process.


# ## Optimal stopping problem
#
# > This approach presumes that the investor wants to commit only two trades: entering the position, and
#     liquidating it.
#
# Suppose the investor already has the position the value process of which $(Y_t)_{t>0}$ follows the CIR process. After liquidating the position at the time $\tau$ e receives the value $(Y_{\tau})$ and pays a
# constant transaction cost $c_s \in \mathbb{R}$ To maximize the expected discounted value we need to solve
# the optimal stopping problem:


# $$ V^{\chi}(y) = \underset{\tau \in T}{\sup} \mathbb{E}({e^{-r \tau} (Y_{\tau} - c_s)| Y_0 = y})$$
#
# where $T$ denotes the set of all possible stopping times and $r > 0$ is our subjective constant
# discount rate. $V^{\chi}(y)$ represents the expected liquidation value accounted with y.
#
# Current price plus transaction cost  constitute the cost of entering the trade and in combination with $V^{\chi}(y)$
# we can formalize the optimal entry problem:
#
#
# $$J^{\chi}(y) = \underset{\nu \in T}{\sup} \mathbb{E}({e^{-\hat{r} \tau} (V^{\chi}(Y_{\nu}) - Y_{\nu} - c_b)| Y_0 = y})$$
#
# with
#
# $$\hat{r}>0,\ c_b \in \mathbb{R}$$
#
# As an investor our goal is to maximize the expected difference between the current price
# of the position - $Y_{\nu}$ and its' expected liquidation value $V^{\chi}(Y_{\nu})$ minus transaction cost
# $c_b$


# ### The solutions:


#
# Theorem 4.2 (p.85):
#
# **The optimal liquidation problem admits the solution:**
#
# $$
#     V^{\chi}(x) = \begin{cases} (b^{\chi*} - c_s) \frac{F^{\chi}(x)}{F^{\chi}(b^{\chi*})} , & \mbox{if } x \in [0,b^{\chi*})\\
#     \\x - c_s, &  \mbox{ otherwise}  \end{cases}\\
# $$
#
# The optimal liquidation level $b^{\chi*}$ is found from the equation:
#
# $$F^{\chi} (b^{\chi}) - (b^{\chi} - c_s)F'^{\chi}(b^{\chi}) = 0$$
#
# Corresponding optimal liquidation time is given by
#
# $$\tau^{\chi*} = inf [t\geq0:Y_t \geq b^{\chi*}]$$
#
# Theorem 4.4 (p.86):
#
# **The optimal entry timing problem admits the solution:**
#
# $$
#     J(x) = \begin{cases} V^{\chi}(x) - x - c_b, & \mbox{if } x \in [0,d^{\chi*}) \\
#     \\\frac{V^{\chi}(d^{\chi*}) - d^{\chi*} - c_b}{\hat{G^{\chi}}(d^{\chi*})}, & \mbox{if } x \in (d^{\chi*}, \infty)  \end{cases}
# $$
#
# The optimal entry level $d^{\chi*}$ is found from the equation:
#
# $$ \hat{G}^{\chi}(d^{\chi})(V'^{\chi}(d^{\chi}) - 1) - \hat{G}'^{\chi}(d^{\chi})(V^{\chi}(d^{\chi}) - d^{\chi} - c_b) = 0$$
#
# Where "$\hat{\ }$" represents the use of transaction cost and discount rate of entering.


# ## Optimal switching problem
#
# > This approach presumes that the investor can commit an infinite number of trades.
#
#
# If there is no limit on the number of times the investor will open or close the position, the sequential trading times
# are modelled by the stopping times $\nu_1,\tau_1,\nu_2,\tau_2,... \in T$ such that
#
# $$0\leq\nu_1\leq\tau_1\leq\nu_2\leq\tau_2\leq...$$
#
# Where $\nu_i$ are times when the share of a risky asset was bought and $\tau_i$ - when it was sold. In the case
# of pairs trading, we consider our spread as such an asset.


# To find the optimal levels, first, two critical constants have to be denoted:
#
# $$y_s:=\frac{\mu\theta+rc_s}{\mu+r} \\
#     y_b:=\frac{\mu\theta-rc_b}{\mu+r}$$


# Theorem 4.7 (p.56):
#
# **Under optimal switching approach it is optimal to re-enter the market if and only if all of the following conditions
# hold true:**
#
# a) If $y_b>0$
#
# b) The following inequality must hold true:
#
# $$ c_b < \frac{b^{\chi*}-c_s}{F^{\chi}(b^{\chi*})}$$
#
# In case any of the conditions are not met - re-entering the market is deemed not optimal. It would be advised to exit
# at the optimal liquidation price without re-entering, or not enter the position at all. The difference between the
# options depends on whether the investor had already entered the market beforehand,
# or did he or she start with a zero position.


# ## How to use the CIR module
#
# For this module, the most suitable input would be a mean-reverting portfolio or an array of two correlated or co-moving asset prices.
#
# Both optimal stopping and optimal switching levels are used alike in determining the rules of our trading strategy:
#
#
# * If not already entered, enter when the price reaches optimal entry level.
#
# * If not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# However, the difference lies in the applications of the two approaches. The optimal stopping usually 
# has a much longer delay between the buy and sell levels are reached and offers a bigger gain than a one-time entry and liquidation using the optimal switching levels. The optimal switching levels, on the other hand, may provide a bigger cumulative gain by performing the trades multiple times during the same time period.


# Imports:


import numpy as np
import matplotlib.pyplot as plt

from arbitragelab.optimal_mean_reversion import CoxIngersollRoss


# Let's establish a testing sample. Alongside with the numerical example from the book we use generated CIR data with the following parameters:
#
# $\theta$=0.2; $\mu$=0.2; $\sigma$=0.3 


# During this step, we also demonstrate the ability of our model to generate artificial CIR data based on given parameters


# Creating a class instance
example = CoxIngersollRoss()

# We establish our training sample
delta_t = 1/252
np.random.seed(30)
cir_example =  example.cir_model_simulation(n=1000, theta_given=0.2, mu_given=0.2,
                                            sigma_given=0.3, delta_t_given=delta_t)


# Plotting the generated CIR process
plt.figure(figsize=(12, 7))
plt.plot(cir_example);


# Model fitting uses the same functional structure as the OU and XOU modules:


# Model fitting
example.fit(cir_example, data_frequency="D", discount_rate=0.05,
            transaction_cost=[0.001, 0.001])


# Optimal stopping levels can be found separately via respective functions.


# You can separately solve optimal stopping
# and optimal switching problems

# Solving the optimal stopping problem
b = example.optimal_liquidation_level()

d = example.optimal_entry_level()


print("Optimal liquidation level:", round(b,5),
      "\nOptimal entry level:", round(d, 5))


# For the optimal switching level exists a separate function. 


# Solving the optimal switching problem
d_switch, b_switch = example.optimal_switching_levels()


print("Optimal liquidation level:", round(b_switch,5),
      "\nOptimal entry level:", round(d_switch, 5))


# To test the obtained results let's simulate the CIR process based on the fitted model using the respective function.


# Generating a CIR process to calculate the optimal levels
np.random.seed(30)
cir_test = example.cir_model_simulation(n=1000)


# To visualize all the obtained results we use the `cir_plot_levels` function. 


# You can display the results using the plot
fig = example.cir_plot_levels(cir_test, switching=True)

# Adjusting the size of the plot
fig.set_figheight(7)
fig.set_figwidth(12)


# Or you can view the model statistics


# Or you can view the model statistics
example.cir_description(switching=True)


# ## Conclusion


# This notebook describes the Cox-Ingersoll-Ross (CIR) model and how it is applied to mean reverting portfolios. The main goal of the notebook is to show the usage of the module to obtain solutions to the optimal stopping and optimal switching problems.
#
# Both techniques were introduced by *Tim Leung and Xin Li* in [**Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications**](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919)
#
# Key takeaways from the notebook:
#
# * Cox-Ingersoll-Ross model is a tool used to model the behavior of mean-reverting assets.
#
# * Main idea behind the use of the  optimal levels is:
#
#     * If the position is not already entered, enter when the price reaches the optimal entry level.
#
#     * If the position is not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# * An optimal stopping problem formulated for the CIR process allows us to maximize the expected discounted value of one-time entering or liquidating the position by finding the optimal price levels at which trades should be committed.
#
# * An optimal switching problem, on the other hand, allows us to maximize the expected discounted value of an infinite amount of trades(entering or liquidating the position). By finding the optimal price levels at which the repeated trades should be committed for the maximum overall gain. 
#
# * The model should be considered as a building block for the problem with any finite number of sequential trades.


# ## Reference


# 1. Leung, Tim, and Xin Li. Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications. World Scientific Publishing Company, 2451. 



// ---------------------------------------------------

// multivariate_cointegration.py
// arbitrage_research/Cointegration Approach/multivariate_cointegration.py
# Generated from: multivariate_cointegration.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# References: 
#
# - [Galenko, A., Popova, E., and Popova, I. (2012). **Trading in the presence of cointegration.** *The Journal of Alternative Investments*, 15(1):85–97.](http://www.ntuzov.com/Nik_Site/Niks_files/Research/papers/stat_arb/Galenko_2007.pdf)


# # Multivariate Cointegration Framework


# ## Introduction


# The cointegration relations between time series imply that the time series are bound together. Over time the time series
# might drift apart for a short period of time, but they ought to re-converge. A trading strategy on $N \: (N \geq 3)$ cointegrated assets that have a positive expectation of profit can be designed based on this property. 
#
# In this notebook, the trading strategy will be demonstrated, and an empirical example of applying this strategy to four European stock indices will be given.


# ## Multivariate Cointegration


# Cointegration is defined by the stochastic relationships among the asset log returns.
#
# Let $P_i$, where $i = 1, 2, \ldots, N$ denote the price of $N$ assets. The continuously compounded asset
# returns, i.e. log-returns at time $t > 0$ can be written as:
#
# \begin{equation*}
# r_t^i = \ln{P_t^i} - \ln{P_{t-1}^i}
# \end{equation*}
#
# Now construct a process $Y_t$ as a linear combination of the $N$ asset prices:
#
# \begin{equation*}
# Y_t = \sum_{i=1}^N b^i \ln{P_t^i}
# \end{equation*}
#
# where $b^i$ denotes the $i$-th element for a finite vector $\mathbf{b}$.
#
# The corresponding asset returns series $Z_t$ can be defined as:
#
# \begin{equation*}
# Z_t = Y_t - Y_{t-1} = \sum_{i=1}^N b^i r_t^i
# \end{equation*}
#
# Assume that the memory of the process $Y_t$ does not extend into the infinite past, which can be expressed as the
# following expression in terms of the autocovariance of the process $Y_t$:
#
# \begin{equation*}
# \lim_{p \to \infty} \text{Cov} \lbrack Y_t, Y_{t-p} \rbrack = 0
# \end{equation*} 
#
# Then the **log-price** process $Y_t$ is stationary, if and only if the following three conditions on
# **log-returns** process $Z_t$ are satisfied:
#
# \begin{gather*}
# E[Z_t] = 0 \\
# \text{Var }Z_t = -2 \sum_{p=1}^{\infty} \text{Cov} \lbrack Z_t, Z_{t-p} \rbrack \\
# \sum_{p=1}^{\infty} p \text{ Cov} \lbrack Z_t, Z_{t-p} \rbrack < \infty
# \end{gather*}
#
# When $Y_t$ is stationary, the log-price series of the assets are cointegrated.
#
# For equity markets, the log-returns time series can be assumed as stationary and thus satisfy the above conditions.
# Therefore, when it comes to empirical applications, the Johansen test could be directly applied to the log price series
# to derive the vector $\mathbf{b}$.


# ## Trading Strategy


# The core idea of the strategy is to bet on the spread formed by the cointegrated $N$ assets that have gone apart
# but are expected to mean revert in the future.
#
# The trading strategy, using the notation in the above section, can be described as follows.
#
# 1. Estimate the cointegration vector $\hat{\mathbf{b}}$ with Johansen test using training data.
# 2. Construct the realization $\hat{Y}_t$ of the process $Y_t$ by calculating $\hat{\mathbf{b}}^T \ln P_t$, and calculate $\hat{Z}_t = \hat{Y}_t - \hat{Y}_{t-1}$.
# 3. Compute the finite sum $\sum_{p=1}^P \hat{Z}_{t-p}$, where the lag $P$ is an input argument.
# 4. Partition the assets into two sets $L$ and $S$ according to the sign of the element in the cointegration vector $\hat{\mathbf{b}}$. For each asset $i$,
#
# \begin{eqnarray*}
# i \in L \iff b^i \geq 0 \\
# i \in S \iff b^i < 0
# \end{eqnarray*}
#
# 5. Following the formulae below, calculate the number of assets to trade so that the notional of the positions would equal to $C$.
#
# \begin{eqnarray*}
#     \Bigg \lfloor \frac{-b^i C \text{ sgn} \bigg( \sum_{p=1}^{P} Z_{t-p} \bigg)}{P_t^i \sum_{j \in L} b^j} \Bigg \rfloor, \: i \in L \\
#     \Bigg \lfloor \frac{b^i C \text{ sgn} \bigg( \sum_{p=1}^{P} Z_{t-p} \bigg)}{P_t^i \sum_{j \in L} b^j} \Bigg \rfloor, \: i \in S
# \end{eqnarray*}
#
# 6. Open the positions on time $t$ and close the positions on time $t+1$.
# 7. Every once in a while - once per month (22 trading days) for example, re-estimate the cointegration vector. If it is time for a re-estimate, go to step 1; otherwise, go to step 2.
#
#
# The strategy is trading at daily frequency and always in the market.


# ## Usage of the Module


# In this section, the usage of multivariate cointegration trading strategy will be demonstrated with an empirical example of four European stock indices, i.e. DAX (Germany), CAC 40 (France), FTSE 100 (UK), and AEX (Netherlands). Price history from Jan 2nd, 1996 to Dec 28th, 2006 was used. The module allows two missing data imputation methods: forward-fill and polynomial spline. In the following demonstration, missing data due to the difference in working days in different countries was imputed with a forward-fill method in order to avoid the introduction of phantom returns on non-trading days.
#
# Trading for out-of-sample tests starts on Nov 6th, 2001 and ends on Dec 28th, 2006. The cointegration vector $\mathbf{b}$ was estimated using the Johansen test. The notional value of the long positions and short positions each day was set to $\$10 \text{M}$, respectively. To be specific, each day $\$10 \text{M}$ were invested in longs and another $\$10 \text{M}$ were invested in shorts, resulting in a $\$20 \text{M}$ portfolio.


%matplotlib inline


# Importing libraries
import pandas as pd
import numpy as np

from arbitragelab.cointegration_approach.multi_coint import MultivariateCointegration
from arbitragelab.trading import MultivariateCointegrationTradingRule


# Loading data
euro_stocks_df = pd.read_csv("multi_coint.csv", parse_dates=['Date'])
euro_stocks_df.set_index("Date", inplace=True)

# Out-of-sample data split time point
split_point = pd.Timestamp(2001, 11, 6)

# Indexing with DateTimeIndex is always inclusive. Removing the last data point in the training data
train_df = euro_stocks_df.loc[:split_point].iloc[:-1]
trade_df = euro_stocks_df.loc[split_point:]


# Checking train data
train_df.tail()


# Checking test data
trade_df.head()


# ### Optimize and Generate Trading Signal


# For the following test, the cointegration vector will be estimated with all training data and will not be updated monthly, but this can easily be made by re-running the MultivariateCointegration optimization.


# Initializing the optimizer
optimizer = MultivariateCointegration()

# Set the trainging deatset
optimizer.set_train_dataset(train_df)

# Imputing all missing values
optimizer.fillna_inplace(nan_method='ffill')


# Generating the cointegration vector to later use in a trading strategy
coint_vec = optimizer.get_coint_vec()


# Now we can now use the MultivariateCointegrationTradingRule from the Spread Trading module to feed in new price values and get signls - number of shares to trade per asset. With the mechanism of providing price values one by one to the strategy, it's easier to integrate this strategy in an existing trading pipeline.


# Creating a strategy
strategy = MultivariateCointegrationTradingRule(coint_vec)


# Now we use a loop to simulate a live data feed.


# Adding initial price values
strategy.update_price_values(trade_df.iloc[0])

# Feeding price values to the strategy one by one
for ind in range(trade_df[:5].shape[0]):

    time = trade_df.index[ind]
    value = trade_df.iloc[ind]

    strategy.update_price_values(value)

    # Getting signal - number of shares to trade per asset
    pos_shares, neg_shares, pos_notional, neg_notional = strategy.get_signal()

    # Close previous trade
    strategy.update_trades(update_timestamp=time)

    # Add a new trade
    strategy.add_trade(start_timestamp=time, pos_shares=pos_shares, neg_shares=neg_shares)


# Currently open trades in a strategy
open_trades = strategy.open_trades

open_trades


# Only one trade is io all trades but one were closed.


# Checking all closed trades
closed_trades = strategy.closed_trades

closed_trades


# We see the closed trades with signals - a number of shares to trade for each set of prices in our testing dataset.
#
# ### Strategy outputs
#
# We can see the following data:
# * Dictionary key:
#     * Timestamp at which the trade was opened
# * Dctionary value:
#     * t1: Timestamp at which the trade was closed
#     * pt: Prices at which the trade was closed
#     * uuid: Trade ID that can be provided for each trade
#     * start_prices: Prices at which spread was opened
#     * end_prices: Prices at which spread was closed
#     * pos_shares: Ticker and number of shares to go long
#     * neg_shares: Ticker and number of shares to go short 


# ## Discussion


# In general, from the conducted experiments, we discovered that the rolling window setup is better than the cumulative window setup. Also, re-estimating the cointegration vector monthly improves the performance of the strategy. It is better to exclude further history when estimating the cointegration vector, as the cointegration relationship between the $N$ assets are time-varying. It also provides circumstantial evidence that the following assumptions of the model are reasonable:
#
# \begin{eqnarray*}
# \lim_{p \to \infty} \text{Cov} \lbrack Y_t, Y_{t-p} \rbrack = 0 \\
# \sum_{p=1}^{\infty} p \text{ Cov} \lbrack Z_t, Z_{t-p} \rbrack < \infty
# \end{eqnarray*}
#
# These two assumptions indicate that long-term memory for the cointegrated assets will be almost non-existent. 
#
# However, this trading strategy also has its limitations. Since the index value of AEX is much smaller than DAX, FTSE, and CAC 40, the number of AEX shares/contracts that need to be traded is much larger than its counterpart. Therefore, when the prices of the assets are different in the order of magnitude, it is better to double-check the position limit before trading the strategy.


# ## Conclusion


# This notebook demonstrated a trading strategy using the properties of cointegration among $N$ assets, and included an empirical example of trading four European stock indices (AEX, DAX, FTSE, and CAC).
#
# ### Key Takeaways
#
# - The cointegration relation can be defined by the properties of compounded returns rather than asset prices.
# - It is possible to trade a strategy that has positive profit expectancy based on this cointegration relation of $N$ assets.


# ## Reference


# 1. [Galenko, A., Popova, E. and Popova, I., 2012. Trading in the presence of cointegration. The Journal of Alternative Investments, 15(1), pp.85-97.](http://www.ntuzov.com/Nik_Site/Niks_files/Research/papers/stat_arb/Galenko_2007.pdf)



// ---------------------------------------------------

// trading_simulation.py
// arbitrage_research/Cointegration Approach/trading_simulation.py
# Copyright 2019, Hudson and Thames Quantitative Research
# All rights reserved
# Read more: https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/additional_information/license.html

# pylint: disable=invalid-name
"""
This module simulates trading based on the minimum profit trading signal, reports the trades,
and plots the equity curve.
"""

from typing import Optional, Tuple

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd


class TradingSim:
    """
    This class simulates the trades based on an optimized minimum profit trading signal.
    It plots the trading signal on the cointegration error and the equity curves as well.
    """

    def __init__(self, starting_equity: float = np.Inf):
        """
        Setting up a new trading account for simulating the trading strategy.

        :param starting_equity: (float) The amount available to trade in this simulation.
        """

        # Set position and P&L to 0, fund the account with the dollar amount specified
        self._position = np.zeros((2, ))
        self._base_equity_value = starting_equity
        self._total_trades = np.zeros((2,))
        self._report = dict()

        # Record mark-to-market P&L everyday to give a proper view of drawdowns during the trades
        self._pnl = 0.
        self._mtm = None

    def initialize_report(self):
        """
        Initialize the dictionary for trade reports.
        """

        # Dictionary for generating the equity curve and trade report DataFrame
        self._report = {
            "Trade Date": [],
            "Trade Type": [],
            "Leg 1": [],
            "Leg 1 Shares": [],
            "Leg 1 Price": [],
            "Leg 2": [],
            "Leg 2 Shares": [],
            "Leg 2 Price": []}

        self._mtm = {
            "P&L": [],
            "Total Equity": []}

        self._position = np.zeros((2,))
        self._pnl = 0.

    def _trade(self, signals: pd.DataFrame, num_of_shares: np.array):
        """
        Trade the cointegrated pairs based on the optimized signal.

        :param signals: (pd.DataFrame) Dataframe that contains asset prices and trade signals.
        :param num_of_shares: (np.array) Optimized number of shares to trade.
        """

        # Generate report
        self.initialize_report()

        # Trading periods in the trade_df
        period = len(signals)

        # Add a flag to let the simulator know if a U-trade is currently open or a L-trade
        current_trade = 0

        # Start trading
        entry_price = np.zeros((2, ))

        for i in range(period):
            current_price = signals.iloc[i, [0, 1]].values

            # Check mark-to-market P&L
            trade_pnl = np.dot(current_price - entry_price, self._position)

            # Record mark-to-market P&L
            self._mtm['P&L'].append(trade_pnl)
            self._mtm['Total Equity'].append(self._base_equity_value + trade_pnl)

            if current_trade == 0:
                # No position, and the opening trade condition is satisfied
                # Before opening the trade, check if the dollar constraint allows us to open
                capital_req = np.dot(current_price, num_of_shares)

                # Capital requirement satisfied, open the position
                if capital_req <= self._base_equity_value:
                    # Record the entry price.
                    entry_price = current_price

                    # Do we open a U-trade or L-trade?
                    if signals['otc_U'].iloc[i]:
                        # U-trade, short share S1, long share S2
                        self._report['Trade Type'].append("U-trade Open")
                        self._position = num_of_shares * np.array([-1, 1])
                        current_trade = 1

                    elif signals['otc_L'].iloc[i]:
                        # L-trade, long share S1, short share S2
                        self._report['Trade Type'].append("L-trade Open")
                        self._position = num_of_shares * np.array([1, -1])
                        current_trade = -1
                    else:
                        # No opening condition met, forward to next day
                        continue

                    # Bookkeeping
                    self._report['Trade Date'].append(signals.index[i].date())
                    self._report['Leg 1'].append(signals.columns[0])
                    self._report['Leg 2'].append(signals.columns[1])
                    self._report['Leg 1 Price'].append(entry_price[0])
                    self._report['Leg 2 Price'].append(entry_price[1])
                    self._report['Leg 1 Shares'].append(self._position[0])
                    self._report['Leg 2 Shares'].append(self._position[1])

                # Make sure the trade will not be closed on the same day (using elif)

            else:
                # We have a trade on
                if current_trade == 1 and signals['ctc_U'].iloc[i]:
                    # The open trade is a U-trade
                    self._report['Trade Type'].append("U-trade Close")
                    self._total_trades[0] += 1

                elif current_trade == -1 and signals['ctc_L'].iloc[i]:
                    # The open trade is a L-trade
                    self._report['Trade Type'].append("L-trade Close")
                    self._total_trades[1] += 1

                else:
                    # No condition triggered, just forward to next day
                    continue

                # Bookkeeping
                self._report['Trade Date'].append(signals.index[i].date())
                self._report['Leg 1'].append(signals.columns[0])
                self._report['Leg 2'].append(signals.columns[1])
                self._report['Leg 1 Price'].append(current_price[0])
                self._report['Leg 2 Price'].append(current_price[1])
                self._report['Leg 1 Shares'].append(-1 * self._position[0])
                self._report['Leg 2 Shares'].append(-1 * self._position[1])

                # Add the final profit to base equity value.

                self._base_equity_value += trade_pnl

                # Clear the trade book.
                current_trade = 0
                entry_price = np.zeros((2,))
                self._position = np.zeros((2,))

                # Will not close the trade on the same day (using elif)

    def summary(self, signals: pd.DataFrame, num_of_shares: np.array) -> pd.DataFrame:
        """
        Trade the strategy and generate the trade reports.

        :param signals: (pd.DataFrame) Dataframe that contains asset prices and trade signals.
        :param num_of_shares: (np.array) Optimized number of shares to trade.
        :return (pd.DataFrame, np.array): A dataframe that contains each opening/closing trade details, P&L,
            and equity curves; a NumPy array that represents the number of U-trade and L-trade over the period
        """

        self._trade(signals, num_of_shares)
        report_df = pd.DataFrame(self._report)
        return report_df

    def get_pnl(self, signals: pd.DataFrame) -> pd.DataFrame:
        """
        Retrieve the daily mark-to-market P&L of trading the strategy.

        :param signals: (pd.DataFrame) Dataframe that contains the trading signal.
        :return: (pd.DataFrame) Dataframe that contains the mark-to-market P&L every trading day.
        """

        # Build the P&L dataframe
        pnl_df = pd.DataFrame(self._mtm)

        # Set the date index
        pnl_df.index = signals.index
        return pnl_df

    def _plot_signals(self, signals: pd.DataFrame, num_of_shares: np.array, cond_lines: np.array,
                      figw: float = 15, figh: float = 10, start_date: Optional[pd.Timestamp] = None,
                      end_date: Optional[pd.Timestamp] = None) -> plt.Figure:
        """
        Plot the spread and the signals.

        :param signals: (pd.DataFrame) Dataframe that contains the trading signal.
        :param num_of_shares: (np.array) Numpy array that contains the number of shares.
        :param cond_lines: (np.array) Numpy array that contains the trade initiation/close signal line.
        :param figw: (float) Figure width.
        :param figh: (float) Figure height.
        :param start_date: (pd.Timestamp) The starting point of the plot.
        :param end_date: (pd.Timestamp) The end point of the plot.
        :return: (plt.Figure) The object of the plot.
        """

        # Retrieve the report
        report_df = pd.DataFrame(self._report)

        # Define the ticks on the x-axis
        years = mdates.YearLocator()  # every year
        months = mdates.MonthLocator()  # every month
        years_fmt = mdates.DateFormatter('\n%Y')
        months_fmt = mdates.DateFormatter('%b')

        # Plot the price action of each leg as well as the cointegration error
        fig, axes = plt.subplots(2, 1, sharex=True, figsize=(figw, figh), gridspec_kw={'height_ratios': [2.5, 1]})

        # Plot prices
        axes[0].plot(signals.iloc[:, 0], label=signals.columns[0])
        axes[0].plot(signals.iloc[:, 1], label=signals.columns[1])
        axes[0].legend(loc='upper left', fontsize=12)
        axes[0].tick_params(axis='y', labelsize=14)

        # Plot cointegration error
        axes[1].plot(signals.iloc[:, 2], label='spread')
        axes[1].legend(loc='best', fontsize=12)

        # Plot signal lines
        axes[1].axhline(y=cond_lines[1], color='black')  # Closing condition
        axes[1].axhline(y=cond_lines[0], color='red')  # L-trade opens
        axes[1].axhline(y=cond_lines[2], color='green')  # U-trade opens

        # Formatting the tick labels
        axes[1].xaxis.set_major_locator(years)
        axes[1].xaxis.set_major_formatter(years_fmt)
        axes[1].xaxis.set_minor_locator(months)
        axes[1].xaxis.set_minor_formatter(months_fmt)
        axes[1].tick_params(axis='x', labelsize=14)
        axes[1].tick_params(axis='y', labelsize=14)

        # Define the date range of the plot
        if start_date is not None and end_date is not None:
            axes[1].set_xlim((start_date, end_date))

        # Plot arrows for buy and sell signal
        for idx in range(len(report_df)):
            trade_type = report_df.iloc[idx]['Trade Type']
            trade_date = report_df.iloc[idx]['Trade Date']
            arrow_xpos = mdates.date2num(trade_date)
            arrow_ypos = signals.loc[pd.Timestamp(trade_date)]['coint_error']

            # Green arrow for opening U-trade, red arrow for opening L-trade, black arrow for closing the trade.
            if trade_type == "U-trade Open":
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, 15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='green'))
            elif trade_type == "L-trade Open":
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, -15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='red'))
            elif trade_type == "L-trade Close":
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, 15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='black'))
            else:
                axes[1].annotate("", (arrow_xpos, arrow_ypos), xytext=(0, -15),
                                 textcoords='offset points', arrowprops=dict(arrowstyle='-|>', color='black'))

        fig.suptitle("Optimal Pre-set Boundaries and Trading Signals", fontsize=20)
        return fig

    def _plot_pnl_curve(self, signal: pd.DataFrame, figw: float = 15., figh: float = 10.,
                        start_date: Optional[pd.Timestamp] = None,
                        end_date: Optional[pd.Timestamp] = None) -> plt.Figure:
        """
        Plot the equity curve (marked to market daily) trading the strategy.

        :param signal: (pd.DataFrame) Dataframe containing the trading signal.
        :param figw: (float) Figure width.
        :param figh: (float) Figure heght.
        :param start_date: (pd.Timestamp) The starting point of the plot.
        :param end_date: (pd.Timestamp) The end point of the plot.
        :return: (plt.Figure) The object of the plot.
        """

        # Build the equity curve dataframe
        equity_curve_df = pd.DataFrame(self._mtm)

        # Set up date index
        equity_curve_df.index = signal.index

        # Define the ticks on the x-axis
        years = mdates.YearLocator()  # every year
        months = mdates.MonthLocator()  # every month
        years_fmt = mdates.DateFormatter('\n%Y')
        months_fmt = mdates.DateFormatter('%b')

        # Plot the equity curve
        fig, ax = plt.subplots(figsize=(figw, figh))
        ax.plot(equity_curve_df['Total Equity'])

        # Formatting the tick labels
        ax.xaxis.set_major_locator(years)
        ax.xaxis.set_major_formatter(years_fmt)
        ax.xaxis.set_minor_locator(months)
        ax.xaxis.set_minor_formatter(months_fmt)
        ax.tick_params(axis='x', labelsize=14)
        ax.tick_params(axis='y', labelsize=14)

        # Define the date range of the plot
        if start_date is not None and end_date is not None:
            ax.set_xlim((start_date, end_date))

        fig.suptitle("P&L Curve of the Trading Strategy", fontsize=20)
        return fig

    def plot_strategy(self, signal: pd.DataFrame, num_of_shares: np.array, cond_lines: np.array,
                      figw: float = 15, figh: float = 10, start_date: Optional[pd.Timestamp] = None,
                      end_date: Optional[pd.Timestamp] = None) -> Tuple[plt.Figure, plt.Figure]:
        """
        Plot the trading signal and the PnL curve of the trading strategy

        :param signal: (pd.DataFrame) Dataframe containing the trading signal.
        :param num_of_shares: (np.array) Numpy array that contains the number of shares.
        :param cond_lines: (np.array) Numpy array that contains the trade initiation/close signal line.
        :param figw: (float) Figure width.
        :param figh: (float) Figure heght.
        :param start_date: (pd.Timestamp) The starting point of the plot.
        :param end_date: (pd.Timestamp) The end point of the plot.
        :return: (plt.Figure, plt.Figure) The object of the trading signal plot; the object of the P&L curve plot.
        """

        # A wrapper function to plot two figures
        fig1 = self._plot_signals(signal, num_of_shares, cond_lines, figw=figw, figh=figh,
                                  start_date=start_date, end_date=end_date)
        fig2 = self._plot_pnl_curve(signal, figw=figw, figh=figh, start_date=start_date, end_date=end_date)

        return fig1, fig2


// ---------------------------------------------------

// sparse_mean-reverting_portfolio_selection.py
// arbitrage_research/Cointegration Approach/sparse_mean-reverting_portfolio_selection.py
# Generated from: sparse_mean-reverting_portfolio_selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# References:
#
# * [d'Aspremont, A. (2011). Identifying small mean-reverting portfolios. *Quantitative Finance*, 11(3), pp.351-364.](https://arxiv.org/pdf/0708.3048.pdf)
# * [Cuturi, M. and d'Aspremont, A. (2015). Mean-reverting portfolios: Tradeoffs between sparsity and volatility. *arXiv preprint arXiv:1509.05954.*](https://arxiv.org/abs/1509.05954)


# # Sparse Mean-reverting Portfolio Selection


# ## Introduction


# Assets that exhibit significant mean-reversion are difficult to find in efficient markets. As a result, investors focus on creating long-short asset baskets to form a mean-reverting portfolio whose aggregate value shows mean-reversion. Classic solutions including cointegration or canonical correlation analysis can only construct dense mean-reverting portfolios, i.e. they include every asset in the investing universe. These portfolios have shown significant such as higher transaction costs, worse P&L interpretability, and inability to capture meaningful statistical arbitrage opportunities. On the other hand, sparse mean-reverting portfolios, which require trading as few assets as possible, can mitigate these shortcomings.
#
# This research notebook will showcase three approaches to constructing a sparse mean-reverting portfolio using **ArbitrageLab**:
#
# 1. Covariance selection and penalized regression techniques to narrow down the investing universe;
# 2. Greedy search to construct sparse mean-reverting portfolios;
# 3. Semidefinite programming (SDP) approach to construct sparse mean-reverting portfolios above a volatility threshold.
#
# A few key concepts will be introduced first to provide a better understanding of the sparse mean-reverting portfolio selection problem and an explanation to why these three approaches are effective.


# ## Mean-reversion Strength Metrics and Proxies


# ### Ornstein-Uhlenbeck Process and Mean-reversion Speed


# The straightforward solution is to assume the portfolio value follows an Ornstein-Uhlenbeck (OU) process and use the mean-reversion speed parameter $\lambda$ to measure the mean-reversion strength.


# $$
# \begin{gather*}
# dP_t = \lambda (\bar{P} - P_t)dt + \sigma dZ_t \\
# P_t = \mathbf{x}^T S_t \\
# \lambda > 0
# \end{gather*}
# $$
#
# where $P_t$ is the portfolio value at time $t$, $\bar{P}$ is the average portfolio value, $Z_t$ is a standard Brownian motion, $\mathbf{x}$ is the weight vector of each asset in the portfolio, and $S_t$ is the individual asset prices at time $t$. The objective is to maximize $\lambda$ by adjusting $\mathbf{x}$ under the constraints that $\lVert \mathbf{x} \rVert_2 = 1$ and $\lVert \mathbf{x} \rVert_0 = k$, where 
#
# * $\lVert \mathbf{x} \rVert_0$ denotes the number of non-zero elements in $\mathbf{x}$; this constraint is also referred to as *cardinality constraint*;
# * $k > 0$ is an integer which represents the number of assets that should be included in the portfolio.


# If the OU mean-reversion speed $\lambda$ can be expressed as a function of the portfolio weight vector $\mathbf{x}$, maximizing $\lambda$ will give the optimal asset weightings $\mathbf{x}$ during the process. However, this is rather difficult. Instead, three other mean-reversion strength proxies are employed to solve the sparse mean-reverting portfolio selection problem:
#
# 1. Predictability based on Box-Tiao canonical decomposition.
# 2. Portmanteau statistic.
# 3. Crossing statistic.
#
# Meanwhile, the OU mean-reversion speed $\lambda$ is only used to evaluate the sparse portfolios generated by ArbitrageLab.


# ### Predictability and Box-Tiao Canonical Decomposition


# Assume that the asset prices $S_t$ follows a vector autoregressive process of order one - a VAR(1) process.
#
# $$
# S_t = S_{t-1} A + Z_t
# $$
#
#
# where $A$ is a $n \times n$ square matrix and $Z_t$ is a vector of i.i.d. Gaussian noise with $Z_t \sim N(0, \Sigma)$, independent of $S_{t-1}$.


# Assume also that the portfolio value $P_t$ follows the recursion:
#
# $$
# P_t = \hat{P}_{t-1} + \varepsilon_t
# $$
#
# where $\hat{P}_{t-1}$ is a predictor of $x_t$ built upon all past portfolio values recorded up to $t-1$,
# and $\varepsilon_t$ is a vector i.i.d. Gaussian noise, where $\varepsilon_t \sim N(0, \Sigma)$, independent
# of all past portfolio values $P_0, P_1, \ldots, P_{t-1}$.


# Substitute the portfolio value with the linear combination of the asset price, and use the VAR(1) model as the predictor $\hat{P}_{t-1}$:
#
# $$
# \mathbf{x}^T S_t = \mathbf{x}^T {S}_{t-1} A + \mathbf{x}^T \varepsilon_t
# $$


# Now calculate the variance for both sides of the equation. Since the variance would not change if the mean is shifted, it is safe to assume that the mean price of each asset is zero. The variance equation can be written as:
#
# $$
# \mathbf{x}^T \Gamma_0 \mathbf{x} = \mathbf{x}^T A^T \Gamma_0 A \mathbf{x} + \Sigma
# $$
#
# where $\Gamma_0$ are the covariance matrices of and $S_t$.


# Define the predictability of the portfolio value process as:
#
# $$
# \nu = \frac{\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}}{\mathbf{x}^T \Gamma_0 \mathbf{x}}
# $$
#
# Then the above equation can be written as:
#
# $$
# 1 = \nu + \frac{\Sigma}{\mathbf{x}^T \Gamma_0 \mathbf{x}}
# $$
#
# The interpretation of $\nu$ is straightforward. When $\nu$ is small, the variance of the Gaussian noise $\Sigma$ dominates and the portfolio value process will look like noise and is more strongly mean-reverting. Otherwise, the variance of the predicted value $\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}$ dominates and the portfolio value process can be accurately predicted on average. More importantly, the predictability $\nu$ is a function of the portfolio weights $\mathbf{x}$, and thus the optimal portfolio can be obtained during the optimization.


# Without the cardinality constraint $\lVert \mathbf{x} \rVert_0 = k$, optimizing predictability can be summarized as a generalized eigenvalue problem:
#
# $$
# \det (\lambda \Gamma_0 - A^T \Gamma_0 A) = 0
# $$
#
# To find the portfolio weight that forms a portfolio with the most (or the least) predictability, just calculate the eigenvector that corresponds to the maximal (or minimal) eigenvalue of the matrix $\Gamma_0^{-1} A^T \Gamma_0 A$. This process is Box-Tiao canonical decomposition. An example of combining covariance selection and Box-Tiao canonical decomposition to form a sparse mean-reverting portfolio will be shown in the later part of this research notebook.


# ### Portmanteau Statistic


# Portmanteau statistic of order $p$ (Ljung and Box, 1978) tests if a process is a white noise.
# By definition, the portmanteau statistic is 0 if a process is a white noise. Therefore, maximizing mean-reversion strength
# is equivalent to minimizing the portmanteau statistic.
#
# The advantage of the portmanteau statistic over the Box-Tiao predictability is that this statistic requires no modeling
# assumptions. The disadvantage, on the other hand, is higher computational complexity. The estimate of the portmanteau
# statistic of order $p$ is given as follows:
#
# $$
# \hat{\phi}_p(y) = \frac{1}{p} \sum_{k=1}^p \Big( \frac{\mathbf{x}^T \gamma_k \mathbf{x}}{\mathbf{x}^T \gamma_0 \mathbf{x}} \Big)^2
# $$
#
# where $\gamma_k$ is the sample lag-$k$ autocovariance matrix, defined as:
#
# $$
# \begin{align*}
# \gamma_k & \equiv \frac{1}{T-k-1} \sum_{t=1}^{T-k}\tilde{S}_t \tilde{S}_{t+k}^T \\
# \tilde{S}_t & \equiv S_t - \frac{1}{T} \sum_{t=1}^T S_t
# \end{align*}
# $$


# ### Crossing Statistic


# Kedem and Yakowitz (1994) define the crossing statistic of a univariate process $x_t$ as the expected number of
# crosses around 0 per unit of time:
#
# $$
# \xi(x_t) = \mathbf{E} \Bigg[ \frac{\sum_{t=2}^T \mathbf{1}_{\{x_t x_{t-1} \leq 0 \}}}{T-1} \Bigg]
# $$
#
# For a stationary AR(1) process, the crossing statistic can be reformulated with the cosine formula:
#
# $$
# \xi(x_t) = \frac{\arccos (a)}{\pi}
# $$
#
# where $a$ is the first-order autocorrelation, or the AR(1) coefficient of the stationary AR(1) process, where $\lvert a \rvert < 1$. The function $y = \arccos (a)$ is monotonic decreasing with respect to $a$
# when $\lvert a \rvert < 1$. Therefore, stronger mean-reversion strength implies a greater crossing statistic,
# which in turn implies a smaller first-order autocorrelation. To extend this result to the multivariate case,
# Cuturi (2015) proposed to minimize $\mathbf{x}^T \gamma_1 \mathbf{x}$ and ensure that all absolute higher-order
# autocorrelations $\lvert \mathbf{x}^T \gamma_k \mathbf{x} \rvert, \, k > 1$ are small.


# ## Constructing the Sparse Mean-reverting Portfolio


# This section will showcase how ArbitrageLab help construct the sparse mean-reverting portfolios with the three approaches mentioned previously. All examples uses daily price data of 45 international equity ETFs from Jan 1st, 2016 to Jan 27th, 2021.


# ### Covariance Selection via Graphical LASSO and Structured VAR(1) Estimate via Penalized Regression


# The Box-Tiao canonical decomposition relies on estimates of both the covariance matrix $\Gamma_0$ and the VAR(1)
# coefficient matrix $A$ of the asset prices. Using an $\ell_1$-penalty, as shown in d'Aspremont (2011),
# is able to simultaneously obtain numerically stable estimates and isolate key idiosyncratic dependencies
# in the asset prices. The penalized estimates of $\Gamma_0$ and $A$ provides different perspective on the
# conditional dependencies and their graphical representations help cluster the assets into several smaller groups.


# #### Covariance Selection


# Covariance selection is a process where the maximum likelihood estimation of the covariance matrix $\Gamma_0$ is
# penalized by setting a certain number of coefficients in the inverse covariance matrix $\Gamma_0^{-1}$ to zero.
# Zeroes in $\Gamma_0^{-1}$ corresponds to conditionally independent assets in the model, and the penalized, or
# sparse, estimate of $\Gamma_0^{-1}$ is both numerically robust and indicative of the underlying structure of the
# asset price dynamics.
#
# The sparse estimate of $\Gamma_0^{-1}$ is obtained by solving the following optimization problem:
#
# $$
# \max_X \log \det X - \mathbf{Tr} (\Sigma X) - \alpha \lVert X \rVert_1
# $$
#
#
# where $\Sigma = \gamma_0$ is the sample covariance matrix, $\alpha> 0$ is the $\ell_1$-regularization
# parameter, and $\lVert X \rVert_1$ is the sum of the absolute value of all the matrix elements.


# #### Structured VAR(1) Estimate via Penalized Regression


# Recall that under a VAR(1) model assumption, the asset prices $S_t$ follow the following process:
#
# $$
#     S_t = S_{t-1} A + Z_t
# $$
#
# For most financial time series, the noise terms are correlated such that $Z_t \sim N(0, \Sigma)$, where the noise
# covariance is $\Sigma$. In this case, the VAR(1) coefficient matrix $A$ has to be directly estimated from
# the data. A structured (penalized), or sparse, estimate of $A$ can be obtained column-wise via a LASSO regression
# by minimizing the following objective function:
#
# $$
#     a_i = \argmin_x \lVert S_{it} - S_{t-1}x \rVert^2 + \lambda \lVert x \rVert_1
# $$
#
# where $a_i$ is a column of the matrix $A$, and $S_{it}$ is the price of asset $i$.
#
# The sparse estimate of $A$ can be also obtained by applying a more aggressive penalty under a multi-task LASSO
# model. The objective function being minimized is:
#
# $$
#     \argmin_A \lVert S_{t} - S_{t-1}A \rVert^2 + \alpha \sum_i \sqrt{\sum_j a_{ij}^2}
# $$
#
# where $a_{ij}$ is the element of the matrix $A$.
#
# The multi-task LASSO model will suppress the coefficients in an entire column to zero, but its estimate is less robust
# than the column-wise LASSO regression in terms of numerical stability. 


# #### Clustering


# If the Gaussian noise in the VAR(1) model is uncorrelated:
#
# $$
#     S_t = S_{t-1} A + Z_t, \; Z_t \sim N(0, \sigma \mathbf{I}), \, \sigma > 0
# $$
#
# then Gilbert (1994) has shown that the graph of the inverse covariance matrix $\Gamma_0^{-1}$ and the graph of
# $A^T A$ share the same structure, i.e. the graph of $\Gamma_0^{-1}$ and $A^T A$ are disconnected along
# the same clusters of assets.
#
# When the Gaussian noise $Z_t$ is correlated, however, the above relation is no longer valid, but it is still
# possible to find common clusters between the graph of penalized estimate of $\Gamma_0^{-1}$ and penalized estimate
# of $A^T A$. This will help find a much smaller investing universe for sparse mean-reverting portfolios. 


# #### Example


# Import libraries.
import networkx as nx
import numpy as np
import pandas as pd

import matplotlib.dates as mdates
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.sparse_mr_portfolio import SparseMeanReversionPortfolio

%matplotlib inline


# Load data.
etf_df = pd.read_csv("Country_ETF.csv", parse_dates=["Dates"])
etf_df.set_index("Dates", inplace=True)
etf_df.fillna(method='ffill', inplace=True)
etf_df.columns = [x.split()[0] for x in etf_df.columns]

# Construct the sparse mean-reverting portfolio selection class.
etf_sparse_portf = SparseMeanReversionPortfolio(etf_df)


# Check the data.
etf_sparse_portf.assets.head(5)


# Calculate the penalized estimates.
sparse_var_est = etf_sparse_portf.LASSO_VAR_fit(1.4, threshold=7, multi_task_lasso=True)
_, sparse_prec_est = etf_sparse_portf.covar_sparse_fit(0.89)

# Generate the clusters.
multi_LASSO_cluster_graph = etf_sparse_portf.find_clusters(sparse_prec_est, sparse_var_est)
multi_LASSO_clusters = list(sorted(nx.connected_components(multi_LASSO_cluster_graph), key=len, reverse=True))

# Check the first two clusters.
print("1st cluster: {}\nAsset count: {}".format(multi_LASSO_clusters[0], len(multi_LASSO_clusters[0])))
print("2nd cluster: {}\nAsset count: {}".format(multi_LASSO_clusters[1], len(multi_LASSO_clusters[1])))


# <div class="alert alert-block alert-info">
# <b>Tip:</b> Covariance selection determines the size of the clusters. When fitting the graphical LASSO model, apply a larger regularizer parameter to make the covariance matrix as sparse as possible such that smaller clusters are obtained. </div> 


# <div class="alert alert-block alert-info">
# <b>Tip:</b> Both graphical LASSO model and LASSO regression are sensitive to the scale of the data. Use standardized data for this step, otherwise the LASSO models will not be able to yield a reasonable, or even feasible, solution.</div> 


# The second cluster looks quite sparse. Now perform a Box-Tiao canonical decomposition on this small cluster.


# Construct another class on the cluster obtained previously.
small_etf_sparse_portf = SparseMeanReversionPortfolio(etf_df[multi_LASSO_clusters[1]])

# Check the data.
small_etf_sparse_portf.assets.head(5)


# Perform Box-Tiao canonical decomposition.
bt_weights = small_etf_sparse_portf.box_tiao()

# Retrieve the portfolio weights that corresponds to the smallest eigenvalue.
sparse_portf_weight1 = bt_weights[:, -1]

# Check weights and pretty print the results.
sparse_portf_weight_df = pd.DataFrame(sparse_portf_weight1.reshape(-1, 1))
sparse_portf_weight_df.columns = ['Weight']
sparse_portf_weight_df.index = small_etf_sparse_portf.assets.columns

sparse_portf_weight_df


# Retrieve the portfolio weights that yield the least predictability, i.e. strongest mean-reversion. Check the OU mean-reversion coefficient and half-life and plot the portfolio value in Figure 1.


# Form the portfolio.
sparse_portf1 = small_etf_sparse_portf.assets @ sparse_portf_weight1

# Fit an OU-model to see the mean-reversion speed parameter and half-life.
coeff, hl = small_etf_sparse_portf.mean_rev_coeff(sparse_portf_weight1, small_etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


def plot_portf(portf_data, title):
    years = mdates.YearLocator()  # every year
    months = mdates.MonthLocator()  # every month
    years_fmt = mdates.DateFormatter('%Y')

    fig, ax = plt.subplots(1,1, figsize=(15, 5), sharex=True)

    ax.xaxis.set_major_locator(years)
    ax.xaxis.set_major_formatter(years_fmt)
    ax.xaxis.set_minor_locator(months)
    ax.tick_params(axis='y', labelsize=14)
    ax.tick_params(axis='x', labelsize=12)

    ax.set_title(title, fontsize=18)
    ax.plot(portf_data, label='Weak Mean-reversion')
    ax.axhline(portf_data.mean(), linewidth=0.3, linestyle='--', color=(0, 0, 0, 0.85))
    ax.set_xlim(pd.Timestamp(2016, 1, 1), pd.Timestamp(2021, 2,1))

    plt.show()


plot_portf(sparse_portf1, "Sparse Portfolio from Covariance Selection and Structured VAR(1) Estimate")


# Half-life is around 9 days, which gives enough number of trades. Figure 1 also shows that the portfolio is decently mean-reverting. 


# ### Greedy Search


# It has been already shown in the previous section that constructing a mean-reverting portfolio from a set of assets using Box-Tiao canonical decomposition is equivalent to solving the generalized eigenvalue problem,
#
# $$
# \det (\lambda \Gamma_0 - A^T \Gamma_0 A) = 0
# $$
#
# and retrieve the eigenvector corresponding to the smallest eigenvalue. This generalized eigenvalue problem can be also
# written in the variational form as follows:
#
# $$
#     \lambda_{\text{min}}(A^T \Gamma_0 A, \Gamma_0) = \min_{\mathbf{x} \in \mathbb{R}^n} \frac{\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}}{\mathbf{x}^T \Gamma_0 \mathbf{x}}
# $$
#
# However, once the cardinality constraint has been added, the resulting sparse generalized eigenvalue problem is equivalent to subset selection, which is an NP-hard problem. 
#
# $$
# \begin{align*}
# \text{minimize } & \frac{\mathbf{x}^T A^T \Gamma_0 A \mathbf{x}}{\mathbf{x}^T \Gamma_0 \mathbf{x}} \\
# \text{subject to } & \lVert \mathbf{x} \rVert_0 = k \\
# & \lVert \mathbf{x} \rVert_2 = 1
# \end{align*}
# $$
#
# Since no polynomial time solutions are available to get the global optimal solution, greedy search is thus used to get good approximate solutions of the problem, which will have a polynomial time complexity. Greedy search builds the solution recursively, i.e. it always tries to add the asset that results in the least increase in predictability to the current "optimal" solution. Figure 2 demonstrates the working process of the greedy algorithm with a toy example.


from IPython.display import Image
Image(filename='images/greedy_demo.gif')


# #### Examples


# The example in this section uses data of all 45 ETFs and attempts to construct a sparse mean-reverting portfolio with greedy algorithm. The sample covariance matrix and the least-square VAR(1) estimate were used here to demonstrate that the greedy algorithm can work well even without the preprocessing step with covariance selection and penalized regression.


# Calculate least-square VAR(1) estimate and sample covariance.
full_var_est = etf_sparse_portf.least_square_VAR_fit()
full_cov_est = etf_sparse_portf.autocov(0, use_standardized=False)

# Use greedy algorithm to calculate portfolio weights.
greedy_weight = etf_sparse_portf.greedy_search(8, full_var_est, full_cov_est, maximize=False)

# Pretty print the weights.
greedy_weight_df = pd.DataFrame(greedy_weight.reshape(-1, 1))
greedy_weight_df.columns = ['Weight']
greedy_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
greedy_weight_df[greedy_weight_df['Weight'] != 0]


# Build the portfolio and check the OU model fit.
greedy_portf = etf_sparse_portf.assets @ greedy_weight

coeff, hl = etf_sparse_portf.mean_rev_coeff(greedy_weight.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(greedy_portf.squeeze(), "Sparse Portfolio Constructed by Greedy Algorithm")


# The sparse portfolio built by greedy algorithm has similar volatility to the previous one, but its mean-reversion is more stable, especially during the Coronavirus crash in 2020. The OU half-life of this portfolio is longer at 27.5 days, which suggests a lower trading frequency for a possible mean-reversion strategy.


# ### Convex Relaxation


# An alternative to greedy search is to relax the cardinality constraint and reformulate the original non-convex optimization problem
#
# $$
# \begin{align*}
# \text{minimize } & \frac{\mathbf{x}^T \mathbf{A} \mathbf{x}}{\mathbf{x}^T \mathbf{B} \mathbf{x}} \\
# \text{subject to } & \lVert \mathbf{x} \rVert_0 = k \\
# & \lVert \mathbf{x} \rVert_2 = 1
# \end{align*}    
# $$
#
# into a convex one. The concept "convex" means "when an optimal solution is found, then it is guaranteed to be the
# best solution". The convex optimization problem is formed in terms of the symmetric matrix $X = \mathbf{x} \mathbf{x}^T$:


# $$
# \begin{align*}
# \text{minimize } & \frac{\mathbf{Tr} (\mathbf{A} X)}{\mathbf{Tr} (\mathbf{B} X)} \\
# \text{subject to } & \mathbf{1}^T \lvert X \rvert \mathbf{1} \leq k \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# The objective function is the quotient of the traces of two matrices, which is only quasi-convex. Even if this quasi-convex optimization problem can be transformed into a convex optimization problem, or semidefinite programming (SDP) to be exact, with a change of variables, this formulation of SDP still suffers from a few drawbacks:
#
# 1. It has numerical stability issues;
# 2. It cannot proper handle volatility constraints;
# 3. It cannot optimize mean-reversion strength proxies other than predictability.
#
# Therefore, this module followed the regularizer form of the SDP proposed by Cuturi (2015) to mitigate these drawbacks.


# The predictability optimization SDP is as follows:
#
# $$
# \begin{align*}
# \text{minimize } & \mathbf{Tr} (\gamma_1 \gamma_0^{-1} \gamma_1^T X) + \rho \lVert X \rVert_1 \\
# \text{subject to } & \mathbf{Tr} (\gamma_0 X) \geq V \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# The portmanteau statistic optimization SDP is as follows:
# $$
# \begin{align*}
# \text{minimize } & \sum_{i=1}^p \mathbf{Tr} (\gamma_i X)^2 + \rho \lVert X \rVert_1 \\
# \text{subject to } & \mathbf{Tr} (\gamma_0 X) \geq V \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# The crossing statistic optimization SDP is as follows:
# $$
# \begin{align*}
# \text{minimize } & \mathbf{Tr}(\gamma_1 X) + \mu \sum_{i=2}^p \mathbf{Tr} (\gamma_i X)^2 + \rho \lVert X \rVert_1 \\
# \text{subject to } & \mathbf{Tr} (\gamma_0 X) \geq V \\
# & \mathbf{Tr} (X) = 1 \\
# & X \succeq 0
# \end{align*}
# $$


# where $\rho>0$ is the $\ell_1$-regularization parameter, $\mu>0$ is a specific regularization parameter
# for crossing statistic optimization, and $V > 0$ is the portfolio variance lower threshold.


# In some restricted cases, the convex relaxation are tight, which means the optimal solution of the SDP is exactly the
# optimal solution to the original non-convex problem. However, in most cases this correspondence is not guaranteed and the
# optimal solution of these SDPs has to be deflated into a rank one matrix $xx^T$ where $x$ can be considered
# as a good candidate for portfolio weights with the designated cardinality $k$. This module uses Truncated Power
# method (Yuan and Zhang, 2013) as the deflation method to retrieve the leading sparse vector of the optimal solution
# $X^*$ that has $k$ non-zero elements.


# #### Examples: Predictability


# The examples in the following section also uses data of all 45 ETFs to demonstrate the effectiveness of the convex relaxation framework.


# Solve the predictability SDP.
sdp_pred_vol_result = etf_sparse_portf.sdp_predictability_vol(rho=0.001, variance=5, 
                                                              max_iter=5000, use_standardized=False,
                                                              verbose=False)

# Deflate the optimization result into a weight vector.
sdp_pred_vol_weights = etf_sparse_portf.sparse_eigen_deflate(sdp_pred_vol_result, 8, verbose=False)


# <div class="alert alert-block alert-warning">
#
# <b>Note:</b> Running `sparse_eigen_deflate` method after getting the SDP result is a required step. In most cases, the SDP result is a full-rank matrix rather than a rank-one matrix, so it cannot be translated into a feasible portfolio weight vector with the desired cardinality unless `sparse_eigen_deflate` has been run immediately afterwards.
#
# </div>


# Pretty print the weights.
sdp_pred_vol_weight_df = pd.DataFrame(sdp_pred_vol_weights.reshape(-1, 1))
sdp_pred_vol_weight_df.columns = ['Weight']
sdp_pred_vol_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
sdp_pred_vol_weight_df[sdp_pred_vol_weight_df['Weight'] != 0]


# <div class="alert alert-block alert-info">
# <b>Tip:</b> Always check the weights to see if there are significantly smaller weights than the others. If yes, then this means the regularizer parameter was set too large for the desired cardinality. Reduce the regularizer parameter until all the weights are approximately at the same scale. </div> 


# Build the portfolio and check the OU model fit.
sdp_pred_vol_portf = etf_sparse_portf.assets @ sdp_pred_vol_weights

coeff, hl = etf_sparse_portf.mean_rev_coeff(sdp_pred_vol_weights.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(sdp_pred_vol_portf.squeeze(), "Sparse Portfolio Constructed by SDP framework, Minimizing Predictability")


# #### Examples: Portmanteau


# Solve the portmanteau SDP.
sdp_portmanteau_vol_result = etf_sparse_portf.sdp_portmanteau_vol(rho=0.001, variance=5, nlags=3,
                                                                  max_iter=10000, use_standardized=False,
                                                                  verbose=False)

# Deflate the optimization result into a weight vector.
sdp_portmanteau_vol_weights = etf_sparse_portf.sparse_eigen_deflate(sdp_portmanteau_vol_result, 8, verbose=False)


# <div class="alert alert-block alert-info">
#
# <b>Tip:</b> Use `verbose=True` to output `cvxpy` solver info to confirm if an accurate solution has been obtained. 
#
# </div> 


# Pretty print the weights.
sdp_portmanteau_vol_weight_df = pd.DataFrame(sdp_portmanteau_vol_weights.reshape(-1, 1))
sdp_portmanteau_vol_weight_df.columns = ['Weight']
sdp_portmanteau_vol_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
sdp_portmanteau_vol_weight_df[sdp_portmanteau_vol_weight_df['Weight'] != 0]


# Build the portfolio and check the OU model fit.
sdp_portmanteau_vol_portf = etf_sparse_portf.assets @ sdp_portmanteau_vol_weights

coeff, hl = etf_sparse_portf.mean_rev_coeff(sdp_portmanteau_vol_weights.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(sdp_portmanteau_vol_portf.squeeze(), 
           "Sparse Portfolio Constructed by SDP framework, Minimizing Portmanteau Statistic")


# #### Examples: Crossing Statistic


# Solve the crossing statistic SDP.
sdp_crossing_vol_result = etf_sparse_portf.sdp_crossing_vol(rho=0.002, mu=0.1, variance=5, nlags=4,
                                                            max_iter=10000, use_standardized=False,
                                                            verbose=False)

# Deflate the optimization result into a weight vector.
sdp_crossing_vol_weights = etf_sparse_portf.sparse_eigen_deflate(sdp_crossing_vol_result, 8, verbose=False)


# Pretty print the weights.
sdp_crossing_vol_weight_df = pd.DataFrame(sdp_crossing_vol_weights.reshape(-1, 1))
sdp_crossing_vol_weight_df.columns = ['Weight']
sdp_crossing_vol_weight_df.index = etf_sparse_portf.assets.columns

# Print only the non-zero weights.
sdp_crossing_vol_weight_df[sdp_crossing_vol_weight_df['Weight'] != 0]


# Build the portfolio and check the OU model fit.
sdp_crossing_vol_portf = etf_sparse_portf.assets @ sdp_crossing_vol_weights

coeff, hl = etf_sparse_portf.mean_rev_coeff(sdp_crossing_vol_weights.squeeze(), etf_sparse_portf.assets)
print("Mean-rev Coefficient: {}; half-life: {}".format(coeff, hl))


plot_portf(sdp_crossing_vol_portf.squeeze(), 
           "Sparse Portfolio Constructed by SDP framework, Minimizing Crossing Statistic")


# Minimizing portmanteau statistic and crossing statistic yielded similar mean-reverting portfolios, both of which have a slightly longer OU half-life than the greedy algorithm result. On the other hand, minimizing predictability has generated a noisier portfolio with a shorter OU half-life.


# ## Discussion


# The notebook has demonstrated how to use ArbitrageLab to construct sparse mean-reverting portfolios using covariance selection techniques and structured VAR(1) estimate, greedy algorithm, and convex relaxation framework with SDP. The module allows considerable flexibility and efficiency for sparse mean-reverting portfolio construction.
#
# Among the three approaches, greedy algorithm is the most robust due to its simplicity and the stableness of the portfolio it generates. As shown in the example, the portfolio generated by greedy algorithm was able to stand the test of a market crash. Moreover, greedy algorithm runs fast. Therefore, it is the top choice if given a completely new dataset.
#
# Convex relaxation framework has shown greater flexibility compared to the other two approaches as it allows addition of other constraints. The examples here has shown that a volatility lower bound could be added such that meaningful statistical arbitrage opportunities are available as the deviation from portfolio mean is greater. However, the mean-reversion strengths of the portfolios are slightly weaker than the one generated by the greedy algorithm, which is likely because the convex relaxation is not tight.
#
# Covariance selection and structured VAR(1) estimate approach can narrow down the investing universe effectively. The examples have shown that the 45 international equity ETFs have been readily clustered into a 14-asset and an 8-asset subset from which a sparse mean-reverting portfolio can be constructed using Box-Tiao canonical decomposition. Note that this approach can be used in conjunction with the greedy algorithm and convex relaxation approach. For example, greedy algorithm can be directly applied to the aforementioned 14-asset subset.


# ## Conclusion


# This research notebook has discussed in detail the theoretical background for solving sparse mean-reversion portfolio selection problem and how ArbitrageLab can help build them in three different approaches.
#
# ### Key Takeaways
#
# * Greedy algorithm is the most robust approach among the three. It is recommended to try greedy algorithm first when given a new dataset.
# * Convex relaxation is the most flexible approach among the three. Extra constraints can be added, for example, a volatility lower threshold. However, the quality of the solution may be compromised due to an extra deflation step to yield the sparse weight vector.
# * Covariance selection and structured VAR(1) estimate is more of a preprocessing step among the three. This approach can efficiently narrow down the investing universe. Box-Tiao canonical decomposition, greedy algorithm, and convex relaxation can be all applied afterwards on the resulting asset subsets.
# * This module helps select which assets to trade and allocate the capital for each selected asset. It does **NOT** help decide the mean-reversion trading strategy. The users can choose whichever strategies they may find suitable for the highly mean-reverting portfolio.


# ## References


# 1. [Cuturi, M. and d'Aspremont, A., 2015. Mean-reverting portfolios: Tradeoffs between sparsity and volatility. arXiv preprint arXiv:1509.05954.](https://arxiv.org/pdf/1509.05954.pdf)
# 2. [d'Aspremont, A., 2011. Identifying small mean-reverting portfolios. Quantitative Finance, 11(3), pp.351-364.](https://arxiv.org/pdf/0708.3048.pdf)
# 3. [Fogarasi, N. and Levendovszky, J., 2012. Improved parameter estimation and simple trading algorithm for sparse, mean-reverting portfolios. In Annales Univ. Sci. Budapest., Sect. Comp, 37, pp. 121-144.](http://www.hit.bme.hu/~fogarasi/Fogarasi12Improved.pdf)
# 4. [Gilbert, J.R., 1994. Predicting structure in sparse matrix computations. SIAM Journal on Matrix Analysis and Applications, 15(1), pp.62-79.](https://www.osti.gov/servlets/purl/6987948)
# 5. [Kedem, B. and Yakowitz, S., 1994. Time series analysis by higher order crossings (pp. 115-143). New York: IEEE press.](http://www2.math.umd.edu/~bnk/HOC.Japan02.pdf)
# 6. [Ljung, G.M. and Box, G.E., 1978. On a measure of lack of fit in time series models. Biometrika, 65(2), pp.297-303.](https://apps.dtic.mil/sti/pdfs/ADA049397.pdf)
# 7. [Natarajan, B.K., 1995. Sparse approximate solutions to linear systems. SIAM journal on computing, 24(2), pp.227-234.](https://epubs.siam.org/doi/abs/10.1137/S0097539792240406?journalCode=smjcat)
# 8. [Yuan, X.T. and Zhang, T., 2013. Truncated Power Method for Sparse Eigenvalue Problems. Journal of Machine Learning Research, 14(4).](https://www.jmlr.org/papers/volume14/yuan13a/yuan13a.pdf)



// ---------------------------------------------------

// mean_reversion.py
// arbitrage_research/Cointegration Approach/mean_reversion.py
# Generated from: mean_reversion.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Algorithmic Trading: Winning Strategies and Their Rationale__ _by_ Ernest P. Chan


# # Mean Reversion


# This description of the mean-reverting processes closely follows the work of _Ernest P. Chan_ __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146). 
#
# Additional information on the Engle-Granger cointegration test can be found in a paper by _Faik Bilgili_ __Stationarity and cointegration tests: Comparison of Engle-Granger and Johansen methodologies__ [available here](https://mpra.ub.uni-muenchen.de/75967/1/MPRA_paper_75967.pdf).


# ## Introduction


# Mean-reverting processes and events often occur in nature. Observations of the processes that have a mean-reverting nature tend to move to their average value over time. However, as mentioned in the work of E.P. Chan, most financial price series are not mean-reverting.
#
# The upside is that we can construct advanced financial instruments from multiple simple ones thus obtaining the desired property. Observation series (stock, commodity prices, etc.) that can be combined to achieve a mean-reverting process are called *cointegrating*. The approach described above allows us to use the properties of mean-reverting processes to generate profit.


# ## Augmented Dickey–Fuller (ADF) test
#
# According to Ernest P. Chan:
# "The mathematical description of a mean-reverting price series is that the change of the price series in the next period is proportional to the difference between the mean price and the current price. This gives rise to the ADF test, which tests whether we can reject the null hypothesis that the proportionality
# constant is zero."
#
# The ADF test is based on the idea that the current price level gives us information about the future price level: if it's lower than the mean, the next move will be upwards, and vice versa.
#
# The ADF test uses the linear model that describes the price changes as follows:
#
# $$\Delta y(t) = \lambda y(t-1) + \mu + \beta t + \alpha_1 \Delta y(t-1) + ... \alpha_k \Delta y(t-k) + \epsilon_t$$
#
# where $\Delta y(t) \equiv y(t) - y(t-1)$, $\Delta y(t) \equiv y(t-1) - y(t-2)$, ...
#
# The hypothesis that is being tested is: $\lambda = 0$. For simplicity we assume the drift term to be zero ($\beta = 0$). If we reject the hypothesis, this means that the next price move depends on the current price level.


# ## The Half-life period
#
# Mean reversion tests, such as ADF usually require at least 90 percent certainty. But in practice, we can create strategies that are profitable even at lower certainty levels. The measure $\lambda$ can be used to calculate the *half-life*, which indicates how long it takes for a price to mean revert:
#
# $$Half-life = -log(2) / \lambda$$
#
# Furthermore, we can see that if the $\lambda$ value is positive, the price series are not mean-reverting. If it's close to zero, the half-life is very long and the strategy won't be profitable due to slow mean reversion.
#
# Half-life period can be helpful to determine some of the parameters to use in the trading strategy. Say, if the half-life period is 20 days, then using 5 days backward-looking window for moving average or volatility calculation may not give the best results.
#
# The most common approach is to use two cointegrated price series to construct a portfolio. This is done by simultaneously going long on one asset and short on the other, with an appropriate capital allocation for each asset. This approach is also called a "pairs trading strategy". However, the approach can be extended to three and more assets.


# ## Johansen cointegration test
#
# This is one of the most widely used cointegration tests, it's upside is that, first it allows multiple price series for stationarity testing, and second it provides hedge ratios for price series used to combine elements into a stationary portfolio.
#
# To understand how to test the cointegration of more than two variables, we can transform the equation used in the ADF test to a vector form. So $y(t)$ would be vectors representing multiple price series, and the $\lambda$ and $\alpha$ are matrices. We also assume that the drift term is zero ($\beta t = 0$). So the equation can be rewritten as follows:
#
# $$\Delta Y(t) = \Lambda Y(t-1) + M + A_1 \Delta Y(t-1) + ... + A_k \Delta Y(t-k) + \epsilon_t$$


# This way we can test the hypothesis of $\Lambda = 0$, in which case, we don't have cointegration present. Denoting the rank of the obtained matrix $\Lambda$ as $r$ and the number of price series as $n$, the number of independent portfolios that can be formed is equal to $r$.
#
# The Johansen test calculates the $r$ and tests the hypotheses of $r = 0$ (cointegrating relationship exists), $r \le 1$, ..., $r \le n - 1$. In case all the above hypotheses are rejected, the result is that $r = n$ and the eigenvectors of the $\Lambda$ can be used as hedge ratios to construct a mean-reverting portfolio.
#
# Note that the Johansen test is independent of the order of the price series, in contrast to the CADF test.


# ## Engle-Granger cointegration test
#
#
# The cointegration testing approach proposed by Engle-Granger allows us to test whether two or more price series are cointegrated of a given order.
#
# The Engle-Granger cointegration test is performed as follows:
#
# - First, we need to determine the order of integration of variables $x$ and $y$
#   (or $y_{1}, y_{2}, ...$ in case of more than two variables). If they are integrated of the same order, we can apply the cointegration test.
# - Next, if the variables are integrated of order one at the previous step, the following regressions can be performed:
#
# $$x_t = a_0 + a_1 y_t + e_{1,t},$$


# $$y_t = b_0 + b_1 x_t + e_{2,t}$$
#
# - Finally we run the following regressions and test for unit root for each equation:
#
# $$\Delta e_{1,t} = a_1 e_{1, t-1} + v_{1, t},$$


# $$\Delta e_{2,t} = a_2 e_{2, t-1} + v_{2, t}$$


# If we cannot reject the null hypotheses that $|a_1| = 0$ and $|a_2| = 0$, we cannot reject the hypothesis that the variables are not cointegrated.
#
# The hedge ratios for constructing a mean-reverting portfolio in the case of the Engle-Granger test are set to $1$ for the $x$ variable and the coefficient $-a_1$ for the $y$ variable (or $-a_1, -a_2, ..$ in case of multiple $y_i$ price series).
#
# The Engle-Granger cointegration test implemented in the ArbitrageLab package assumes that the first step of the algorithm is passed and that the variables are integrated of order one.


# ## Bollinger Bands trading strategy
#
# By using the Bollinger bands on the Z-scores from the provided spread, we can construct a trading strategy.
# The Z-score is calculated as a normalized deviation of the spread value from its moving average.
# The formula can be written as follows:
#
# $$Zscore_{t} = \frac{S_{t} - MA(S_{t}, T_{MA})}{std(S_{t}, T_{std})}$$
#
# Where:
#
# - $S_{t}$ is the spread value at time $t$.
#
# - $MA(S_{t}, T_{MA})$ is the moving average of the spread calculated
#   using a backward-looking $T_{MA}$ window.
#
# - $std(S_{t}, T_{std})$ is the rolling standard deviation of the spread
#   calculated using a backward-looking $T_{std}$ window.
#
# The idea is to enter a position only when the price deviates by more than *entryZscore* standard deviations
# from the mean. This parameter can be optimized in a training set.
#
# Also, the look-back windows for calculating the mean and the standard deviation are the parameters that
# can be optimized. We can later exit the strategy when the price reverts to the *exitZscore* standard
# deviations from the mean (*exitZscore* $<$ *entryZscore*).
#
# If *exitZscore* $= -$ *entryZscore*, we will exit when the price moves beyond the opposite band,
# also triggering a trading signal of the opposite sign. At either time we can have either zero or one
# unit (long or short) invested, so capital allocation is easy.
#
# If the look-back window is short and we set a small *entryZscore* and *exitZscore*, the holding period
# will be shorter and we get more round trip trades and generally higher profits.
#
# The strategy object is initialized with a window for a simple moving average, a window for
# simple moving st. deviation, and entry and exit label Z-Scores.
#
# The `update_spread_value` method allows adding new spread values one by one - when they are available.
# At each stage, the `check_entry_signal` method checks if the trade should be entered according to the
# above-described logic. If the trade should be opened, it can be added to the internal dictionary using the
# `add_trade` method.
#
# As well, the `update_trades` method can be used to check if any trades should be closed.
# If so, the internal dictionaries are updated, and the list of the closed trades at this stage is returned.


# ## Usage of the Algorithms


# Let's use the above tools on real data. 
#
# First, we will use the cointegration tests on datasets to determine if it's possible to construct a mean-reverting portfolio. Then we will create such a portfolio, find its half-lif, and run trading strategies on it. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# Following the example in the Optimal Mean Reversion module, we will use Gold Shares ETF (GLD), Gold Miners ETF (GDX), and Junior Gold Miners ETF (GDXJ) to construct a portfolio of three elements.


# Loading data
train_data =  yf.download("GLD GDX GDXJ", start="2016-01-01", end="2018-01-01")
test_data =  yf.download("GLD GDX GDXJ", start="2018-01-02", end="2020-01-01")

# Taking close prices for chosen instruments
train_three_elements = train_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

test_three_elements = test_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

# Looking at the downloaded data
train_three_elements.head()


# ### Johansen test


# Now let's run the cointegration tests and analyze the results.


# Initialising an object containing needed methods
j_portfolio = al.cointegration_approach.JohansenPortfolio()

# Fitting the data on a dataset of three elements with constant term
j_portfolio.fit(train_three_elements, det_order=0)

# Getting results of the eigenvalue and trace Johansen tests
j_eigenvalue_statistics = j_portfolio.johansen_eigen_statistic
j_trace_statistics = j_portfolio.johansen_trace_statistic
j_cointegration_vectors = j_portfolio.cointegration_vectors
j_hedge_ratios = j_portfolio.hedge_ratios


# First, the eigenvalue statistic test
j_eigenvalue_statistics


# Using the eigenvalue statistic test, we can see that our eigenvalue statistics are above the 95% significance level values for two elements - GLD and GDXJ. And they are all above the 90% significance level values.
#
# So at a 90% significance level, three elements are cointegrated, we can construct three possible mean-reverting portfolios using the coefficients from the 
# *j_cointegration_vectors* variable.


# Next, the alternative trace statistic test
j_trace_statistics


# Cointegration vectors
j_cointegration_vectors


# We have slightly different results for the trace statistic test. Here, all three trace statistics are above the 95% significance level values.
#
# According to this test, at a 95% significance level, three elements are cointegrated, we can construct three possible mean-reverting portfolios using the hedge ratios  from the same *j_hedge_ratios* variable.


# Hedge ratios to construct a mean-reverting spread 
j_hedge_ratios


# ### Engle-Granger test


# Initialising an object containing needed methods
eg_portfolio = al.cointegration_approach.EngleGrangerPortfolio()

# Fitting the data on a dataset of three elements with constant term
eg_portfolio.fit(train_three_elements, add_constant=True)

# Getting results of the Engle-Granger test
eg_adf_statistics = eg_portfolio.adf_statistics
eg_cointegration_vectors = eg_portfolio.cointegration_vectors
eg_hedge_ratios = eg_portfolio.hedge_ratios


# Looking at the statistic from the last step of the Engle-Granger test
eg_adf_statistics


# Cointegration vectors
eg_cointegration_vectors


# Using the ADF statistic test output, we can see that our statistic is above the 95% significance level value.
#
# So at a 95% significance level, our elements are cointegrated, we can construct a mean-reverting portfolio using the coefficients from the *eg_hedge_ratios* variable.


eg_hedge_ratios


# As described in the theoretical part, the coefficient for the first element is $1$, while other two are equal to negative regression coefficients.


# ### Constructing spread series


# For constructing spread series it's advised to use the functionality of the Hedge Ratios module.


from arbitragelab.hedge_ratios import (get_ols_hedge_ratio, get_tls_hedge_ratio,
                                       get_johansen_hedge_ratio, get_box_tiao_hedge_ratio,
                                       get_minimum_hl_hedge_ratio, get_adf_optimal_hedge_ratio,
                                       construct_spread)


# Calculating hedge ratios using the supported algorithms
ols_hedge_ratio, _, _, _ = get_ols_hedge_ratio(train_three_elements, dependent_variable='GLD',
                                               add_constant=False)
print(f'OLS hedge ratio for SPY/QQQ spread is {ols_hedge_ratio}')

tls_hedge_ratio, _, _, _ = get_tls_hedge_ratio(train_three_elements, dependent_variable='GLD')
print(f'TLS hedge ratio for SPY/QQQ spread is {tls_hedge_ratio}')

joh_hedge_ratio, _, _, _ = get_johansen_hedge_ratio(train_three_elements, dependent_variable='GLD')
print(f'Johansen hedge ratio for SPY/QQQ spread is {joh_hedge_ratio}')

box_tiao_hedge_ratio, _, _, _ = get_box_tiao_hedge_ratio(train_three_elements, dependent_variable='GLD')
print(f'Box-Tiao hedge ratio for SPY/QQQ spread is {box_tiao_hedge_ratio}')

hl_hedge_ratio, _, _, _, opt_object = get_minimum_hl_hedge_ratio(train_three_elements,
                                                                 dependent_variable='GLD')
print(f'Minimum HL hedge ratio for SPY/QQQ spread is {hl_hedge_ratio}')

adf_hedge_ratio, _, _, _, opt_object = get_adf_optimal_hedge_ratio(train_three_elements,
                                                                   dependent_variable='GLD')
print(f'Minimum ADF t-statistic hedge ratio for SPY/QQQ spread is {adf_hedge_ratio}')



# Let's use the Johansen hedge ratio to construct a spread.


# Constructing spread
j_spread = construct_spread(test_three_elements, hedge_ratios=joh_hedge_ratio)


# Plotting Johansen spread series
j_spread.plot(title='Johansen spread series', figsize=(10,5));


# Spreads using other hedge ratios can be constructed in the same way.


# ### Calculating the half-life of obtained portfolios 


# Importing the needed function
from arbitragelab.cointegration_approach import get_half_life_of_mean_reversion


j_half_life = get_half_life_of_mean_reversion(j_spread)

print('Half-life of the Johansen spread is', j_half_life, 'days.')


# The half-life of the Johansen spread is relatively short.
#
# The half-life calculation can be used to determine which spread poses more interest for trading as it is reverting to its mean value faster, allowing for more potential trades to be opened over the same time period.


# ### Applying Bollinger Bands trading strategy


# This trading strategy requires defining entry and exit Z-scores. Let's choose the entry Z-score - upon which the strategy will enter a short position, going short one unit of a portfolio. This position will be closed once the exit Z-score delta will be traveled in the opposite direction.
#
# In our example, we will be using an entry Z-score of 2.5 and an exit Z-score delta of 3.


# Importing the needed function
from arbitragelab.trading import BollingerBandsTradingRule


# Creating a strategy
strategy = BollingerBandsTradingRule(sma_window=20, std_window=20,
                                     entry_z_score=2.5, exit_z_score_delta=3)


# Now we feed Johansen spread values one by one to the strategy object.
#
# With a live data feed the spread value would be provided to the strategy object in the same way.


# Adding initial spread value
strategy.update_spread_value(j_spread[0])

# Feeding spread values to the strategy one by one
for time, value in j_spread.iteritems():
    strategy.update_spread_value(value)

    # Checking if logic for opening a trade is triggered
    trade, side = strategy.check_entry_signal()

    # Adding a trade if we decide to trade signal
    if trade:
        strategy.add_trade(start_timestamp=time, side_prediction=side)

    # Update trades, close if logic is triggered
    close = strategy.update_trades(update_timestamp=time)


# Now we can see which positions were opened and closed during this time period.


# Currently open trades in a strategy
open_trades = strategy.open_trades

open_trades


# So all open trades were closed.


# Checking all closed trades
closed_trades = strategy.closed_trades

closed_trades


# A total of 12 trades were open during this period.
#
# ### Strategy outputs
#
# We can see the following data:
# * Dictionary key:
#     * Timestamp at which the trade was opened
# * Dctionary value:
#     * t1: Timestamp at which the trade was closed
#     * pt: Z-score at which the trade was closed
#     * uuid: Trade ID that can be provided for each trade
#     * start_value: Spread value at which spread was opened
#     * end_value: Spread value at which spread was closed
#     * side: Side of the trade '-1' for short and '1' for long
#     * initial_z_score: Z-score at which the trade was opened


# We can see that each of the closed trades during this period was profitable - the spread value change was in the right direction, according to the trade side.
#
# This strategy can be furhter adjusted by choosing different entry and exit Z-score deltas, by optimizing them in a training set first.


# ## Conclusion


# This notebook describes the Mean Reversion module tools - cointegration tests and trading strategies. Also, it shows how these tools can be used on real data and that they can output profitable trading signals.
#
# The algorithms and the descriptions used in this notebook were described by _Ernest P. Chan_ in the book __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146).
#
# Key takeaways from the notebook:
#
# - Mean-reverting processes tend to move to their average value over time. However, most financial price series are not mean-reverting.
# - Financial instruments can be combined in a portfolio that has mean-reverting properties.
# - ADF test allows testing if the next price move depends on the current price level.
# - Half-life of a mean-reverting process indicates how long it takes for a price to mean revert.
# - Both Johansen and Engle-Granger cointegration tests allow testing if we can construct a mean-reverting portfolio out of multiple price series, and if so, which combination of these elements should we use to construct a portfolio.
# - Constructed portfolios can be used in the Linear and Bollinger Bands trading strategies.
# - The Bollinger Bands  strategy  deals with the issues present in the linear strategy: infinitesimal portfolio rebalances and no predefined buying power.



// ---------------------------------------------------

// minimum_profit_optimization.py
// arbitrage_research/Cointegration Approach/minimum_profit_optimization.py
# Generated from: minimum_profit_optimization.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# Reference:
# 1. [Lin, Y.-X., McCrae, M., and Gulati, C., 2006. **Loss protection in pairs trading through minimum profit bounds: a cointegration approach**](http://downloads.hindawi.com/archive/2006/073803.pdf)
#
# 2. [Puspaningrum, H., Lin, Y.-X., and Gulati, C. M. 2010. **Finding the optimal pre-set boundaries for pairs trading strategy based on cointegration technique**](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1040&context=cssmwp)


# # Minimum Profit Optimization


# ## Introduction


# A common pairs trading strategy is to "fade the spread", i.e. to open a trade when the spread is sufficiently far away
# from its equilibrium in anticipation of the spread reverting to the mean. Within the context of cointegration, the
# spread refers to cointegration error, and in the remainder of this documentation "spread" and "cointegration error" will
# be used interchangeably.
#
# The concept of "sufficiently far away from the equilibrium of the spread", i.e. a pre-set boundary chosen to open a trade, need to be quantified. This boundary can affect the minimum total profit (MTP) over a specific trading horizon. The higher the pre-set boundary for opening trades, the higher the profit per trade but the lower the trade numbers. The opposite applies to lowering the boundary values. The number of trades over a specified trading horizon is determined jointly by the average trade duration and the average inter-trade interval.
#
# In this notebook, we will demonstrate how to optimize the pre-set boundary that would maximize the MTP for cointegration error
# following an AR(1) process by numerically estimating the average trade duration, average inter-trade interval, and the
# average number of trades based on the mean first-passage time.


# ## Model Assumptions


# Before getting into the nuts and bolts of the optimization process, the assumptions of the cointegration pairs trading strategy need to be articulated.
#
# - The price of two assets ($S_1$ and $S_2$) are cointegrated over the relevant time period, which includes both in-sample and out-of-sample (trading) period.
# - The cointegration error follows a stationary AR(1) process.
# - The cointegration error is symmetrically distributed so that we can apply the optimal boundary on both sides of the mean.
# - Short sales are permitted or possible through a broker and there is no interest charged for the short sales and no cost for trading.
# - The cointegration coefficient $\beta > 0$, where a cointegration relationship is defined as:
#
# \begin{equation}
#     P_{S_1,t} - \beta P_{S_2,t} = \varepsilon_t
# \end{equation}


# ## Minimum Profit per Trade


# Denote a trade opened when the cointegration error $\varepsilon_t$ overshoots the pre-set upper boundary $U$
# as a **U-trade**, and similarly, a trade opened when $\varepsilon_t$ falls through the pre-set lower
# boundary $L$ as an **L-trade**. Without loss of generality, it can be assumed that the mean of $\varepsilon_t$ is 0. 


# ### Minimum Profit per U-trade


# The setup of a U-trade is as follows:
#
# - When $\varepsilon_t \geq U$ at $t_o$, we open a trade by selling $N$ of asset $S_1$ and buying $\beta N$ of asset $S_2$.
# - When $\varepsilon_t \leq 0$ at $t_c$, we close the trade.
#
# The profit per trade would thus be:
#
# \begin{equation}
#     P = N (P_{S_1, t_o} - P_{S_1, t_c}) + \beta N (P_{S_2, t_c} - P_{S_2, t_o})
# \end{equation}
#
# Since the two assets are cointegrated during the trade period, substitute the cointegration relationship into
# the above equation and derive the following:
#
# \begin{align*}
#     P & =  N (P_{S_1, t_o} - P_{S_1, t_c}) + \beta N (P_{S_2, t_c} - P_{S_2, t_o}) \\
#       & =  N (P_{S_1, t_o} - \beta P_{S_2, t_o}) - N (P_{S_1, t_c} - \beta P_{S_2, t_c}) \\
#       & =  N \varepsilon_{t_o} - N \varepsilon_{t_c} \\
#       & \geq N U
# \end{align*}


# ### Minimum Profit per L-trade


# The setup of an L-trade is as follows:
#
# - When $\varepsilon_t \leq -U$ at $t_o$, we open a trade by buying $N$ of asset $S_1$ and selling $\beta N$ of asset $S_2$.
# - When $\varepsilon_t \geq 0$ at $t_c$, we close the trade.
#
# The profit per trade would thus be:
#
# \begin{equation}
#     P = N (P_{S_1, t_c} - P_{S_1, t_o}) + \beta N (P_{S_2, t_o} - P_{S_2, t_c})
# \end{equation}
#
# Since the two assets are cointegrated during the trade period, substitute the cointegration relationship into
# the above equation and derive the following:
#
# \begin{align*}
#     P & =  N (P_{S_1, t_c} - P_{S_1, t_o}) + \beta N (P_{S_2, t_o} - P_{S_2, t_c}) \\
#       & =  N (P_{S_1, t_c} - \beta P_{S_2, t_c}) - N (P_{S_1, t_o} - \beta P_{S_2, t_o}) \\
#       & =  N \varepsilon_{t_c} - N \varepsilon_{t_o} \\
#       & \geq N U
# \end{align*}


# **Both U-trades and L-trades would make a minimum profit per trade of $U$ if one unit of the cointegrated pair ($N=1$) was traded.**


# ## Minimum Total Profit (MTP)


# Based on the assumption that the boundary to open trades would be symmetrically applied on both sides of the spread mean, we will focus only on U-trade in this section. We can simply double the results to take L-trades into account.
#
# After deriving the minimum profit per U-trade, the next question of interest is the MTP over a trading horizon $[0,T]$. We would like to find the optimal boundary $U$ that can allow enough trades within the trading horizon as well as maximize the total profit. Define MTP as a function of the boundary $U$ as follows.
#
# \begin{equation}
#     MTP(U) = \Big( \frac{T}{{TD}_U + I_U} - 1 \Big) U
# \end{equation}
#
# where ${TD}_U$ is the trade duration and $I_U$ is the inter-trade interval. The derivation from the previous section proved that the minimum profit of one trade is $U$. Therefore, the number of U-trades within the trading horizon is given by:
#
# \begin{equation}
#     \frac{T}{{TD}_U + I_U} - 1
# \end{equation}


# According to the model assumptions, the cointegration error $\varepsilon_t$ follows a stationary AR(1) process. 
#
# \begin{equation}
# \varepsilon_t = \phi \varepsilon_{t-1} + a_t \qquad a_t \sim N(0, \sigma_a^2) \text{ i.i.d}
# \end{equation}


# Without loss of generality, we assume $E(\varepsilon_t) = 0$. Then ${TD}_U$ can be estimated by calculating the average time of $\varepsilon_t$ to pass 0 for the first time given the initial value of $\varepsilon_{t} = U$.
#
# \begin{equation}
#     {TD}_U = E(\mathcal{T}_{0, \infty}(U)) = \lim_{b \to \infty} \frac{1}{\sqrt{2 \pi} \sigma_a} \int_0^b E(\mathcal{T}_{0, b}(s)) \text{ exp} \Big( - \frac{(s- \phi U)^2}{2 \sigma_a^2} \Big) ds + 1
# \end{equation}


# Similarly, $I_U$ can be estimated by calculating the average time of $\varepsilon_t$ to pass $U$ for the first time given the initial value of $\varepsilon_t = 0$.
#
# \begin{equation}
#     I_U = E(\mathcal{T}_{- \infty, U}(0)) = \lim_{-b \to - \infty} \frac{1}{\sqrt{2 \pi} \sigma_a} \int_{-b}^U E(\mathcal{T}_{-b, U}(s)) \text{ exp} \Big( - \frac{s^2}{2 \sigma_a^2} \Big) ds + 1
# \end{equation}


# ### Numerically Estimating the Integral in the Mean First-passage Time of an AR(1) Process


# The crux of the optimization algorithm is thus calculating the above two integrals.
#
# Consider a stationary AR(1) process:
#
# \begin{equation}
#     Y_t = \phi Y_{t-1} + \xi_t
# \end{equation}
#
# where $-1 < \phi < 1$, and
# \begin{equation}
#     \xi_t \sim N(0, \sigma_{\xi}^2) \text{ i.i.d}
# \end{equation}
#
# The mean first-passage time over the interval $\lbrack a, b \rbrack$ of $Y_t$, starting at initial state $y_0 \in \lbrack a, b \rbrack$, which is denoted by $E(\mathcal{T}_{a,b}(y_0))$, is given by
#
# \begin{equation}
#     E(\mathcal{T}_{a,b}(y_0)) = \frac{1}{\sqrt{2 \pi}\sigma_{\xi}}\int_a^b E(\mathcal{T}_{a,b}(u)) \text{ exp} \Big( - \frac{(u-\phi y_0)^2}{2 \sigma_{\xi}^2} \Big) du + 1
# \end{equation}


# This integral equation can be solved numerically using the Nystrom method, i.e. by solving the following linear equations:
#
# $$
# \begin{pmatrix}
#     1 - K(u_0, u_0) & -K(u_0, u_1) & \ldots & -K(u_0, u_n) \\
#     -K(u_1, u_0) & 1 - K(u_1, u_1) & \ldots & -K(u_1, u_n) \\
#     \vdots & \vdots & \vdots & \vdots \\
#     -K(u_n, u_0) & -K(u_n, u_1) & \ldots & 1-K(u_n, u_n)
# \end{pmatrix}
# \begin{pmatrix}
#     E_n(\mathcal{T}_{a,b}(u_0)) \\
#     E_n(\mathcal{T}_{a,b}(u_1)) \\
#     \vdots \\
#     E_n(\mathcal{T}_{a,b}(u_n))
# \end{pmatrix}
# =   
# \begin{pmatrix}
#     1 \\
#     1 \\
#     \vdots \\
#     1 \\
# \end{pmatrix} 
# $$
#
# where $E_n(\mathcal{T}_{a,b}(u_0))$ is a discretized estimate of the integral, and the Gaussian kernel function $K(u_i, u_j)$ is defined as:
#
# \begin{equation}
#     K(u_i, u_j) = \frac{h}{2 \sqrt{2 \pi} \sigma_{\xi}} w_j  \text{ exp} \Big( - \frac{(u_j - \phi u_i)^2}{2 \sigma_{\xi}^2} \Big)
# \end{equation}
#
# and the weight $w_j$ is defined by the trapezoid integration rule:
#
# \begin{equation}
#     w_j = \begin{cases}
#     1 & j = 0 \text{ and } j = n \\
#     2 & 0 < j < n, j \in \mathbb{N}
#     \end{cases}
# \end{equation}
#
# The time complexity for solving the above linear equation system is $O(n^3)$ (see [here](https://www.netlib.org/lapack/lug/node71.html) for an introduction of the time complexity of `numpy.linalg.solve` ), which is the most time-consuming part of this procedure.


# Note that the integrals in the definition of ${TD}_U$ and $I_U$ the integral limit goes to infinity. To approximate the infinity limit, the following stylized fact proves to be useful: for a stationary AR(1) process $\{ \varepsilon_t \}$, the probability that the absolute value of the process $\vert \varepsilon_t \vert$ is greater than 5 times the standard deviation of the process $5 \sigma_{\varepsilon}$ is close to 0. Therefore, $5 \sigma_{\varepsilon}$ can be used as an approximation of the infinity limit in the integrals.


# ## Optimization


# Based on the above discussion, the numerical algorithm to optimize the pre-set boundary that maximizes MTP can be readily given.
#
# 1. Perform Engle-Granger or Johansen test to derive the cointegration coefficient $\beta$.<br>
# 2. Fit the cointegration error $\varepsilon_t$ to an AR(1) process and retrieve the AR(1) coefficient and the fitted residual.<br>
# 3. Calculate the standard deviation of cointegration error ($\sigma_{\varepsilon}$) and the fitted residual ($\sigma_a$).<br>
# 4. Generate a sequence of pre-set upper bounds $U_i$, where $U_i = i \times 0.01, \> i = 0, \ldots, b/0.01$, and $b = 5 \sigma_{\varepsilon}$.<br>
# 5. For each $U_i$,<br>
#   a. Calculate ${TD}_{U_i}$.<br>
#   b. Calculate $I_{U_i}$. *Note: this is the main bottleneck of the optimization speed.*<br>
#   c. Calculate $MTP(U_i)$.<br>
# 6. Find $U^{*}$ such that $MTP(U^{*})$ is the maximum.<br>
# 7. Set a desired minimum profit $K \geq U^{*}$ and calculate the number of assets to trade according to the following equations:<br>
# \begin{eqnarray}
# N_{S_2} = \Big \lceil \frac{K \beta}{U^{*}} \Big \rceil \\
# N_{S_1} = \Big \lceil \frac{N_{S_2}}{\beta} \Big \rceil
# \end{eqnarray}


# ## Want to Test with Simulated Data First?


# We also provided the functionality of simulating cointegrated series in case there is trouble finding cointegrated asset pairs from empirical data. 
#
# The simulations are based on the following cointegration model:
#
# \begin{gather*}
#     P_{S_1}(t) + \beta P_{S_2}(t) = \varepsilon_t \\
#     P_{S_2}(t) - P_{S_2}(t-1) = e_t
# \end{gather*}


# where $\varepsilon_t$ and $e_t$ are AR(1) processes.
#
# \begin{eqnarray*}
# \varepsilon_t - \phi_1 \varepsilon_{t-1} = c_1 + \delta_{1,t} \qquad \delta_{1,t} \sim N(0, \sigma_1^2) \\
# e_t - \phi_2 e_{t-1} = c_2 + \delta_{2,t} \qquad \delta_{2,t} \sim N(0, \sigma_2^2)
# \end{eqnarray*}


# The module allows simulation of multiple cointegrated series all at once.


# ## Usage of the Algorithms


# ### Cointegration Simulation
#
# Firstly, we will showcase the usage of the simulation module.
#
# The following codeblocks will simulate a batch of cointegrated series that are defined by the following parameters.


# \begin{eqnarray*}
#             \phi_1 & = & 0.95 \\
#             c_1 & = & 1.5 \\
#             \sigma_1 & = & 0.5 \\
#             \phi2 & = & 0.9 \\
#             c_2 & = & 0.05 \\
#             \sigma_2 & = & 1.0 \\
#             \beta & = & -0.6 \\
# \end{eqnarray*}


# Import libraries
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.coint_sim import CointegrationSimulation
from arbitragelab.cointegration_approach.minimum_profit import MinimumProfit

# Import helper module for trading simulation
from trading_simulation import TradingSim


# Initialize the data simulator
# Generate 20 cointegrated series, each of the series have 250 data points
coint_simulator = CointegrationSimulation(20, 250)


# Set the parameters for e_t
price_params = {
    "ar_coeff": 0.95,
    "white_noise_var": 0.5,
    "constant_trend": 1.5}

# Set the parameters for epsilon_t
coint_params = {
    "ar_coeff": 0.9,
    "white_noise_var": 1.,
    "constant_trend": 0.05,
    "beta": -0.6}

# Load the parameters
coint_simulator.load_params(price_params, target='price')
coint_simulator.load_params(coint_params, target='coint')


# The simulation can be done either by purely following the recurrence relation given by the AR(1) process, or using the `statsmodels` package. Here we use the `statsmodels` package to simulate the cointegrated series according to the above-mentioned parameters.


# Using statsmodels package to simulate the cointegrated series
s1_series, s2_series, coint_errors = coint_simulator.simulate_coint(initial_price=100., use_statsmodels=True)


# Plot the simulated cointegrated series and the cointegration error.


# Plot an example of the simulated cointegrated series and cointegration error
coint_sim_fig = coint_simulator.plot_coint_series(s1_series[:, 0], s2_series[:, 0], coint_errors[:, 0])


# ### Minimum Profit Optimization


# In this section, the usage of the minimum profit optimization and the resulting trading strategy will be presented with empirical data. The assets involved in this example are two S&P 500 stocks, Ametek Inc. (Ticker: AME) and Dover Corp. (Ticker: DOV). 


# Read price series data, set date as index
data = pd.read_csv('AME-DOV.csv', parse_dates=['Date'])
data.set_index('Date', inplace=True)

# Show how the price data is structured
data.head(5)


# Although the paper suggested the usage of a 12-month training period and a 6-month trading period with daily data, our results found that the choice of 12-month and 6-month is arbitrary and the performance was not affected if a longer training period was used. In the following demonstration, daily data of both stocks from Jan 4th, 2016 to Dec 31st, 2018 were used for training, and the trading period starts from Jan 2nd, 2019 to Nov 23th, 2020.


# Split the entire price history into training and trading period
# 2019 Jan 1st was a market holiday so it is a perfect split point
train_df = data[data.index < pd.Timestamp(2019, 1, 1)]
trade_df = data[data.index >= pd.Timestamp(2019, 1, 1)]


# Initialize the minimum profit optimizer
optimizer = MinimumProfit()

# Set the trainging deatset
optimizer.set_train_dataset(train_df)


# The next step is to test if the two stocks stayed cointegrated during the training period. Here we used the Engle-Granger test to test for cointegration and retrieve the cointegration coefficient.


beta_eg, epsilon_t_eg, ar_coeff_eg, ar_resid_eg = optimizer.fit(use_johansen=False, sig_level="95%")


# If the asset pair is not cointegrated at the designated significance level (here we set it to 95\%), then the `fit()` function will output a warning message that the pair was not cointegrated at the 95\% level. Here the warning did not appear, which means the stock pair (AME-DOV) was cointegrated during the entire training period. 
#
# We can now check the properties of the cointegration error and see how well it follows an AR(1) process.


print("The cointegration coefficient is: {}".format(beta_eg))
print("The AR(1) coefficient of the cointegration error is: {}".format(ar_coeff_eg))

sigma_e = epsilon_t_eg.std()
sigma_a = ar_resid_eg.std()

print("The standard deviation of the cointegration error is: {}".format(sigma_e))
print("The standard deviation of the fitted AR(1) process residual is: {}".format(sigma_a))
print("The ratio of the AR(1) residual std-dev to "
      "cointegration error std-dev is: {}".format(sigma_a / sigma_e))
print("Value of sqrt(1 - phi^2) is: {}".format(np.sqrt(1 - ar_coeff_eg ** 2)))


# According to the above results, we can see that the cointegration error indeed follows a stationary AR(1) process with two pieces of proof.
#
# 1. The standard deviation of the cointegration error and the standard deviation of the AR(1) process residual have the following relationship:
#
# \begin{equation}
# \sigma_a = \sqrt{1 - \phi^2} \sigma_{\varepsilon}
# \end{equation}
#
# And the fitting result has shown that $\frac{\sigma_a}{\sigma_{\varepsilon}} = 0.21$ and $\sqrt{1 - \phi^2} = 0.18$, which is relatively close.
#
# 2. The absolute value of the AR(1) coefficient is less than 1 ($\phi = 0.9831 < 1$).
#
# Now we proceed to optimize the upper bound.
#
# **Note:** This process will take a while, so a progress bar has been provided.


# Optimize the pre-set boundaries based on the fitted parameters
optimal_ub, _, _, optimal_mtp, optimal_num_of_trades = optimizer.optimize(ar_coeff_eg, epsilon_t_eg, 
                                                                          ar_resid_eg, len(train_df))


# Let's check the optimal upper bound, the optimal minimal total profit, and the optimal total number of U-trades. We can simply double the minimal total profit and the number of U-trades to take into account L-trades.


print("The optimal upper-bound is: {}".format(optimal_ub))
print("The optimal minimal total profit over the in-sample period with only U-trades: ${:.2f}".format(optimal_mtp))
print("The optimal total number of U-trades is: {}".format(np.floor(optimal_num_of_trades)))
print("The optimal minimal total profit over the in-sample period with both "
      "U-trades and L-trades: ${:.2f}".format(2. * optimal_mtp))
print("The optimal total number of U-trades and L-trades is: {}".format(2. * np.floor(optimal_num_of_trades)))


# The above results correspond to trading one unit of the AME-DOV pair. The optimal upper-bound is $\$2.05$, which is also the minimum profit per trade according to the derivation in the **Minimum Profit Per Trade** section.


# ### Trading the Strategy


# We need to get the optimal levels to enter and exit a trade.


# Obtain levels to open and close trades and number of shares to trade on in-sample data
num_of_shares_is, trading_levels_is = optimizer.get_optimal_levels(optimal_ub, optimal_mtp, 
                                                                   beta_eg, epsilon_t_eg)

# Show the number of shares to trade
print("The number of AME and DOV shares to trade is", num_of_shares_is)
# Show the entry and exit levels
print("The trading levels for the AME and DOV shares spread are", trading_levels_is)


# The number of shares has been decided by the optimizer. One unit of trade corresponds to trading 8 shares of AME and 7 shares of DOV. 
#
# The output trading levels are:
# * To enter a long trade: $-5.49$
# * To close a trade: $-3.44$
# * To enter a short trade: $-1.39$
#
# Now we can now use the MinimumProfitTrading rule from the Spread Trading module to feed in new spread values and check times when a trade shouldbne opened. With the mechanism of providing spread values one by one to the strategy, it's easier to integrate this strategy in an existing trading pipeline.


# Importing the needed function
from arbitragelab.trading import MinimumProfitTradingRule


# Construct spread values
spread = optimizer.construct_spread(trade_df, beta_eg)


# Creating a strategy
strategy = MinimumProfitTradingRule(shares=num_of_shares_is, optimal_levels=trading_levels_is)


# Now we use a loop to simulate a live data feed.


# Adding initial spread value
strategy.update_spread_value(spread[0])

# Feeding spread values to the strategy one by one
for time, value in spread.iteritems():
    strategy.update_spread_value(value)

    # Checking if logic for opening a trade is triggered
    trade, side = strategy.check_entry_signal()

    # Adding a trade if we decide to trade signal
    if trade:
        strategy.add_trade(start_timestamp=time, side_prediction=side)

    # Update trades, close if logic is triggered
    close = strategy.update_trades(update_timestamp=time)


# Currently open trades in a strategy
open_trades = strategy.open_trades

open_trades


# So all trades but one were closed.


# Checking all closed trades
closed_trades = strategy.closed_trades

closed_trades


# A total of 20 trades were open during this period, out of which 19 were closed.
#
# ### Strategy outputs
#
# We can see the following data:
# * Dictionary key:
#     * Timestamp at which the trade was opened
# * Dctionary value:
#     * t1: Timestamp at which the trade was closed
#     * uuid: Trade ID that can be provided for each trade
#     * start_value: Spread value at which spread was opened
#     * end_value: Spread value at which spread was closed
#     * side: Side of the trade '-1' for short and '1' for long


# From the above results, we can see that each of the closed trades during this period was profitable - the spread value change was in the right direction, according to the trade side.


# ## Conclusion


# This notebook described how to design a cointegrated pair trading strategy by optimizing the pre-set boundaries to "fade the spread" to maximize the minimum total profit over a trading period. The assumptions of the cointegration model, the optimization algorithm, and the application of the algorithm to an empirical pair have been demonstrated in detail.
#
# ### Key takeaways
#
# * When trading a cointegrated asset pair, two elements are required to guarantee a minimum profit per trade.
#   - Build the spread by weighting the assets by the cointegration coefficient.
#   - Fade the spread at a pre-set boundary and square the trade when the spread returns to its mean.
# * The spread must maintain cointegrated during the training and the trading period.
# * If the spread follows a stationary AR(1) process, the optimal pre-set boundary can be estimated with mean first-passage time.
# * A numerical algorithm has been given to calculate the mean first-passage time.
# * The optimal boundary maximizes the total minimum profit over a trading period.
# * By assuming a symmetric distribution of the spread, the optimal boundary is applied over both sides of the spread mean, so the spread can be faded both when overvalued and undervalued.
# * The strategy can be scaled up by setting a higher minimum profit per trade.


# ## Reference


# 1. [Lin, Y.-X., McCrae, M., and Gulati, C. (2006). Loss protection in pairs trading through minimum profit bounds: A cointegration approach.Journal of Applied Mathematics and Decision Sciences, 2006(1):1–14.](http://downloads.hindawi.com/archive/2006/073803.pdf)
# 2. [Puspaningrum, H., Lin, Y.-X., and Gulati, C. M. (2010).  Finding the optimal pre-set boundaries for pairs trading strategy based on cointegration technique. Journal of  Statistical  Theory and Practice, 4(3):391–419.](https://ro.uow.edu.au/cgi/viewcontent.cgi?article=1040&context=cssmwp)



// ---------------------------------------------------

// Copula_Strategy_Mispricing_Index.py
// arbitrage_research/Copula Approach/Copula_Strategy_Mispricing_Index.py
# Generated from: Copula_Strategy_Mispricing_Index.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Copula Strategy Using Mispricing Index
# This notebook demonstrates the usage of the `copula_strategy_mpi` module.
# The framework of which was originally proposed in
#
# * Xie, W., Liew, R.Q., Wu, Y. and Zou, X., 2016. Pairs trading with copulas. 
#
# For a fundamental introduction of concepts in copula, please refer to the documentation for this module, or the basic copula strategy notebook.
# The rest of the notebook is written assuming the reader in possession of basic understanding of those concepts.
#
# **Warning:**
# The authors claimed a relatively robust 8-10% returns from this strategy in the formation period (6 mo).
# We are pretty positive that the rules proposed in the paper were implemented correctly in the `CopulaStrategyMPI`
# module with thorough unit testing on every possible case, and thus there is very unlikely to have mistakes.
# However the P&L is very sensitive to the opening and exiting parameters value, input data and copula choice,
# and it cannot lead to the claimed returns, after trying all the possible interpretations of ambiguities.
#
# We still implement this module for people who intend to explore possibilities with copula methods, however the user should be
# aware of the nature of the proposed framework.
# Interested reader may read through the *Possible Issues Discussion* part and see where this strategy can be improved.
# We also marked in the source code where the trading logics are implemented for you to change them quickly as you wish.


# ## Introduction to the Strategy Framework
# For convenience, the **mispricing index** implemented in the strategy will be referred to as **MPI** when no ambiguity arises.


# ### How is the MPI Strategy Constructed?
# At first glance, the MPI strategy documented in [Xie et al. 2016] looks quite bizarre.
# However, it is reasonably consistent when one goes through the logic of its construction:
# In order to use returns to generate trading signals, one needs to be creative about utilizing the information.
# It is one thing to know the dependence structure of a pair of stocks, it is another thing to trade based on it because intrinsically stocks are traded on prices, not returns.
#
# If one takes conditional probabilities as a distance measure on the spread, then it is natural to think about how far the returns can cumulatively drive the prices apart (or together), thereby introducing trading opportunities.
#
# Hence we introduce the following concepts for the strategy framework:
#
# #### Mispricing Index
# **MPI** is defined as the conditional probability (cumulative density) of returns, i.e., 
# $$
# MI_t^{X\mid Y} = P(R_t^X < r_t^X \mid R_t^Y = r_t^Y)
# $$
# $$
# MI_t^{Y\mid X} = P(R_t^Y < r_t^Y \mid R_t^X = r_t^X)
# $$
# for stocks $(X, Y)$ with returns random variables at day $t$: $(R_t^X, R_t^Y)$ and returns value at day $t$: $(r_t^X, r_t^Y)$.
# Those two values determine how mispriced each stock is, based on that day's return.
# Note that so far only one day's return information contributes, and we want to add it up to cumulatively use returns to gauge how mispriced the stocks are.
# Therefore we introduce the **flag** series:
#
# #### Flag and Raw Flag
# A more descriptive name than flag, in my opinion, would be the **cumulative mispricing index**.
# The **raw flag** series (with a star) is the cumulative sum of daily MPIs minus 0.5, i.e.,
# $$
# FlagX^*(t) = FlagX^*(t-1) + (MI_t^{X\mid Y} - 0.5), \quad FlagX^*(0) = 0.
# $$
# $$
# FlagY^*(t) = FlagY^*(t-1) + (MI_t^{Y\mid X} - 0.5), \quad FlagY^*(0) = 0.
# $$
# Or equivalently
# $$
# FlagX^*(t) = \sum_{s=0}^t (MI_s^{X\mid Y} - 0.5)
# $$
# $$
# FlagY^*(t) = \sum_{s=0}^t (MI_s^{Y\mid X} - 0.5)
# $$
# If one plots the raw flags series, they look quite similar to cumulative returns from their price series, which is what they were designed to do:
# Accumulate information from daily returns to reflect information on prices.
# Therefore, you may consider it as a fancy way to represent the returns series.
#
# However, the **real flag** series (without a star, $FlagX(t), FlagY(t)$) **will be reset to $0$** whenever there is an exiting signal, which brings us to the trading logic.


# ### Trading Logic
#
# The authors propose a **dollar-neutral** trade scheme worded as follows:
#
# Suppose stock $X$, $Y$ are associated with $FlagX$, $FlagY$ respectively.
#
# Opening rules: ($D = 0.6$ in the paper)
# * When $FlagX$ reaches $D$, we short-sell stock $X$ and buy stock $Y$ in **equal amounts**. ($-1$ Position)
# * When $FlagX$ reaches $-D$, we short-sell stock $Y$ and buy stock $X$ in **equal amounts**. ($1$ Position)
# * When $FlagY$ reaches $D$, we short-sell stock $Y$ and buy stock $X$ in **equal amounts**. ($1$ Position)
# * When $FlagY$ reaches $-D$, we short-sell stock $X$ and buy stock $Y$ in **equal amounts**. ($-1$ Position)
#
# Exiting rules: ($S = 2$ in the paper)
# * If trades are opened based on $FlagX$, then they are closed if $FlagX$ returns to zero or reaches stop-loss position $S$ or $-S$.
#
# * If trades are opened based on $FlagY$, then they are closed if $FlagY$ returns to zero or reaches stop-loss position $S$ or $-S$.
#
# * After trades are closed, both $FlagX$ and $FlagY$ are reset to $0$.
#
# The rationale behind the dollar-neutral choice might be that (the authors did not mention this), because the signals are generated by returns, it makes sense to "reset" returns when entering into a long/short position.


# #### Ambiguities
# The authors did not specify what will happen if the followings occur:
#
# 1. When $FlagX$ reaches $D$ (or $-D$) and $FlagY$ reaches $D$ (or $-D$) together.
# 2. When in a long(or short) position, receives a short(or long) trigger.
# 3. When receiving an opening and exiting signal together.
# 4. When the position was open based on $FlagX$ (or $FlagY$), $FlagY$ (or $FlagX$) reaches $S$ or $-S$.
#
# Here is our take on the above issues:
#
# 1. Do nothing.
# 2. Change to the trigger position. For example, long position with a short trigger will go short.
# 3. Go for the exiting signal.
# 4. Do nothing.


# #### Choices for Open and Exit Logic
#
# The above default logic is essentially an OR-OR logic for open and exit: When at least one of the 4 open conditions is satisfied, an
# open signal (long or short) is triggered;
# Similarly for the exit logic, to exit only one of them needs to be satisfied.
# The opening trigger is in general too sensitive and leads to too many trades, and [Rad et al. 2016] suggested using AND-OR logic instead.
# Thus, to achieve more flexibility, we allow the user to choose AND, OR for both open and exit logic and hence there are 4 possible combinations.
# Based on our tests we found AND-OR to be the most reasonable choice in general, but in certain situations other choices may have an edge.
#
# The default is OR-OR, as suggested in [Xie et al. 2014], and you can switch to other logic in the `get_positions_and_flags` method
# by setting `open_rule` and `exit_rule` to your own liking.
# For instance `open_rule='and'`, `exit_rule='or'`.


# ## `CopulaStrategyMPI` Functionality ##
# Tools included in the module enables the users to conduct the following workflow:
#
# * Initiate the `CopulaStrategyMPI` class with options to set custom $D$ value `opening_triggers` and $S$ value `stop_loss_positions`, and the option to side-load a fitted `Copula` or `MixedCopula` object.
# * Calculate returns from price series using `to_returns` method.
# * Calculate daily MPIs from returns using `calc_mpi` method.
#   (This step is optional and you do not need the MPI series to do other things.
#   Other methods that uses MPIs will calculate them internally.)
# * Fit returns to different kinds of copulas using `fit_copula` method.
# * Get positions and flags from testing/trading data, using `get_positions_and_flags` method.
# * Translate positions to the number of units to hold for a dollar-neutral approach, using `positions_to_units_dollar_neutral` method.
#
# A few notes:
#
# 1. This module is pandas based, and is built on top of the `BasicCopulaStrategy` class.
# 2. The output is *very sensitive* to the values of `opening_triggers` and `stop_loss_positions`, to the point that it may determine whether this strategy profits or loses.
# 3. If you just want to have the raw flag series, simply toggle `enable_reset_flag` to `False`. But this logically conflicts with the exiting logic generation so do not use the trading signal under this condition.
# 4. If you want to have the raw flag series AND trade on it, you need to set `stop_loss_positions` to `(-np.inf, np.inf)` and toggle `enable_reset_flag` to `False`. Although based on our tests, this strategy is not likely to profit.


# ## Usage ##
# The demonstration part has the following sections using real-world data:
# 1. Importing and Data Pre-processing.
# 2. Fitting Data to Different Copulas with Training Data.
# 3. Generating Trading Positions using Testing Data.
# 4. Possible Issues Discussion.


# ### 1. Importing and Data Preprocessing
#
# I have also included the 2009-2011 data for BKD and ESC. Simply uncomment accordingly for cell 2, 3 and 4.


# Importing libraries and modules
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
import statsmodels.api as sm

from arbitragelab.trading import MPICopulaTradingRule
import arbitragelab.copula_approach.copula_calculation as ccalc
from arbitragelab.copula_approach.archimedean import (Gumbel, Clayton, Frank, Joe, N13, N14)
from arbitragelab.copula_approach.elliptical import (StudentCopula, GaussianCopula)


# Importing data

# I also included the 2009-2011 data. Uncomment below to use.
# pair_prices = pd.read_csv(r'BKD_ESC_2009_2011.csv', index_col=0, parse_dates=True)

pair_prices = pd.read_csv(r'BKD_ESC_2008_2009_June.csv', index_col=0, parse_dates=True)


# Plotting price series
plt.figure(dpi=120)
plt.plot(pair_prices['BKD'], label='BKD')
plt.plot(pair_prices['ESC'], label='ESC')

# plt.axvline(dt.date(2012, 1, 3), color='red')  # Training testing split date

plt.axvline(dt.date(2008, 12, 31), color='red')  # Training testing split date
plt.legend()
plt.grid()
plt.gcf().autofmt_xdate()
plt.title(r'Original Price Series of BKD and ESC')
plt.show()


# Initiate the analysis module
CSMPI = MPICopulaTradingRule()

# Training and testing split. Note that this module works with returns series.
# training_length = 756# From 01/02/2008 to 12/30/2008 (m/d/y)
training_length = 252 # From 01/02/2008 to 12/30/2008 (m/d/y)

prices_train = pair_prices.iloc[: training_length, :]
returns_train = CSMPI.to_returns(prices_train)

prices_test = pair_prices.iloc[training_length : , :]
returns_test = CSMPI.to_returns(prices_test)

# Empirical CDF for the training set.
cdf1 = ccalc.construct_ecdf_lin(returns_train['BKD'])
cdf2 = ccalc.construct_ecdf_lin(returns_train['ESC'])

cdf1_price = ccalc.construct_ecdf_lin(prices_train['BKD'])
cdf2_price = ccalc.construct_ecdf_lin(prices_train['ESC'])


# Let's at first scatter plot the training data to have a grasp on its dependence structure, and determine which copula it should fit.


# Scatter plot the training data.
fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10,5), dpi=100)
axs[0].scatter(cdf1(returns_train['BKD']), cdf2(returns_train['ESC']), s=2)
axs[0].set_aspect('equal', adjustable='box')
axs[0].set_title(r'From Return Series, Training Data')

axs[1].scatter(cdf1_price(prices_train['BKD']), cdf2_price(prices_train['ESC']), s=2, color='r')
axs[1].set_aspect('equal', adjustable='box')
axs[1].set_title(r'From Price Series, Training Data')
plt.tight_layout()
plt.show()


# **Note: We only use the returns series for the MPI strategy. The price series scatter plot is just for visual comparison.**
#
# The returns plot exhibits some tail dependencies on the lower-left corner.
# There might also be some tail dependencies on the upper-right corner, but it is not directly obvious.
# When compared to the price series scatter plot, the price series shows a much stronger lower dependency and no clear upper dependency.
# In terms of market movement, the two plots indicate that both returns and prices tend to move downward together, but not upward together, at least the interdependence is not as strong as downward movements.
# Considering the training data is in year 2008, those scatter plots make sense.
#
# ### 2. Fitting Data to Different Copulas with Training Data
#
# Regarding which copulas best describe the structure, we propose using N14 copula log-likelihood score, and their ability to capture tail dependencies at both corners.
# One may fit to as much the copulas as they wish in the `copula_approach` package.


# Fit to Gumbel, Frank, Clayton, Joe, N14, Gaussian and Student-t copulas respectively, and print the scores.
# You may see warnings coming from fitting a mixed copula. In this case, adjust values for gamma_scad, a_scad.
fit_result_gumbel, copula_gumbel, cdf_x_gumbel, cdf_y_gumbel =\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=Gumbel)

fit_result_frank, copula_frank, cdf_x_frank, cdf_y_frank =\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=Frank)

fit_result_clayton, copula_clayton, cdf_x_clayton, cdf_y_clayton =\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=Clayton)

fit_result_joe, copula_joe, cdf_x_joe, cdf_x_joe=\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=Joe)

fit_result_n14, copula_n14, cdf_x_n14, cdf_y_n14=\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=N14)

fit_result_gauss, copula_gauss, cdf_x_gauss, cdf_y_gauss =\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=GaussianCopula)

fit_result_t, copula_t, cdf_x_t, cdf_y_t=\
    ccalc.fit_copula_to_empirical_data(x=returns_train['BKD'], y=returns_train['ESC'], copula=StudentCopula)

# Print fit scores
print(fit_result_gumbel)
print(fit_result_frank)
print(fit_result_clayton)
print(fit_result_joe)
print(fit_result_n14)
print(fit_result_gauss)
print(fit_result_t)


# Note that we are working with the returns series, which in general is much noisier than price series, and has a lower level of correlation.


# Plotting copulas
fig, ax = plt.subplots(figsize=(5,7), dpi=100)

ax.scatter(cdf1(returns_train['BKD']), cdf2(returns_train['ESC']), s=1)
ax.set_aspect('equal', adjustable='box')
ax.set_title(r'BKD and ESC Returns Series, Training Data')
ax.set_xlim([-0.05, 1.05])
ax.set_ylim([-0.05, 1.05])
plt.tight_layout()

plt.show()


# Gumbel
copula_gumbel.plot_scatter(num_points=training_length);


# Frank
copula_frank.plot_scatter(num_points=training_length);


# Clayton
copula_clayton.plot_scatter(num_points=training_length);


# Joe
copula_joe.plot_scatter(num_points=training_length);


# N14
copula_n14.plot_scatter(num_points=training_length);


# Gaussian
copula_gauss.plot_scatter(num_points=training_length);


# Student-t
copula_t.plot_scatter(num_points=training_length);


# Now we choose Student-t and N14 copula to generate trading signals.
#
# ### 3. Generating Trading Positions using Testing Data
#
# We are using a dollar neutral strategy, and the buy/sell/hold is based on "flag" series, calculated from accumulations of mispricing indices.
# For more detail, please refer to the beginning of the notebook.
#
# Here we use the default OR-OR open-exit logic with the Student-t copula, and AND-OR open-exit logic with the
# N14 copula.


# Make two strategy classes with the fitted Student and N14 copula.
CSMPI_t = MPICopulaTradingRule(opening_triggers=(-0.2, 0.2), stop_loss_positions=(-2, 2))
CSMPI_t.set_copula(copula_t)
CSMPI_t.set_cdf(cdf_x_t, cdf_y_t)

CSMPI_n14 = MPICopulaTradingRule(opening_triggers=(-0.5, 0.5), stop_loss_positions=(-2.1, 2.1))
CSMPI_n14.set_copula(copula_n14)
CSMPI_n14.set_cdf(cdf_x_n14, cdf_y_n14)

# Get positions and flag series
positions_t, flags_t = CSMPI_t.get_positions_and_flags(
    returns=returns_test, enable_reset_flag=True)
positions_n14, flags_n14 = CSMPI_n14.get_positions_and_flags(
    returns=returns_test, enable_reset_flag=True,
    open_rule='and', exit_rule='or')

# Shift the positions by 1 day
positions_t = positions_t.shift(1)
positions_n14 = positions_n14.shift(1)
positions_t[0] = 0
positions_n14[0] = 0


# As a referece we also plot the normalized price series from the two stocks.
plt.figure(figsize=(10, 3), dpi=150)
plt.plot(prices_test['BKD'] / prices_test['BKD'][0], label='BKD prices')
plt.plot(prices_test['ESC'] / prices_test['ESC'][0], label='ESC prices')
plt.title('Normalized Prices in the Trading Period')
plt.legend()
plt.grid()
plt.show()

# Plot positions and flags
fig, axs = plt.subplots(4, 1, gridspec_kw={'height_ratios': [1, 0.4, 1, 0.4]}, figsize=(10,10), dpi=150)
fig.suptitle('Flags and Corresponding Positions')

# Plotting flags for Student-t copula
axs[0].plot(flags_t['BKD'], label='BKD')
axs[0].plot(flags_t['ESC'], label='ESC')
axs[0].title.set_text('Flags, Student-t Copula')
axs[0].legend()
axs[0].grid()
axs[0].set_yticks([-2, -1, -0.6, 0, 0.6, 1, 2])

# Plotting positions for Student-t copula
axs[1].plot(positions_t, label='Positions', color='brown')
axs[1].title.set_text('Positions, Student-t Copula')
axs[1].grid()
axs[1].set_yticks([-1,0,1])

# Plotting flags for N14 copula
axs[2].plot(flags_n14['BKD'], label='BKD')
axs[2].plot(flags_n14['ESC'], label='ESC')
axs[2].title.set_text('Flags, N14 Copula')
axs[2].legend()
axs[2].grid()
axs[2].set_yticks([-2, -1, -0.6, 0, 0.6, 1, 2])

# Plotting positions for N14 copula
axs[3].plot(positions_n14, label='Positions', color='brown')
axs[3].title.set_text('Positions, N14 Copula')
axs[3].grid()
axs[3].set_yticks([-1,0,1])

fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Avoid title overlap
plt.show()


# #### Forming Equity Curves
# The authors propose a dollar-neutral trading strategy and therefore, we can represent the P&L of the strategy in returns.


# 1. Calculate the unit holding series
units_df_t = CSMPI.positions_to_units_dollar_neutral(prices_df=prices_test, positions=positions_t)
units_df_n14 = CSMPI.positions_to_units_dollar_neutral(prices_df=prices_test, positions=positions_n14)

# Calculate returns from the strategy, i.e., assuming 1$ initial investment and calculate P&L.
# 2. Calculate Daily P&L of the strategy based on the units holding for each security
portfolio_pnl_t = returns_test['BKD'] * units_df_t['BKD'] + returns_test['ESC'] * units_df_t['ESC']
portfolio_pnl_n14 = returns_test['BKD'] * units_df_n14['BKD'] + returns_test['ESC'] * units_df_n14['ESC']

# 3. Calculate and plot the equity curve
equity_t = portfolio_pnl_t.cumsum()
equity_n14 = portfolio_pnl_n14.cumsum()

# Plotting
fig, axs = plt.subplots(4, 1, gridspec_kw={'height_ratios': [0.5, 0.2, 0.2, 0.5]}, figsize=(10,10), dpi=150)

# As a referece we also plot the normalized price series from the two stocks.
axs[0].plot(prices_test['BKD'] / prices_test['BKD'][0], label='BKD prices')
axs[0].plot(prices_test['ESC'] / prices_test['ESC'][0], label='ESC prices')
axs[0].set_title('Normalized Prices in the Trading Period')
axs[0].legend()
axs[0].grid()

# Plot the units series
axs[1].plot(units_df_t['BKD'], label='BKD units')
axs[1].plot(units_df_t['ESC'], label='ESC units')
axs[1].set_title('Amount of Shares to Hold According to Student-t Copula')
axs[1].legend()
axs[1].grid()

axs[2].plot(units_df_n14['BKD'], label='BKD units')
axs[2].plot(units_df_n14['ESC'], label='ESC units')
axs[2].set_title('Amount of Shares to Hold According to N14 Copula')
axs[2].legend()
axs[2].grid()

# Plot the daily P&L
axs[3].plot(equity_t, label='Student-t', color='firebrick')
axs[3].plot(equity_n14, label='N14', color='forestgreen')
axs[3].set_title('Equity Curves, Plotted in Returns')
axs[3].legend()
axs[3].grid()

fig.tight_layout()
plt.show()


# ### 4. Possible Issues Discussion
# The following are critiques for the default strategy.
# For a thorough comparison in large amounts of stocks across several decades, read For the AND-OR strategy, read more 
# in [Rad et al. 2016] on comparisons with other common strategies, using the AND-OR logic.
#
# 1. The strategy's outcome is quite sensitive to the values of opening and exiting triggers to the point that a well-fitted copula with a not good sets of parameters can actually lose money.
#
# 2. The trading signal is generated from the flags series, and the flags series will be calculated from the copula that we use to model.
#    Therefore the explainability suffers.
#    Also, it is based on the model in second order, and therefore the flag series and the suggested positions will be quite different across different copulas, making it not stable and not directly comparable mutually.
#
# 3. The way the flags series are defined does not handle well when both stocks are underpriced/overpriced concurrently.
#
# 4. Because of flags will be reset to 0 once there is an exiting signal, it implicitly models the returns as martingales that do not depend on the current price level of the stock itself and the other stock.
#    Such an assumption may be situational, and the user should be aware. (White noise returns do not imply that the prices are well cointegrated.)
#
# 5. The strategy is betting the flags series having dominating mean-reversion behaviors, for a pair of cointegrated stocks.
#    It is not mathematically clear what justifies the rationale.
#
# 6. If accumulating mispricing index is basically using returns to reflect prices, and the raw flags look basically the same as normalized prices, why not just directly use normalized prices?


# ## Conclusion
#
# (This section follows from [Xie et al. 2016])
#
# * Copulas are able to capture both linear and non-linear relations between random variables, it is possible that the proposed method captures strong non-linear associations that the traditional methods such as distance measure tend to neglect.
#
# * Using returns overcomes the criticism that the prices series not being stationary, as used by [Liew et al. 2013].


# ## References ##
# - [Xie, W., Liew, R.Q., Wu, Y. and Zou, X., 2016. Pairs trading with copulas. The Journal of Trading, 11(3), pp.41-52.](https://efmaefm.org/0efmameetings/EFMA%20ANNUAL%20MEETINGS/2014-Rome/papers/EFMA2014_0222_FullPaper.pdf)
# - [Liew, R.Q. and Wu, Y., 2013. Pairs trading: A copula approach. Journal of Derivatives & Hedge Funds, 19(1), pp.12-30.](https://dr.ntu.edu.sg/bitstream/10220/17826/1/jdhf20131a.pdf)
# - [Rad, H., Low, R.K.Y. and Faff, R., 2016. The profitability of pairs trading strategies: distance, cointegration and copula methods. Quantitative Finance, 16(10), pp.1541-1558.](https://www.tandfonline.com/doi/pdf/10.1080/14697688.2016.1164337?casa_token=X0-FdUqDsv4AAAAA:ZFothfEHF-dO2-uDtFo2ZuFH0uzF6qijsweHD888yfPx3OZGXW6Szon1jvA2BB_AsgC5-kGYreA4fw)



// ---------------------------------------------------

// vine_copula_partner_selection.py
// arbitrage_research/Copula Approach/vine_copula_partner_selection.py
# Generated from: vine_copula_partner_selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Implementation of Partner Selection Procedures
#
# This notebook demonstrates the usage of the `vine_copula_partner_selection` module.
#
# This module contains implementation of the four partner selection approaches
# mentioned in Section 3.1.1 of 
#
# - [Statistical Arbitrage with Vine Copulas. (2016)](https://www.econstor.eu/bitstream/10419/147450/1/870932616.pdf) by Stübinger,  Mangold, and Krauss.
#
#
# ## Abstract
#
# In this paper, Stubinger, Mangold and Krauss developed a multivariate statistical arbitrage strategy based on vine copulas -
# a highly flexible instrument for linear and nonlinear multivariate dependence modeling. Pairs trading is a relative-value arbitrage strategy,
# where an investor seeks to profit from mean-reversion properties of the price spread between two co-moving securities.
# Existing literature focused on using bivariate copulas to model the dependence structure between two stock return time series,
# and to identify mispricings that can potentially be exploited in a pairs trading application.
#
# This paper proposes a multivariate copula-based statistical arbitrage framework, where specifically,
# for each stock in the S&P 500 data base, the three most suitable partners are selected by leveraging different selection criteria.
# Then, the multivariate copula models are benchmarked to capture the dependence structure of the selected quadruples.
# Later on, the paper focusses on the generation of trading signals and backtesting.
#
#
# ## Introduction
#
# This module will focus on the various Partner Selection procedures and their implementations, as described in the paper.
# For every stock in the S&P 500, a partner triple is identified based on adequate measures of association.
# The following four partner selection approaches are implemented:
#
# - Traditional Approach - baseline approach where the high dimensional relation between the four stocks is approximated by their pairwise bivariate correlations via Spearman’s $\rho$;
#
# - Extended Approach - calculating the multivariate version of Spearman’s $\rho$ based on Schmid and Schmidt (2007);
#
# - Geometric Approach - involves calculating the sum of euclidean distances from the 4-dimensional hyper-diagonal;
#
# - Extremal Approach - involves calculating a non-parametric $\chi^2$ test statistic based on Mangold (2015) to measure the degree of deviation from independence.
#
# Firstly, all measures of association are calculated using the ranks of the daily discrete returns of our samples.
# Ranked transformation provides robustness against outliers.
#
# Secondly, only the top 50 most highly correlated stocks are taken into consideration for each target stock, to reduce the computational burden.
#
# The traditional, the extended, and the geometric approach share a common feature - they measure the deviation from linearity in ranks.
# All three aim at finding the quadruple that behaves as linearly as possible to ensure that there is an actual relation between its components to model.
# While it is true that this aspiration for linearity excludes quadruples with components that are not connected (say, independent),
# it also rules out nonlinear dependencies in ranks.
# On the other hand, the extremal approach tries to maximize the distance to independence with focus on the joint extreme observations.


# ---


# ## Imports and Loading the Dataset


from arbitragelab.copula_approach.vine_copula_partner_selection import PartnerSelection
from arbitragelab.copula_approach.vine_copula_partner_selection_utils import get_sector_data
import pandas as pd


# Dataset contains daily pricing data for all stocks in S&P 500. Data from the year 2019 is taken into consideration for this notebook.
#
# When a PartnerSelection object is created, daily returns of the stocks and their corresponding ranked returns are calculated and stored as attributes. 


# Importing DataFrame of daily pricing data for all stocks in S&P 500.(atleast 12 months data)
df = pd.read_csv('./sp500_2019.csv', parse_dates=True, index_col='Date').dropna()
# Instantiating the partner selection module.
ps = PartnerSelection(df)

#Loading the sector data for every ticker in S&P 500
constituents = pd.read_csv('./sp500_constituents-detailed.csv', index_col='Symbol')


# For each one of the methods below,
# ```
# - ps.traditional()
# - ps.extended()
# - ps.geometric()
# - ps.extremal()
# ```
# we can give the number of target stocks(in alphabetical order in S&P 500) to be considered as input.
#
# For example, for
# ```ps.traditional(5)``` , the final quadruples for the first 5 tickers in S&P 500 according to alphabetical order are returned.


# ## Traditional Approach
#
# As a baseline approach, the high dimensional relation between the four stocks is approximated
# by their pairwise bi-variate correlations via Spearman’s $\rho$.
# We used ranked returns data for this approach. In addition to the robustness obtained by rank transformation,
# it allows to capture non-linearities in the data to a certain degree.
#
# The procedure is as follows:
#
# - Calculate the sum of all pairwise correlations for all possible quadruples, consisting of a fixed target stock.
#
# - Quadruple with the largest sum of pairwise correlations is considered the final quadruple and saved to the output matrix.


# Calculating final quadruples using traditional approach for first 10 target stocks.
traditional_Q = ps.traditional(10)
print(pd.Series(traditional_Q))


# ```ps.plot_selected_pairs``` can be used to plot the cumulative daily returns of the stocks in the list of quadruples given as input.


# Plotting the cumulative daily returns of the final quadruples.
ps.plot_selected_pairs(traditional_Q);


# ```get_sector_data``` returns name and sector data of stocks in the quadruple given as input.


# Displaying Name and Sector Info of Securities in the final Quadruples.
for quadruple in traditional_Q:
    display(get_sector_data(quadruple,constituents))


# ## Extended Approach
#
# Schmid and Schmidt (2007) introduce multivariate rank based measures of association.
# This paper generalizes Spearman’s $\rho$ to arbitrary dimensions - a natural extension of the traditional approach.
#
# In contrast to the strictly bi-variate case, this extended approach – and the two following approaches –
# directly reflect multivariate dependence instead of approximating it by pairwise measures only.
# This approach provides a more precise modeling of high dimensional association and thus a better performance in trading strategies.
#
# The procedure is as follows:
#
# - Calculate the multivariate version of Spearman’s $\rho$ for all possible quadruples, consisting of a fixed target stock.
#
# - Quadruple with the largest value is considered the final quadruple and saved to the output matrix.
#
# $d$ denotes the number of stocks daily returns observed from day $1$ to day $n$. $X_i$ denotes the $i$-th stock's return.
#
# 1. We calculate the empirical cumulative density function (ECDF) $\hat{F}_i$ for stock $i$.
#
# 2. Calculate quantile data for each $X_{i}$, by
#
# $$
#     \begin{align}
#     \hat{U}_i = \frac{1}{n} (\text{rank of} \ X_i) = \hat{F}_i(X_i)
#     \end{align}
# $$
#
# The formula for the three estimators are given below, as in the paper.
#
# $$
#     \begin{align}
#        \hat{\rho}_1 = h(d) \times \Bigg\{-1 + \frac{2^d}{n} \sum_{j=1}^n \prod_{i=1}^d (1 - \hat{U}_{ij}) \Bigg\} \\
#        \hat{\rho}_2 = h(d) \times \Bigg\{-1 + \frac{2^d}{n} \sum_{j=1}^n \prod_{i=1}^d \hat{U}_{ij} \Bigg\} \\
#        \hat{\rho}_3 = -3 + \frac{12}{n {d \choose 2}} \times \sum_{k<l} \sum_{j=1}^n (1-\hat{U}_{kj})(1-\hat{U}_{lj}) \\
#    \end{align}
# $$
#
# Where:
#
# $$
#     \begin{align}
#        h(d) = \frac{d+1}{2^d - d -1}
#     \end{align}
# $$
#
# We use the mean of the above three estimators as the final measure used to return the final quadruple.


# Calculating final quadruples using extended approach for first 10 target stocks.
extended_Q = ps.extended(10)
print(pd.Series(extended_Q))


# ## Geometric Approach
#
# This approach tries to measure the geometric relation between the stocks in the quadruple.
#
# Consider the relative ranks of a bi-variate random sample, where every observation takes on values in the $[0,1] \times [0,1]$ square.
# If there exists a perfect linear relation among both the ranks of the components of the sample,
# a plot of the relative ranks would result in a perfect line of dots between the points (0,0) and (1,1) – the diagonal line.
# However, if this relation is not perfectly linear, at least one point differs from the diagonal.
# The Euclidean distance of all ranks from the diagonal can be used as a measure of deviation from linearity, the diagonal measure.
#
# Hence, we try to find the quadruple $Q$ that leads to the minimal value of the sum of these Euclidean distances.
#
# The procedure is as follows:
#
# - Calculate the four dimensional diagonal measure for all possible quadruples, consisting of a fixed target stock.
#
# - Quadruple with the smallest diagonal measure is considered the final quadruple and saved to the output matrix.
#
#
# The diagonal measure in four dimensional space is calculated using the following equation,
#
# $$
#     \begin{align}
#     \sum_{i=1}^{n} | (P - P_{1}) - \frac{(P - P_{1}) \cdot (P_{2} - P_{1})}{| P_{2} -P_{1} |^{2}} (P_{2} - P_{1}) |
#     \end{align}
# $$
#
# where,
#
# $$
#     \begin{align}
#         P_{1} = (0,0,0,0), \\
#         P_{2} = (1,1,1,1)
#     \end{align}
# $$
#
# are points on the hyper-diagonal, and
#
# $$
#     \begin{align}
#     P = (u_{1},u_{2},u_{3},u_{4})
#     \end{align}
# $$    
#
# where $u_i$ represents the ranked returns of a stock $i$ in quadruple.


# Calculating final quadruples using geometric approach for first 10 target stocks.
geometric_Q = ps.geometric(10)
print(pd.Series(geometric_Q))


#Showing output quadruples of the above three approaches,

display(pd.DataFrame([pd.Series(traditional_Q),pd.Series(extended_Q), pd.Series(geometric_Q)], index=['Traditional','Extended','Geometric']).T)


# Comparing the outputs of the three approaches we can see sublte differences in the partners selected for the first 10 target stocks.
#
# Traditional and Extended Approaches generate the exact same quadruples for all 10 targets.
#
# For A, AAL, AAPL, ABBV, ABMD and ADBE, Geometric approach generated a different quadruple compared to the other two.


#Checking the sector data of quadruples for ticker 'AAPL',

#Quadruple generated by traditional and extended.
display(get_sector_data(['AAPL', 'MCHP', 'TXN', 'MXIM'] ,constituents))

#Quadruple generated by geometric approach.
display(get_sector_data(['AAPL', 'GOOG', 'GOOGL', 'MSFT'],constituents))


# ## Extremal Approach
#
# Mangold (2015) proposes a nonparametric test for multivariate independence.
# The resulting $\chi^2$ test statistic can be used to measure the degree of deviation from independence, so dependence.
# The value of the measure increases on the occurence of an abnormal number of joint extreme events.
#
#
# The procedure is as follows:
#
# - Calculate the $\chi^2$ test statistic for all possible quadruples, consisting of a fixed target stock.
#
# - Quadruple with the largest test statistic is considered the final quadruple and saved to the output matrix.
#
# Given below are the steps to calculate the $\chi^2$ test statistic described in Proposition 3.3 of Mangold (2015):
#
# These steps assume a 4-dimensional input.
#
# 1) Analytically calculate the 4-dimensional Nelsen copula from Definition 2.4 in Mangold (2015)
#
#
# $$
#     \begin{align}
#     C_{\theta}(u_{1}, u_{2}, u_{3}, u_{4})
#     \end{align}
# $$
#
#
# 2) Analytically calculate the corresponding density function of 4-dimensional copula:
#
#
# $$
#     \begin{align}
#        c_{\theta}(u_{1}, u_{2}, u_{3}, u_{4}) = \frac{\partial^{4}}{\partial u_{1} \partial u_{2} \partial u_{3}\partial u_{4}}C_{\theta}(u_{1}, u_{2}, u_{3}, u_{4})
#     \end{align}
# $$
#
#
# 3) Calculate the Partial Derivative of above density function $w.r.t \ \theta$.
#
#
# $$
#     \begin{align}
#       \dot{c_{\theta}} = \frac{\partial c_{\theta}(u_{1}, u_{2}, u_{3}, u_{4})}{\partial \theta}
#     \end{align}
# $$
#
#
# 4) Calculate the Test Statistic for p-dimensional rank test:
#
#
# $$
#     \begin{align}
#           T=n \boldsymbol{T}_{p, n}^{\prime} \Sigma\left(\dot{c}_{\theta_{0}}\right)^{-1} \boldsymbol{T}_{p, n} \stackrel{a}{\sim} \chi^{2}(q)
#     \end{align}
# $$
#
# where,
#
# $$
#     \begin{align}
#         \boldsymbol{T}_{p, n}=\mathbb{E}\left[\left.\frac{\partial}{\partial \theta} \log c_{\theta}(B)\right|_{\theta=\theta_{0}}\right] \\
#     \\
#     \Sigma\left(\dot{c}_{0}\right)_{i, j}=\int_{[0,1]^{p}}\left(\left.\frac{\partial c_{\theta}(\boldsymbol{u})}
#     {\partial \theta_{i}}\right| _{\boldsymbol{\theta}=\mathbf{0}}\right) \times\left(\left.\frac{\partial c_{\theta}(\boldsymbol{u})}
#     {\partial \theta_{j}}\right |_{\theta=0}\right) \mathrm{d} \boldsymbol{u}
#     \end{align}
# $$
#
#


# Calculating final quadruples using extremal approach for first 10 target stocks.
extremal_Q = ps.extremal(10)
print(pd.Series(extremal_Q))


#Showing output quadruples of the above four approaches,

display(pd.DataFrame([pd.Series(traditional_Q),pd.Series(extended_Q), pd.Series(geometric_Q), pd.Series(extremal_Q)], index=['Traditional','Extended','Geometric', 'Extremal']).T)


# For AAP, ABT and ACN Extremal Approach generates different final quadruple compared to the other three methods.


#Checking the sector data of quadruples for ticker 'ABT',

#Quadruple generated by traditional and extended.
display(get_sector_data(['ABT', 'DHR', 'TMO', 'A'] ,constituents))

#Quadruple generated by geometric approach.
display(get_sector_data(['ABT', 'BSX', 'SYK', 'ISRG'],constituents))


# ## Conclusion
#
# This notebook describes the proposed Partner Selection Framework, also showcasing example usage of the implemented framework.
#
# The first two procedures seem to generate the same final set of quadruples for every target stock in the universe. Another important takeaway is the Industry/Sub-Sector of the stocks in most of the final quadruples are highly correlated, even though clustering methods were not used in this framework.


# ## References
#
# * [Stübinger, Johannes; Mangold, Benedikt; Krauss, Christopher; 2016. Statistical Arbitrage with Vine Copulas.](https://www.econstor.eu/bitstream/10419/147450/1/870932616.pdf)
# * [Schmid, F., Schmidt, R., 2007. Multivariate extensions of Spearman’s rho and related statis-tics. Statistics & Probability Letters 77 (4), 407–416.](https://wisostat.uni-koeln.de/fileadmin/sites/statistik/pdf_publikationen/SchmidSchmidtSpearmansRho.pdf)
# * [Mangold, B., 2015. A multivariate linear rank test of independence based on a multipara-metric copula with cubic sections. IWQW Discussion Paper Series, University of Erlangen-N ̈urnberg.](https://www.statistik.rw.fau.de/files/2016/03/IWQW-10-2015.pdf)
#



// ---------------------------------------------------

// CVine_Copula_Strategy.py
// arbitrage_research/Copula Approach/CVine_Copula_Strategy.py
# Generated from: CVine_Copula_Strategy.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # C-vine Copula Trading Strategy
#
# This notebook demonstrates the essential usage of the `vinecop_generate` and `vinecop_strategy` module.
# The strategy is an implementation of
#
# * [Statistical Arbitrage with Vine Copulas(2016)](https://www.iwf.rw.fau.de/files/2015/12/11-2016-1.pdf)
#
# **Note**: the literature above only utilizes C-vine copula, instead of D-vine or more general R-vine.


# ## C-vine Copula Overview
#
# Vine copula is created to address the problem of modeling the dependencies of multiple random variables.
# Instead of using an N-dimensional copula directly, it decomposes the probability density into conditional probabilities, and further decomposes conditional probabilities into bivariate copulas.
# To sum up, **the idea is that we decompose the high dimensional pdf into bivariate copulas densities and marginal densities**.
#
# For example, for 3 random variables we decompose the joint density $f(x_1, x_2, x_3)$ as follows:


from IPython.display import Image
Image(filename='images/3d_vinecop_decomposition.png')


# where $f_i(x_i)$ is the marginal density for random variable $X_i$ and $c_{ij}$ is the bivariate copula for random variable $X_i$ and $X_j$ linking the density of $f_i, f_j$, and $c_{ij|k}$ is the bivariate copula that links the density of $c_{ik}, c_{jk}$.
#
# This tree structure classifies the type of vine copula: each node stands for a density, either a joint density computed from a copula or a marginal density.
# The most generic stricture is the R-vine (Regular-vine), and the tree must satisfy the following conditions:
#
# 1. For $N$ many random variables there are $N$ levels in the tree, with level $i (i = 1,\cdots, N)$ having $N-i+1$ nodes.
# 2. Proximity condition: every edge between 2 nodes contributes to a joint density node in the next level.
# 3. Every layer must be a connected graph, and the edge number must be 1 less than the node number.
#
# C-vine (Canonical-vine) and D-vine (Drawable-vine) are special cases for an R-vine, and they are the most often used models in applications due to interpretability.
# C-vine requires that, at every level of the tree, there must be a **center node** that connects to every other nodes.
# D-vine requires that, every level of the tree is a **path**, i.e., all the nodes are chained like a 1-D linked list.
# See below for a visual explanation.


Image(filename='images/Rvine_Cvine_Dvine.png')


# The advantage of using a C-vine comes from the trading strategy: At first we choose a cohort of stocks, say 4 of them.
# Within the cohort there exists a target stock, and the long/short signals are generated with respect to this target stock.
# Hence the C-vine model that highlights a governing component at each level of the tree is a natural choice.
#
# We will not dive further into the mathematics of vine copula and we now move onto the key ideas of the trading strategy.
# The take-home message is that, **vine copula is a powerful tool to generate mispricing info through probability for a stock, when compared to other stocks**.
# Interested readers can check the documentation which has a nice summary of all the key concepts, or check the landmark paper [Selecting and estimating regular vine copulae and application to financial returns](https://arxiv.org/pdf/1202.2002).


# ## Workflow and the Strategy
#
# To sum up, the workflow is as follows:
#
# 1. Select stocks cohorts, and translate into pseudo-observations.
# 2. Fit to data to figure out the C-vine structure.
# 3. Calculate conditional cumulative densities from joint probability densities, and then formulate mispricing index and cumulative mispricing index (CMPI).
# 4. Formlate signals and trade based on Bollingerband of CMPI.
#
# We discuss briefly what each part means in application.
# We also exclusively use stocks daily returns for our data.
#
# ### 1. Select stocks and translate into pseudo-observations.
# Selecting trading candidates is a serious topic, and can largely determine if a strategy is profitable or not.
# There are 4 methods proposed in [Stübinger et al. 2018]: pairwise Spearman's rho, multi-dimensional Spearman's rho, sum of distance in quantile plots to hyper-diagonal, and a copula-chi-square test for dependence.
# Loosely speaking, the goal is to find stocks cohorts that are heavily dependent, such that a mean-reversion bet on relative mispricings is profitable.
# For more details in implementation, please refer to `vine_copula_partner_selection.ipynb`.
#
# After choosing a collection of cohorts and their corresponding target stocks, we split the returns data into training and trading subsets.
# We map the training data into their corresponding quantiles (pseudo-observations) using empirical CDFs (ECDFs).
#
#


import arbitragelab.copula_approach.vinecop_generate as vinecop_generate
import arbitragelab.copula_approach.vinecop_strategy as vinecop_strategy
import pandas as pd
import matplotlib.pyplot as plt
import os
import numpy as np

from arbitragelab.copula_approach.vine_copula_partner_selection import PartnerSelection
from arbitragelab.copula_approach.copula_calculation import to_quantile
# import arbitragelab.util.data_importer as data_importer


# Import data
sp500_prices = pd.read_csv('./prices_10y_SP500.csv',
                           index_col=0, parse_dates=True).fillna(method='ffill')
sp500_returns = sp500_prices.pct_change().fillna(0)

# Training and testing split
returns_train = sp500_returns.iloc[:800].drop(columns='SPY')
returns_test = sp500_returns.iloc[800:1200].drop(columns='SPY')

prices_spy_test = sp500_prices['SPY'].iloc[800:1200]


# #### Partner selection
# Use the extended approach to select 10 cohorts. The 1st stock in each cohort is the target stock:
#
# for example, `A` is the target stock for the cohort `[[A, PKI, TMO, MTD]]`.


# Intantiate a partner selection module and use the extended approach (multi dimensional Spearman's rho)
ps = PartnerSelection(returns_train)
extended_Q = ps.extended(10)
print(pd.Series(extended_Q))
# For time sake, we will only use the first 4 cohorts to demonstrate the trading module
cohorts = extended_Q[:4]

# Alternatively you can use the following methods. Uncomment to use them
# traditional_Q = ps.traditional(10)
# extended_Q = ps.extended(10)
# extremal_Q = ps.extremal(10)


# Translate the returns into quantiles data for the cohorts
# All the tickers we are interested in
all_tickers = list(set(ticker for cohort in cohorts for ticker in cohort))
subset_rts_train = returns_train[all_tickers]
subset_rts_test = returns_test[all_tickers]

# Train the ECDF and get the quantiles data for the training set
subset_quantiles_train, cdfs = to_quantile(subset_rts_train)
# Also get the numerical index for tickers in each cohort for the data set
num_cohorts_idx = []
cdfs_cohorts = []
for cohort in cohorts:
    num_cohort_idx = [subset_rts_train.columns.get_loc(ticker) for ticker in cohort]
    num_cohorts_idx.append(num_cohort_idx)
    cdfs_cohort = [cdfs[idx] for idx in num_cohort_idx]
    cdfs_cohorts.append(cdfs_cohort)
print(num_cohorts_idx)


# ### 2. Fit C-vine copulas for each of the cohorts
#
# We need the quantiles data and a target stock index to fit a C-vine copula.
# Here the target stock index is indexed from 1, not 0. It indicates in the DataFrame `data`, which one is the target stock.
#
# You can also print out the tree structure for the i-th cohort's vine copula by
#
# `print(structures[i])`


# Fit C-vine copulas, this is slow.
cvinecops = []
structures = []
for cohort_number, cohort in enumerate(cohorts):
    quantiles_data_train = subset_quantiles_train[cohort]
    cvinecop = vinecop_generate.CVineCop()
    structure = cvinecop.fit_auto(data=quantiles_data_train, pv_target_idx=1, if_renew=True)
    
    cvinecops.append(cvinecop)
    structures.append(structure)


# ### 3. Formulate CMPIs using the trading class
#
# Mispricing index (MPI) are just conditional cumulative probabilities, which can be calculated directly from the vine copula by numerical integration.
# Cumulative mispricing index (CMPI) is the de-meaned sum of MPIs, i.e.,
#
# `CMPIs = (MPIs - 0.5).cumsum()`
#
# CMPI is used to determine whether the target stock is overpriced or underpriced compared to other stocks in the cohort.
# CMPIs should be resonably similar in shape to cumulative log returns.
# This can be used as a sanity check on whether our model fit makes sense.


# Calculate CMPIs
mpis_cohorts = []
cmpis_cohorts = []
cvcss = []
for cohort_number, cohort in enumerate(cohorts):
    # Initiate the trading class
    cvcs = vinecop_strategy.CVineCopStrat(cvinecops[cohort_number])
    cvcss.append(cvcs)
    # Calculate MPIs for the trading period
    mpis_trading = cvcs.calc_mpi(returns=subset_rts_test[cohort], cdfs=cdfs_cohorts[cohort_number],
                                 subtract_mean=True)
    mpis_cohorts.append(mpis_trading)
    
    cmpis_trading = mpis_trading.cumsum()
    cmpis_cohorts.append(cmpis_trading)


# For cohort 0, plot the cmpis against the log prices for the trading period
sum_log_returns = (subset_rts_test + 1).apply(np.log, axis=0).cumsum()

fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [1, 1]}, figsize=(10,8), dpi=150)

axs[0].set_title('Cumulative Log Returns of Stocks')
axs[0].plot(sum_log_returns['A'], label='A, target')
axs[0].plot(sum_log_returns['PKI'], label='PKI')
axs[0].plot(sum_log_returns['TMO'], label='TMO')
axs[0].plot(sum_log_returns['MTD'], label='MTD')
axs[0].plot(np.log(prices_spy_test) - np.log(prices_spy_test)[0], label='SPY, benchmark')
axs[0].plot()
axs[0].grid()
axs[0].legend()

axs[1].set_title('Cumulative Mispricing Index of the Target Stock')
axs[1].plot(cmpis_cohorts[0], label='CMPI')
axs[1].grid()

fig.autofmt_xdate()

plt.show()


# **Spoiler**: this is only the cohort that ends up losing money in the final equity curve check.
# The reason may come from that all the 4 stocks are not behaving like the SPY at all.
# Hence when traded against SPY the method falls apart.
# This may be a valid place for potential improvements of the strategy.


# For cohort 1, plot the cmpis against the log prices for the trading period

fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [1, 1]}, figsize=(10,8), dpi=150)

axs[0].set_title('Cumulative Log Returns of Stocks')
axs[0].plot(sum_log_returns['AAL'], label='AAL, target')
axs[0].plot(sum_log_returns['LUV'], label='LUV')
axs[0].plot(sum_log_returns['UAL'], label='UAL')
axs[0].plot(sum_log_returns['ALK'], label='ALK')
axs[0].plot(np.log(prices_spy_test) - np.log(prices_spy_test)[0], label='SPY, benchmark')
axs[0].legend()
axs[0].grid()

axs[1].set_title('Cumulative Mispricing Index of the Target Stock')
axs[1].plot(cmpis_cohorts[1], label='CMPI')
axs[1].grid()

fig.autofmt_xdate()

plt.show()


# ### 4. Trade using Bollinger band
#
# The trading scheme follows a dollar neutral strategy: if the position is long, then we long 1/2 dollar the target stock and short accordingly 1/2 dollar SPY, and vice versa for short positions.
#
# The Bollinger band is just a fixed length moving average of CMPI, together with upper limit and lower limit determined by a constant multiplied by moving standard deviations.
# Signal is generated as follows:
#
# * When the CMPI > upper limit of the Bollinger band, Short
# * When the CMPI < lower limit of the Bollinger band, Long
# * When the CMPI crosses with the moving average, Exit
# * Else, Do nothing
#
# See below for an example:


Image(filename='images/Bollinger_band_example.png')


# We now calculate positions, bollinger band and units for all of the cohorts
positions_cohorts = []
bband_cohorts = []
units_cohorts = []
for cohort_number, cohort in enumerate(cohorts):
    positions, bollinger_band = cvcss[cohort_number].get_positions_bollinger(
        returns=subset_rts_test[cohort], cdfs=cdfs_cohorts[cohort_number],
        mpis=mpis_cohorts[cohort_number]+0.5, if_return_bollinger_band=True, threshold_std=2)
    
    units = cvcss[cohort_number].positions_to_units_against_index(
        target_stock_prices=sp500_prices[cohort[0]][800:1200],
        index_prices=prices_spy_test,
        positions=positions.shift(1),
        multiplier=200)
    
    positions_cohorts.append(positions)
    bband_cohorts.append(bollinger_band)
    units_cohorts.append(units)


# ### 5. Calcluate portfolio P&L and equity curve
#
# Now for all cohorts we calculate the daily P&L, and their equity curve.
# Then we total together to get the sum.
# We expect the risk shrinks as the number of cohorts increases due to diversification.


equity_cohorts = []
for cohort_number, cohort in enumerate(cohorts):
    portfolio_pnl = subset_rts_test[cohort[0]] * units_cohorts[cohort_number][cohort[0]] \
                  + sp500_returns['SPY'][800:1200] * units_cohorts[cohort_number]['SPY']
    equity = portfolio_pnl.cumsum()
    equity_cohorts.append(equity)
    
total_equity = equity_cohorts[0] + equity_cohorts[1] + equity_cohorts[2] + equity_cohorts[3]


fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [1, 1]},figsize=(10,8), dpi=200)

axs[0].plot(equity_cohorts[0], label='Cohort 0')
axs[0].plot(equity_cohorts[1], label='Cohort 1')
axs[0].plot(equity_cohorts[2], label='Cohort 2')
axs[0].plot(equity_cohorts[3], label='Cohort 3')
axs[0].legend()
axs[0].grid()
axs[0].set_title('Equity Curve in Dollars, for 100 Dollar Investment Limit')

axs[1].plot(total_equity, label='total equity')
axs[1].plot(total_equity - equity_cohorts[1], label='total equity excluding cohort 1')
axs[1].legend()
axs[1].grid()
axs[1].set_title('Equity Curve Sum')

fig.tight_layout()

plt.show()


# ## Conclusion
# Vine copula provides a very flexible approach in modeling multi-variate dependencies. The C-vine structure specifically highlights a dominant component at every level of the tree, ideal for our “1-vs-the rest” trading strategy for capturing statistical arbitrage among multiple stocks, which non-quant strategies often omit or are still primitive.
#
# As promising as it looks, just like any other methods, it inevitably bears  some minor drawbacks:
#
# 1. High start-up cost: to understand this method, the user needs to understand copula modeling from scratch, and also how to interprete vine copula models from end to end.
#
# 2. High computation cost: For a cohort of 4 stocks and 3 years of daily training data + 1 year of test data, it takes about 30 seconds to fit and generate positions. This can hardly be optimized further since the fitting algorithm is already written in an optimized C++ library. And the computation time should scale up in O(N!)
#
# This is just for fitting and generating positions without factoring into the time for stocks selection.
#
# Interpretability: Since the exact fitting algorithms are quite complicated, the interpretability may suffer in back tracking possible fitting issues and how to evaluate each fit. Also, the high dimension makes it difficult to produce an intuitive plot just to visually check if the model is correct, unlike bivariate copulas.


# ## References
# - [Stübinger, J., Mangold, B. and Krauss, C., 2018. Statistical arbitrage with vine copulas. Quantitative Finance, 18(11), pp.1831-1849](https://www.tandfonline.com/doi/pdf/10.1080/14697688.2018.1438642?casa_token=olPBPI2bc3IAAAAA:8QViZfM9C0pbxGrarr-BU-yO2Or_wkCF_Pvk4dJFppjNtFzWjfM7W14_oc_ztl_1csHe4gFfloEWyA)
# - [Joe, H. and Kurowicka, D. eds., 2011. Dependence modeling: vine copula handbook. World Scientific.](https://www.worldscientific.com/worldscibooks/10.1142/7699)
# - [Yu, R., Yang, R., Zhang, C., Špoljar, M., Kuczyńska-Kippen, N. and Sang, G., 2020. A Vine Copula-Based Modeling for Identification of Multivariate Water Pollution Risk in an Interconnected River System Network. Water, 12(10), p.2741.](https://www.mdpi.com/2073-4441/12/10/2741/pdf)
# - [Dissmann, J., Brechmann, E.C., Czado, C. and Kurowicka, D., 2013. Selecting and estimating regular vine copulae and application to financial returns. Computational Statistics & Data Analysis, 59, pp.52-69.](https://arxiv.org/pdf/1202.2002)



// ---------------------------------------------------

// Copula_Notebook_Liew_etal(legacy).py
// arbitrage_research/Copula Approach/Copula_Notebook_Liew_etal(legacy).py
# Generated from: Copula_Notebook_Liew_etal(legacy).ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Copula Pair's Trading Basic Strategy
#
# This notebook demonstrates the usage of the `copula_strategy` module.
# The framework is originally proposed in 
# - Liew, Rong Qi, and Yuan Wu. "Pairs trading: A copula approach."
# - Stander, Yolanda, Daniël Marais, and Ilse Botha. "Trading strategies with copulas."
#
# ## A Bare Minimum Intro to Copula
#
# ### What Merit Does It Have
# Consider having a pair of cointegrated stocks. 
# By analyzing their time series, one can calculate their standardized price gap as part of a distance approach, or project their long-run mean as in a cointegrated system as part of a cointegration approach. 
# However, none of the two methods are built with the assumptions on distributions from the stocks' time series, which may lead to unused information.
# Further, just using a single parameter (i.e., Euclidean distance) to quantify two coupled time series might have fundamentally simplified the problem too much.
# The copula model naturally incorporates their marginal distributions, together with other interesting properties from each copula inherited from their own structures, e.g., tail dependency for capturing rare and/or extreme moments like large, cointegrated swings in the market.
#
# Briefly speaking, a copula is a tool to capture details of how two random variables are “correlated”.
# By having a more detailed modeling framework, we expect the pairs trading strategy followed to be more realistic and robust, and possibly to bring more trading opportunities.
#
# ### Definitions for Bivariate Copula
# (**Definition using Sklar's Theorem**) For two random variables $S_1$, $S_2 \in [-\infty, \infty]$.
# $S_1$ and $S_2$ have their own fixed, continuous CDFs $F_1, F_2$.
# Consider their (cumulative) joint distribution $H(s_1, s_2) := P(S_1 \le s_1, S_2 \le s_2)$.
# Now take the uniformly distributed quantile random variable $U_1(S_1)$, $U_2(S_2)$, for every pair
# $(u_1, u_2)$ drawn from the pair's quantile we define the **bivariate copula**
# $C: [0, 1] \times [0, 1] \rightarrow [0, 1]$ as:
#
# $$
#     \begin{align}
#     C(u_1, u_2) &= P(U_1 \le u_1, U_2 \le u_2) \\
#     &= P(S_1 \le F_1^{-1}(u_1), S_2 \le F_2^{-1}(u_2)) \\
#     &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))
#     \end{align}
# $$
# where $F_1^{-1}$ and $F_2^{-1}$ are quasi-inverse of the marginal CDFs $F_1$ and $F_2$.
# The definition, although mathematically more fundamental, is not used as much in trading.
# Instead, cumulative conditional probabilities and copula density are used more often, which we will define as below:
#
# ### Conditional Probabilities
# $$
#     \begin{align}
#     P(U_1\le u_1 | U_2 = u_2) &:= \frac{\partial C(u_1, u_2)}{\partial u_2}, \\
#     P(U_2\le u_2 | U_1 = u_1) &:= \frac{\partial C(u_1, u_2)}{\partial u_1}.
#     \end{align}
# $$
# ### Copula Density
#
# $$
#     c(u_1 , u_2) := \frac{\partial^2 C(u_1, u_2)}{\partial u_1 \partial u_2},
# $$
# which by definition is the probability density.
#
# For interested readers, Nelsen's book [An Introduction to Copulas](https://www.springer.com/gp/book/9780387286594) and note [Properties and applications of copulas: A brief survey](http://w4.stern.nyu.edu/ioms/docs/sg/seminars/nelsen.pdf) presents more rigorously and thoroughly on the subject.


# ## How to Use Copula for Trading
#
# ### Use Conditional Probabilities
# This was originally proposed in [Liew et al., 2013] and [Stander et al., 2013].
# We start with a pair of stocks of interest $S_1$ and $S_2$, which can be selected by various methods.
# For example, using the Engle-Granger test for cointegration.
# By consensus, we define the spread as $S_1$ in relation to $S_2$.
# e.g. Short the spread means buying $S_1$ and/or selling $S_2$.
#
# Use **cumulative log return** data of the stocks during the training/formation period, we proceed with a **pseudo-MLE** fit to establish a copula that reflects the relation of the two stocks during the training/formation period.
#
# **Note**: <br>
# the type of processed data fed in need to be **approximately stationary**.
# i.e., $\mathbb{E}[X(t_1)] \approx \mathbb{E}[X(t_2)]$ for time series $X$, for all $t_1, t_2$ in
# the scope of interest.
# For example, if we model each stock's price to have a log-Normal distribution, then the price itself cannot be stationary after some time.
# Using cumulative log return has the same issue if the time span is sufficiently long.
# One can consider just using the daily return or its logarithm instead, given that the stock's price has a log-Normal distribution. i.e., $\frac{X(t+1)}{X(t)}$ or $\ln \left( \frac{X(t+1)}{X(t)} \right)$.
#
# - $u_i \in [0, 1]$ is the quantile of trading period data mapped by a CDF formed in the training period.
# - When $P(U_1\le u_1 | U_2 = u_2) < 0.5$, then stock 1 is considered under-valued.
# - When $P(U_1\le u_1 | U_2 = u_2) > 0.5$, then stock 1 is considered over-valued.
#
# Now we define an upper threshold $b_{up}$ (e.g. $0.95$) and a lower threshold $b_{lo}$ (e.g. $0.05$),
# then the logic goes as follows:
#
# - If $P(U_1\le u_1 | U_2 = u_2) \le b_{lo}$ and $P(U_2\le u_2 | U_1 = u_1) \ge b_{up}$, then stock 1 is
#   undervalued, and stock 2 is overvalued. Hence we long the spread.
# - If $P(U_2\le u_2 | U_1 = u_1) \le b_{lo}$ and $P(U_1\le u_1 | U_2 = u_2) \ge b_{up}$, then stock 2 is
#   undervalued, and stock 1 is overvalued. Hence we short the spread.
# - If one of the conditional probabilities cross the boundary of $0.5$ in relation to its previous time step, then we exit the position, as we consider the position is no longer valid.


# ### `CopulaStrategy` Class Functionalities
# Tools presented in this module enable the user to:
# * Transform and fit pair's price data to a given type of copula;
# * Sample and plot from a given copula;
# * Generate trading positions given the pair's data using a copula:
#     - Feed in training lists (i.e., data from 2016-2019) and thus generate a position list.
#     - Feed in a single pair's data point (i.e., EOD data from just today) and thus generate a single position.
#
# There are 8 commonly used ones that are now available: `Gumbel`, `Frank`, `Clayton`, `Joe`, `N13`, `N14`,
# `Gaussian`, and `Student` (Student-t).
# They are all subclasses of the class `Copula`, and they share some common repertoire of methods and attributes.
# However, most of the time, the user is not expected to directly use the copulas.
# All trading related functionalities are stated above, and included in the `CopulaStrategy` class.
#
# The user may choose to fit the pair's data to all provided copulas, then compare the information criterion scores (AIC,
# SIC, HQIC) to decide the best copula. One can further use the fitted copula to generate trading positions by giving
# thresholds from data.
#
# **Very Important Note**:<br>
# For `Student` copula, the user can choose to provide the degrees of freedom parameter $\nu$, as an extra input argument `nu`.
# Use $\nu = \text{sample size} - 1$ is strongly discouraged, as data from time series are clearly not mutually independent instances. For $\nu > 12$, consider using the `Gaussian` copula instead.
#
# ### `copula_calculation` Module
# To be able to calculate marginal CDFs (quantile function) for plotting purposes or processing other data to be able to feed to some copula, one may use its `find_marginal_cdf` function,
# although `ECDF` from `statsmodels.distributions.empirical_distribution` suffices for a general plotting purpose.
# This function finds an empirical CDF based on given training data, and does not generate $0$ or $1$, instead gives numbers sufficiently close to $0$ and $1$.
# This is done because when some copulas take values in $0$ or $1$, the density and/or marginal conditional probability may become $\infty$.
#
# ## Usage
# The demonstratration part has the following sections using real world data:
# 1. Importing and Data Pre-processing.
# 2. Fitting Data to Different Copulas with Training Data.
# 3. Generating Trading Positions using Testing Data.
# 4. Possible Issues Discussion.


# ### 1. Importing and Data Pre-processing


# Importing libraries and modules
import pandas as pd
import numpy as np
import datetime as dt  # For plotting x-axis as dates
import matplotlib.pyplot as plt
import statsmodels.api as sm
from arbitragelab.copula_approach.copula_strategy import CopulaStrategy
import arbitragelab.copula_approach.copula_calculation as ccalc


# For the copula approach to work, the stocks pair need to be cointegrated to begin with.
# Here we choose BKD and ESC, the same as the author, and use their daily closing price from the start of 2009 to the end of 2012.
# Note that the original price series is not what the copula will use. Instead, we use cumulative return data implied from the price series.


# Importing data
pair_prices = pd.read_csv(r'BKD_ESC_2009_2011.csv', index_col=0)
# Formatting dates
formatted_dates = [dt.datetime.strptime(d,'%m/%d/%Y').date() for d in pair_prices.index]
pair_prices.index = formatted_dates

BKD_series = pair_prices['BKD'].to_numpy()
ESC_series = pair_prices['ESC'].to_numpy()


# Now we take a look at the price series for the whole period.
# They indeed have cointegrated behavior at first glance.
# We then do a training and testing split to fit and simulate trading.


plt.figure(dpi=120)
plt.plot(formatted_dates, BKD_series, label='BKD')
plt.plot(formatted_dates, ESC_series, label='ESC')
plt.axvline(dt.date(2012, 1, 3), color='red')  # Training testing split date
plt.legend()
plt.grid()
plt.gcf().autofmt_xdate()
plt.title(r'Original Price Series of BKD and ESC')
plt.show()


# Initiate the analysis module
CS = CopulaStrategy()

# Calculate cumulative log return
BKD_clr = CS.cum_log_return(BKD_series)
ESC_clr = CS.cum_log_return(ESC_series)

# Training and testing split
training_length = 756# From 01/02/2009 to 12/30/2011 (m/d/y)

BKD_train = BKD_clr[ : training_length]
ESC_train = ESC_clr[ : training_length]
BKD_train_prices = BKD_series[ : training_length]
ESC_train_prices = ESC_series[ : training_length]
train_dates = formatted_dates[ : training_length]

BKD_test = BKD_clr[training_length : ]
ESC_test = ESC_clr[training_length : ]
BKD_test_prices = BKD_series[training_length : ]
ESC_test_prices = ESC_series[training_length : ]
test_dates = formatted_dates[training_length : ]

# Empirical CDF for the training set.
# This step is necessary for plotting.
cdf1 = ccalc.find_marginal_cdf(BKD_train)
cdf2 = ccalc.find_marginal_cdf(ESC_train)


# ### 2. Fitting Data to Different Copulas with Training Data.
# Here we fit to every copula type available in the module, and print out the scores in terms of SIC, AIC, and HQIC. The lower the score (Note they are all negative), the better the fit.
#
# **Note**:<br>
# 1. `fit_copula` returns the fit result, the fitted copula, and marginal CDFs for the two input lists. Here the `cdf1`, `cdf2` is exactly the same as directly using `find_marginal_cdf` in `copula_calculation` module.
# 2. For `Student` copula, the MLE of $\nu$ is slow.


# Fit different copulas, store the results in dictionaries
fit_result_gumbel, copula_gumbel, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Gumbel')

fit_result_frank, copula_frank, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Frank')

fit_result_clayton, copula_clayton, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Clayton')

fit_result_joe, copula_joe, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Joe')

fit_result_n13, copula_n13, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='N13')

fit_result_n14, copula_n14, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='N14')

fit_result_gauss, copula_gauss, cdf1, cdf2 =\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Gaussian')

fit_result_t, copula_t, cdf1, cdf2=\
    CS.fit_copula(s1_series=BKD_train, s2_series=ESC_train, copula_name='Student')


# print all the fit scores
print(fit_result_gumbel)
print(fit_result_frank)
print(fit_result_clayton)
print(fit_result_joe)
print(fit_result_n13)
print(fit_result_n14)
print(fit_result_gauss)
print(fit_result_t)


# It seems by the score, N13 and Student-t are reasonable choices of copula that fit our training data the best.
# Clayton also look somewhat reasonable, although the tail dependency on the upper right corner is not taken into account of.
# So at first, we take a look at their information.


print(copula_n13.describe(), '\n')
print(copula_t.describe(), '\n')
print(copula_clayton.describe(), '\n')


# Now we plot the training data (as an empirical copula), together with $5$ other copulas fitted.


fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9,18), dpi=100)
# axs[0, 0]: Empirical.
axs[0, 0].scatter(cdf1(BKD_train), cdf2(ESC_train), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'Empirical from BKD and ESC, Training Data')
axs[0, 0].set_xlim([0,1])
axs[0, 0].set_ylim([0,1])
plt.tight_layout()

# axs[0, 1]: N13.
CS.graph_copula(copula_name='N13', theta=copula_n13.theta,
                ax=axs[0, 1], s=1, num=len(ESC_train))

# axs[1, 0]: Student-t.
CS.graph_copula(copula_name='Student', cov=copula_t.cov, nu=copula_t.nu,
                ax=axs[1, 0], s=1, num=len(ESC_train))
# axs[1, 1]: Joe.
CS.graph_copula(copula_name='Joe', theta=copula_joe.theta,
                ax=axs[1, 1], s=1, num=len(ESC_train))
# axs[2, 0]: Frank.
CS.graph_copula(copula_name='Frank', theta=copula_frank.theta,
                ax=axs[2, 0], s=1, num=len(ESC_train))
# axs[2, 1]: Gumbel.
CS.graph_copula(copula_name='Gumbel', theta=copula_gumbel.theta,
                ax=axs[2, 1], s=1, num=len(ESC_train))
# axs[3, 0]: Clayton.
CS.graph_copula(copula_name='Clayton', theta=copula_clayton.theta,
                ax=axs[3, 0], s=1, num=len(ESC_train))
# axs[3, 1]: Gaussian.
CS.graph_copula(copula_name='Gaussian', cov=copula_gauss.cov,
                ax=axs[3, 1], s=1, num=len(ESC_train))
plt.show()


# ### 3. Generate Trading Positions Using Test Data
# At first we plot the testing data (as an empirical copula) and the two chosen copulas, together with three other copulas fitted.


fig, axs = plt.subplots(nrows=4, ncols=2, figsize=(9,18), dpi=100)
# axs[0, 0]: Empirical.
axs[0, 0].scatter(cdf1(BKD_test), cdf2(ESC_test), s=1)
axs[0, 0].set_aspect('equal', adjustable='box')
axs[0, 0].set_title(r'Empirical from BKD and ESC, Testing Data')
axs[0, 0].set_xlim([0,1])
axs[0, 0].set_ylim([0,1])

# axs[0, 1]: N13.
CS.graph_copula(copula_name='N13', theta=copula_n13.theta,
                ax=axs[0, 1], s=1, num=len(ESC_test))

# axs[1, 0]: Student-t.
CS.graph_copula(copula_name='Student', cov=copula_t.cov, nu=copula_t.nu,
                ax=axs[1, 0], s=1, num=len(ESC_test))
# axs[1, 1]: Joe.
CS.graph_copula(copula_name='Joe', theta=copula_joe.theta,
                ax=axs[1, 1], s=1, num=len(ESC_test))
# axs[2, 0]: Frank.
CS.graph_copula(copula_name='Frank', theta=copula_frank.theta,
                ax=axs[2, 0], s=1, num=len(ESC_test))
# axs[2, 1]: Gumbel.
CS.graph_copula(copula_name='Gumbel', theta=copula_gumbel.theta,
                ax=axs[2, 1], s=1, num=len(ESC_test))
# axs[3, 0]: Clayton.
CS.graph_copula(copula_name='Clayton', theta=copula_clayton.theta,
                ax=axs[3, 0], s=1, num=len(ESC_test))
# axs[3, 1]: Gaussian.
CS.graph_copula(copula_name='Gaussian', cov=copula_gauss.cov,
                ax=axs[3, 1], s=1, num=len(ESC_test))

plt.tight_layout()
plt.show()


# Also, one can run an information test with the fitted copulas using test data.
# This is a necessary step to see how consistent or robust the chosen copula model is.
# If the answer changes greatly from the training data, there are several possibilities, for example:
# * Testing data differ significantly from training data in terms of distributions, although they are still cointegrated.
# * The two stocks should be modeled by another copula that is not included in this module.
# * The market has fundamentally changed and as a result, the two stocks have decoupled.
#
# As a rule of thumb, it is always a good practice to look at multiple models instead of using only one, especially when the outcome is quite sensitive to inputs.
#
# As can be seen from the outputs below, N13 is no longer the best copula to describe the test data.


# Running information tests for the test data with the fitted copula
test_ic_n13 = CS.ic_test(BKD_test, ESC_test, cdf1, cdf2, copula=copula_n13)
test_ic_t = CS.ic_test(BKD_test, ESC_test, cdf1, cdf2, copula=copula_t)
test_ic_clayton = CS.ic_test(BKD_test, ESC_test, cdf1, cdf2, copula=copula_clayton)
print(test_ic_n13)
print(test_ic_t)
print(test_ic_clayton)


# Then we get trading positions from the two chosen copulas.
#
# **Note**:<br>
# Although theoretically, one can change the threshold to potentially bring more trading opportunities, it needs to proceed with caution, as the recommended positions are very sensitive to the choice of copula and training data if the thresholds are not as strict.


# #### Plot Trading Positions
# Plot trading positions with recalculated cumulative log return (CLR) from test data, with the starting reference day as day 1 in the test data.
#
# Here, 0 means no position, 1 means long the spread, -1 means short the spread.
#
# **Note**:<br>
# This is not the same as cumulative log returns used in the test data.
# Suppose the training/testing split happens on day $755$, then the CLR is still calculated as $\ln(S(t))-\ln(S(0))$ for $t>755$.
# However, we are plotting $\ln(S(t))-\ln(S(756))$, so that it is easier to see the spread and evaluate our recommended trading positions.
# It is thus can be understood as a *repositioned log price*, or *reset cumulative log return*.


# Generate Trading Positions
# Use N13 copula.
# Instantiate the CopulaStrategy with the fitted N13 copula.
CS_n13 = CopulaStrategy(copula=copula_n13)
positions_n13 = CS_n13.analyze_time_series(BKD_test, ESC_test, cdf1, cdf2,
                                           upper_threshold=0.95,
                                           lower_threshold=0.05,
                                           start_position=0)  # Author used 0.95 and 0.05
# Use Student-t Copula.
# Instantiate the CopulaStrategy with the fitted Student-t copula.
CS_t = CopulaStrategy(copula=copula_t)
positions_t = CS_t.analyze_time_series(BKD_test, ESC_test, cdf1, cdf2,
                                         upper_threshold=0.95,
                                         lower_threshold=0.05,
                                         start_position=0)
# Use Clayton copula.
# Instantiate the CopulaStrategy with the fitted Clayton copula.
CS_clayton = CopulaStrategy(copula=copula_clayton)
positions_clayton = CS_t.analyze_time_series(BKD_test, ESC_test, cdf1, cdf2,
                                             upper_threshold=0.95,
                                             lower_threshold=0.05,
                                             start_position=0)

# Roll all positions forward by 1 day
positions_n13 = np.roll(positions_n13, 1)
positions_t = np.roll(positions_t, 1)
positions_clayton = np.roll(positions_clayton, 1)

# Reset position at day 0
positions_n13[0] = 0
positions_t[0] = 0
positions_clayton[0] = 0


fig, axs = plt.subplots(4, 1, gridspec_kw={'height_ratios': [3, 0.7, 0.7, 0.7]}, figsize=(10,10), dpi=150)
fig.suptitle('Copula Trading Strategy Results')
# Plotting repositioned log prices
axs[0].plot(test_dates, BKD_test-BKD_test[0], label='BKD', color='cornflowerblue')
axs[0].plot(test_dates, ESC_test-ESC_test[0], label='ESC', color='seagreen')
axs[0].title.set_text('(Repositioned) Log Prices')
axs[0].legend()
axs[0].grid()
# Plotting positions from N13
axs[1].plot(positions_n13, label='Positions', color='darkorange')
axs[1].title.set_text('Positions from N13 Copula')
axs[1].set_yticks([-1,0,1])
# Plotting positions from Student-t
axs[2].plot(positions_t, label='Positions', color='darkorange')
axs[2].title.set_text(r'Positions from Student-t Copula')
axs[2].set_yticks([-1,0,1])
# Plotting positions from Clayton
axs[3].plot(positions_clayton, label='Positions', color='darkorange')
axs[3].title.set_text(r'Positions from Clayton Copula')
axs[3].set_yticks([-1,0,1])

fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Avoid title overlap
plt.show()


# #### Equity Curve for the Portfolio
# 1. Defining weights (hedging ratio)
# 2. Calculating returns
# 3. Portfolio prices
# 4. Equity curve for copula strategy
#
# There are multiple methods to define weights.
# For example, using OLS or Johansen test on training data.
# Here we define weights as below to reflect the (repositioned) series mentioned above:
# At first, we define the unnormalized weight $w_{1}^* = w_{BKD}$ and $w_2^* = w_{ESC}$ for $S_1 = S_{BKD}$ and $S_2 = S_{ESC}$ as follows:
# $$
# w_{1}^* S_{1}[0] - w_{2}^* S_{2}[0] = 0
# $$
# with another arbitrary linear constraint
# $$w_1^* - w_2^* = 1.$$
# So that it has value $0$ initially.
# Hence
# $$
# w_1^* = -\frac{S_2[0]}{S_1[0]-S_2[0]}, \quad
# w_2^* = -\frac{S_1[0]}{S_1[0]-S_2[0]}.
# $$
# Then we normalize them so that
# $$
# w_1 = \frac{w_1^*}{w_1^* + w_2^*}, \quad
# w_2 = \frac{w_2^*}{w_1^* + w_2^*}
# $$


# 1. Calculating weights
w1_star = -ESC_test_prices[0] / (BKD_test_prices[0] - ESC_test_prices[0])
w2_star = w1_star - 1

w1 = w1_star / (w1_star + w2_star)
w2 = w2_star / (w1_star + w2_star)
print('Unnormalized weight: \n\
w1_star={}, \nw2_star={},\n\
Normalized weight:\n\
w1={} \nw2={}'.format(w1_star, w2_star, w1, w2))


# 2. Calculating Portfolio Series and daily P&L
portfolio_prices = w1 * BKD_test_prices - w2 * ESC_test_prices
portfolio_pnl = np.diff(portfolio_prices, prepend=0)

# 3. Plotting portfolio prices
fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(test_dates, portfolio_prices)
ax.title.set_text('Unit Portfolio Value for Pair ("BKD", "ESC") Calcualted From Price Series')
ax.grid()
fig.autofmt_xdate()
plt.show()

# 4. Calculating strategy daily P&L
pnl_n13 = portfolio_pnl * positions_n13
pnl_t = portfolio_pnl * positions_t
pnl_clayton = portfolio_pnl * positions_clayton
equity_n13 = pnl_n13.cumsum()
equity_t = pnl_t.cumsum()
equity_clayton = pnl_clayton.cumsum()

fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(test_dates, equity_n13, label='N13')
ax.plot(test_dates, equity_t, '--', label=r'Student-t')
ax.plot(test_dates, equity_clayton, ':', label=r'Clayton')
ax.title.set_text('Strategy Performance on Unit Portfolio Calcualted From Daily P&L')
ax.grid()
fig.autofmt_xdate()
ax.legend()
plt.show()


# ### 4. Possible Issues Discussion
#
# #### Data Processing
# Log prices or CLR are not fundamentally stationary, and especially in real data, the training period may differ significantly from the testing period, rendering testing period data out of range.
# One may consider other alternatives.
#
# #### Copula Fitting
# The fitting results are very sensitive to the inputs.
# And more often than not, the fit score among copulas are not very different from each other.
# In this case, one should look at the few best-suited copulas as candidates, instead of using a single copula.
#
# #### Copula Modeling
# Modeling and trading a pair of stocks' movements using copula, as implemented in the module, treats each stocks time series data as a random variable with stationary distribution.
# Though mathematically valid, this approach does not take into account each random variable as a time series, and re-shuffle the training data will yield the exact same result in terms of  fitted copula parameters and thus recommended positions.
# However, when compared to a vanilla Euclidean distance approach, this is one step further.
#
# Moreover, all the commonly used copulas are either Archimedean (Gumbel, Frank, etc.) or Elliptic (Student-t, Gaussian), which share some nice properties such as symmetry.
# However, the best-suited copula behind a pair of stocks may not necessarily be symmetric.


# ## Conclusion
# (This section follows from [Liew et al. 2013])<br>
# Copula introduces delicate assumptions on the exact coupled structure, along with other nice properties, of two random variables.
# In the pairs trading context, two stocks time series.
#
# * When compared to the Euclidean distance or cointegration approach, copula does not rely on assumptions of linear association or correlation coefficients as a measure of dependency.
#
# * Copula-based approach results in a far richer set of information, such as the shape and nature of the dependency between the stock pairs, thereby leading to potentially more robust modeling of the pair.
#
# * Some copula choices measure well with upper and lower tail dependencies of different extent, in an environment that considers both linear and non-linear relationship. 
#
# * Copulas possess an attractive property of being invariant under strictly monotone transformations of random variables. In other words, the same copula will be obtained regardless of whether the analyst is using price series or log price series.


# ## References ##
# - [Liew, R.Q. and Wu, Y., 2013. Pairs trading: A copula approach. Journal of Derivatives & Hedge Funds, 19(1), pp.12-30.](https://link.springer.com/article/10.1057/jdhf.2013.1)
# - [Stander, Y., Marais, D. and Botha, I., 2013. Trading strategies with copulas. Journal of Economic and Financial Sciences, 6(1), pp.83-107.](https://www.researchgate.net/publication/318054326_Trading_strategies_with_copulas)



// ---------------------------------------------------

// Copula_Strategy_Basic.py
// arbitrage_research/Copula Approach/Copula_Strategy_Basic.py
# Generated from: Copula_Strategy_Basic.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Copula Pair's Trading Basic Strategy
#
# This notebook demonstrates the usage of the `BasicCopulaTradingRule` from the Trading Strategies module.
# The framework is originally proposed in 
# - Liew, Rong Qi, and Yuan Wu. "Pairs trading: A copula approach."
# - Stander, Yolanda, Daniël Marais, and Ilse Botha. "Trading strategies with copulas."
#
# ## A Bare Minimum Intro to Copula
#
# ### What Merit Does It Have
# Consider having a pair of cointegrated stocks. 
# By analyzing their time series, one can calculate their standardized price gap as part of a distance approach, or project their long-run mean as in a cointegrated system as part of a cointegration approach. 
# However, none of the two methods are built with the assumptions on distributions from the stocks' time series, which may lead to unused information.
# Further, just using a single parameter (i.e., Euclidean distance) to quantify two coupled time series might have fundamentally simplified the problem too much.
# The copula model naturally incorporates their marginal distributions, together with other interesting properties from each copula inherited from their own structures, e.g., tail dependency for capturing rare and/or extreme moments like large, cointegrated swings in the market.
#
# Briefly speaking, a copula is a tool to capture details of how two random variables are “correlated”.
# By having a more detailed modeling framework, we expect the pairs trading strategy followed to be more realistic and robust, and possibly to bring more trading opportunities.


# ### Definitions for Bivariate Copula
# (**Definition using Sklar's Theorem**) For two random variables $S_1$, $S_2 \in [-\infty, \infty]$.
# $S_1$ and $S_2$ have their own fixed, continuous CDFs $F_1, F_2$.
# Consider their (cumulative) joint distribution $H(s_1, s_2) := P(S_1 \le s_1, S_2 \le s_2)$.
# Now take the uniformly distributed quantile random variable $U_1(S_1)$, $U_2(S_2)$, for every pair
# $(u_1, u_2)$ drawn from the pair's quantile we define the **bivariate copula**
# $C: [0, 1] \times [0, 1] \rightarrow [0, 1]$ as:
#
# $$
#     \begin{align}
#     C(u_1, u_2) &= P(U_1 \le u_1, U_2 \le u_2) \\
#     &= P(S_1 \le F_1^{-1}(u_1), S_2 \le F_2^{-1}(u_2)) \\
#     &= H(F_1^{-1}(u_1), F_2^{-1}(u_2))
#     \end{align}
# $$
# where $F_1^{-1}$ and $F_2^{-1}$ are quasi-inverse of the marginal CDFs $F_1$ and $F_2$.
# The definition, although mathematically more fundamental, is not used as much in trading.
# Instead, cumulative conditional probabilities and copula density are used more often, which we will define as below:
#
# ### Conditional Probabilities
# $$
#     \begin{align}
#     P(U_1\le u_1 | U_2 = u_2) &:= \frac{\partial C(u_1, u_2)}{\partial u_2}, \\
#     P(U_2\le u_2 | U_1 = u_1) &:= \frac{\partial C(u_1, u_2)}{\partial u_1}.
#     \end{align}
# $$
# ### Copula Density
#
# $$
#     c(u_1 , u_2) := \frac{\partial^2 C(u_1, u_2)}{\partial u_1 \partial u_2},
# $$
# which by definition is the probability density.
#
# For interested readers, Nelsen's book [An Introduction to Copulas](https://www.springer.com/gp/book/9780387286594) and note [Properties and applications of copulas: A brief survey](http://w4.stern.nyu.edu/ioms/docs/sg/seminars/nelsen.pdf) presents more rigorously and thoroughly on the subject.


# ## Basic Strategy (Conditional Probabilities Threshold)
#
# ### Logic
# This was originally proposed in [Liew et al., 2013] and [Stander et al., 2013].
# We start with a pair of stocks of interest $S_1$ and $S_2$, which can be selected by various methods.
# For example, using the Engle-Granger test for cointegration.
# By consensus, we define the spread as $S_1$ in relation to $S_2$.
# e.g. Short the spread means buying $S_1$ and/or selling $S_2$.
#
# Use **cumulative log return** data of the stocks during the training/formation period, we proceed with a **pseudo-MLE** fit to establish a copula that reflects the relation of the two stocks during the training/formation period.
#
# **Note**: <br>
# the type of processed data fed in need to be **approximately stationary**.
# i.e., $\mathbb{E}[X(t_1)] \approx \mathbb{E}[X(t_2)]$ for time series $X$, for all $t_1, t_2$ in
# the scope of interest.
# For example, if we model each stock's price to have a log-Normal distribution, then the price itself cannot be stationary after some time.
# Using cumulative log return has the same issue if the time span is sufficiently long.
# One can consider just using the daily return or its logarithm instead, given that the stock's price has a log-Normal distribution. i.e., $\frac{X(t+1)}{X(t)}$ or $\ln \left( \frac{X(t+1)}{X(t)} \right)$.
#
# - $u_i \in [0, 1]$ is the quantile of trading period data mapped by a CDF formed in the training period.
# - When $P(U_1\le u_1 | U_2 = u_2) < 0.5$, then stock 1 is considered under-valued.
# - When $P(U_1\le u_1 | U_2 = u_2) > 0.5$, then stock 1 is considered over-valued.
#
# Now we define an upper threshold $b_{up}$ (e.g. $0.95$) and a lower threshold $b_{lo}$ (e.g. $0.05$),
# then the logic goes as follows:
#
# - If $P(U_1\le u_1 | U_2 = u_2) \le b_{lo}$ and $P(U_2\le u_2 | U_1 = u_1) \ge b_{up}$, then stock 1 is
#   undervalued, and stock 2 is overvalued. Hence we long the spread.
# - If $P(U_2\le u_2 | U_1 = u_1) \le b_{lo}$ and $P(U_1\le u_1 | U_2 = u_2) \ge b_{up}$, then stock 2 is
#   undervalued, and stock 1 is overvalued. Hence we short the spread.
# - If **both/either** conditional probabilities cross the boundary of $0.5$, then we exit the position, as we consider the position no longer valid.


# ### Ambiguities and Comments
# The authors did not specify what will happen if the followings occur:
#
# 1. When there is an open signal and an exit signal.
# 2. When there is an open signal and currently there is a position.
# 3. When there is a long and short signal together.
#
# Here is our take:
#
# 1. Exit signal overrides open signal.
# 2. Flip the position to the signal's suggestion. For example, originally have a short position, and receives
#    a long signal, then the position becomes long.
# 3. Technically this should never happen with the default trading logic. However, if it did happen for whatever 
#    reason, long + short signal will lead to no opening signal and the positions will not change, unless there
#    is an exit signal and that resets the position to 0.
#
# For exiting a position, the authors proposed using **'and'** logic: Both conditional probabilities need to cross $0.5$.
# However, we found this too strict and sometimes fails to exit a position when it should.
# Therefore we also provide the **'or'** logic: At least one of the conditional probabilities cross $0.5$.


# ### `BasicCopulaTradingRule` Class Functionalities
#
# Tools presented in this module enable the user to:
#
#   - Fit a selected copula to a training dataset (i.e., data from 2016-2019) and generate emirical CDFs.
#   - Feed in single observations one-by-one (i.e., EOD data from just today) and check if the strategy generates a signal.
#   - If a signal is generated, the position can be opened and added to an internal dictionary of the class that tracks positions.
#   - At any time it's possible to check if the trategy generates a signal to close any of the opened positions. If so, ID's of these positions will be returned and the internal dictionary of the class withh be updated.
#
# There are 8 commonly used pure copulas that are now available: `Gumbel`, `Frank`, `Clayton`, `Joe`, `N13`, 
# `N14`, `GaussianCopula`, and `StudentCopula` (Student-t), and 2 mixed copulas `CFGMixCop` (Clayton-Frank-Gumbel) and 
# `CTGMixCop` (Clayton-Student-Gumbel).
# They are all subclasses of the class `Copula` and `MixedCopula`, and they share some common repertoire of methods and attributes.
#
# The user can chose the appropriate copula, fit data to it and then use it in the trading strategy. This modular approach makes it easier to construct multiple strategies and test hypothesis easier.
#
# The user may choose to fit the pair's data to all provided copulas, then compare the information criterion scores (AIC,
# SIC, HQIC, Log-likelihood) to decide the best copula. One can further use the fitted copula to generate trading positions by giving thresholds from data.
#
# **Very Important Note**:<br>
# For `StudentCopula`, the user can choose to provide the degrees of freedom parameter $\nu$, as an extra input argument `nu`.
# Use $\nu = \text{sample size} - 1$ is strongly discouraged, as data from time series are clearly not mutually independent instances. For $\nu > 12$, consider using the `GaussianCopula` instead.
#
# Mixed copulas especially `CTGMixCop` is relatively slow to fit. However, it generally provides the best log-likelihood score.


# ### `copula_calculation` Module
# To be able to calculate marginal CDFs (quantile function) for plotting purposes or processing other data to be able to feed to some copula, one may use its `construct_ecdf_lin` function.
# It is a wrapper around `ECDF` function from `statsmodels`, and allows linear interpolation between points instead of using a step function as `ECDF`.
#
# This function finds an empirical CDF based on given training data, and does not generate $0$ or $1$, instead gives numbers sufficiently close to $0$ and $1$.
# This is done because when some copulas take values in $0$ or $1$, the calculation of density and/or marginal conditional probability may suffer numerical issues.
#
# ## Usage
# The demonstration part has the following sections using real-world data:
# 1. Importing and Data Pre-processing.
# 2. Fitting Data to Different Copulas with Training Data.
# 3. Generating Trading Positions using Testing Data.
# 4. Possible Issues Discussion.


# ### 1. Importing and Data Pre-processing


# Importing libraries and modules
import pandas as pd
import numpy as np
import datetime as dt  # For plotting x-axis as dates
import matplotlib.pyplot as plt
import statsmodels.api as sm

from arbitragelab.trading import BasicCopulaTradingRule
import arbitragelab.copula_approach.copula_calculation as ccalc
from arbitragelab.copula_approach.archimedean import (Gumbel, Clayton, Frank, Joe, N13, N14)
from arbitragelab.copula_approach.elliptical import (StudentCopula, GaussianCopula)


# For the copula approach to work, the stocks pair need to be cointegrated to begin with.
# Here we choose BKD and ESC, the same as the author, and use their daily closing price from the start of 2009 to the end of 2012.
# Note that the original price series is not what the copula will use. Instead, we use cumulative return data implied from the price series.


# Importing data
pair_prices = pd.read_csv(r'BKD_ESC_2009_2011.csv', index_col=0, parse_dates=True)


# Now we take a look at the price series for the whole period.
# They indeed have cointegrated behavior at first glance.
# We then do a training and testing split to fit and simulate trading.


# Plotting data
plt.figure(dpi=120)
plt.plot(pair_prices['BKD'], label='BKD')
plt.plot(pair_prices['ESC'], label='ESC')
plt.axvline(dt.date(2012, 1, 3), color='red')  # Training testing split date
plt.legend()
plt.grid()
plt.gcf().autofmt_xdate()
plt.title(r'Original Price Series of BKD and ESC')
plt.show()


# Initiate the analysis module
BCS = BasicCopulaTradingRule()

# Training and testing split
training_length = 756 # From 01/02/2009 to 12/30/2011 (m/d/y)

prices_train = pair_prices.iloc[: training_length]
prices_test = pair_prices.iloc[training_length :]

# Empirical CDF for the training set.
# This step is only necessary for plotting.
cdf1 = ccalc.construct_ecdf_lin(prices_train['BKD'])
cdf2 = ccalc.construct_ecdf_lin(prices_train['ESC'])


# ### 2. Fitting Data to Different Copulas with Training Data.
# Here we fit to every copula type available in the module, and print out the scores in terms of SIC, AIC, and HQIC. The lower the score (Note they are all negative), the better the fit.
#
# **Note**:<br>
# 1. `fit_copula_to_empirical_data` returns the fit result, the fitted copula, and marginal CDFs for the two input lists. Here the `cdf1`, `cdf2` is exactly the same as directly using `construct_ecdf_lin` function in `copula_calculation` module.
# 2. For `Student` copula, the MLE of $\nu$ is slow.
# 3. For mixed copulas, the fit is slow however they generally gives the best score.
# 4. You may need to tune the fitting parameters for mixed copulas: gamma_scad, a_scad to avoid warnings outputs.


# Fit different copulas, store the results in dictionaries
fit_result_gumbel, copula_gumbel, cdf_x_gumbel, cdf_y_gumbel =\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=Gumbel)

fit_result_frank, copula_frank, cdf_x_frank, cdf_y_frank =\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=Frank)

fit_result_clayton, copula_clayton, cdf_x_clayton, cdf_y_clayton =\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=Clayton)

fit_result_joe, copula_joe, cdf_x_joe, cdf_x_joe=\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=Joe)

fit_result_n14, copula_n14, cdf_x_n14, cdf_y_n14=\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=N14)

fit_result_gauss, copula_gauss, cdf_x_gauss, cdf_y_gauss =\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=GaussianCopula)

fit_result_t, copula_t, cdf_x_t, cdf_y_t=\
    ccalc.fit_copula_to_empirical_data(x=prices_train['BKD'], y=prices_train['ESC'], copula=StudentCopula)


# **Note**: The warnings generated above is due to the optimization package by scipy not strictly following the prescribed bounds in the mixed copula fitting algo. And based on our experiments it does not influence the fit result with statistical significance. To completely get rid of the warnings, you will need to choose specific values `gamma_scad` and `a_scad` of the fitting parameter, and this heavily depends on the data."


# Print all the fit scores
print(fit_result_gumbel)
print(fit_result_frank)
print(fit_result_clayton)
print(fit_result_joe)
print(fit_result_n14)
print(fit_result_gauss)
print(fit_result_t)


# It seems by the score, Student-t and Gaussian are reasonable choices of copula that fit our training data the best.
# Clayton also looks somewhat reasonable, although the tail dependency on the upper right corner is not taken into account.
# So at first, we take a look at their information.


# Print copula descriptions
print(copula_t.describe(), '\n')
print(copula_gauss.describe(), '\n')
print(copula_clayton.describe(), '\n')


# Now we plot the training data (as an empirical copula), together with $5$ other copulas fitted.


# Plotting copulas
fig, ax = plt.subplots(figsize=(5,7), dpi=100)

ax.scatter(prices_train['BKD'].apply(cdf1), prices_train['ESC'].apply(cdf2), s=1)
ax.set_aspect('equal', adjustable='box')
ax.set_title(r'Empirical from BKD and ESC, Training Data')
ax.set_xlim([-0.02, 1.02])
ax.set_ylim([-0.02, 1.02])
plt.tight_layout()


# N14
copula_n14.plot_scatter(num_points=training_length);


# Student-t
copula_t.plot_scatter(num_points=training_length);


# Joe
copula_joe.plot_scatter(num_points=training_length);


# Frank
copula_frank.plot_scatter(num_points=training_length);


# Gumbel
copula_gumbel.plot_scatter(num_points=training_length);


# Clayton
copula_clayton.plot_scatter(num_points=training_length);


# Gaussian
copula_gauss.plot_scatter(num_points=training_length);


# ### 3. Generate Trading Positions Using Test Data
# At first, we plot the testing data (as an empirical copula) and the two chosen copulas.


# Plotting copulas
fig, ax = plt.subplots(figsize=(5,7), dpi=100)

ax.scatter(prices_test['BKD'].map(cdf1), prices_test['ESC'].map(cdf2), s=1)
ax.set_aspect('equal', adjustable='box')
ax.set_title(r'Empirical from BKD and ESC, Test Data')
ax.set_xlim([-0.02, 1.02])
ax.set_ylim([-0.02, 1.02])
plt.tight_layout()

test_length = len(prices_test)


# Student-t
copula_t.plot_scatter(num_points=test_length);


# Gaussian
copula_gauss.plot_scatter(num_points=test_length);


# Clayton
copula_clayton.plot_scatter(num_points=test_length);


# Then we get trading positions from the two chosen copulas.
#
# **Note**:<br>
# Although theoretically, one can change the threshold to potentially bring more trading opportunities, it needs to proceed with caution, as the recommended positions are very sensitive to the choice of copula and training data if the thresholds are not as strict.


# #### Trading Simulation
#
# As the trading strategy takes new observations one by one, we'll simulate such process.
#
# So first we use the fitted copulas and CDFs to initialize trading strategies. Then we feed observations one by one, checking if the logic to open a long/short trade is triggered. If so, we take the trade and update the internal dictionary of the trading strategy class.
#
# We also check if any of the currently open positions should be closed, and if so, we again update the internal dictionary.
#
# In the end of the simulation we analyze the opened and closed positions during the period.


# Generate Trading Positions
# Using 'and' logic by default.

# ========== Use Student-t Copula ==========
# Instantiate the strategy
BCTR_t = BasicCopulaTradingRule(exit_rule='and', open_probabilities=(0.5, 0.95),
                                exit_probabilities=(0.9, 0.5))
# Adding copula, cdf for X and Y to strategy
BCTR_t.set_copula(copula_t)
BCTR_t.set_cdf(cdf_x_t, cdf_y_t)
# Setting initial probabilities
BCTR_t.current_probabilities = (0.5, 0.5)

# ========== Use Gaussian Copula ==========
# Instantiate the strategy
BCTR_gauss = BasicCopulaTradingRule(exit_rule='and', open_probabilities=(0.5, 0.95),
                                    exit_probabilities=(0.9, 0.5))
# Adding copula, cdf for X and Y to strategy
BCTR_gauss.set_copula(copula_gauss)
BCTR_gauss.set_cdf(cdf_x_gauss, cdf_y_gauss)
# Setting initial probabilities
BCTR_gauss.current_probabilities = (0.5, 0.5)

# ========== Use Clayton Copula ==========
# Instantiate the strategy
BCTR_clayton = BasicCopulaTradingRule(exit_rule='and', open_probabilities=(0.5, 0.95),
                                      exit_probabilities=(0.9, 0.5))
# Adding copula, cdf for X and Y to strategy
BCTR_clayton.set_copula(copula_clayton)
BCTR_clayton.set_cdf(cdf_x_clayton, cdf_y_clayton)
# Setting initial probabilities
BCTR_clayton.current_probabilities = (0.5, 0.5)

# ========== Use Student-t Copula with 'or' logic ==========
# Instantiate the strategy
BCTR_t_or = BasicCopulaTradingRule(exit_rule='or', open_probabilities=(0.5, 0.95),
                                   exit_probabilities=(0.9, 0.5))
# Adding copula, cdf for X and Y to strategy
BCTR_t_or.set_copula(copula_t)
BCTR_t_or.set_cdf(cdf_x_t, cdf_y_t)
# Setting initial probabilities
BCTR_t_or.current_probabilities = (0.5, 0.5)


# Now we can simulate a trading process, feeding observations one by one to the strategy and checking the outputs.


# Simulate trading process on test data for StudentCopula
for time, values in prices_test.iterrows():
    x_price = values['BKD']
    y_price = values['ESC']

    # Adding price values
    BCTR_t.update_probabilities(x_price, y_price)
    
    # Check if it's time to enter a trade
    trade, side = BCTR_t.check_entry_signal()

    # Close previous trades if needed
    BCTR_t.update_trades(update_timestamp=time)

    if trade:  # Open a new trade if needed
        BCTR_t.add_trade(start_timestamp=time, side_prediction=side)

# Finally, check open trades at the end of the simulation
open_trades_t = BCTR_t.open_trades

# And all trades that were opened and closed
closed_trades_t = BCTR_t.closed_trades


# No currently open trades
len(open_trades_t)


# And 22 trades were opened and closed during the testing period
len(closed_trades_t)


# Getting trades for other strategies.


# Simulate trading process on test data for StudentCopula
for time, values in prices_test.iterrows():
    x_price = values['BKD']
    y_price = values['ESC']

    # Adding price values
    BCTR_gauss.update_probabilities(x_price, y_price)
    BCTR_clayton.update_probabilities(x_price, y_price)
    BCTR_t_or.update_probabilities(x_price, y_price)
    
    # Check if it's time to enter a trade
    trade_gauss, side_gauss = BCTR_gauss.check_entry_signal()
    trade_clayton, side_clayton = BCTR_clayton.check_entry_signal()
    trade_t_or, side_t_or = BCTR_t_or.check_entry_signal()

    # Close previous trades if needed
    BCTR_gauss.update_trades(update_timestamp=time)
    BCTR_clayton.update_trades(update_timestamp=time)
    BCTR_t_or.update_trades(update_timestamp=time)

    if trade_gauss:  # Open a new trade if needed
        BCTR_gauss.add_trade(start_timestamp=time, side_prediction=side_gauss)
    if trade_clayton:  # Open a new trade if needed
        BCTR_clayton.add_trade(start_timestamp=time, side_prediction=side_clayton)
    if trade_t_or:  # Open a new trade if needed
        BCTR_t_or.add_trade(start_timestamp=time, side_prediction=side_t_or)
        
# Finally, check open trades at the end of the simulation
open_trades_gauss = BCTR_gauss.open_trades
open_trades_clayton = BCTR_clayton.open_trades
open_trades_t_or = BCTR_t_or.open_trades

# And all trades that were opened and closed
closed_trades_gauss = BCTR_gauss.closed_trades
closed_trades_clayton = BCTR_clayton.closed_trades
closed_trades_t_or = BCTR_t_or.closed_trades


# Let's look at one of the `closed_trades` logs


closed_trades_gauss


# ## Strategy outputs
#
# We can see the following data:
#
# * Dictionary key:
#   * Timestamp at which the trade was opened
# * Dctionary value:
#   * t1: Timestamp at which the trade was closed
#   * exit_proba: Conditional probabilities of X and Y at which the trade was closed
#   * uuid: Trade ID that can be provided for each trade
#   * side: Side of the trade '-1' for short and '1' for long
#   * initial_proba: Conditional probabilities of X and Y at which the trade was opened
#
# We can see that trades were opened starting from late April 2012 and were all closed at the end of July 2012. They can be treated as multiple trades, or we can follow a logic of only one open trade at a time.
#
# This strategy can be adjusted by choosing different open and close probabilities.


# Following a logic of one open trade at a time - using the first trade
open_time = list(closed_trades_gauss.keys())[0]
close_time = list(closed_trades_gauss.values())[0]['t1']
position = list(closed_trades_gauss.values())[0]['side']

# Creating dataframe for viaualization
positions_gauss = pd.DataFrame(0, index=prices_test.index, columns=['Gauss Positions'])
positions_gauss[(open_time < positions_gauss.index) & (positions_gauss.index< close_time)] = position


# Plotting generated positions
fig, axs = plt.subplots(2, 1, gridspec_kw={'height_ratios': [3, 0.7]}, figsize=(10,6), dpi=150)
fig.suptitle('Copula Trading Strategy Results')

# Plotting repositioned log prices
axs[0].plot((prices_test['BKD'] / prices_test['BKD'][0]).map(np.log), label='BKD', color='cornflowerblue')
axs[0].plot((prices_test['ESC'] / prices_test['ESC'][0]).map(np.log), label='ESC', color='seagreen')
axs[0].title.set_text('Repositioned Log Prices')
axs[0].legend()
axs[0].grid()

# Plotting position from Gaussian copula strategy
axs[1].plot(positions_gauss , label='Positions', color='darkorange')
axs[1].title.set_text('Positions from Gaussian Copula, AND logic')
axs[1].set_yticks([-1,0,1])

fig.tight_layout(rect=[0, 0.03, 1, 0.95])  # Avoid title overlap
plt.show()


# #### Equity Curve for the Portfolio
# 1. Defining weights (hedging ratio)
# 2. Calculating returns
# 3. Portfolio prices
# 4. Equity curve for copula strategy
#
# There are multiple methods to define weights.
# For example, using OLS or Johansen test on training data.
# Here we define weights as below to reflect the (repositioned) series mentioned above:
# At first, we define the unnormalized weight $w_{1}^* = w_{BKD}$ and $w_2^* = w_{ESC}$ for $S_1 = S_{BKD}$ and $S_2 = S_{ESC}$ as follows:
# $$
# w_{1}^* S_{1}[0] - w_{2}^* S_{2}[0] = 0
# $$
# with another arbitrary linear constraint
# $$w_1^* - w_2^* = 1.$$
# So that it has value $0$ initially.
# Hence
# $$
# w_1^* = -\frac{S_2[0]}{S_1[0]-S_2[0]}, \quad
# w_2^* = -\frac{S_1[0]}{S_1[0]-S_2[0]}.
# $$
# Then we normalize them so that
# $$
# w_1 = \frac{w_1^*}{w_1^* + w_2^*}, \quad
# w_2 = \frac{w_2^*}{w_1^* + w_2^*}
# $$


# 1. Calculating weights
w1_star = -prices_test['ESC'][0] / (prices_test['BKD'][0] - prices_test['ESC'][0])
w2_star = w1_star - 1

w1 = w1_star / (w1_star + w2_star)
w2 = w2_star / (w1_star + w2_star)

print('Unnormalized weight: \n\
w1_star={}, \nw2_star={},\n\
Normalized weight:\n\
w1={} \nw2={}'.format(w1_star, w2_star, w1, w2))


# 2. Calculating Portfolio Series and daily P&L
portfolio_prices = w1 * prices_test['BKD'] - w2 * prices_test['ESC']
portfolio_pnl = np.diff(portfolio_prices, prepend=0)

# 3. Plotting portfolio prices
fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(portfolio_prices)
ax.title.set_text('Unit Portfolio Value for Pair ("BKD", "ESC") Calcualted From Price Series')
ax.grid()
fig.autofmt_xdate()
plt.show()

# 4. Calculating strategy daily P&L
pnl_gauss = portfolio_pnl * positions_gauss.values.T
equity_gauss = pnl_gauss.cumsum()

fig, ax = plt.subplots(figsize=(10,3), dpi=150)
ax.plot(equity_gauss, '--', color='g',label=r'Gauss AND')
ax.title.set_text('Strategy Performance on Unit Portfolio Calcualted From Daily P&L')
ax.grid()
fig.autofmt_xdate()
ax.legend()
plt.show()


# ### 4. Possible Issues Discussion
#
# #### Data Processing
# Log prices or CLR are not fundamentally stationary, and especially in real data, the training period may differ significantly from the testing period, rendering testing period data out of range.
# One may consider other alternatives.
#
# #### Copula Fitting
# The fitting results are very sensitive to the inputs.
# And more often than not, the fit score among copulas are not very different from each other.
# In some cases, one should look at the few best-suited copulas as candidates, instead of using a single copula.
#
# #### Copula Modeling
# Modeling and trading a pair of stocks' movements using copula, as implemented in the module, treats each stocks time series data as a random variable with stationary distribution.
# Though mathematically valid, this approach does not take into account each random variable as a time series, and re-shuffle the training data will yield the exact same result in terms of  fitted copula parameters and thus recommended positions.
# However, when compared to a vanilla Euclidean distance approach, this is one step further.
#
# Moreover, all the commonly used copulas are either Archimedean (Gumbel, Frank, etc.) or Elliptic (Student-t, Gaussian), which share some nice properties such as symmetry.
# However, the best-suited copula behind a pair of stocks may not necessarily be symmetric.


# ## Conclusion
# (This section follows from [Liew et al. 2013])<br>
# Copula introduces delicate assumptions on the exact coupled structure, along with other nice properties, of two random variables.
# In the pairs trading context, two stocks time series.
#
# * When compared to the Euclidean distance or cointegration approach, copula does not rely on assumptions of linear association or correlation coefficients as a measure of dependency.
#
# * Copula-based approach results in a far richer set of information, such as the shape and nature of the dependency between the stock pairs, thereby leading to potentially more robust modeling of the pair.
#
# * Some copula choices measure well with upper and lower tail dependencies of different extent, in an environment that considers both linear and non-linear relationship. 
#
# * Copulas possess an attractive property of being invariant under strictly monotone transformations of random variables. In other words, the same copula will be obtained regardless of whether the analyst is using price series or log price series.


# ## References ##
# - [Liew, R.Q. and Wu, Y., 2013. Pairs trading: A copula approach. Journal of Derivatives & Hedge Funds, 19(1), pp.12-30.](https://link.springer.com/article/10.1057/jdhf.2013.1)
# - [Stander, Y., Marais, D. and Botha, I., 2013. Trading strategies with copulas. Journal of Economic and Financial Sciences, 6(1), pp.83-107.](https://www.researchgate.net/publication/318054326_Trading_strategies_with_copulas)



// ---------------------------------------------------

// pearson_distance_approach.py
// arbitrage_research/Distance Approach/pearson_distance_approach.py
# Generated from: pearson_distance_approach.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Pearson Approach


# This description of the distance approach closely follows the paper by _Chen et al._ __Empirical investigation of an equity pairs trading strategy__  [available here](http://www.pbcsf.tsinghua.edu.cn/research/chenzhuo/paper/Empirical%20Investigation%20of%20an%20Equity%20Pairs%20Trading%20Strategy.pdf). 


# ## Introduction


# After the distance approach was introduced in the paper by Gatev et al. (2006), a lot of research has been
# conducted to further develop the original distance approach. One of the adjustments is the Pearson correlation
# approach proposed by Chen et al.(2012). In this paper, the authors use the same data set and time frame as
# in the work by Gatev et al.(2006) but they used Pearson correlation on return level for forming pairs.
# In the formation period(5 years in the paper), pairwise return correlations for all pairs in the universe
# are calculated based on monthly return data. Then the authors construct a new variable, Return Divergence
# ($D_{i j t}$), to capture the return divergence between a single stock’s return and its pairs-portfolio returns:
#
#
# $$D_{i j t}=\beta\left(R_{i t}-R_{f}\right)-\left(R_{j t}-R_{f}\right)$$
#
#
# where $\beta$ denotes the regression coefficient of stock's monthly return $R_{i t}$ on its
# pairs-portfolio return $R_{j t}$, and $R_{f}$ is the risk-free rate
#
# The hypothesis in this approach is that if a stock’s return deviates from its pairs portfolio returns more
# than usual, this divergence is expected to be reversed in the next month. And the returns of this stock are
# expected to be abnormally high/low in comparison to other stocks.
#
# Therefore, after calculating the return divergence of all the stocks,  a long-short portfolio is constructed
# based on the long and short ratio given by the user.


# ## Pairs portfolio formation step
#
#
# This stage of PearsonStrategy consists of the following steps:
#
# 1. **Data preprocessing**
#
# As the method has to compute all of the pairs’ correlation values in the following steps, for $m$ stocks,
# there are $\frac{m*(m-1)}{2}$ correlations to be computed in the formation period. As the number of
# observations for the correlations grows exponentially with the number of stocks, this estimation is
# computationally intensive.
#
# Therefore, to reduce the computation burden, this method uses monthly stock returns data in the formation
# period (ex. 60 monthly stock returns if the formation period is 5 years). If the daily price data is given,
# the method calculates the monthly returns before moving into the next steps.
#
# 2. **Finding pairs**
#
# For each stock, the method finds $n$ stocks with the highest correlations to the stock as its pairs using
# monthly stock returns. After pairs are formed, returns of pairs, which refer to as pairs portfolios in the paper,
# are needed to create $beta$ in the following step. Therefore, this method uses two different weighting
# metrics in calculating the returns.
#
#
# The first is an equal-weighted portfolio. The method by default computes the pairs portfolio returns as the
# equal-weighted average returns of the top n pairs of stocks. The second is a correlation-weighted portfolio.
# If this metric is chosen, the method uses the stock’s correlation values to each of the pairs and forms a
# portfolio weighted by these values and the weights are calculated by the formula:
#
#
#
# $$w_{k}=\frac{\rho_{k}}{\sum_{i=1}^{n} \rho_{i}}$$
#
# where $w_{k}$ is the weight of stock k in the portfolio and $\rho_{i}$ is a correlation of the stock
# and one of its pairs.
#
# 3. **Calculating beta**
#
# After pairs portfolio returns are calculated, the method derives beta from the monthly return of the stock and
# its pairs portfolio. By using linear regression, setting stock return as independent variable and pairs portfolio
# return as the dependent variable, the methods set beta as a regression coefficient. Then the beta is stored in a
# dictionary for future uses in trading. 


# ## Trading signals generation
#
#
# After calculating the betas for all of the stocks in the formation period, the next step is to generate trading
# signals by calculating the return divergence for each of the stocks. In this method, test data is not necessarily
# required if only a trading signal for the last month of formation period is needed. However, if one wants to
# see the backtesting results of the method and test with test data, a successive dataset after the formation period
# is required to generate the proper trading signals. The steps are as follows:
#
# 1. **Data Preprocessing**
#
# The same data preprocessing is done with the formation period as the data needs to be in the same format. As in
# the formation period, risk free rate can be given in the form of either a float number of a series of data.
#
# 2. **Calculating the Return Divergence**
#
# For every month of test data, starting from the very last month of the train data, return divergences are calculated
# with the beta created in the formation period. The equation for calculating the return divergence is in the first section
# of this documentation. Note that while the return divergence between a portfolio of $n$ most-highly correlated stocks
# with stock i and stock $i$ is used as a sorting variable, only individual stock $i$ enters the portfolio
# construction, not those $n$ stocks. The portfolio of those $n$ stocks only serves as a benchmark for portfolio sorting.
#
# 3. **Finding Trading Signals**
#
# Then, all stocks are sorted in descending order based on their previous month's return divergence.  If the percentages
# of long and short stocks are given, say $p\%$ and $q\%$, the top $p\%$ of the sorted stocks are chosen
# for the long stocks and the bottom $q\%$ of the sorted stocks are chosen for the short stocks. If a user wants to
# construct a dollar-neutral portfolio, one should choose the same percentage for $p$ and $q$. Finally,
# a new dataframe is created with all of the trading signals: 1 if a stock is in a long position, -1 if it is in a short
# position and 0 if it does not have any position.


# ## Results output 
# The PearsonStrategy class contains multiple methods to get results in the desired form.
#
# Functions that can be used to get data:
#
# - **get_trading_signal()** outputs trading signal in monthly basis. 1 for a long position, -1 for a short position and 0 for closed position.
#
# - **get_beta_dict()** outputs beta, a regression coefficients for each stock, in the formation period.
#
# - **get_pairs_dict()** outputs top n pairs selected during the formation period for each of the stock.


# ## Usage of the Algorithms


# Let's use the above strategy on real data. 
#
# First, we will choose a training period of 12 months to form pairs. Second, we'll create trading signals for the following 6 months window. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns


# ### Loading data


# As a dataset, we'll download the price time series for 272 stocks over a period from 01.2018 to 07.2019. First 5 years of data will be used for training and the following 1 year for trading signal generation and analysis of results. 
#
# Here, we'll use tickers from S&P 500, which are in 4 main industry groups and will use Treasury Yield 10 Years (^TNX) rate as a risk-free rate. However, any source of data can be applied the same as this.


# Get industry data from Wikipedia 
table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
stock_table = table[0]

# Indutry groups to use are below
industry_group = ['Information Technology','Industrials','Financials',
                  'Health Care']

# Get tickers from S&P 500 which are in those industry groups
ticker_industry = stock_table[stock_table['GICS Sector']
                              .isin(industry_group)].reset_index(drop=True)

# Get a dataframe of ticker and industry group
ticker_industry = ticker_industry[['Symbol','GICS Sector']]

# Get tickers to use as a list
tickers = ticker_industry['Symbol'].to_list()
remove_tickers = ['CARR','ABC','BRK.B','VNT','OTIS','OGN'] # Removed tickers
tickers = [ticker for ticker in tickers if ticker not in remove_tickers]

# Get a dictionary of industry group
industry_dict = pd.Series(ticker_industry['GICS Sector'].values,
                          index=ticker_industry['Symbol']).to_dict()

# Loading data
train_data =  yf.download(tickers, start="2015-01-02", end="2018-12-31")
test_data =  yf.download(tickers, start="2019-01-02", end="2019-12-31")

# Taking close prices for chosen instruments
train_data = train_data["Adj Close"]
test_data = test_data["Adj Close"]

# Risk free rate for train and test
risk_free_train = yf.download("^TNX", start="2015-01-02", end="2018-12-31")
risk_free_test = yf.download("^TNX", start="2019-01-02", end="2019-12-31")

# Get a series of risk free rate from the data
risk_free_train = risk_free_train['Adj Close']/100
risk_free_test = risk_free_test['Adj Close']/100

# Looking at the downloaded data
train_data.head()


# The risk free rate series should look like below.


risk_free_train.head()


# ### Forming pairs portfolios


# Now let’s form pairs portfolios and calculate beta for trading signals generation later. 


# Initialising an object containing needed methods
strategy = al.distance_approach.PearsonStrategy()

# Forming pairs portfolio with training data
strategy.form_portfolio(train_data, risk_free_train)


# Variation of the strategy can be applied by changing the number of pairs in the pairs portfolio or changing the weight metric. 


# Initialising an object containing needed methods
strategy_corr = al.distance_approach.PearsonStrategy()

# Forming pairs portfolio with training data
strategy_corr.form_portfolio(train_data, risk_free_train, num_pairs=30,
                             weight='correlation')


# For example, if we look at a pair of stocks (‘A’ and ‘TMO’), we can see that the monthly return moves very similarly to each other in the plot. 


# Get a pair for the stock 'A'
A_pairs_5 = strategy_corr.pairs_dict['A'][:1]

# Appending the stock 'A' to the list 
A_pairs_5.append('A')

# Get monthly return data for both stocks
A_pairs_return = strategy_corr.monthly_return.loc[:,A_pairs_5]

# Plot the return graph
A_pairs_return.plot(figsize=(12,8));


# Also, if we look at the beta, we can see a big difference between beta values of different stocks. Below is a figure showing two stocks with high beta (_ENPH_) and low beta (_RE_). We can clearly see that the stock with high beta has a bigger regression coefficient than the one with low beta.


# Get beta and pairs for the strategy
beta,pairs = strategy.beta_dict, strategy.pairs_dict

# Sort the beta dictionary based on the value
beta_sorted = {k: v for k, v in sorted(beta.items(), 
                                       key=lambda item: item[1])}

# Get pairs and return of ENPH and RE
ENPH_pairs, RE_pairs = pairs['ENPH'], pairs['RE']
ENPH_return = strategy.monthly_return.loc[:,'ENPH']
RE_return = strategy.monthly_return.loc[:,'RE']

# Get pairs return of both stocks
ENPH_pairs_return = strategy.monthly_return.loc[:,ENPH_pairs] 
RE_pairs_return = strategy.monthly_return.loc[:,RE_pairs] 

# Get mean return for both of the stock pairs
ENPH_pairs_mean = ENPH_pairs_return.mean(axis=1)
RE_pairs_mean = RE_pairs_return.mean(axis=1)

# Create new dataframes to store both of the stock returns
ENPH = pd.concat([ENPH_return, ENPH_pairs_mean], axis=1)
RE = pd.concat([RE_return, RE_pairs_mean], axis=1)

# Reset the columns for clarification
ENPH.columns = ['ENPH','ENPH_Pairs_Portfolio']
RE.columns = ['RE', 'RE_Pairs_Portfolio']

# Set the fig settings
fig, (ax1, ax2) = plt.subplots(2, 1,figsize=(8,15))
fig.suptitle('Linear Regression Plot of ENPH, RE and its Pairs Portfolio')
fig.subplots_adjust(top=0.95)

# Plot two graphs
sns.regplot(ax=ax1, x="ENPH_Pairs_Portfolio", y="ENPH", data=ENPH)
ax1.set(ylim=(-0.5,0.5))

sns.regplot(ax=ax2, x="RE_Pairs_Portfolio",y="RE",data=RE);
ax2.set(ylim=(-0.5,0.5));


# ### Generating trading signals


# Now let's generate trading signals for the testing dataset.
#
# For correlation strategy, we'll adjust the long and short percentage of stocks in the final portfolio to see the differences.


# Gernerating trading signals for both of the strategies
strategy.trade_portfolio(test_data, risk_free_test)
strategy_corr.trade_portfolio(test_data, risk_free_test, long_pct=0.05, short_pct=0.05)


# To check the trading signals for the strategy, we can just use the get_trading_signal method to call the dataframe as below.


# Get trading signal for strategy
strategy.get_trading_signal()


# Now let’s see some examples of equity curves generated by PearsonStrategy. 


def calc_cum_return(strategy):
    """
    Calculates cumulative return of each strategy in testing period.
    
    :param strategy: (PearsonStrategy) A strategy using PearsonStrategy.
    :return: (pd.Series) Series of returns.
    """
    
    # Get trading signals and monthly return on test period
    trading_signal = strategy.get_trading_signal()
    test_monthly_return = strategy.test_monthly_return 

    # Mulitply the both dataframe to get a dataframe of returns with trading signal
    test_traded_return = test_monthly_return*trading_signal

    return_mean_list = [] # Monthly average return list
    
    # Loop through every month to calculate the mean return 
    for month in test_traded_return.index:
        
        month_series = test_traded_return.loc[month]
        # To track the number of stocks traded and the sum of return for each month
        count = 0 # Number of stocks traded
        return_sum = 0 # Sum of the monthly return
        # Loop through every stock in the month
        for stock_return in month_series:

            return_sum += stock_return
            count+=1
        
        # Add mean return for every stock in the month
        return_mean_list.append(return_sum/count)
    
    return_series = pd.Series(return_mean_list, index=test_traded_return.index)
    return_series = return_series.rename('Monthly Return')
    
    return return_series


# For basic strategy, we have a monthly return series as below and the equity curve is given as well. 


calc_cum_return(strategy)


basic_strategy = (calc_cum_return(strategy) + 1).cumprod()
basic_strategy_plot = basic_strategy-1
basic_strategy_plot.plot(title='Pearson Strategy investemnt portfolio equity curve - basic strategy', figsize=(10,5));
print('Investment portfolio value rose to ',basic_strategy.iloc[-1])


# For correlation strategy, we have a monthly return series as below and the equity curve is given as well. 


calc_cum_return(strategy_corr)


corr_strategy = (calc_cum_return(strategy_corr) + 1).cumprod()
corr_strategy_plot = corr_strategy-1
corr_strategy_plot.plot(title='Pearson Strategy investemnt portfolio equity curve - basic strategy', figsize=(10,5));
print('Investment portfolio value rose to ',corr_strategy.iloc[-1])


# ## Conclusion


# This notebook describes the Pearson Strategy class and its functionality. Also, it shows how the stages of the method (pairs portfolio formation and trading signals generation) can be used on real data and that this method can output profitable trading signals.
#
# The algorithms and the descriptions used in this notebook were described the paper by _Chen et al._ __Empirical investigation of an equity pairs trading strategy__  [available here](http://www.pbcsf.tsinghua.edu.cn/research/chenzhuo/paper/Empirical%20Investigation%20of%20an%20Equity%20Pairs%20Trading%20Strategy.pdf). 
#
#
# Key takeaways from the notebook:
#
# - The Pearson distance approach has two work stages - pairs portfolio formation and trading signals generation.
#
# - The Pearson approach works as follows:
#     - First, after data is preprocessed, the method finds pairs for every stock based on the Pearson correlation. 
#     - Then, by using linear regression, setting stock return as independent variable and pairs portfolio return as the dependent variable, the methods set beta as a regression coefficient.
#     - After the pairs portfolio formation period, trading signals can be generated with test data.
#     - The method selects long and short stocks for each month in the testing period by calculating the return divergence for each stock.
#     - Then the trading signals are generated. 
#
# - There can be two different ways of forming pairs portfolios.
#     - By default, the method calculates equally weighted portfolios.
#     - However, it can also calculates the portfolio return weighted by its pairs correlation coefficients.


# ## References ##
# - [Chen, H., Chen, S., Chen, Z. and Li, F., 2019. Empirical investigation of an equity pairs trading strategy. Management Science, 65(1), pp.370-389.](http://www.pbcsf.tsinghua.edu.cn/research/chenzhuo/paper/Empirical%20Investigation%20of%20an%20Equity%20Pairs%20Trading%20Strategy.pdf)
# - [Perlin, M.S., 2009. Evaluation of pairs-trading strategy at the Brazilian financial market. Journal of Derivatives & Hedge Funds, 15(2), pp.122-136.](https://link.springer.com/article/10.1057/jdhf.2009.4)
# - [Krauss, C., 2017. Statistical arbitrage pairs trading strategies: Review and outlook. Journal of Economic Surveys, 31(2), pp.513-545.](https://www.econstor.eu/bitstream/10419/116783/1/833997289.pdf)



// ---------------------------------------------------

// basic_distance_approach_comparsion.py
// arbitrage_research/Distance Approach/basic_distance_approach_comparsion.py
# Generated from: basic_distance_approach_comparsion.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Basic Distance Approach Comparison


# This notebook aims to give a comprehensive comparison of different pairs selection methods described in our previous notebook for the basic distance approach. As in the work by Do and Faff (2010), we’ll make a portfolio for the top 20 pairs of each selection method. All selection methods are as below:
#
# 1. **Basic Strategy**: By calculating the Euclidean square distance for each of the pairs, the $n$ closest pairs are selected.
#
#
# 2. **Industry Strategy**: By calculating the Euclidean square distance for each of the pairs within the same industry group, the $n$ closest pairs are selected.
#
#
# 3. **Zero-crossings Strategy**: By sorting the pairs based on the number of spread zero crossings in the formation period, the top $n$ pairs by number of zero crossings are selected.
#
#
# 4. **Variance Strategy**: By sorting the pairs based on their historical standard deviation in the formation period, the top $n$ pairs with higher standard deviation are selected.
#
#
# 5. **Cointegration Strategy**: By conducting a cointegration test, we can pick the pairs from the basic strategy that are actually cointegrated, out of them we can pick the top $n$ pairs with the smallest p-values are selected.


# # Introduction


# We’ll use ArbitrageLab’s Basic Distance Approach module to show comparisons in this notebook. Therefore, we highly recommend seeing [here](https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/distance_approach/distance_approach.html) for more details of the module.
#
# This notebook goes as follows:
# - First, we load data and modules we need for comparisons. One may use different sources of data if the data structure is the same as in the given example.
# - Second, by using the 5 different pairs selection methods, we’ll create 5 different strategies.
# - After pairs are selected, the empirical test is conducted using test data to see which strategy performs better in a given period. 


# # Implementation


import itertools
import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import statsmodels.tsa.stattools as ts 


# ## Loading Data


# As we did in the Basic Distance Approach research notebook(basic_distance_appraoch.ipynb), we'll download price time series for 272 stocks over a period from 01.2018 to 07.2019. First 12 months of data will be used for training and the following 6 months for trading signal generation and analysis of results. 
#
# Also, for industry-group based selection, we’ll use 4 major industries in S&P 500 stocks: Information Technology, Industrials, Financials and Healthcare. We’ll download this data from [Wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) but any source of data can be used if we follow the dictionary style as below.


# Get industry data from Wikipedia 
table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
stock_table = table[0]

# Indutry groups to use are below
industry_group = ['Information Technology','Industrials','Financials',
                  'Health Care']

# Get tickers from S&P 500 which are in those industry groups
ticker_industry = stock_table[stock_table['GICS Sector']
                              .isin(industry_group)].reset_index(drop=True)

# Get a dataframe of ticker and industry group
ticker_industry = ticker_industry[['Symbol','GICS Sector']]

# Get tickers to use as a list
tickers = ticker_industry['Symbol'].to_list()
remove_tickers = ['CARR','ABC','BRK.B','VNT','OTIS'] # Removed tickers
tickers = [ticker for ticker in tickers if ticker not in remove_tickers]

# Get a dictionary of industry group
industry_dict = pd.Series(ticker_industry['GICS Sector'].values,
                        index=ticker_industry['Symbol']).to_dict()

# Loading data
train_data =  yf.download(tickers, start="2018-01-03", end="2019-01-01")
test_data =  yf.download(tickers, start="2019-01-02", end="2019-07-01")

# Taking close prices for chosen instruments
train_data = train_data["Adj Close"]
test_data = test_data["Adj Close"]

# Looking at the downloaded data
train_data.head()


# ## Pair Selection Methods


# To learn the details of each of the pairs selection methods, please refer to our [documentation](https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/distance_approach/distance_approach.html). 
#
# As mentioned above, we’ll select the top 15 pairs for each strategy. However, for the cointegration strategy, we’ll select the top 50 pairs first and test the cointegrating relationship between pairs so that we can pick the pairs taht are actually cointegrated. 


# Defining three different pair selection criteria 
strategy_basic = al.distance_approach.DistanceStrategy()
strategy_industry = al.distance_approach.DistanceStrategy()
strategy_zero_crossing = al.distance_approach.DistanceStrategy()
strategy_variance = al.distance_approach.DistanceStrategy()
strategy_coint = al.distance_approach.DistanceStrategy()

# Performing the pairs formation step and picking top 15 pairs
strategy_basic.form_pairs(train_data, num_top=15)
strategy_industry.form_pairs(train_data, industry_dict=industry_dict, 
                             num_top=15)
strategy_zero_crossing.form_pairs(train_data, method='zero_crossing', 
                             industry_dict=industry_dict, num_top=15)
strategy_variance.form_pairs(train_data, method='variance',
                             industry_dict=industry_dict, num_top=15)
strategy_coint.form_pairs(train_data, industry_dict=industry_dict, 
                          num_top=50)


# Here, by using a module from [statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.coint.html), the Null hypothesis is that there is no cointegration, the alternative hypothesis is that there is a cointegrating relationship. If the p-value is small, below a critical size, then we can reject the hypothesis that there is no cointegrating relationship.
#
# Below is a simple function returning a list containing tuples of two strings that are cointegrated. One may use the function to test the cointegration of the pairs. There is also a cointegration module in our ArbitrageLab so we highly recommend reading [here](https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/cointegration_approach/cointegration_tests.html) and utilizing it as well.


def get_coint_pairs(pairs, train_data, critical_val=0.05):
    """
    Selects pairs which are cointegrated at the given critical values only.

    For cointegration test, Engle-Granger two-step cointegration test is used.

    :param pairs: (list) List containing tuples of two strings, for names of elements in a pair.
    :param train_data: (pd.DataFrame) Dataframe with training data used to create asset pairs.
    :param critical_val: (float) Critical value to reject the null hypothesis. By default, it's 0.05.
    :return: (list) List containing tuples of two strings that are cointegrated.
    """

    # Make an empty dictionary for cointegrated pairs
    pairs_coint = {}

    # Perform hypothesis test for every pair in the pairs list
    for pair in pairs:

        # Select each stock in the pair
        first_ticker = pair[0]
        second_ticker = pair[1]

        # Get data for each of the stock with scaling parameter
        first_stock = train_data[first_ticker].fillna(method='ffill')

        second_stock = train_data[second_ticker].fillna(method='ffill')

        # Conduct a cointegration test and get p-value
        p_value = ts.coint(first_stock.values, second_stock.values)[1]

        # Check whether the p-value is below the critical value and add to a list if it is.
        if p_value < critical_val:
            pairs_coint[pair] = p_value

    return pairs_coint


# If we get more than 15 pairs after the cointegrations testing, we will pick the top ones based on the p-values.


# Get pairs and scaling parameter for cointegration strategy
pairs_coint = strategy_coint.get_pairs()
scaling_coint = strategy_coint.get_scaling_parameters()

# Get a dictionary of pairs and p-value after the cointegration test
pairs_coint = get_coint_pairs(pairs_coint, train_data)

# Sort a dictionary by its item values
pairs_coint = dict(sorted(pairs_coint.items(), key=lambda item: item[1]))

# Get only top 15 pairs from the dictionary
num_top = 15
pairs_coint = list(dict(itertools.islice(pairs_coint.items(), num_top)).keys())

# Assign the selected pairs as the strategy's pairs
strategy_coint.pairs = pairs_coint


# We’ll just use the `get_pairs()` method for other strategies to get the top 15 pairs.


# Getting a list of created pairs for each of the strategy
pairs_basic = strategy_basic.get_pairs()
pairs_industry = strategy_industry.get_pairs()
pairs_zero_crossing = strategy_zero_crossing.get_pairs()
pairs_variance = strategy_variance.get_pairs()


# Let’s see all the pairs selected by each method. We’ll see the profitability of each pair in the table below after testing all the strategies.


# Showing pairs selected by each method
pair_dataframe = pd.DataFrame([pd.Series(pairs_basic), pd.Series(pairs_industry),pd.Series(pairs_zero_crossing), 
                               pd.Series(pairs_variance),pd.Series(pairs_coint)],
                              index =['Basic','Industry','Zero-Crossing','Variance','Cointegration']).T

display(pair_dataframe)


# ## Testing strategies


# Now we’ll test each of the strategies by forming an equally weighted portfolio with the selected pairs. 
#
# Here, we follow the trading rule by Do and Faff (2010). If the portfolio value exceeds two historical deviations, a sell signal is generated - we expect the price of the first element to decrease and the price of the second element to increase. And if the value of the portfolio is below minus two historical deviations, a buy signal is generated.
#
# An open position is closed when the portfolio value crosses the zero mark - or when the prices of elements in a pair cross. So at any given time, we have one (buy or sell) or none active positions opened. This makes cost allocation for the strategy easier. The resulting trading signals are target quantities of portfolios to hold for each pair (with values -1, 0, or +1).
#
# In the get_portfoio_return function below, we generate these signals and trade based on these for every portfolio. Then we average the returns of the portfolios by equal weight for simple comparison. 


def get_portfolio_return(strategy, test_data):
    """
    Ouputs a series of portfolio returns for a given strategy.

    :params strategy: (Class) DistanceStratgey Class with pairs formed with form_pair().
    :params test_data: (pd.DataFrame) Dataframe with testing data used to create trading signals.
    :return: (pd.Series) Series of portfolio returns in the test period.
    """

    # Getting a list of created pairs for each chosen pair portfolio
    pairs = strategy.get_pairs()

    # Performing the signal generation stage using (2 * st. variation) as a threshold
    strategy.trade_pairs(test_data, divergence=2)

    # Getting series of portfolio values, trading signals, and normalized price series of elements for each chosen pair
    index_columns = [str(pair) for pair in pairs]
    portfolio_series = strategy.get_portfolios()
    portfolio_series = portfolio_series.loc[:,portfolio_series.columns.isin(index_columns)]
    trading_signals = strategy.get_signals()
    trading_signals = trading_signals.loc[:,trading_signals.columns.isin(index_columns)]

    # Returns of elemrnts in a test dataset
    test_data_returns = (test_data / test_data.shift(1) - 1)[1:]

    # Making empty dataframe and list for storing returns of the pairs
    total_return = pd.Series([], dtype='float64')
    pair_return = []
    
    for pair in pairs:
        first_stock, second_stock = pair
        portfolio_returns_scaled = test_data_returns[first_stock] * 0.5 - test_data_returns[second_stock] * 0.5
        portfolio_returns_scaled = portfolio_returns_scaled * (trading_signals[str(pair)].shift(1))
        portfolio_price_scaled = (portfolio_returns_scaled + 1).cumprod()

        # Equity curve of our scaled portfolio price
        equity_curve_scaled = portfolio_price_scaled - 1
        
        # Appends the return of the pair into the list and total return into the dataframe
        pair_return.append(equity_curve_scaled[-1])
        total_return = total_return.add(equity_curve_scaled, fill_value=0)
    
    # Reformatting the returns of the pairs for better visualization later
    pair_return = [ '%.4f' % value for value in pair_return ]
    
    # Calculates the average return of the portfolio
    total_return = total_return/len(pairs)
    
    return pair_return, total_return


# Below are the results of the test for every portfolio. 


# Get portfolio return for basic strategy
pair_return_basic, portfolio_basic = get_portfolio_return(strategy_basic, test_data)

# Plot the equity curve and print the excess return of the portfolio
portfolio_basic.plot(title='Distance Strategy investment portfolio equity curve - Basic Strategy', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_basic[-1])


# Get portfolio return for industry strategy
pair_return_industry, portfolio_industry = get_portfolio_return(strategy_industry, test_data)

# Plot the equity curve and print the excess return of the portfolio
portfolio_industry.plot(title='Distance Strategy investment portfolio equity curve - Industry Strategy', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_industry[-1])


# Get portfolio return for variance strategy
pair_return_zero_crossing, portfolio_zero_crossing = get_portfolio_return(strategy_zero_crossing, test_data)

# Plot the equity curve and print the excess return of the portfolio
portfolio_zero_crossing.plot(title='Distance Strategy investment portfolio equity curve - Zero-Crossing Strategy',
                             figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_zero_crossing[-1])


# Get portfolio return for variance strategy
pair_return_variance, portfolio_variance = get_portfolio_return(strategy_variance, test_data)

# Plot the equity curve and print the excess return of the portfolio
portfolio_variance.plot(title='Distance Strategy investment portfolio equity curve - Variance Strategy', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_variance[-1])


# Get portfolio return for cointegration strategy
pair_return_coint, portfolio_coint = get_portfolio_return(strategy_coint, test_data)

# Plot the equity curve and print the excess return of the portfolio
portfolio_coint.plot(title='Distance Strategy investment portfolio equity curve - Cointegration Strategy', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_coint[-1])


# Before we wrap up, one may look into the details of portfolio returns for each pair as below. One thing to note is that although the variance method has the highest portfolio return among the five methods, it has two pairs with zero returns implying that these pairs are not traded at all during the test period. As it might not be the most desirable result we want from pairs trading, some other combinations of methods could be tried using this module. 


# Showing pairs selected by each method and its returns
pair_returns = pd.DataFrame([pairs_basic, pair_return_basic,pairs_industry, pair_return_industry,
                pairs_zero_crossing, pair_return_zero_crossing,pairs_variance,pair_return_variance,
                pairs_coint, pair_return_coint]).T
columns = ['Basic','Return' ,'Industry','Returns','Zero-Crossing','Returns','Variance','Returns','Cointegration','Returns']
pair_returns.columns = columns
display(pair_returns)


# # Conclusion


# Among the 5 different strategies, the variance method performs better than the others in the test period. Analyzing the plots, the variance strategy portfolio shows a more steady performance in comparison to other strategies. But still, it showed a drawdown in June 2019.
#
# As this notebook conducted testing distance approach strategies for a specific period in time, this might not always be the case that the variance strategy performs the best. The proposed code examples can be used to analyze which modification would better suit a specific case, we encourage to test these strategies on different time periods and data sets.
#
# Key takeaways from the notebook:
# - As in the work by Do and Faff (2010), the basic method performs worst among 5 different strategies, which means additional pairs selection criteria might be needed other than the smallest Euclidean distance.
# - The variance method performs well as theoretically, the pairs with a higher standard deviation in the formation period assure more volatile spreads, resulting in higher profit opportunities in the testing period.
# - Conducting a cointegration test to select pairs that have a high cointegrating relationship works as the cointegration strategy performs better than the basic and industry strategy, supporting the hypothesis mentioned in the work of Krauss (2017).


# ## References ##
# - [Gatev, E., Goetzmann, W.N. and Rouwenhorst, K.G., 2006. Pairs trading: Performance of a relative-value arbitrage rule. The Review of Financial Studies, 19(3), pp.797-827.](https://www.stat.rutgers.edu/home/hxiao/fsrm588_2013/gatev.pdf)
# - [Do, B. and Faff, R., 2012. Are pairs trading profits robust to trading costs?. Journal of Financial Research, 35(2), pp.261-287.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1707125)
# - [Do, B. and Faff, R., 2010. Does simple pairs trading still work?. Financial Analysts Journal, 66(4), pp.83-95](https://www.jstor.org/stable/25741293?seq=1)
# - [Krauss, C., 2017. Statistical arbitrage pairs trading strategies: Review and outlook. Journal of Economic Surveys, 31(2), pp.513-545.](https://www.econstor.eu/bitstream/10419/116783/1/833997289.pdf)



// ---------------------------------------------------

// basic_distance_approach.py
// arbitrage_research/Distance Approach/basic_distance_approach.py
# Generated from: basic_distance_approach.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Basic Distance Approach


# This description of the distance approach closely follows the paper by _Gatev, E., Goetzmann, W. N.,_ and _Rouwenhorst, K. G._ __Pairs Trading: Performance of a Relative Value Arbitrage Rule__  [available here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=141615). 


# ## Introduction


# The distance approach works as follows:
# - First, a historical period is defined, cumulative returns for assets in this period are normalized.
# - Second, using the Euclidean squared distance on the normalized price time series, $n$ closest pairs of assets are picked.
# - After the pairs are formed, the trading period starts, and the trading signals are generated. The mechanism
#   behind this process if the following:
#   - If the difference between the price of elements in a pair diverged by
#     more than 2 standard deviations (calculated for each pair during the training period), the positions are
#     opened - long for the element with a lower price in a portfolio and short for an element with a higher price
#     in a portfolio.
#   - These positions are closed when the normalized prices cross or when the trading period ends.
#
# Using this standard description, the distance approach is a parameter-free strategy.
#
# **Note:** No cointegration tests (as opposed to the mean reversion approach) are being performed in the distance
# approach. As spotted in the work by Krauss (2015), dependencies found using this approach can be spurious.
# This also leads to higher divergence risks, and as shown in the work by Do and Faff (2010), up to 32% of
# pairs identified by this method are not converging.
#
# There are, however, possible adjustments to this strategy, like selecting pairs based on different criteria such as the number of zero crossings or historical variance, choosing distances other than the Euclidean square distance, adjusting the threshold to enter a trade for each pair, etc. 


# ## Pairs formation step
#
# This stage of the DistanceStrategy consists of the following steps:
#
# 1. **Normalization of the input data.**
#
# To use the Euclidean square distance, the training price time series are being normalized using the following
# formula:
#
# $$P_{normalized} = \frac{P - min(P)}{max(P) - min(P)}$$
#
# where $P$ is the training price series of an asset, $min(P)$ and $max(P)$ are the minimum and maximum values from the price series.
#
# 2. **Finding pairs.**
#
# Using the normalized price series, the distances between each pair of assets are calculated. These
# distances are then sorted in the ascending order and the $n$ closest pairs are picked (our
# function also allows skipping a number of first pairs, so one can choose pairs 10-15 to study).
#
# The distances between elements (Euclidean square distance - SSD) are calculated as:
#
# $$SSD = \sum^{N}_{t=1} (P^1_t - P^2_t)^{2}$$
#
# where $P^1_t$ and $P^2_t$ are normalized prices at time $t$ for the first and
# the second elements in a pair.
#
# Using the prices of elements in a pair a portfolio is being constructed - the difference between
# their normalized prices.
#
# 3. **Calculating historical volatility.**
#
# For $n$ portfolios (differences between normalized price series of elements) calculated in the
# previous step, their volatility is being calculated. Historical standard deviations of these portfolios
# will later be used to generate trading signals.


# ## Pair selection criteria
#
# As basic pairs formation confirms declining profitability in pairs trading, some other refined pair selection criteria have emerged. Here, we describe three different methods from the basic approach in selecting pairs for trading. 
#
# First is only allowing for matching securities within the same industry group. The second is sorting selected pairs based on the number of zero-crossings in the formation period and the third is sorting selected pairs based on the historical standard deviation where pairs with high standard deviation are selected.
#
# 1. **Pairs within the same industry group**
#
# In the pairs formation step above, one can add this method when finding pairs in order to match securities within the same industry group. 
#
# With a dictionary containing the name/ticker of the securities and each corresponding industry group, the securities are first separated into different industry groups. Then, by calculating the Euclidean square distance for each of the pair within the same group, the $n$ closest pairs are selected(our function also allows skipping a number of first pairs, so one can choose pairs 10-15 to study). 
#
# This pair selection criterion can be used alongside other methods such as zero-crossings or variance if one gives a dictionary of industry group as an input. This will allow sorting pairs by alternative criteria but within one industry group.
#
# 2. **Pairs with a higher number of zero-crossings**
#
# The number of zero crossings in the formation period has a positive relation to the future spread convergence according to the work by Do and Faff (2010). 
#
# After pairs were matched either within the same industry group or from different industries, the top $n$ pairs that had the highest number of zero crossings during the formation period are admitted to the portfolio we select. This method incorporates the time-series dimension of the historical data in the form of the number of zero crossings. 
#
# 3. **Pairs with a higher historical standard deviation**
#
# The historical standard deviation calculated in the formation period can also be a criterion to sort selected pairs. 
#
# After pairs were matched, we can sort them based on their historical standard deviation in the formation period. According to the work of Do and Faff(2010), as having a small SSD decreases the variance of the spread, this approach should increase the expected profitability of the method.
#
# 4. **Pairs with a cointegrating relationship(Optional)**
#
# For the pairs matched, a cointegration test can also be conducted to select pairs that have cointegrating relationships. 
#
# By using an augmented Engle-Granger two-step cointegration test with a critical value, the pairs having a lower p-value than the critical value are selected. These pairs are expected to have better performance during the trading period, as proposed in the work by Krauss (2017).


# ## Trading signals generation
#
#
# After pairs were formed, we can proceed to the second stage of the DistanceStrategy - trading
# signals generation. The input to this stage is a dataframe with testing price series for assets - not
# used in the pairs formation stage.
#
# This stage of the DistanceStrategy consists of the following steps:
#
# 1. **Normalization of the input data.**
#
# Using the same approach as in the pairs formation stage, we normalize the input trading dataset using
# the same maximum and minimum historical values from the training price series.
#
# 2. **Portfolios creation.**
#
# In this step, the portfolios are being constructed based on the asset pairs chosen in the pairs
# formation step. Portfolio values series are differences between normalized price series of elements
# in a pair - as we're opening a long position for the first element in a pair and a short position for
# the second element in a pair. A buy signal generated by the strategy means going long on the first
# element and short on the second. A sell signal means the opposite - going short on the first element
# and long on the second element.
#
# 3. **Generating signals.**
#
# If the portfolio value exceeds two historical deviations, a sell signal is generated - we expect
# the price of the first element to decrease and the price of the second element to increase. And if
# the value of the portfolio is below minus two historical deviations, a buy signal is generated.
#
# An open position is closed when the portfolio value crosses the zero mark - or when the prices of
# elements in a pair cross. So at any given time, we have one (buy or sell) or none active positions
# opened. This makes cost allocation for the strategy easier. Resulting trading signals are target
# quantities of portfolios to hold for each pair (with values -1, 0, or +1).


# ## Results output and plotting
#
# The DistanceStrategy class contains multiple methods to get results in the desired form.
#
# Functions that can be used to get data:
#
# - **get_signals()** outputs generated trading signals for each pair.
#
# - **get_portfolios()** outputs values series of each pair portfolios.
#
# - **get_scaling_parameters()** outputs scaling parameters from the training dataset used to normalize data.
#
# - **get_pairs()** outputs a list of tuples, containing chosen top pairs in the pairs formation step.
#
# - **get_num_crossing()** outputs a list of tuples, containing chosen top pairs with its number of zero-crossings.
#
# Functions that can be used to plot data:
#
# - **plot_pair()** plots normalized price series for elements in a given pair and the corresponding
#   trading signals for portfolio of these elements.
#
# - **plot_portfolio()** plots portfolio value for a given pair and the corresponding trading signals.


# ## Usage of the Algorithms


# Let's use the above strategy on real data. 
#
# First, we will choose a training period of 12 months to form pairs. Second, we'll create trading signals for the following 6 months window. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import statsmodels.tsa.stattools as ts 


# ### Loading data


# As a dataset, we'll download the price time series for 272 stocks over a period from 01.2018 to 07.2019. First 12 months of data will be used for training and the following 6 months for trading signal generation and analysis of results. 
#
# Also, for industry-group based selection, we’ll use 4 major industries in S&P 500 stocks: Information Technology, Industrials, Financials and Healthcare. We’ll download this data from [Wikipedia](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) but any source of data can be used if we follow the dictionary style as below.


# Get industry data from Wikipedia 
table = pd.read_html('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')
stock_table = table[0]

# Indutry groups to use are below
industry_group = ['Information Technology','Industrials','Financials',
                  'Health Care']

# Get tickers from S&P 500 which are in those industry groups
ticker_industry = stock_table[stock_table['GICS Sector']
                              .isin(industry_group)].reset_index(drop=True)

# Get a dataframe of ticker and industry group
ticker_industry = ticker_industry[['Symbol','GICS Sector']]

# Get tickers to use as a list
tickers = ticker_industry['Symbol'].to_list()
remove_tickers = ['CARR','ABC','BRK.B','VNT','OTIS'] # Removed tickers
tickers = [ticker for ticker in tickers if ticker not in remove_tickers]

# Get a dictionary of industry group
industry_dict = pd.Series(ticker_industry['GICS Sector'].values,
                          index=ticker_industry['Symbol']).to_dict()

# Loading data
train_data =  yf.download(tickers, start="2018-01-03", end="2019-01-01")
test_data =  yf.download(tickers, start="2019-01-02", end="2019-07-01")

# Taking close prices for chosen instruments
train_data = train_data["Adj Close"]
test_data = test_data["Adj Close"]

# Looking at the downloaded data
train_data.head()


# ### Forming pairs


# Now let's form pairs and calculate historical volatilities for chosen portfolio pairs based on training data.


# Initialising an object containing needed methods
strategy = al.distance_approach.DistanceStrategy()

# Performing the pairs formation step and picking top 20 pairs
strategy.form_pairs(train_data, num_top=20)

# Getting scaling values used to normalize data, a list of created pairs and historical volatility for each chosen pair portfolio
scaling_parameters = strategy.get_scaling_parameters()
pairs = strategy.get_pairs()
historical_std = strategy.train_std


# Looking at the scaling parameters 
scaling_parameters


# These scaling parameters can be used to calculate weights for elements when creating a portfolio.


# Looking at top closest pairs 
pairs


# For example, if we have a pair portfolio of ('MSFT', 'V'), we can construct series of their normalized prices as:


# Normalizing the price series on our own (already done inside the DistanceStrategy class)
MSFT_series_scaled = (train_data['MSFT'] - scaling_parameters['min_value']['MSFT']) / \
                  (scaling_parameters['max_value']['MSFT'] - scaling_parameters['min_value']['MSFT'])

V_series_scaled = (train_data['V'] - scaling_parameters['min_value']['V']) / \
                   (scaling_parameters['max_value']['V'] - scaling_parameters['min_value']['V'])


# Plotting the results
plt.figure(figsize=(12,5))

ax1 = MSFT_series_scaled.plot(color='blue', label='MSFT normalized series')
ax2 = V_series_scaled.plot(color='red', label='V normalized series')

plt.legend()
plt.show()


# Looking at top closest pairs 
pairs


# These pairs will be used during the trading signal generation stage.


# Looking at historical standard deviations of pair portfolios
historical_std


# Generally, we can observe that with the increase of Euclidean distance between pairs the volatility is also rising.


# ### Alternative pair selection methods


# As basic pairs formation confirms declining profitability in pairs trading, some other refined pair selection criteria have emerged. Here, we describe three different methods from the basic approach in selecting pairs for trading.
#
# First is only allowing for matching securities within the same industry group. The second is sorting selected pairs based on the number of zero-crossings in the formation period and the third is sorting selected pairs based on the historical standard deviation where pairs with high standard deviation are selected.


# Now let’s make strategies for each of the selection method. 


# Defining three different pair selection criteria 
strategy_industry = al.distance_approach.DistanceStrategy()
strategy_zero_crossing = al.distance_approach.DistanceStrategy()
strategy_variance = al.distance_approach.DistanceStrategy()

# Performing the pairs formation step and picking top 20 pairs
strategy_industry.form_pairs(train_data, industry_dict=industry_dict, 
                             num_top=20)
strategy_zero_crossing.form_pairs(train_data, method='zero_crossing', 
                             industry_dict=industry_dict, num_top=20)
strategy_variance.form_pairs(train_data, method='variance',
                             industry_dict=industry_dict, num_top=20)

# Getting a list of created pairs for each of the strategy
pairs_industry = strategy_industry.get_pairs()
pairs_zero_crossing = strategy_zero_crossing.get_pairs()
pairs_variance = strategy_variance.get_pairs()

# Getting scaling values used to normalize data
scaling_industry = strategy_industry.get_scaling_parameters()
scaling_zero_crossing = strategy_zero_crossing.get_scaling_parameters()
scaling_variance = strategy_variance.get_scaling_parameters()


# **(Optional) Cointegration Test**
#
# For the pairs selected, we’ll conduct a cointegration test to see which pairs are actually cointegrated, as these pairs may perform better during the trading period. We’ll use the augmented Engle-Granger two-step cointegration test here and as the results of the test only belong to the data we selected, one may perform different tests on different datasets.
#
# A brief introduction to Engle-Granger two-step cointegration test method is in below.
#
# For two stocks in a pair, let’s say stock 1 and stock 2, as we do not know $\beta$, we estimate it by using ordinary least squares, and then run the stationarity test on the estimated $u_{t}$ series. 
#
# $$stock1_{t}-\beta stock2_{t}=u_{t}$$
#
# Here, by using a module from [statsmodels](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.coint.html), the Null hypothesis is that there is no cointegration, the alternative hypothesis is that there is cointegrating relationship. If the p-value is small, below a critical size, then we can reject the hypothesis that there is no cointegrating relationship.


# Below is a simple function returning a list containing tuples of two strings that are cointegrated. One may use the function to test the cointegration of the pairs. There is also a cointegration module in our Arbitrage Lab so we highly recommend reading [here](https://hudson-and-thames-arbitragelab.readthedocs-hosted.com/en/latest/cointegration_approach/cointegration_tests.html) and utilize it as well.


def get_coint_pairs(pairs, train_data, critical_val = 0.05):
    """
    Selects pairs which are cointegrated at the given critical values only.

    For cointegration test, Engle-Granger two-step cointegration test is used.

    :param pairs: (list) List containing tuples of two strings, for names of elements in a pair.
    :param train_data: (pd.DataFrame) Dataframe with training data used to create asset pairs.
    :param critical_val: (float) Critical value to reject the null hypothesis. By default, it's 0.05.
    :return: (list) List containing tuples of two strings that are cointegrated.
    """

    # Make an empty list for cointegrated pairs
    pairs_coint = []

    # Perform hypothesis test for every pair in the pairs list
    for pair in pairs:

        # Select each stock in the pair
        first_ticker = pair[0]
        second_ticker = pair[1]

        # Get data for each of the stock with scaling parameter
        first_stock = train_data[first_ticker].fillna(method='ffill')

        second_stock = train_data[second_ticker].fillna(method='ffill')

        # Conduct a cointegration test and get p-value
        p_value = ts.coint(first_stock.values, second_stock.values)[1]

        # Check whether the p-value is below the critical value and add to a list if it is.
        if p_value < critical_val:
            pairs_coint.append(pair)

    return pairs_coint


# Pairs that have a cointegrating relationship in standard strategy.


get_coint_pairs(pairs, train_data)


# Pairs that have a cointegrating relationship within the same industry.


get_coint_pairs(pairs_industry, train_data)


# ### Generating trading signals


# Now let's generate trading signals for the testing dataset.
#
# We are testing the standard strategy here but strategies using other methods can also be used if one changes the original strategy to other strategies in the codes below.


# Performing the signal generation stage using (2 * st. variation) as a threshold
strategy.trade_pairs(test_data, divergence=2)

# Getting series of portfolio values, trading signals, and normalized price series of elements for each chosen pair
portfolio_series = strategy.get_portfolios()
trading_signals = strategy.get_signals()
normalized_prices = strategy.normalized_data


# Looking at calculated portfolio value series
portfolio_series.head()


# Looking at generated trading signals - target quantities of portfolios to hold
trading_signals.head()


# Also normalized price series for each asset
normalized_prices.head()


# The DistanceStrategy class also allows plotting data for a chosen pair. Looking again at the list of chosen pairs to pick a pair to plot.


# Looking at top closest pairs 
pairs


# Let's look at normalized prices, portfolio values and generated trading signals for the pair ('CMA', 'RF') - with number 4 (counting from zero).


# Plotting normalized price series of elements in a pair
figure_pair = strategy.plot_pair(4)


# Plotting portfolio value series
figure_portfolio = strategy.plot_portfolio(4)


# This pair of stocks is moving similarly over the testing period. A signal to open a sell position on a portfolio is generated in the middle of February 2019 and a signal to close this position is generated around April of 2019.
#
# As the long asset in a portfolio is 'CMA' and the short asset is 'RF', the signal to sell a portfolio means we should sell 'CMA' and buy 'RF'.
#
# We can either buy and sell one share for each asset in a pair or calculate weights for 'CMA' and 'RF' based on starting prices and scaling parameters.
#
# First, we should scale by starting prices of each stock and next by scaling parameters ($max(P) - min(P)$) (returns are proportional to initial prices of stocks and negatively proportional to the scaling parameter):
#
# - $Scale_{1} = \frac{P_{1}^{0}}{P_{1}^{0} + P_{2}^{0}} * \frac{(max(P_{2}) - min(P_{2})}{(max(P_{1}) - min(P_{1})) + (max(P_{2}) - min(P_{2}))}$
#
# - $Scale_{2} = \frac{P_{2}^{0}}{P_{1}^{0} + P_{2}^{0}} * \frac{(max(P_{1}) - min(P_{1})}{(max(P_{1}) - min(P_{1})) + (max(P_{2}) - min(P_{2}))}$


# Looking at the scaling parameters (min and max values) used for elements in a portfolio
pair_scales = scaling_parameters.loc[['CMA', 'RF']]

pair_scales


# So the scaling parameters for 'CMA' and 'RF' are
maxmin_CMA = pair_scales.loc['CMA'][1] - pair_scales.loc['CMA'][0]
maxmin_RF = pair_scales.loc['RF'][1] - pair_scales.loc['RF'][0]

scale_CMA = (test_data['CMA'][0] / (test_data['CMA'][0] + test_data['RF'][0])) * (maxmin_RF / (maxmin_CMA + maxmin_RF))
scale_RF = (test_data['RF'][0] / (test_data['CMA'][0] + test_data['RF'][0])) * (maxmin_CMA / (maxmin_CMA + maxmin_RF))

print('Scaling parameter for CMA is ', scale_CMA)
print('Scaling parameter for RF is ', scale_RF)


# Now, let's check how much profit would this distance strategy generate on a given ('CMA', 'RF') pair.


# Returns of elemrnts in a test dataset
test_data_returns = (test_data / test_data.shift(1) - 1)[1:]

test_data_returns.head()


# For unscaled portfolio we'll invest 50% into the 'CMA' asset and 50% in the 'RF' asset. 
#
# For scaled portfolio we should calculate the weights - make scales for 'CMA' and 'RF' sum up to 1.


weight_CMA = scale_CMA / (scale_CMA + scale_RF)
weight_RF = 1 - weight_CMA

print("For scaled portfolio we'll invest ", round(weight_CMA, 3), "% into the CMA asset.")
print("And ", round(weight_RF, 3), "% into the RF asset.")


# Let's test that weight parameters are calculated right


# Pair portfolio price from returns using weight parameters
pair_portfolio_returns = test_data_returns['CMA'] * weight_CMA - test_data_returns['RF'] * weight_RF
pair_portfolio_price = (pair_portfolio_returns + 1).cumprod()
pair_portfolio_price.plot(title="Portfolio values for pair ('CMA', 'RF') calcualted from returns based on weights", figsize=(10,5));


# As we can see, this price performance matches the pair portfolio price performance from the DistanceStrategy class plot_portfolio() function. 


# Invested portfolio prices for scaled and unscaled weights
portfolio_returns_unscaled = test_data_returns['CMA'] * 0.5 - test_data_returns['RF'] * 0.5
portfolio_returns_unscaled = portfolio_returns_unscaled * (trading_signals["('CMA', 'RF')"].shift(1))
portfolio_price_unscaled = (portfolio_returns_unscaled + 1).cumprod()

portfolio_returns_scaled = test_data_returns['CMA'] * weight_CMA - test_data_returns['RF'] * weight_RF
portfolio_returns_scaled = portfolio_returns_scaled * (trading_signals["('CMA', 'RF')"].shift(1))
portfolio_price_scaled = (portfolio_returns_scaled + 1).cumprod()


# Equity curve of our unscaled portfolio price
equity_curve_unscaled = portfolio_price_unscaled - 1

equity_curve_unscaled.plot(title='Distance Strategy investemnt portfolio equity curve - unscaled weights', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_price_unscaled[-1])


# Equity curve of our scaled portfolio price
equity_curve_scaled = portfolio_price_scaled - 1

equity_curve_scaled.plot(title='Distance Strategy investemnt portfolio equity curve - scaled weights', figsize=(10,5));
print('Investment portfolio value rose to ', portfolio_price_scaled[-1])


# So using trading signals from the Distance Strategy for this particular example resulted in the equity curve of our investment portfolio increasing from 1 in mid-January 2019 to around 1.0357 in late May 2019 for the scaled portfolio and 1.0352 for the unscaled one.


# ## Conclusion


# This notebook describes the Distance Strategy class and its functionality. Also, it shows how the stages of the method (pairs formation and trading signals generation) can be used on real data and that this method can output profitable trading signals.
#
# The algorithms and the descriptions used in this notebook were described by _Gatev, E., Goetzmann, W. N.,_ and _Rouwenhorst, K. G._ in the paper __Pairs Trading: Performance of a Relative Value Arbitrage Rule__  [available here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=141615).
#
# Key takeaways from the notebook:
# - The distance approach can be divided into two stages - pairs formation and trading signals generation.
# - The distance approach works as follows:
#   - First, a historical period is defined, cumulative returns for assets in this period are normalized.
#   - Second, using the Euclidean squared distance on the normalized price time series, $n$ closest pairs of assets are picked.
#   - During the treading period, the trading signals are generated. The mechanism behind this process is the following:
#   - If the difference between the price of elements in a pair diverged by
#     more than 2 standard deviations (calculated for each pair during the training period), the positions are
#     opened - long for the element with a lower price in a portfolio and short for an element with a higher price
#     in a portfolio.
#   - These positions are closed when the normalized prices cross or when the trading period ends.
# - Other refined pair selection criteria have emerged:
#   - First is only allowing for matching securities within the same industry group.
#   - The second is sorting selected pairs based on the number of zero-crossings in the formation period.
#   - The third is sorting selected pairs based on the historical standard deviation where pairs with high standard deviation     are selected.
# - No cointegration tests are being performed in the distance approach by default, so dependencies found using this approach can be spurious.
# - We show how a cointegration test can also be conducted to select pairs that have cointegrating relationships. These pairs are expected to have better performance during the trading period.


# ## References ##
# - [Gatev, E., Goetzmann, W.N. and Rouwenhorst, K.G., 2006. Pairs trading: Performance of a relative-value arbitrage rule. The Review of Financial Studies, 19(3), pp.797-827.](https://www.stat.rutgers.edu/home/hxiao/fsrm588_2013/gatev.pdf)
# - [Do, B. and Faff, R., 2012. Are pairs trading profits robust to trading costs?. Journal of Financial Research, 35(2), pp.261-287.](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1707125)
# - [Do, B. and Faff, R., 2010. Does simple pairs trading still work?. Financial Analysts Journal, 66(4), pp.83-95](https://www.jstor.org/stable/25741293?seq=1)
# - [Krauss, C., 2017. Statistical arbitrage pairs trading strategies: Review and outlook. Journal of Economic Surveys, 31(2), pp.513-545.](https://www.econstor.eu/bitstream/10419/116783/1/833997289.pdf)



// ---------------------------------------------------

// regime_switching_arbitrage_rule.py
// arbitrage_research/Time Series Approach/regime_switching_arbitrage_rule.py
# Generated from: regime_switching_arbitrage_rule.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __A regime-switching relative value arbitrage rule.__ by _Bock, M._ and _Mestel, R._


# # Statistical Arbitrage Strategy Based on the Markov Regime-Switching Model


# ## Introduction
#
# This notebook demonstrates an implementation of the strategy described in [Bock, M. and Mestel, R. (2009). A regime-switching relative value arbitrage rule.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.453.3576&rep=rep1&type=pdf)
#
# Traditional pairs trading strategies are prone to failures when fundamental or economic reasons cause a structural break and the pair of assets that were expected to move together are no longer having a strong relationship. Such a break may result in asset price spread having abnormally high deviations failing to revert to its historical mean values. Under these circumstances, betting on the spread to revert to its historical mean would result in a loss.
#
# To overcome the problem of detecting whether the deviations are temporary or longer-lasting, Bock, M. and Mestel, R. (2009) bridge the literature on Markov regime-switching models and the scientific work on statistical arbitrage to develop a set of useful trading rules for pairs trading.


# ---


# ## Assumptions
#
# ### Series Formed by the Trading Pair
#
# The model defines the price series $X_t$ formed by the trading pair as, 
#
# $X_t = \mu_{s_t} + \epsilon_t$,
#
# where 
#
# $E[\epsilon_t] = 0$, $\sigma^2_{\epsilon_t} = \sigma^2_{s_t}$ and $s_t$ denotes the current regime determined by the Markov regime-switching model.
#
# ### Markov Regime-Switching Model
#
# A two-state, first-order Markov-switching process for $s_t$ is considered with the following transition probabilities:
#
# $\Bigg\{ \begin{matrix}
# prob[s_t = 1 | s_{t-1} = 1] = p \\
# prob[s_t = 2 | s_{t-1} = 2] = q \\
# \end{matrix}$ 
#
# where
# $1$ indicates a regime with a higher mean ($\mu_{1}$) while
# $2$ indicates a regime with a lower mean ($\mu_{2}$).
#
# ### Strategy
#
# The idea behind the Markov regime-switching model strategy is straightforward. Suppose the spread process is in the high regime, and we observe that it deviated below the mean by a certain amount. In that case, we take a long trade on the spread only if the probability of the process staying in the high regime is bigger than a certain threshold. In the low regime, the logic is exactly the opposite.
#
# The trading signal $z_t$ is determined in the following way:
#
# $Case\ 1 \ \ current\ regime = 1$
#
# $z_t = \left\{\begin{array}{l}
# +1,\ if\ X_t \leq \mu_1 - \delta \cdot \sigma_1 \wedge P(s_t = 1 | X_t) \geq \rho \\
# -1,\ if\ X_t \geq \mu_1 + \delta \cdot \sigma_1 \\ 
# 0,\ otherwise
# \end{array}\right.$
#
# $Case\ 2 \ \ current\ regime = 2$
#
# $z_t = \left\{\begin{array}{l}
# +1,\ if\ X_t \leq \mu_2 - \delta \cdot \sigma_2 \\
# -1,\ if\ X_t \geq \mu_2 + \delta \cdot \sigma_2 \wedge P(s_t = 2 | X_t) \geq \rho\\ 
# 0,\ otherwise
# \end{array}\right.$
#
# where
#
# $P(\cdot)$ denotes the smoothed probabilities for each state, 
#
# $\delta$ and $\rho$ denote the standard deviation sensitivity parameter and the probability threshold of the trading strategy, respectively.
#
# To be more specific, the trading signal can be described as,
#
# $Case\ 1 \ \ current\ regime = 1$
#
# $\left\{\begin{array}{l}
# Open\ a\ long\ trade,\ if\ X_t \leq \mu_1 - \delta \cdot \sigma_1 \wedge P(s_t = 1 | X_t) \geq \rho \\
# Close\ a\ long\ trade,\ if\ X_t \geq \mu_1 + \delta \cdot \sigma_1 \\ 
# Open\ a\ short\ trade,\ if\ X_t \geq \mu_1 + \delta \cdot \sigma_1 \\
# Close\ a\ short\ trade,\ if\ X_t \leq \mu_1 - \delta \cdot \sigma_1 \wedge P(s_t = 1 | X_t) \geq \rho \\
# Do\ nothing,\ otherwise
# \end{array}\right.$
#
# $Case\ 2 \ \ current\ regime = 2$
#
# $\left\{\begin{array}{l}
# Open\ a\ long\ trade,\ if\ X_t \leq \mu_2 - \delta \cdot \sigma_2 \\
# Close\ a\ long\ trade,\ if\ X_t \geq \mu_2 + \delta \cdot \sigma_2 \wedge P(s_t = 2 | X_t) \geq \rho\\ 
# Open\ a\ short\ trade,\ if\ X_t \geq \mu_2 + \delta \cdot \sigma_2 \wedge P(s_t = 2 | X_t) \geq \rho\\ 
# Close\ a\ short\ trade,\ if\ X_t \leq \mu_2 - \delta \cdot \sigma_2 \\
# Do\ nothing,\ otherwise
# \end{array}\right.$


# ---


# ## Steps to Execute the Strategy
#
# ### Step 1: Select a Trading Pair
# In the original paper, the authors used the DJ STOXX 600 component as the asset pool and applied the cointegration test to perform pairs selection, meaning that if the prices of assets in a pair are cointegrated, they are picked as good candidates to be used in the strategy.
#
# ### Step 2: Construct the Spread Series
# In their work, authors use $\frac{P^A_t}{P^B_t}$ as the spread series. One can use the same method as in the paper, or other ways of constructing the spread series, like $(P^A_t/P^A_0) - \beta \cdot (P^B_t/P^B_0)$ or $ln(P^A_t/P^A_0) - \beta \cdot ln(P^B_t/P^B_0)$ .
#
# ### Step 3: Estimate the Parameters of the Markov Regime-Switching Model 
# Fit the Markov regime-switching model to the spread series with a rolling time window to estimate $
# \mu_1$, $\mu_2$, $\sigma_1$, $\sigma_2$ and the current regime.
#
# ### Step 4: Determine the Signal of the Strategy
# Determine the current signal based on the strategy and estimated parameters.
#
# ### Step 5: Decide the Trade
# Decide the trade based on the signal at time $t$ and the position at time $t - 1$. Possible combinations are listed below:
#
# $$Position_{t - 1}$$|$$Open\ a\ long\ trade$$|$$Close\ a\ long\ trade$$|$$Open\ a\ short\ trade$$|$$Close\ a\ short\ trade$$|$$Trade\ Action$$|$$Position_{t}$$
# :---|:---|:---|:---|:---|:---|:---
# 0|True|False|False|X|$$Open\ a\ long\ trade$$|1
# 0|False|X|True|False|$$Open\ a\ short\ trade$$|-1
# 0|Otherwise||||$$Do\ nothing$$|0
# +1|False|True|False|X|$$Close\ a\ long\ trade$$|0
# +1|False|X|True|False|$$Close\ a\ long\ trade\ and\ open\ a\ short\ trade$$|-1
# +1|Otherwise||||$$Do\ nothing$$|+1
# -1|False|X|False|True|$$Close\ a\ short\ trade$$|0
# -1|True|False|False|X|$$Close\ a\ short\ trade\ and\ open\ a\ long\ trade$$|1
# -1|Otherwise||||$$Do\ nothing$$|-1
#
# where X denotes the don't-care term, the value of X could be either True or False.


# ---


# ## Example Usage of the Module


import matplotlib.pyplot as plt
import yfinance as yf
from arbitragelab.time_series_approach.regime_switching_arbitrage_rule import RegimeSwitchingArbitrageRule

plt.rcParams['figure.figsize'] = (24, 12)


# Loading data
data =  yf.download("CL=F NG=F", start="2015-01-01", end="2017-01-01")["Adj Close"]

# Constructing spread series
Ratt = data["NG=F"]/data["CL=F"]

# Creating a class instance for getting the positions
RSAR = RegimeSwitchingArbitrageRule(delta = 1.5, rho = 0.6)

# Setting window size
window_size = 60


%%time
# Getting current signal
signal = RSAR.get_signal(Ratt[-window_size:], switching_variance = False, silence_warnings = True)
print("Open a long trade:", signal[0]) 
print("Close a long trade:", signal[1]) 
print("Open a short trade:", signal[2]) 
print("Close a short trade:", signal[3])


%%time
# Getting signals on a rolling basis
signals = RSAR.get_signals(Ratt, window_size, switching_variance = True, silence_warnings = True)
print(signals.shape)


%%time
# Deciding the trades based on the signals
trades = RSAR.get_trades(signals)
print(trades.shape)


# Plotting trades
fig = RSAR.plot_trades(Ratt, trades)
plt.show()


# ---


# ## Custom Strategy
#
# If the user is not satisfied with the default trading strategy described in the paper, one can use <code>`RegimeSwitchingArbitrageRule.change_strategy`</code> to modify it.


# ### Example
#
# $Case\ 1 \ \ current\ regime = 1$
#
# $\left\{\begin{array}{l}
# Open\ a\ long\ trade,\ if\ X_t \leq \mu_1 - \delta \cdot \sigma_1\\
# Close\ a\ long\ trade,\ if\ X_t \geq \mu_1\\ 
# Open\ a\ short\ trade,\ if\ X_t \geq \mu_1 + \delta \cdot \sigma_1 \\
# Close\ a\ short\ trade,\ if\ X_t \leq \mu_1\\
# Do\ nothing,\ otherwise
# \end{array}\right.$
#
# $Case\ 2 \ \ current\ regime = 2$
#
# $\left\{\begin{array}{l}
# Open\ a\ long\ trade,\ if\ X_t \leq \mu_2 - \delta \cdot \sigma_2 \wedge P(s_t = 2 | X_t) \geq 0.7\\
# Close\ a\ long\ trade,\ if\ X_t \geq \mu_2\\ 
# Open\ a\ short\ trade,\ if\ X_t \geq \mu_2 + \delta \cdot \sigma_2 \wedge P(s_t = 2 | X_t) \geq \rho\\ 
# Close\ a\ short\ trade,\ if\ X_t \leq \mu_2\\
# Do\ nothing,\ otherwise
# \end{array}\right.$


# Creating a class instance for getting the positions
RSAR = RegimeSwitchingArbitrageRule(delta = 1.5, rho = 0.6)

# Setting window size
window_size = 60


# Changing rules in the high regime
ol_rule = lambda Xt, mu, delta, sigma: Xt <= mu - delta*sigma
cl_rule = lambda Xt, mu, delta, sigma: Xt >= mu
os_rule = lambda Xt, mu, delta, sigma: Xt >= mu + delta*sigma
cs_rule = lambda Xt, mu, delta, sigma: Xt <= mu

RSAR.change_strategy("High", "Long", "Open", ol_rule)
RSAR.change_strategy("High", "Long", "Close", cl_rule)
RSAR.change_strategy("High", "Short", "Open", os_rule)
RSAR.change_strategy("High", "Short", "Close", cs_rule)

# Changing rules in the low regime
ol_rule = lambda Xt, mu, delta, sigma, prob: Xt <= mu - delta*sigma and prob >= 0.7
cl_rule = lambda Xt, mu, delta, sigma: Xt >= mu
os_rule = lambda Xt, mu, delta, sigma, prob, rho: Xt >= mu + delta*sigma and prob >= rho
cs_rule = lambda Xt, mu, delta, sigma: Xt <= mu

RSAR.change_strategy("Low", "Long", "Open", ol_rule)
RSAR.change_strategy("Low", "Long", "Close", cl_rule)
RSAR.change_strategy("Low", "Short", "Open", os_rule)
RSAR.change_strategy("Low", "Short", "Close", cs_rule)


%%time
# Getting signals on a rolling basis
signals = RSAR.get_signals(Ratt, window_size, switching_variance = True, silence_warnings = True)

# Deciding the trades based on the signals
trades = RSAR.get_trades(signals)


# Plotting trades
fig = RSAR.plot_trades(Ratt, trades)
plt.show()


# ## Conclusion
#
# This notebook demonstrated a statistical arbitrage strategy based on the Markov regime-switching model, which is very different from the traditional one.
#
# Key takeaways from the notebook:
# - A new way to construct statistical arbitrage strategies.
# - A way to detect structural break on the spread formed by the pair.


# ## References
#
# 1. [A regime-switching relative value arbitrage rule. Bock, M. and Mestel, R. (2009).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.453.3576&rep=rep1&type=pdf)



// ---------------------------------------------------

// H_strategy.py
// arbitrage_research/Time Series Approach/H_strategy.py
# Generated from: H_strategy.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Pairs trading based on statistical variability of the spread process.__ by _Bogomolov T._


# # H-Strategy


# ## Introduction
#
# This notebook demonstrates an implementation of the strategy described in [Bogomolov, T. (2013). Pairs trading based on statistical variability of the spread process.](https://www.researchgate.net/publication/263339291_Pairs_trading_based_on_statistical_variability_of_the_spread_process)
#
# In this paper, author proposes a novel approach to pairs trading based on the idea of Renko and Kagi charts. This approach exploits statistical information about the variability of the tradable process. There is no need for complicated mathematical derivations, and no strict assumptions have to be made regarding the spread price properties. The trading strategy based on Renko and Kagi models is built on defining how far the process should move in one direction from the turning point before trading in the opposite direction becomes potentially profitable by measuring the variability of the process.


# ## H-construction
#
# Suppose $P(t)$ is a continuous time series on the time interval $[0, T]$.
#
# ### Renko construction
#
# #### Step 1: Generate the Renko Process
# The Renko process $X(i)$ is defined as,
#
# $X(i) : X(i) = P(\tau_i)$, $i = 0, 1, ..., N$,
#
# where $\tau_i$, $i = 0, 1, ..., N$ is an increasing sequence of time moments such that for some arbitrary $H > 0$, $\tau_0 = 0$ and $P(\tau_0) = P(0)$,
#
# $H \leq \max \limits_{t \in [0,T]} P(t) - \min \limits_{t \in [0,T]} P(t)$,
#
# $\tau_i = inf\{u \in [\tau_{i - 1}, T] : |P(u) − P(\tau_{i - 1})| = H\}$.
#
# #### Step 2: Determine Turning Points
#
# We create another sequence of time moments $\{(\tau^a_n, \tau^b_n), n = 0, 1, ..., M\}$ based on the
# sequence ${\tau_i}$. The sequence $\{\tau^a_n\}$ defines time moments when the renko process $X(i)$ has a local
# maximum or minimum, that is the process $X(i) = P(\tau_i)$ changes its direction, and the sequence
# $\{\tau^b_n\}$ defines the time moments when the local maximum or minimum is detected.
#
# More precisely, when take $\tau^a_0 = \tau_0$ and $\tau^b_0 = \tau_1$ then
#
# $\tau^b_n = min\{\tau_i > \tau^b_{n-1}: (P(\tau_i) − P(\tau_{i-1}))(P(\tau_{i-1}) − P(\tau_{i-2})) < 0\}$, 
#
# $\tau^a_n = \{\tau_{i - 1} : \tau^b_n = \tau_i\}$.
#
# ### Kagi construction
#
# The Kagi construction is similar to the Renko construction with the only difference being that to create the sequence of time moments $\{(\tau^a_n, \tau^b_n), n = 0, 1, ..., M\}$ for the Kagi construction we use local maximums and minimums of the process $P(t)$ rather than the process $X(i)$ derived from it.
#
# The sequence $\{\tau^a_n\}$ then defines the time moments when the price process $P(t)$ has a local maximum or minimum and the sequence $\{\tau^b_n\}$ defines the time moments when that local maximum or minimum is recognized, which is the time when the process $P(t)$ moves away from its last local maximum or minimum by a distance equal to $H$.
#
# More precisely, $\tau^a_0$, $\tau^b_0$ and $S_0$ is defined as,
#
# $\tau^b_0 = inf\{u \in [0, T] : \max \limits_{t \in [0,u]} P(t) − \min \limits_{t \in [0,u]} P(t) = H\}$,
#
# $\tau^a_0 = inf\{u < \tau^b_0: |P(u) − P(\tau^b_0)| = H\}$,
#
# $S_0 = sign(P(\tau^a_0) − P(\tau^b_0))$,
#
# where $S_0$ can take two values: $1$ for a local maximum and $−1$ for a local minimum.
#
# Then we define $(\tau^a_n, \tau^b_n)$, $n > 0$ recursively. The construction of the full sequence $\{(\tau^a_n, \tau^b_n), n = 0, 1, ..., M\}$ is done inductively by alternating the following cases.
#
# $Case\ 1: \ \ S_{n-1} = -1$
#
# if $S_{n-1} = -1$, then $\tau^a_n, \tau^b_n$ and $S_n$ is defined as,
#
# $\tau^b_n = inf\{u \in [\tau^a_{n-1}, T] : P(u) − \min \limits_{t \in [\tau^a_{n-1}\ \ ,\ u]} P(t) = H\}$,
#
# $\tau^a_n = inf\{u < \tau^b_n: P(u) = \min \limits_{t \in [\tau^a_{n-1}\ \ ,\ \tau^b_n]} P(t)\}$,
#
# $S_n = 1$.
#
#
# $Case\ 2: \ \ S_{n-1} = 1$
#
# if $S_{n-1} = 1$, then $\tau^a_n, \tau^b_n$ and $S_n$ is defined as,
#
# $\tau^b_n = inf\{u \in [\tau^a_{n-1}, T] : \max \limits_{t \in [\tau^a_{n-1}\ \ ,\ u]} P(t) - P(u) = H\}$,
#
# $\tau^a_n = inf\{u < \tau^b_n: P(u) = \max \limits_{t \in [\tau^a_{n-1}\ \ ,\ \tau^b_n]} P(t)\}$,
#
# $S_n = -1$.


# ## H-statistics
#
# ### H-inversion
#
# H-inversion counts the number of times the process $P(t)$ changes its direction for selected $H$, $T$ and $P(t)$. It is straightforward that mean-reverting processes tend to have higher H-inversions, so when we are choosing which pair of assets to trade, the level of H-inversion of their spread is a good metric to use. The higher the H-inversion, the more pair is suitable for a mean-reverting strategy.
#
# Formally, it is calculated as
#
# $N_T (H, P) = \max \{n : \tau^{b}_{n} = T\} = N$,
#
# where $H$ denotes the threshold of the H-construction, and $P$ dnotes the process $P(t)$.
#
# ### H-distances
#
# H-distances counts the sum of vertical distances between local maximums and minimums to the power $p$.
#
# It is calculated as
#
# $V^p_T (H, P) = \sum_{n = 1}^{N}|P(\tau^a_n) − P(\tau^a_{n−1})|^p$.
#
# ### H-volatility
#
# H-volatility of order p measures the variability of the process $P(t)$ for selected $H$ and $T$. We can see from the formula that the H-volatility calculates the average distance between adjacent turning points. If the H-volatility is high, the process tends to move in the same direction for a significant amount before turning its direction. Therefore, it’s better to pick a pair with lower H-volatility to be used in a pairs trading strategy.
#
# It is calculated as
#
# $\xi^p_T = {V^p_T (H, P)}/{N_T (H, P)}$.


# ## Strategies
#
# ### Momentum Strategy
#
# The investor buys (sells) an asset at a stopping time $\tau^b_n$ when he or she recognizes that the process passed its previous local minimum
# (maximum) and the investor expects a continuation of the movement. The signal $s_t$ is given by
#
# $s_t = \left\{\begin{array}{l}
# +1,\ if\ t = \tau^b_n\ and\ P(\tau^b_n) - P(\tau^a_n) > 0\\
# -1,\ if\ t = \tau^b_n\ and\ P(\tau^b_n) - P(\tau^a_n) < 0\\
# 0,\ otherwise
# \end{array}\right.$
#
# where $+1$ indicates $opening\ a\ long\ trade\ or\ closing\ a\ short\ trade$, $-1$ indicates $opening\ a\ short\ trade\ or\ closing\ a\ long\ trade$ and $0$ indicates $holding\ the\ previous\ position$.
#
# The profit from one trade according to the momentum H-strategy over time from $\tau^b_{n−1}$ to $\tau^b_{n}$ is
#
# $Y_{\tau^b_n} = (P(\tau^b_n) − P(\tau^b_{n−1})) · sign(P(\tau^a_n) − P(\tau^a_{n−1}))$
#
# and the total profit from time $0$ till time $T$ is
#
# $Y_T(H, P) = (\xi^1_T (H, P) − 2H) \cdot N_T (H, P)$
#
# ### Contrarian Strategy
#
# The investor sells (buys) an asset at a stopping time $\tau^b_n$ when he or she decides that the process has passed far enough from its previous local minimum
# (maximum), and the investor expects a movement reversion. The signal $s_t$ is given by
#
# $s_t = \left\{\begin{array}{l}
# +1,\ if\ t = \tau^b_n\ and\ P(\tau^b_n) - P(\tau^a_n) < 0\\
# -1,\ if\ t = \tau^b_n\ and\ P(\tau^b_n) - P(\tau^a_n) > 0\\
# 0,\ otherwise
# \end{array}\right.$
#
# where $+1$ indicates $opening\ a\ long\ trade\ or\ closing\ a\ short\ trade$, $-1$ indicates $opening\ a\ short\ trade\ or\ closing\ a\ long\ trade$ and $0$ indicates $holding\ the\ previous\ position$.
#
# The profit from one trade according to the momentum H-strategy over time from $\tau^b_{n−1}$ to $\tau^b_{n}$ is
#
# $Y_{\tau^b_n} = (P(\tau^b_n) − P(\tau^b_{n−1})) · sign(P(\tau^a_{n−1}) - P(\tau^a_n))$,
#
# and the total profit from time $0$ till time $T$ is
#
# $Y_T(H, P) = (2H - \xi^1_T (H, P)) \cdot N_T (H, P)$.
#
# ### Properties
#
# It is clear that the choice of H-strategy depends on the value of H-volatility. if $\xi^1_T > 2H$, then to achieve a positive profit the investor should
# employ a momentum H -strategy; if $\xi^1_T < 2H$ then the investor should use a contrarian H-strategy.
#
# Suppose $P(t)$ follows the Wiener process, the H-volatility $\xi^1_T = 2H$. As a result, it is impossible to profit from the trading on the process $P(t)$. We can also see that H-volatility $\xi^1_T = 2H$ is a property of a martingale. Likewise $\xi^1_T > 2H$ could be a property of a sub-martingale or a super-martingale or a process regularly switching over time from a sub-martingale to a super-martingale and back.
#
# In the reference paper, the author proposes that for any mean-reverting process, regardless of its distribution, the H-volatility is less than $2H$. Hence, theoretically, trading the mean-reverting process by the contrarian H-strategy is profitable for any choice of $H$.


# ## Pairs Selection
# - Purpose: Select trading pairs from the assets pool by using the properties of the H-construction.
# - Algorithm:
#     - Determine the assets pool and the length of historical data.
#     - Take log-prices of all assets based on the history, combine them in all possible pairs and build spread process for each pair.
#         - $spread_{ij} = log(P_i) - log(P_j)$
#     - For each spread process, calculate its standard deviation, and set it as the threshold of the H-construction.
#     - Determine the construction type of the H-construction.
#         - It could be either Renko or Kagi.
#     - Build the H-construction on the spread series formed by each possible pairs.
#     - The top N pairs with the highest/lowest H-inversion are used for pairs trading.
#         - Mean-reverting process tends to have higher H-inversion.


import os

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf

from arbitragelab.time_series_approach.h_strategy import HConstruction
from arbitragelab.time_series_approach.h_strategy import HSelection


# ## Pairs Trading Backtesting Example
#
# ### Pairs Selection
# - Frequency: Reselect every six months.
# - H-selection parameters: 
#     - Assets pool: S&P 500 Components, updated at the beginning of each year.
#     - The length of historical data: 12-month history.
#     - Construction type: Kagi
#     - Selection: The top 20 pairs with the highest H-inversion.
#
# ### Trading Strategy
# - Construction type: Kagi
# - Signals type: contrarian.
# - Trading period: Start trading all pairs from the first day of the trading period and constantly stay in the market for up to six months.
#
# ### Others
# - Backtesting interval: 2005/01/01 ~ 2011/01/01
# - Transaction costs: 0.2% per trade for the pair and about 0.4% per round trip for the pair.


# Getting the date of the first day of each quarter
dates = pd.date_range("2004-01-01", "2011-01-01", freq = "QS")

# Setting parameters for getting training and testing intervals
interval = 4
overlap = 2

train_date = []
test_date = []

# Determining training and testing intervals
i = 0
while i < len(dates) - interval - overlap:
    trd = (dates[i].date(), dates[i + interval].date())
    tsd = (dates[i + interval].date(), dates[i + interval + overlap].date())
    
    train_date.append(trd)
    test_date.append(tsd)
    
    i += 2
    
# Printing out some of the training and testing intervals
for train, test in zip(train_date[:5], test_date[:5]):
    print("Training:", train[0], "~", train[1], "Testing:", test[0], "~", test[1])


# Getting data path
root_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))
path_1 = os.path.join(root_path, "Sample-Data", "S&P 500_Components.csv")
path_2 = os.path.join(root_path, "Sample-Data", "S&P 500_Close_Data.csv")


# Reading data
components = pd.read_csv(path_1, index_col = 0)
data_df = pd.read_csv(path_2, index_col = 0)
data_df.index = pd.to_datetime(data_df.index)
tickers_with_data = list(data_df.columns)


def cal_daily_returns(price_data, position):
    cum_return = position*(price_data - price_data.iloc[0])/price_data.iloc[0]
    daily_returns = (cum_return + 1).pct_change().dropna()
    return daily_returns


# Setting parameters
construction_type = "Kagi"
signals_type = "contrarian"
selection_method = "highest"
transaction_costs = 0.002
num = 20

# Creating series for storing backtesting results
total_returns = pd.Series(1, index = [test_date[0][0]], dtype = float)
total_returns_with_costs = pd.Series(1, index = [test_date[0][0]], dtype = float)
total_returns.index = pd.to_datetime(total_returns.index)
total_returns_with_costs.index = pd.to_datetime(total_returns_with_costs.index)

# Starting backtesting
for train, test in zip(train_date, test_date):
    train_year = str(train[0].year) # Getting the year of the start of the training interval
    tickers = list(components[train_year].dropna()) # Getting S&P 500 components for the year
    valid_tickers = list(set(tickers_with_data) & set(tickers)) # Determining which Tickers have historical data
    
    train_data = data_df[valid_tickers][train[0] : train[1]] # Getting required data for pairs selection
    train_data = train_data.dropna(how = "all", axis = 0).dropna(how = "all", axis = 1)
    
    # Getting pairs for trading
    minimum_length = len(train_data)*0.8
    hs = HSelection(train_data, construction_type)
    hs.select(minimum_length = minimum_length)
    pairs = hs.get_pairs(num, selection_method, False)
    
    # Creating lists for storing cumulative returns in the test interval for each pair
    returns_series_list = []
    returns_series_with_costs_list = []
    
    print("Training:", train[0], "~", train[1])
    print("Testing:", test[0], "~", test[1])
    
    # Calculating the cumulative returns in the test interval for each pair
    for i in pairs:
        threshold = i[1] # Getting the threshold of the H-construction
        pair_tickers = list(i[2])
        pair_data = data_df[pair_tickers][test[0] : test[1]] # Getting required data for backtesting
        pair_data = pair_data.dropna(axis = 0).dropna(axis = 1)
        
        if len(pair_data.columns) < 2:
            print("data error")
            continue
        
        series = np.log(pair_data[pair_tickers[0]]) - np.log(pair_data[pair_tickers[1]]) # Constructing spread series
        hc = HConstruction(series, threshold, construction_type) # Creating a class object
        signals = hc.get_signals(signals_type) # Getting the signals in the test interval
        trade_actions = signals[signals != 0] # Getting the actual trade actions (+1 for Open a long trade, -1 for Open a short trade)
        trade_dates_interval = [(trade_actions.index[i].date(), trade_actions.index[i + 1].date()) for i in range(len(trade_actions) - 1)] # Getting the intervals between two adjacent trade actions
        if len(trade_actions) > 0 and trade_actions.index[-1].date() < test[1]:
            trade_dates_interval.append([trade_actions.index[-1].date(), test[1]])
            
        # Creating series for storing cumulative returns
        returns_series = pd.Series(1, index = [test[0]], dtype = float)
        returns_series.index = pd.to_datetime(returns_series.index)
        returns_series_with_costs = pd.Series(1, index = [test[0]], dtype = float)
        returns_series_with_costs.index = pd.to_datetime(returns_series_with_costs.index)
        
        # Calculating the cumulative returns for each interval between two adjacent trade actions, then connecting them together
        for i, j in zip(trade_dates_interval, trade_actions.values):
            interval_data = pair_data.loc[i[0]:i[1]].dropna(how = "all", axis = 0).dropna(how = "all", axis = 1)
            if len(interval_data) <= 1:
                continue
                
            daily_returns = interval_data.copy()
            daily_returns[pair_tickers[0]] = cal_daily_returns(daily_returns[pair_tickers[0]], j) # if j == 1 (-1), long (short) the first stock and short (long) the second stock
            daily_returns[pair_tickers[1]] = cal_daily_returns(daily_returns[pair_tickers[1]], -j)
            daily_returns = daily_returns.dropna(how = "all", axis = 0).dropna(how = "all", axis = 1)
            daily_returns_with_costs = daily_returns.copy()
            daily_returns_with_costs.iloc[0] = daily_returns_with_costs.iloc[0] - transaction_costs
            daily_returns_with_costs.iloc[-1] = daily_returns_with_costs.iloc[-1] - transaction_costs
            cum_returns = (daily_returns + 1).cumprod().mean(axis = 1) # Assuming we have equal weights at the first day of the interval between two adjacent trade actions
            cum_returns_with_costs = (daily_returns_with_costs + 1).cumprod().mean(axis = 1)
            returns_series = pd.concat([returns_series, cum_returns*returns_series[-1]])
            returns_series_with_costs = pd.concat([returns_series_with_costs, cum_returns_with_costs*returns_series_with_costs[-1]])
        
        returns_series_list.append(returns_series)
        returns_series_with_costs_list.append(returns_series_with_costs)
        
    pairs_returns = pd.DataFrame(returns_series_list).T.ffill().mean(axis = 1) # Assuming we have equal weights at the first day of the test interval
    total_returns = pd.concat([total_returns, pairs_returns*total_returns[-1]])
    pairs_returns_with_costs = pd.DataFrame(returns_series_with_costs_list).T.ffill().mean(axis = 1)
    total_returns_with_costs = pd.concat([total_returns_with_costs, pairs_returns_with_costs*total_returns_with_costs[-1]])


# Getting benckmark
start = total_returns.index[0].date().isoformat()
end = total_returns.index[-1].date().isoformat()
SP500 = yf.download("^GSPC", start=start, end=end)["Adj Close"]


# Plotting results
total_returns.plot(figsize=(16, 10), label = "Stratrgy")
total_returns_with_costs.plot(label = "Stratrgy with transaction costs")
(SP500.pct_change() + 1).cumprod().plot(label = "Benckmark")
plt.legend()
plt.show()


# ## Live Trading Example
#
# If users want to actually use this strategy for a live trading, they can use the following method to complete. It mainly consists of two steps. The first step is to transmit the latest price to the object, and the second step is to obtain the latest signal.


# Loading data
data =  yf.download("KO PEP", start="2020-01-01", end="2021-07-01")["Adj Close"]

# Constructing spread series
series = np.log(data["KO"]) - np.log(data["PEP"])

# Splitting the series for demonstation
historical_series = series[:-1]
threshold = historical_series.std()
latest_data = series[-1:]
print(latest_data)


# Creating a class object
hc = HConstruction(historical_series, threshold, "Renko")

# Transmiting the latest price to the object
hc.extend_series(latest_data)

# Getting the latest signal
print("Latest Signal:")
display(hc.get_signals()[-1:])


# ## Conclusion
#
# This notebook demonstrated a nonparametric approach to pairs trading based on the idea of Renko and Kagi charts, which is very different from the traditional one.
#
# Key takeaways from the notebook:
# - H-inversion is astatistic that captures the degree of mean reversion.
# - H-volatility is a statistic that measures the variability of the process.
# - A new way to select pairs and generate trading signals for statistical arbitrage strategies.


# ## References
#
# 1. [Pairs trading based on statistical variability of the spread process. Bogomolov, T. (2013).](https://www.researchgate.net/publication/263339291_Pairs_trading_based_on_statistical_variability_of_the_spread_process)



// ---------------------------------------------------

// quantile_time_series.py
// arbitrage_research/Time Series Approach/quantile_time_series.py
# Generated from: quantile_time_series.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __A Machine Learning based Pairs Trading Investment Strategy__ by _Simão Moraes Sarmento_ and _Nuno_ _Horta_


# # Quantile Time Series Strategy


# ## Introduction


# Usually, when a mean-reverting spread is constructed, a researcher will use the Z-score Bollinger Band strategy to trade a portfolio. However, the time series approach may be applied to model the spread dynamics and enter long/short spread positions.


# ## Modeling the spread difference
#
# - Firstly, let's define a spread in 2-dimensional space as $$S_t = Y_t - \beta X_t$$ Parameter $\beta$ can be defined by either using the Engle-Granger or Johansen cointegration approach.
# - Secondly, we need to come up with a spread prediction model - it can be ARIMA, ANN, RNN, or other time series prediction models. Let's define spread prediction at time $t$ as $\hat S_t$.
# - On the third step, we split spread differences $S_t - S_{t-1}$ into positive ($f_{+}$) and negative ($f_{-}$). We define bottom negative differences quantile as $Q_{f_{-}}$ and top positive differences quantile as $Q_{f_{+}}$.


from IPython.display import Image
Image(filename='Quantile_time_series/quantile_thresholds.png')


# ## Quantile-based trading
# In the time series approach, our trading rule can be described as
#
# $$(\hat S_{t+1} - S_{t}) \geq Q_{f_{+}} \Rightarrow OPEN LONG $$ $$ (\hat S_{t+1} - S_{t}) \leq Q_{f_{-}} \Rightarrow OPEN SHORT $$ $$0 \leq \hat S_{t+1} - S_{t} \leq Q_{f_{+}} \Rightarrow HOLDLONG$$ $$Q_{f_{-}} \leq \hat S_{t+1} - S_{t} \leq 0 \Rightarrow HOLDSHORT$$
#
# - OPENLONG/OPENSHORT means opening a new long/short position if none was opened before.
# - HOLDLONG/HOLDSHORT means holding long/short positions if one was opened before.
# - We should exit a long position if HOLDLONG condition was not satisfied, we should exit a short position if HOLDSHORT condition was not satisfied.
#
# **Note**: we use __90%__ and __5%__ quantiles for top positive and bottom negative quantiles thresholds respectively and 0 value as exit threshold, however, the researcher may decide what are the most optimal parameters based on a specific strategy.


Image(filename='Quantile_time_series/trading_example.png')


# ## Spread prediction model
#
# In the notebook, we use __Auto ARIMA(p, d, q)__ approach to generate spread predictions. 
# Non-seasonal ARIMA models are generally denoted ARIMA(p,d,q) where parameters p, d, and q are non-negative integers, __p__ is the order (number of time lags) of the autoregressive model, __d__ is the degree of differencing (the number of times the data have had past values subtracted), and __q__ is the order of the moving-average model.
# In order to choose, best fit ARIMA model parameters we minimize the **Akaike information criterion (AIC)** value: $$AIC = 2k - 2ln(L)$$ where $k$ - number of model parameters and $L$ - likelihood function.
#
# As a part of arbitragelab, we have an Auto ARIMA fit and prediction module which utilizes the **pmdarima** package to find the best model fit.


# ## Usage of the Algorithms


# Let's use the algorithm described above and its performance to Quantile Time Series Strategy. 
#
# - Firstly, we will use the Engle-Granger test to construct a mean-reverting portfolio. 
# - Secondly, we will fit the Auto ARIMA model and generate predictions.
# - In the third step, we will model spread differences and find trading thresholds.
# - Finally, we will generate trading signals using Auto ARIMA and fit thresholds.


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# Following the example in the Optimal Mean Reversion module, we will use Gold Shares ETF (GLD), Gold Miners ETF (GDX), and Junior Gold Miners ETF (GDXJ) to construct a portfolio of three elements.


# Loading data
train_data =  yf.download("GLD GDX GDXJ", start="2016-01-01", end="2018-01-01")
test_data =  yf.download("GLD GDX GDXJ", start="2018-01-02", end="2019-08-01")

# Taking close prices for chosen instruments
train_three_elements = train_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

test_three_elements = test_data["Adj Close"][["GLD", "GDX", "GDXJ"]]

# Looking at the downloaded data
train_three_elements.head()


# ### Engle-Granger test


# Initialising an object containing needed methods
eg_portfolio = al.cointegration_approach.EngleGrangerPortfolio()

# Fitting the data on a dataset of three elements with constant term
eg_portfolio.fit(train_three_elements, add_constant=True)

# Getting results of the Engle-Granger test
eg_adf_statistics = eg_portfolio.adf_statistics
eg_cointegration_vectors = eg_portfolio.cointegration_vectors


# Looking at the statistic from the last step of the Engle-Granger test
eg_adf_statistics


# Using the ADF statistic test output, we can see that our statistic is above the 95% significance level value.
#
# So at a 95% significance level, our elements are cointegrated, we can construct a mean-reverting portfolio using the coefficients from the *eg_cointegration_vectors* variable.


eg_cointegration_vectors


# As described in the theoretical part, the coefficient for the first element is $1$, while other two are equal to negative regression coefficients.


# ### Constructing portfolios


# Calculating returns of our elements (ETFs)
train_three_elements_returns = (train_three_elements / train_three_elements.shift(1) - 1)[1:]
test_three_elements_returns = (test_three_elements / test_three_elements.shift(1) - 1)[1:]

train_three_elements_returns.head()


# Also adding weights to take initial prices of our ETFs into account
weights  = train_three_elements.iloc[0] / abs(train_three_elements.iloc[0]).sum()


# Weights of elements for the Engle-Granger portfolio
eg_cointegration_vectors.loc[0]


# Scaling weights so they sum up to 1
eg_scaled_vectors = eg_cointegration_vectors.loc[0] / abs(eg_cointegration_vectors.loc[0]).sum()

eg_scaled_vectors


# Calculating portfolio values during the training period
eg_portfolio_returns = (train_three_elements_returns * eg_scaled_vectors * weights).sum(axis=1)
eg_portfolio_price = (eg_portfolio_returns + 1).cumprod()


# Plotting Engle-Granger portfolio price
eg_portfolio_price.plot(title='Engle-Granger portfolio price', figsize=(10,5));


# ### Fit Auto ARIMA model


# Initializing the Auto ARIMA model
arima_model = al.time_series_approach.AutoARIMAForecast(start_p=1, start_q=1, max_p=10, max_q=10)


# Finding the best fitting model without silencing the warnings
arima_model.get_best_arima_model(y_train=eg_portfolio_price, verbose=True, silence_warnings = False)


# We can see that we are getting quite a few ConvergenceWarning, so we can assume that the Auto ARIMA method doesn't quite fit to this input data.
#
# There is an option to silence the warning for this particular function by using the _silence_warnings_ flag.


# Finding the best fitting model and silencing the warnings
arima_model.get_best_arima_model(y_train=eg_portfolio_price, verbose=True, silence_warnings = True)


# ### Get trading thresholds (quantiles)


# Plotting portfolio differences
eg_portfolio_price.diff().plot(title='Portfolio differences', figsize=(10,5));


# Init strategy class
time_series_trading = al.time_series_approach.QuantileTimeSeriesTradingStrategy(long_quantile=0.8, short_quantile=0.2)

# Fit portfilio to find Q_{-} and Q_{+}
time_series_trading.fit_thresholds(eg_portfolio_price)


# Plot thresholds used for trading
time_series_trading.plot_thresholds();


# ### Generate out-of-sample ARIMA predictions


# Creating a test portfolio based on the EG test results
test_three_elements_returns = (test_three_elements / test_three_elements.shift(1) - 1)[1:]

# Also adding weights to take initial prices of ETFs into account
weights  = test_three_elements.iloc[0] / abs(test_three_elements.iloc[0]).sum()

test_portfolio_returns = (test_three_elements_returns * eg_scaled_vectors * weights).sum(axis=1)

test_portfolio_price = (test_portfolio_returns + 1).cumprod()


# The progress bar was added to the prediction generation process to check the execution state of the method.


# Generate out-of-sample ARIMA prediction
oos_prediction = arima_model.predict(y=test_portfolio_price, silence_warnings = True)


# Compare results
plt.figure(figsize=(12,5))

ax1 = test_portfolio_price[2:].plot(label='Real price', figsize=(10,5))
ax2 = oos_prediction[2:].plot(label ='Auto ARIMA predictions', figsize=(15,9), title='Out-of-sample Auto ARIMA predictions vs Real values')

plt.legend(loc=2)
plt.show()


# ### Applying Quantile Time Series trading strategy


# Use the difference between prediction and actual value to trade the spread
for prediction, actual in zip(oos_prediction, test_portfolio_price):
    time_series_trading.get_allocation(predicted_difference=prediction-actual, exit_threshold=0)


# Plot positions created using a quantile time series strategy
positions = pd.Series(index=test_portfolio_price.index, data=time_series_trading.positions)

# Getting only short and only long positions
long_positions = positions[positions == 1]
short_positions = positions[positions == -1]

# Plottign the positions
test_portfolio_price.plot(title='Engle-Granger OOS portfolio price', figsize=(10,5), label='portfolio')
plt.scatter(long_positions.index, test_portfolio_price.loc[long_positions.index], color='green', label='long positions')
plt.scatter(short_positions.index, test_portfolio_price.loc[short_positions.index], color='red', label='short positions')
plt.legend(loc='best');


# Generate equity curve
equity_curve = (positions.shift(1) * test_portfolio_price.diff()).cumsum()
equity_curve.plot(title='Quantile Time Series Strategy equity curve', figsize=(10, 5));


# As we can see, the strategy generates negative returns due to the bad quality of the ARIMA model prediction. Let's see how the model works if we have a "perfect prediction" - the spread value for the next day.


# ### Perfect prediction results


# Init strategy class
perfect_series_trading = al.time_series_approach.QuantileTimeSeriesTradingStrategy(long_quantile=0.8, short_quantile=0.2)

# Getting the threshold values
perfect_series_trading.fit_thresholds(eg_portfolio_price)


# Use difference between prediction and actual value to trade the spread
for prediction, actual in zip(test_portfolio_price.shift(-1), test_portfolio_price):
    perfect_series_trading.get_allocation(predicted_difference=prediction-actual, exit_threshold=0)


positions = pd.Series(index=test_portfolio_price.index, data=perfect_series_trading.positions)


# Generate equity curve for "perfect" prediction.
perfect_equity_curve = (positions.shift(1) * test_portfolio_price.diff()).cumsum()
perfect_equity_curve.plot(title='Quantile Time Series Strategy with perfect predictions equity curve', figsize=(10, 5));


# ## Conclusion


# This notebook describes a new strategy from the Time Series Approach - the Quantile Time Series Strategy. The key part of the strategy is to generate time series prediction and trade when the predicted value deviates from the current spread value. In the notebook, we used the Auto ARIMA as a tool to generate spread value prediction.
#
# The algorithms and the descriptions used in this notebook were described in the book by _Simão Moraes Sarmento_ and _Nuno_ _Horta_ __A Machine Learning based Pairs Trading Investment Strategy__ [available here](https://www.springer.com/gp/book/9783030472504).
#
# Key takeaways from the notebook:
# - The strategy is based on the expectation that the investor can benefit from an abrupt movement of the spread value.
# - This strategy needs a forecasting algorithm to work, in this case, the Auto ARIMA tool was used.
# - The thresholds to enter the positions are defined based on the quantiles of the percentage change distribution during a formation period. 
# - For the spread forecasting, non-parametric methods can be used, such as Artificial Neural Networks.



// ---------------------------------------------------

// ou_model_optimal_threshold_Bertram.py
// arbitrage_research/Time Series Approach/ou_model_optimal_threshold_Bertram.py
# Generated from: ou_model_optimal_threshold_Bertram.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Analytic solutions for optimal statistical arbitrage trading.__ by _Bertram, W. K._


# # OU Model Optimal Trading Thresholds Bertram


# ## Introduction
#
# This notebook demonstrates an implementation of the methods described in [Bertram, W. K. (2010). Analytic solutions for optimal statistical arbitrage trading.](http://www.stagirit.org/sites/default/files/articles/a_0340_ssrn-id1505073.pdf) for finding optimal trading thresholds.
#
# In this paper, the author derives analytic formulae for statistical arbitrage trading where the price of an asset follows an exponential Ornstein-Uhlenbeck process. By framing the problem in terms of the first-passage time of the process, he first derives the expressions for the mean and the variance of the trade length. Then he derives the formulae for the expected return and the variance of the return per unit of time. Finally, a solution to the problem of choosing optimal trading thresholds is proposed by maximizing the expected return and the Sharpe ratio.


# ---


# ## Assumptions
#
# ### Price of the Traded Security
# The model defines the price of the traded security $p_t$ as,
#
# ${p_t = e^{X_t}};\quad{X_{t_0} = x_0}$
#
# where $X_t$ follows an O-U process and satisfies the following stochastic differential equation,
#
# ${dX_t = {\mu}({\theta} - X_t)dt + {\sigma}dW_t}$
#
# where
# ${\theta}$ is the long-term mean, ${\mu}$ is the speed at which the values will regroup around the long-term mean and ${\sigma}$ is the amplitude of randomness of the O-U process.
#
# ### Trading Strategy
# The trading strategy is defined by entering a trade when $X_t = a$, exiting the trade at $X_t = m$, where $a < m$. The paper here assumes that traders can only make a long trade on the traded security. The original model was adjusted and this assumption was later removed in the work of Zeng, Z. and Lee, C.-G. (2014) to obtain a more versatile trading model.
#
# ### Trading Cycle
# The trading cycle is completed as $X_t$ change from $a$ to $m$, then back to $a$.
#
# ### Trade Length
# The trade length $T$ is defined as the time needed to complete a trading cycle.


# ---


# ## Analytic Formulae
#
# ### Mean and Variance of the Trade Length
# $E[T] = \frac{\pi}{\mu} (Erfi(\frac{(m - \theta)\sqrt{\mu}}{\sigma}) - Erfi(\frac{(a - \theta)\sqrt{\mu}}{\sigma}))$,
#
# where $Erfi(x) = iErf(ix)$ is the imaginary error function.
#
# $V[T] = ({w_1(\frac{(m - \theta)\sqrt{2\mu}}{\sigma})} - {w_1(\frac{(a - \theta)\sqrt{2\mu}}{\sigma})} - {w_2(\frac{(m - \theta)\sqrt{2\mu}}{\sigma})} + {w_2(\frac{(a - \theta)\sqrt{2\mu}}{\sigma})}) / {{\mu}^2}$,
#
# where 
#
# $w_1(z) = (\frac{1}{2} \sum_{k=1}^{\infty} \Gamma(\frac{k}{2}) (\sqrt{2}z)^k / k! )^2 - (\frac{1}{2} \sum_{n=1}^{\infty} (-1)^k \Gamma(\frac{k}{2}) (\sqrt{2}z)^k / k! )^2$,
#
# $w_2(z) = \sum_{k=1}^{\infty} \Gamma(\frac{2k - 1}{2}) \Psi(\frac{2k - 1}{2}) (\sqrt{2}z)^{2k - 1} / (2k - 1)!$,
#
# where $\Psi(x) = \psi(x) − \psi(1)$ and $\psi(x)$ is the digamma function.
#
# ### Mean and Variance of the Trading Strategy Return per Unit of Time
#
# $\mu_s(a,\ m,\ c) = \frac{r(a,\ m,\ c)}{E [T]}$
#
# $\sigma_s(a,\ m,\ c) = \frac{{r(a,\ m,\ c)}^2{V[T]}}{{E[T]}^3}$
#
# where $r(a,\ m,\ c) = (m − a − c)$ gives the continuously compound rate of return for a single trade accounting for transaction cost.


# ---


# ## Optimal Strategies
#
# To calculate an optimal trading strategy, we seek to choose optimal entry and exit thresholds that maximize the expected return or the Sharpe ratio per unit of time for a given transaction cost/risk-free rate.
#
# ### Get Optimal Thresholds by Maximizing the Expected Return / Sharpe Ratio
#
# This paper shows that the maximum expected return/Sharpe ratio occurs when $(m - \theta)^2 = (a - \theta)^2$. Since we have assumed that $a < m$, this implies that $m = 2\theta − a$. Therefore, for a given transaction cost/risk-free rate, the following equation can be maximized to find optimal $a$ and $m$.
#
# $\mu^*_s(a, c) = \frac{r(a, 2\theta − a, c)}{E [T]}$
#
# $S^*(a, c, r_f) = \frac{\mu_s(a, 2\theta − a, c) - r^*}{\sigma_s(a, 2\theta − a, c)}$
#
# where $r^* = \frac{r_f}{E[T]}$ and $r_f$ is the risk free rate.


# ---


# ## Example Usage of the Module


from arbitragelab.time_series_approach.ou_optimal_threshold_bertram import OUModelOptimalThresholdBertram


# Creating a class instance
OUOTB = OUModelOptimalThresholdBertram()

# Initializing OU-process parameter
OUOTB.construct_ou_model_from_given_parameters(theta = 0, mu = 180.9670, sigma = 0.1538)


%%time
# Getting optimal thresholds by maximizing the expected return
a, m = OUOTB.get_threshold_by_maximize_expected_return(c = 0.001)

# Getting the expected return and the variance
E = OUOTB.expected_return(a = a, m = m, c = 0.001)
V = OUOTB.return_variance(a = a, m = m, c = 0.001)

print("Entering a trade when Xt =", a)
print("Exiting the trade at Xt =", m)
print("Expected Return:", E)
print("Variance:", V)


%%time
# Getting optimal thresholds by maximizing the Sharpe ratio
a, m = OUOTB.get_threshold_by_maximize_sharpe_ratio(c = 0.001, rf = 0.01)

# Getting the Sharpe ratio
S = OUOTB.sharpe_ratio(a = a, m = m, c = 0.001, rf = 0.01)

print("Entering a trade when Xt =", a)
print("Exiting the trade at Xt =", m)
print("Sharpe Ratio:", S)


# ---


# ## Reproduction of Numerical Results in the Paper


import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D  
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = (12, 6)


# ### Fig. 1. The expected value and the variance of the trade length as a function of trade entry and exit level.


a_list = np.linspace(-0.025, 0.025, 30)
m_list = np.linspace(-0.025, 0.025, 30)

X, Y = np.meshgrid(a_list, m_list)
skip_points = X > Y # Skipping the points if a > m


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

func = np.vectorize(OUOTB.expected_trade_length)
expected_trade_length = np.array(func(np.ravel(X), np.ravel(Y)))
Z = expected_trade_length.reshape(X.shape)
Z[skip_points] = 0

ax.plot_surface(X, Y, Z)
ax.view_init(30, 240)

ax.set_title('E[Trade Length]')
ax.set_xlabel('a')
ax.set_ylabel('m')
ax.set_zlabel('E[T]')

plt.tight_layout()
plt.show()


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

func = np.vectorize(OUOTB.trade_length_variance)
trade_length_variance = np.array(func(np.ravel(X), np.ravel(Y)))
Z = trade_length_variance.reshape(X.shape)
Z[skip_points] = 0

ax.plot_surface(X, Y, Z)
ax.view_init(30, 240)

ax.set_title('V[Trade Length]')
ax.set_xlabel('a')
ax.set_ylabel('m')
ax.set_zlabel('V[T]')

plt.tight_layout()
plt.show()


# ### Fig. 2. The expected return and the variance of the return as a function of trade entry and exit level in the positive half place.


a_list = np.linspace(-0.025, 0.025, 30)
m_list = np.linspace(-0.025, 0.025, 30)

X, Y = np.meshgrid(a_list, m_list)
C = np.array([0.001]*np.ravel(X).shape[0]) # Transaction costs
skip_points = X > Y # Skipping the points if a > m


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

func = np.vectorize(OUOTB.expected_return)
expected_return = np.array(func(np.ravel(X), np.ravel(Y), C))
negative_points = expected_return < 0 # Positive half place
expected_return[negative_points] = 0 
nan_points = np.isnan(expected_return) # Removing NaN
expected_return[nan_points] = 0

Z = expected_return.reshape(X.shape)
Z[skip_points] = 0

ax.plot_surface(X, Y, Z)
ax.view_init(30, 240)

ax.set_title('E[Return]')
ax.set_xlabel('a')
ax.set_ylabel('m')
ax.set_zlabel(r'$\mu (a, m, c)$')

plt.tight_layout()
plt.show()


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

func = np.vectorize(OUOTB.return_variance)
return_variance = np.array(func(np.ravel(X), np.ravel(Y), C))
negative_points = return_variance < 0 # Positive half place
return_variance[negative_points] = 0 
nan_points = np.isnan(return_variance) # Removing NaN
return_variance[nan_points] = 0

Z = return_variance.reshape(X.shape)
Z[skip_points] = 0

ax.plot_surface(X, Y, Z)
ax.view_init(30, 240)

ax.set_title('V[Return]')
ax.set_xlabel('a')
ax.set_ylabel('m')
ax.set_zlabel(r'${\sigma}^2 (a, m, c)$')

plt.tight_layout()
plt.show()


# ### Fig. 3. The Sharpe ratio as a function of trade entry and exit level in the positive half place.


a_list = np.linspace(-0.025, 0.025, 30)
m_list = np.linspace(-0.025, 0.025, 30)

X, Y = np.meshgrid(a_list, m_list)
C = np.array([0.001]*np.ravel(X).shape[0]) # Transaction costs
R = np.array([0.01]*np.ravel(X).shape[0]) # Risk free rates
skip_points = X > Y # Skipping the points if a > m


fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

func = np.vectorize(OUOTB.sharpe_ratio)
sharpe_ratio = np.array(func(np.ravel(X), np.ravel(Y), C, R))
negative_points = sharpe_ratio < 0 # Positive half place
sharpe_ratio[negative_points] = 0 
nan_points = np.isnan(sharpe_ratio) # Removing NaN
sharpe_ratio[nan_points] = 0

Z = sharpe_ratio.reshape(X.shape)
Z[skip_points] = 0

ax.plot_surface(X, Y, Z)
ax.view_init(30, 240)

ax.set_title('Sharpe Ratio')
ax.set_xlabel('a')
ax.set_ylabel('m')
ax.set_zlabel(r'$S(a, m, c, r)$')

plt.tight_layout()
plt.show()


# ### Fig. 4. The optimal trading bands and corresponding maximum return of the strategy as a function of transaction cost.


c_list = np.linspace(0, 0.01, 30)


fig = OUOTB.plot_target_vs_c(target = "a", method = "maximize_expected_return", c_list = c_list)
plt.show()


fig = OUOTB.plot_target_vs_c(target = "expected_return", method = "maximize_expected_return", c_list = c_list)
plt.show()


# ### Fig. 5. The optimal trading bands and corresponding maximum return of the strategy as a function of the risk-free rate $r_f$.


c = 0.001
rf_list = np.linspace(0, 0.05, 30)


fig = OUOTB.plot_target_vs_rf(target = "a", method = "maximize_sharpe_ratio", rf_list = rf_list, c = c)
plt.show()


fig = OUOTB.plot_target_vs_rf(target = "sharpe_ratio", method = "maximize_sharpe_ratio", rf_list = rf_list, c = c)
plt.show()


# ### Table 1 
# ### Results for maximising the expected return with α = 180.9670, η = 0.1538.


c_list = np.array([0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.015, 0.0175, 0.02])
func = np.vectorize(OUOTB.get_threshold_by_maximize_expected_return)
a_list, m_list = func(c_list)

func = np.vectorize(OUOTB.expected_return)
expected_return_list = func(a_list, m_list, c_list)


table_1 = pd.DataFrame([c_list, a_list, expected_return_list]).T
table_1.columns = ["c", "a", "Expected Return"]
display(table_1)


# ### Table 2
# ### Results for maximising the Sharpe ratio with α = 180.9670, η = 0.1538, c = 0.001.


rf_list = np.array([0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.015, 0.0175, 0.02])
c_list = np.array([0.001] * len(rf_list))
func = np.vectorize(OUOTB.get_threshold_by_maximize_sharpe_ratio)
a_list, m_list = func(c_list, rf_list)

func = np.vectorize(OUOTB.expected_return)
expected_return_list = func(a_list, m_list, c_list)

func = np.vectorize(OUOTB.return_variance)
variance_list = func(a_list, m_list, c_list)

func = np.vectorize(OUOTB.sharpe_ratio)
sharpe_ratio_list = func(a_list, m_list, c_list, rf_list)


table_2 = pd.DataFrame([rf_list, a_list, sharpe_ratio_list, expected_return_list, variance_list]).T
table_2.columns = ["rf", "a", "Sharpe Ratio", "Expected Return", "Variance"]
display(table_2)


# ---


# ## Conclusion
#
# This notebook demonstrated an implementation of the methods for finding optimal trading thresholds under several assumptions and two different objective functions. The numerical results are consistent with the results in the paper, confirming that the implementation seems to be correct.
#
# Key takeaways from the notebook:
# - Analytic way to calculate the expected trade length for the series follows O-U process.
# - A way to reduce optimal trading thresholds finding problem to a simple maximization problem.


# ## References
#
# 1. [Bertram, W. K., Analytic solutions for optimal statistical arbitrage trading. Physica A: Statistical Mechanics and its Applications, 389(11): 2234–2243.](http://www.stagirit.org/sites/default/files/articles/a_0340_ssrn-id1505073.pdf)



// ---------------------------------------------------

// ou_model_optimal_threshold_Zeng.py
// arbitrage_research/Time Series Approach/ou_model_optimal_threshold_Zeng.py
# Generated from: ou_model_optimal_threshold_Zeng.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Pairs trading: optimal thresholds and profitability.__ by _Zeng, Z._ and _Lee, C.-G._


# # OU Model Optimal Trading Thresholds Zeng


# ## Introduction
#
# This notebook demonstrates an implementation of the methods described in [Zeng, Z. and Lee, C.-G. (2014). Pairs trading: optimal thresholds and profitability.](https://www.tandfonline.com/doi/pdf/10.1080/14697688.2014.917806) for finding optimal trading thresholds.
#
# In this paper, the authors enhance the work in [Bertram(2010)](http://www.stagirit.org/sites/default/files/articles/a_0340_ssrn-id1505073.pdf), which only allows long positions on a tradable process when finding the optimal trading thresholds. To also allow short positions, the authors of the first paper derive a polynomial expression for the expectation of the first-passage time of an O-U process with a two-sided boundary. Then they simplify the problem of optimizing the expected return per unit of time to a problem of solving an equation.


# ---


# ## Assumptions
#
# ### Price of the Traded Security
# The model defines the price of the traded security $p_t$ as,
#
# ${p_t = e^{X_t}};\quad{X_{t_0} = x_0}$
#
# where $X_t$ follows an O-U process and satisfies the following stochastic differential equation,
#
# ${dX_t = {\mu}({\theta} - X_t)dt + {\sigma}dW_t}$
#
# where
# ${\theta}$ is the long-term mean, ${\mu}$ is the speed at which the values will regroup around the long-term mean and ${\sigma}$ is the amplitude of randomness of the O-U process.
#
# ### Trading Strategy
# The Trading strategy is defined as: 
#
# $\left\{
# \begin{array}{**lr**} 
# Open\ a\ short\ trade\ when\ Y_t = a_d\ and\ close\ the\ exiting\ short\ trade\ at\ Y_t = b_d.\\
# Open\ a\ long\ trade\ when\ Y_t = -a_d\ and\ close\ the\ exiting\ long\ trade\ at\ Y_t = -b_d.\\
# \end{array}
# \right.
# $
#
# $a_d$ and $b_d$ is the entry and exit thresholds in the dimensionless system, respectively.
#
# The $Y_t$ is a dimensionless series transformed from the original time series $X_t$, 
#
#
# ### Trading Cycle
# The trading cycle is completed as $Y_t$ change from $a_d$ to $b_d$, then back to $a_d$ or $-a_d$.
#
# ### Trade Length
# The trade length $T$ is defined as the time needed to complete a trading cycle.


# ---


# ## Analytic Formulae
#
# ### Mean and Variance of the Trade Length
# $E[T] = \frac{1}{2\mu}\sum_{k=0}^{\infty} \Gamma(\frac{2k + 1}{2})((\sqrt{2}a_d)^{2k + 1} - (\sqrt{2}b_d)^{2k + 1})/ (2k + 1)!$,
#
# $V[T] = \frac{1}{\mu^2}(V[T_{a_d,\ b_d}] + V[T_{-a_d,\ a_d,\ b_d}])$,
#
# where $V[T_{a_d,\ b_d}]$ is the variance of the time taken for the O-U process to travel from $a_d$ to $b_d$,
#
# and $V[T_{-a_d,\ a_d,\ b_d}]$ is the variance of the time taken for the O-U process to travel from $b_d$ to $a_d$ or -$a_d$.
#
# $V[T_{a_d,\ b_d}] = {w_1(a_d)} - {w_1(b_d)} - {w_2(a_d)} + {w_2(b_d)}$,
#
# where 
#
# $w_1(z) = (\frac{1}{2} \sum_{k=1}^{\infty} \Gamma(\frac{k}{2}) (\sqrt{2}z)^k / k! )^2 - (\frac{1}{2} \sum_{n=1}^{\infty} (-1)^k \Gamma(\frac{k}{2}) (\sqrt{2}z)^k / k! )^2$,
#
# $w_2(z) = \sum_{k=1}^{\infty} \Gamma(\frac{2k - 1}{2}) \Psi(\frac{2k - 1}{2}) (\sqrt{2}z)^{2k - 1} / (2k - 1)!$.
#
#
# $V[T_{-a_d,\ a_d,\ b_d}] = E[T^{2}_{-a_d,\ a_d,\ b_d}] - E[T_{-a_d,\ a_d,\ b_d}]^2$,
#
# where
#
# $E[T_{-a_d,\ a_d,\ b_d}] = \frac{1}{2}\sum_{k=1}^{\infty} \Gamma(k)((\sqrt{2}a_d)^{2k} - (\sqrt{2}b_d)^{2k})/ (2k)!$,
#
# $E[T^{2}_{-a_d,\ a_d,\ b_d}] = e^{(b^2_d - a^2_d)/4}[g_1(a_d,\ b_d) - g_2(a_d,\ b_d)]$,
#
# where
#
# $g_1(a_d,\ b_d) = [(m^{''}(\lambda,\ b_d)\ m(\lambda,\ a_d) - m^{'}(\lambda,\ a_d)\ m^{'}(\lambda,\ b_d))/m^2(\lambda,\ a_d)]|_{\lambda = 0}$,
#
# $g_2(a_d,\ b_d) =[(m^{''}(\lambda,\ a_d)\ m(\lambda,\ b_d) + m^{'}(\lambda,\ a_d)\ m^{'}(\lambda,\ b_d))/m^2(\lambda,\ a_d) - 2(m^{'}(\lambda,\ a_d))^2\ m(\lambda,\ b_d)/m^3(\lambda,\ a_d)]|_{\lambda = 0}$,
#
# where $m(\lambda, x) = D_{-\lambda}(x) + D_{-\lambda}(−x)$,
#
# where $D_{-\lambda}(x) = \sqrt{\frac{2}{\pi}} e^{x^2/4} \int_{0}^{\infty} t^{-\lambda} e^{-t^2/2} \cos(xt + \frac{\lambda\pi}{2})dt$.
#
# ### Mean and Variance of the Trading Strategy Return per Unit of Time
#
# $\mu_s(a,\ b,\ c) = \frac{r(a,\ b,\ c)}{E [T]}$
#
# $\sigma_s(a,\ b,\ c) = \frac{{r(a,\ b,\ c)}^2{V[T]}}{{E[T]}^3}$
#
# where $r(a,\ b,\ c) = (|a − b| − c)$ gives the continuously compound rate of return for a single trade accounting for transaction cost,
#
# where $a$, $b$ denotes the entry and exit thresholds, respectively.


# ---


# ## Optimal Strategies
#
# To calculate an optimal trading strategy, we seek to choose optimal entry and exit thresholds that maximize the expected return per unit of time for a given transaction cost.
#
# ### Get Optimal Thresholds by Maximizing the Expected Return
#
# $Case\ 1 \ \ 0 \leqslant b_d \leqslant a_d$
#
# This paper shows that the maximum expected return occurs when $b_d = 0$. Therefore, for a given transaction cost, the following equation can be solved to find optimal $a_d$.
#
# $\frac{1}{2}\sum_{k=0}^{\infty} \Gamma(\frac{2k + 1}{2})((\sqrt{2}a_d)^{2k + 1} / (2k + 1)! = (a - c) \frac{\sqrt{2}}{2}\sum_{k=0}^{\infty} \Gamma(\frac{2k}{2})((\sqrt{2}a_d)^{2k} / (2k + 1)!$
#
# $Case\ 2 \ \ -a_d \leqslant b_d \leqslant 0$
#
# This paper shows that the maximum expected return occurs when $b_d = -a_d$. Therefore, for a given transaction cost, the following equation can be solved to find optimal $a_d$.
#
# $\frac{1}{2}\sum_{k=0}^{\infty} \Gamma(\frac{2k + 1}{2})((\sqrt{2}a_d)^{2k + 1} / (2k + 1)! = (a - \frac{c}{2}) \frac{\sqrt{2}}{2}\sum_{k=0}^{\infty} \Gamma(\frac{2k}{2})((\sqrt{2}a_d)^{2k} / (2k + 1)!$
#
# ### Back Transform from the Dimensionless System
#
# After calculating optimal thresholds in the dimensionless system, we need to use the following formula to transform them back to the original system.
#
# $k = k_d \frac{\sigma}{\sqrt{2\mu}} + \theta$,
#
# where $k_d$ = $a_d$, $b_d$, $-a_d$, $-b_d$ and $k$ = $a_s$, $b_s$, $a_l$, $a_l$,
#
# where
#
# $a_s$, $b_s$ denotes the entry and exit thresholds for a short position,
#
# $a_l$, $b_l$ denotes the entry and exit thresholds for a long position.


# ---


# ## Example Usage of the Module


import numpy as np
import matplotlib.pyplot as plt
from arbitragelab.time_series_approach.ou_optimal_threshold_zeng import OUModelOptimalThresholdZeng

plt.rcParams['figure.figsize'] = (12, 6)


# Creating a class instance
OUOTZ = OUModelOptimalThresholdZeng()

# Initializing OU-process parameter
OUOTZ.construct_ou_model_from_given_parameters(theta = 3.4241, mu = 0.0237, sigma = 0.0081)


%%time
# Getting optimal thresholds by Conventional Optimal Rule
a_s, b_s, a_l, b_l = OUOTZ.get_threshold_by_conventional_optimal_rule(c = 0.02)

print("Entering a short position when Xt =", a_s)
print("Exiting a short position when Xt =", b_s)
print("Entering a long position when Xt =", a_l)
print("Exiting a long position when Xt =", b_l)

E_s = OUOTZ.expected_return(a = a_s, b = b_s, c = 0.02)
V_s = OUOTZ.return_variance(a = a_s, b = b_s, c = 0.02)
E_l = OUOTZ.expected_return(a = a_l, b = b_l, c = 0.02)
V_l = OUOTZ.return_variance(a = a_l, b = b_l, c = 0.02)

print("Short trade expected return:", E_s)
print("Short trade variance:", V_s)
print("Long trade expected return:", E_l)
print("Long trade variance:", V_l)


%%time
# Getting optimal thresholds by New Optimal Rule
a_s, b_s, a_l, b_l = OUOTZ.get_threshold_by_new_optimal_rule(c = 0.02)

print("Entering a short position when Xt =", a_s)
print("Exiting a short position when Xt =", b_s)
print("Entering a long position when Xt =", a_l)
print("Exiting a long position when Xt =", b_l)

E_s = OUOTZ.expected_return(a = a_s, b = b_s, c = 0.02)
V_s = OUOTZ.return_variance(a = a_s, b = b_s, c = 0.02)
E_l = OUOTZ.expected_return(a = a_l, b = b_l, c = 0.02)
V_l = OUOTZ.return_variance(a = a_l, b = b_l, c = 0.02)

print("Short trade expected return:", E_s)
print("Short trade variance:", V_s)
print("Long trade expected return:", E_l)
print("Long trade variance:", V_l)


# Comparison of the expected return between the Conventional Optimal Rule and New Optimal Rule
c_list = np.linspace(0, 0.02, 30)
fig_con = OUOTZ.plot_target_vs_c(target = "expected_return", method = "conventional_optimal_rule", c_list = c_list)
fig_new = OUOTZ.plot_target_vs_c(target = "expected_return", method = "new_optimal_rule", c_list = c_list)
plt.show()


# Combining two figures
ax_con = fig_con.gca()
ax_new = fig_new.gca()

x = ax_con.lines[0].get_xdata()
y_con = ax_con.lines[0].get_ydata()
y_new = ax_new.lines[0].get_ydata()

plt.plot(x, y_con, label = "Conventional Optimal Rule")
plt.plot(x, y_new, label = "New Optimal Rule")
plt.legend()
plt.show()


# Comparison of the variance between the Conventional Optimal Rule and New Optimal Rule 
c_list = np.linspace(0, 0.02, 30)
fig_con = OUOTZ.plot_target_vs_c(target = "return_variance", method = "conventional_optimal_rule", c_list = c_list)
fig_new = OUOTZ.plot_target_vs_c(target = "return_variance", method = "new_optimal_rule", c_list = c_list)
plt.show()


# Combining two figures
ax_con = fig_con.gca()
ax_new = fig_new.gca()

x = ax_con.lines[0].get_xdata()
y_con = ax_con.lines[0].get_ydata()
y_new = ax_new.lines[0].get_ydata()

plt.plot(x, y_con, label = "Conventional Optimal Rule")
plt.plot(x, y_new, label = "New Optimal Rule")
plt.legend()
plt.show()


# ---


# ## Reproduction of Empirical Results in the Paper


# ### Preparing data


import os
import pandas as pd
import datetime

plt.rcParams['figure.figsize'] = (12, 6)


# Getting data path
root_path = os.path.abspath(os.path.join(os.getcwd(), os.pardir))
path_1 = os.path.join(root_path, "Sample-Data", "PEP.csv")
path_2 = os.path.join(root_path, "Sample-Data", "KO.csv")

# Loading data
PEP = pd.read_csv(path_1)
KO = pd.read_csv(path_2)

# Setting index
PEP.index = pd.to_datetime(PEP["Date"])
KO.index = pd.to_datetime(KO["Date"])

# Creating Dataframe
data = pd.DataFrame([PEP["Close"], KO["Close"]]).T
data.columns = ["PEP", "KO"]

# Plotting the data
data.plot()
plt.show()


# Generating spread series that follows O-U process
beta = 0.2187
spread_series = np.log(data["PEP"]) - np.log(data["KO"]) * beta

# Plotting the series
spread_series.plot()
plt.show()


# ### Calculating Optimal Thresholds


# Creating a class instance
OUOTZ = OUModelOptimalThresholdZeng()

# Initializing OU-process parameter
OUOTZ.construct_ou_model_from_given_parameters(theta = 3.4241, mu = 0.0237, sigma = 0.0081)


%%time
# Getting optimal thresholds by New Optimal Rule
a_s, b_s, a_l, b_l = OUOTZ.get_threshold_by_new_optimal_rule(c = 0.02)
print(a_s, b_s, a_l, b_l)


# Plotting the series and thresholds
spread_series.plot(label = "spread")
plt.axhline(a_s, color='r')
plt.axhline(b_s, color='g')
plt.legend();


# ### Backtesting


def backtesting(origin_data, spread_series, a_s, b_s, a_l, b_l, beta , c):
    asset_1 = data[data.columns[0]]
    asset_2 = data[data.columns[1]]
    position = 0
    trade_count = 0
    
    # Setting up columns
    columns = ["Date", data.columns[0] + "-Prices($)", data.columns[0] + "-Action", 
               data.columns[1] + "-Prices($)", data.columns[1] + "-Action", "Total-Returns(%)", "Net-Returns(%)"]
    df = pd.DataFrame(columns = columns)
    
    for i in range(1, len(spread_series)):
        if spread_series[i - 1] < b_l and spread_series[i] >= b_l:
            if position == 1: # If the position is 1, then close the existing long position.
                total_return = spread_series.iloc[i] - spread_series.loc[df["Date"].iloc[-1]]
                df.loc["Trade " + str(trade_count) + " Close"] = [spread_series.index[i], asset_1[i], "Clear positions",
                                                                  asset_2[i], "Clear positions", total_return * 100,
                                                                  (total_return - c) * 100]
                position = 0
                
        if spread_series[i - 1] > b_s and spread_series[i] <= b_s:
            if position == -1: # If the position is -1, then close the existing short position.
                total_return = spread_series.loc[df["Date"].iloc[-1]] - spread_series.iloc[i]
                df.loc["Trade " + str(trade_count) + " Close"] = [spread_series.index[i], asset_1[i], "Clear positions",
                                                                  asset_2[i], "Clear positions", total_return * 100,
                                                                  (total_return - c) * 100]
                position = 0
                
        if spread_series[i - 1] < a_s and spread_series[i] >= a_s:
            if position == 0: # If the position is 0, then short a position on the spread.
                trade_count += 1
                df.loc["Trade " + str(trade_count) + " Open"] = [spread_series.index[i], asset_1[i], "Sell $1", asset_2[i],
                                                                 "Buy $" + str(beta), 0, 0]
                position = -1
                
        if spread_series[i - 1] > a_l and spread_series[i] <= a_l:
            if position == 0: # If the position is 0, then long a position on the spread.
                trade_count += 1
                df.loc["Trade " + str(trade_count) + " Open"] = [spread_series.index[i], asset_1[i], "Buy $1", asset_2[i],
                                                                 "Sell $" + str(beta), 0, 0]
                position = 1
                
    return df


df = backtesting(data, spread_series, a_s, b_s, a_l, b_l, beta, 0.02)

# Details of transaction
display(df)


# Plotting the series, thresholds and trades
trades = spread_series[df["Date"].unique()]
plt.scatter(trades.index, trades.values, color = "k")
spread_series.plot(label = "spread")
plt.axhline(a_s, color='r')
plt.axhline(b_s, color='g')
plt.legend();


# ### Comparison between Two Rules


c_list = np.linspace(0, 0.03, 20)
return_list_new = []
return_list_con = []

for c in c_list:
    a_s_new, b_s_new, a_l_new, b_l_new = OUOTZ.get_threshold_by_new_optimal_rule(c = c)
    a_s_con, b_s_con, a_l_con, b_l_con = OUOTZ.get_threshold_by_conventional_optimal_rule(c = c)
    
    df_new = backtesting(data, spread_series, a_s_new, b_s_new, a_l_new, b_l_new, beta, c)
    df_con = backtesting(data, spread_series, a_s_con, b_s_con, a_l_con, b_l_con, beta, c)
    
    return_new = df_new["Net-Returns(%)"].sum() / 100
    return_con = df_con["Net-Returns(%)"].sum() / 100
    
    return_list_new.append(return_new)
    return_list_con.append(return_con)


plt.plot(c_list, return_list_new, label = "New Optimal Rule")
plt.plot(c_list, return_list_con, label = "Conventional Optimal Rule")

plt.ylabel("Total Return Over The Whole Trading Period")
plt.xlabel("Transaction Cost c")
plt.legend()
plt.show()


# ## Conclusion
#
# This notebook demonstrated an implementation of the methods for finding optimal trading thresholds under several assumptions and the goal to maximize the expected return per unit
# time. The empirical results are similar to the results in the paper, confirming that the implementation seems to be correct.
#
# Key takeaways from the notebook:
# - Analytic way to calculate the expected trade length with a two-sided boundary for the series follows O-U process.
# - A way to reduce optimal trading thresholds finding problem to a simple maximization problem.


# ## References
#
# 1. [Zeng, Z. and Lee, C.-G. Pairs trading:  optimal thresholds and profitability. Quantitative Finance, 14(11): 1881–1893.](https://www.tandfonline.com/doi/pdf/10.1080/14697688.2014.917806)



// ---------------------------------------------------

// optimal_transport.py
// arbitrage_research/Codependence Module/Optimal Transport/optimal_transport.py
# Generated from: optimal_transport.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.__ _by_ Marti et al.


# ## Abstract


# Optimal Copula Transport dependence is a unique measure between two random variables that allows measuring the codependence with respect to similarity to the target codependence type.


# ## Optimal Copula Transport dependence


# This description is based on the paper by _Marti et al._ __“Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.”__  [available here](https://arxiv.org/pdf/1610.09659.pdf).


# ### Optimal Transport and copulas


# As described by Gautier Marti:
#
# “The basic idea of the optimal copula transport dependence measure is rather simple. It relies on leveraging:
#
# - Copulas, which are distributions encoding fully the dependence between random variables.
#
# - A geometrical point of view: Where does the empirical copula stand in the space of copulas? In particular, how far is it from reference copulas such as the Fréchet–Hoeffding copula bounds (copulas associated to comonotone, countermonotone, independent random variables)?”


# ![image](images/optimal_transport_distance.png)
#
# _Dependence is measured as the relative distance from independence to the nearest target-dependence: comonotonicity or countermonotonicity. ([Blog post by Gautier Marti](https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html))_


# “With this geometric view:
#
# - It is rather easy to extend this novel dependence measure to alternative use cases (e.g. by changing the reference copulas).
#
# - It can also allow to look for specific patterns in the dependence structure (generalization of conditional correlation).
#
# With this optimal copula transport tool, one can look for answers to, for example:
#
#  A) “Which pair of assets having ρ=0.7 correlation has the nearest copula to the Gaussian one?”
#
#  B) “Which pairs of assets are both positively and negatively correlated?”
#
#  C) “Which assets occur extreme variations while those of others are relatively small, and conversely?”
#
#  D) “Which pairs of assets are positively correlated for small variations but uncorrelated otherwise?”


# ![image](images/target_copulas.png)
#
#
# _Target copulas (simulated or handcrafted) and their respective nearest copulas which answer questions A,B,C,D. ([Marti et al. 2016](https://arxiv.org/pdf/1610.09659.pdf))_


# ---


# According to the description of the method from [the original paper by Marti et al.](https://arxiv.org/pdf/1610.09659.pdf):


# The idea of the approach is to "target specific dependence patterns and ignore others. We want to target dependence
# which are relevant to such or such problem, and forget about the dependence which are not in the scope of the problems
# at hand, or even worse which may be spurious associations (pure chance or artifacts in the data)."
#
# The proposed codependence coefficient "can be parameterized by a set of target-dependences, and a set
# of forget-dependences. Sets of target and forget dependences can be built using expert hypotheses,
# or by leveraging the centers of clusters resulting from an exploratory clustering of the pairwise dependences.
# To achieve this goal, we will leverage three tools: copulas, optimal transportation, and clustering."
#
# "Optimal transport is based on the underlying theory of the Earth Mover’s Distance. Until very recently,
# optimal transportation distances between distributions were not deemed relevant for machine learning applications
# since the best computational cost known was super-cubic to the number of bins used for discretizing the
# distribution supports which grows itself exponentially with the dimension. A mere distance evaluation
# could take several seconds!"
#
# "Copulas and optimal transportation are not yet mainstream tools, but they have recently gained attention
# in machine learning, and several copula-based dependence measures have been proposed for improving
# feature selection methods".
#
# "Copulas are functions that couple multivariate distribution functions to their univariate marginal distribution functions".
#
# In this implementation, only bivariate copulas are considered, as higher dimensions would cost a high computational burden.
# But most of the results and the methodology presented hold in the multivariate setting.


#  **Theorem 1 (Sklar’s Theorem)** Let $X = (X_i, X_j)$ be  a random  vector with  a joint
#     cumulative distribution function $F$ , and having continuous marginal cumulative
#     distribution functions $F_i, F_j$ respectively. Then, there exists a unique
#     distribution $C$ such that $F(X_i, X_j) = C(F_i(X_i), F_j(X_j))$ .
#     $C$ , the copula of $X$ , is the bivariate distribution of uniform
#     marginals $U_i, U_j := F_i(X_i), F_j(X_j)$


# "Copulas are central for studying the dependence between random variables: their uniform marginals
# jointly encode all the dependence. They allow to study scale-free measures of dependence and are invariant
# to monotonous transformations of the variables. Some copulas play a major role in the measure of dependence,
# namely $\mathcal{W}$ and $\mathcal{M}$ the Frechet-Hoeffding copula bounds, and the independence
# copula $\Pi (u_i,u_j) = u_i u_j$ ".
#
# ![image](images/dependence_copulas.png)
#
# _Copulas measure (left column) and cumulative distribution function (right column) heatmaps for negative dependence (first row), independence (second row), i.e. the uniform distribution over $[0, 1]^2$, and positive dependence (third row) ([Marti et al. 2016](https://arxiv.org/pdf/1610.09659.pdf))_


# **Proposition 1 (Frechet-Hoeffding copula bounds)** For any copula $C: [0, 1]^2 \rightarrow [0, 1]$ and any $(u_i, u_j) \in [0, 1]^2$ the following bounds hold:
#
# $$\mathcal{W} (u_i, u_j) \le C(u_i, u_j) \le \mathcal{M} (u_i, u_j)$$
#
# where $\mathcal{W} (u_i, u_j) = max \{u_i + u_j − 1, 0 \}$ is the copula for countermonotonic random variables and $\mathcal{M} (u_i, u_j) = min \{ u_i, u_j \}$ is the copula for comonotonic random variables.


# "Notice that when working with empirical data, we do not know a priori the margins
# $F_i$ for applying the probability integral transform $U_i := F_i(X_i)$ . Deheuvels has introduced a
# practical estimator for the uniform margins and the underlying copula, the empirical copula transform".


# **Definition 1 (Empirical Copula Transform)** Let $(X^t_i, X^t_j), t = 1, ..., T$ , be $T$ observations
#     from a random vector $(X_i, X_j)$ with continuous margins. Since one cannot directly obtain the corresponding
#     copula observations $(U^t_i, U^t_j) := (F_i(X^t_i), F_j(X^t_j))$ , where $t = 1, ..., T$ , without
#     knowing a priori $F_i$ , one can instead estimate the empirical
#     margins $F^T_i(x) = \frac{1}{T} \sum^T_{t=1} I(X^t_i \le x)$ , to obtain the $T$ empirical
#     observations $(\widetilde{U}^t_i, \widetilde{U}^t_j) := (F^T_i(X^t_i), F^T_j(X^t_j))$ . Equivalently,
#     since $U^t_i = R^t_i / T, R^t_i$ being the rank of observation $X^t_i$ , the empirical copula
#     transform can be considered as the normalized rank transform.


# "The idea of optimal transport is intuitive. It was first formulated by Gaspard Monge in 1781 as a problem to
# efficiently level the ground: Given that work is measured by the distance multiplied by the amount of dirt
# displaced, what is the minimum amount of work required to level the ground? Optimal transport plans and distances
# give the answer to this problem. In practice, empirical distributions can be represented by histograms.
#
# Let $r, c$ be two histograms in the probability simplex $\sum_m = \{x \in R^m_+ : x^T 1_m = 1\}$ .
# Let $U(r, c) = \{ P \in R^{m \times m}_+ | P1_m = r, P^T 1_m = c\}$ be the transportation polytope
# of $r$ and $c$ , that is the set containing all possible transport plans between $r$ and $c$ ".


# **Definition 2 (Optimal Transport)** Given a $m \times m$ cost matrix $M$, the cost of mapping $r$ to
#     $c$ using a transportation matrix $P$ can be quantified as $\langle P, M \rangle _F$ , where $\langle \cdot, \cdot \rangle _F$ is
#     the Frobenius dot-product. The optimal transport between $r$ and $c$ given transportation cost
#     $M$ is thus:
#
# $$d_M(r, c) := min_{P \in U (r, c)} \langle P, M \rangle _F$$


# "Whenever $M$ belongs to the cone of distance matrices, the optimum of the transportation problem
# $d_M(r, c)$ is itself a distance.
#
# Using the optimal transport distance between copulas, we now propose a dependence coefficient which is parameterized
# by two sets of copulas: target copulas and forget copulas".


# **Definition 3 (Target/Forget Dependence Coefficient)** Let ${C^-_l}_l$ be the set of forget-dependence copulas.
#     Let ${C^+_k}_k$ be the set of target-dependence copulas. Let $C$ be the copula of $(X_i, X_j)$ .
#     Let $d_M$ be an optimal transport distance parameterized by a ground metric $M$ . We define
#     the Target/Forget Dependence Coefficient as:
#
# $$TFDC(X_i, X_j; {C^+_k}_k, {C^-_l}_l) := \frac{min_l d_M(C^-_l, C)}{min_l d_M(C^-_l, C) + min_k d_M(C, C^+_k)} \in [0, 1]$$


# "Using this definition, we obtain:
#
# $$TFDC (X_i, X_j; {C:+_k}_k, {C:-_l}_l) = 0 \Leftrightarrow C \in {C^-_l}_l$$
#
# $$TFDC(X_i ,X_j; {C^+_k}_k, {C^-_l}_l) = 1 \Leftrightarrow C \in {C^+_k}_k$$
#
# It is known by risk managers how dangerous it can be to rely solely on a correlation coefficient
# to measure dependence. That is why we have proposed a novel approach to explore, summarize and measure the
# pairwise correlations which exist between variables in a dataset. The experiments show the benefits of the
# proposed method: It allows to highlight the various dependence patterns that can be found between financial
# time series, which strongly depart from the Gaussian copula widely used in financial engineering.
# Though answering dependence queries as briefly outlined is still an art, we plan to develop a rich language so
# that a user can formulate complex questions about dependence, which will be automatically translated into
# copulas in order to let the methodology provide these questions accurate answers".


# ---


# ## Usage of the algorithms


# ##### **Warning!** Optimal Copula Transport dependence is computationally heavy, so calculating the codependence matrix may take some time.


# This part shows how the Optimal Copula Transport dependence can be used to measure codependence between a set of stocks


import arbitragelab as al
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Loading the dataset of stocks
stocks = pd.read_csv('../../Sample-Data/stock_prices.csv')
stocks.set_index('Date', inplace=True)
stocks.index = pd.to_datetime(stocks.index)

# Taking first 10 stocks for better output visualization
stocks = stocks.iloc[:,0:10]
stocks.head()


# Calculating returns of a given dataset of stocks
stocks_returns = stocks.pct_change()[1:]
stocks_returns.iloc[:,0:10].head()


# Calculating Optimal Copula Transport dependence using a comonotone target copula
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='comonotonicity')
print('Optimal Copula Transport dependence between EEM and EWG using a comonotone target copula is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a countermonotone target copula
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='countermonotonicity')
print('Optimal Copula Transport dependence between EEM and EWG using a countermonotone target copula is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a Gaussian target copula with ρ=0.5
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='gaussian', gaussian_corr=0.5)
print('Optimal Copula Transport dependence between EEM and EWG using a Gaussian target copula with ρ=0.5 is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a positive and negative correlation target copula
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='positive_negative')
print('Optimal Copula Transport dependence between EEM and EWG using a positive and negative correlation target copula is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a target copula wheree element has extreme variations and the second has small variations
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='different_variations')
print('Optimal Copula Transport dependence between EEM and EWG using a arget copula with one element has extreme variations and the second has small variations is: ', ot_dist)


# Calculating Optimal Copula Transport dependence using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise
ot_dist = al.codependence.optimal_transport_dependence(stocks_returns['EEM'], stocks_returns['EWG'], target_dependence='small_variations', var_threshold=0.2)
print('Optimal Copula Transport dependence between EEM and EWG using elements being positively correlated for small variations but uncorrelated otherwise: ', ot_dist)


# These codependence measures can also be used on the whole dataframes, the results will be codependence matrices.


# Calculating Optimal Copula Transport dependence between all stocks using a comonotone target copula
ot_matrix_comonotone = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='comonotonicity')

print('Optimal Copula Transport dependence matrix using a comonotone target copula:')
ot_matrix_comonotone


# Calculating Optimal Copula Transport dependence between all stocks using a Gaussian target copula with ρ=0.5
ot_matrix_gaussian = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='gaussian', gaussian_corr=0.5)

# Calculating Optimal Copula Transport dependence between all stocks using a positive and negative correlation target copula
ot_matrix_positive_negative = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='positive_negative')

# Calculating Optimal Copula Transport dependencee between all stocks using a target copula where one element has extreme variations and the second has small variations
ot_matrix_diffvar = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='different_variations')

# Calculating Optimal Copula Transport dependence between all stocks using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise
ot_matrix_diffvar = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='optimal_transport', target_dependence='small_variations', var_threshold=0.2)


# ### Heatmap of Optimal Copula Transport dependence using a comonotone target copula


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(ot_matrix_comonotone, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a comonotone target copula')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with ρ=0.5


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_gaussian, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with ρ=0.5')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a positive and negative correlation target copula


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_positive_negative, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a positive and negative correlation target copula')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a target copula with one element has extreme variations and the second has small variations


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_diffvar, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a target copula where one element has extreme variations and the second has small variations')
plt.show()


# ### Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(ot_matrix_diffvar, ax = ax, cbar_kws={'label': 'OT dependence'})

ax.set_title('Heatmap of Optimal Copula Transport dependence using a Gaussian target copula with elements being positively correlated for small variations but uncorrelated otherwise')
plt.show()


# As seen from the heat maps of OT dependence matrices, various target copulas used in measures are showing specific characteristics of the assets. These measures can help uncover and better analyze relationships between assets.


# ---


# ## Conclusion


# This notebook describes the Optimal Copula Transport dependence measure and how it may be used in real-life applications.  
#
# This dependence measure was described by _Marti et al._ in the work __“Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.”__  [available here](https://arxiv.org/pdf/1610.09659.pdf).
#
# Key takeaways from the notebook:
# - Optimal Copula Transport dependence allows to measure distance between random elements in relation to different target copulas.
# - Optimal Copula Transport dependence is computationally heavy, so it may take some time to calculate the whole codependence matrix.
# - Supported target copulas allow to answer the following questions:
#   - “Which pair of assets having ρ=0.7 correlation has the nearest copula to the Gaussian one?”
#   - “Which pairs of assets are both positively and negatively correlated?”
#   - “Which assets occur extreme variations while those of others are relatively small, and conversely?”
#   - “Which pairs of assets are positively correlated for small variations but uncorrelated otherwise?”
# - Results show that this dependence measure can help uncover dependency types of a given set of elements.


# ## Reference


# 1. Marti, Gautier & Andler, Sébastien & Nielsen, Frank & Donnat, Philippe. (2016). Exploring and measuring non-linear correlations: Copulas, Lightspeed Transportation and Clustering.      Available at: https://arxiv.org/pdf/1610.09659.pdf
#
# 2. Gautier Marti. (2020) Blog post: Measuring non-linear dependence with Optimal Transport. Available at: https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html



// ---------------------------------------------------

// codependence_by_marti.py
// arbitrage_research/Codependence Module/Codependence by Marti/codependence_by_marti.py
# Generated from: codependence_by_marti.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Some contributions to the clustering of financial time series and applications to credit default swaps__ _by_ Gautier Marti


# ## Abstract


# GPR and GNPR distances are a part of a novel technique for measuring the distance between two random variables that allows to separate the measurement of distribution information and the dependence information. A mix of both types of information can be used in a chosen proportion.


# ## GPR and GNPR distances


# This description is based on the paper by _Gautier Marti_ __“Some contributions to the clustering of financial time series and applications to credit default swaps”__  [available here](https://www.researchgate.net/publication/322714557).


# ### Classification of distances


# According to _Gautier Marti_:
#
# "Many statistical distances exist to measure the dissimilarity of two random variables, and therefore two i.i.d. random processes. Such distances can be roughly classified in two
# families:
#
# 1. __distributional distances__, ... which focus on dissimilarity between probability distributions and quantify divergences in marginal behaviours,
#
# 2. __dependence distances__, such as the distance correlation or copula-based kernel dependency measures ..., which focus on the joint behaviours of random variables, generally ignoring their distribution properties.
#
# However, we may want to be able to discriminate random variables both on distribution and dependence. This can be motivated, for instance, from the study of financial assets returns: are two perfectly correlated random variables (assets returns), but one being normally distributed and the other one following a heavy-tailed distribution, similar?
#
# From risk perspective, the answer is no ..., hence the propounded distance of this article".


# ### GPR distance


# From __“Some contributions to the clustering of financial time series and applications to credit default swaps”__ :
#
# __Definition:__ (Distance $d_{\Theta}$ between two random variables). Let $\theta \in [0, 1]$. Let $(X, Y) \in \nu^{2}$ , where $\nu$ is the space of all continuous
# real-valued random variables. Let $G = (G_{X}, G_{Y})$, where $G_{X}$ and $G_{Y}$ are respectively $X$ and $Y$ marginal cdfs. We define the following distance
#
# $$d_{\Theta}^{2}(X, Y) = \Theta d_{1}^{2}(G_{X}(X), G_{Y}(Y)) + (1 - \Theta) d_{0}^{2}(G_{X}, G_{Y})$$
#
# where
#
# $$d_{1}^{2}(G_{X}(X), G_{Y}(Y)) = 3 \mathbb{E}[|G_{X}(X) - G_{Y}(Y)|^{2}]$$
#
# and
#
# $$d_{0}^{2}(G_{X}, G_{Y}) = \frac{1}{2} \int_{R} (\sqrt{\frac{d G_{X}}{d \lambda}} - \sqrt{\frac{d G_{Y}}{d \lambda}})^{2} d \lambda$$


# __Example:__ (Distance $d_{\Theta}$ between two Gaussians). Let $(X, Y)$ be a bivariate Gaussian vector, with $X \sim \mathcal{N}(\mu_{X}, \sigma_{X}^{2})$,
# $Y \sim \mathcal{N}(\mu_{Y}, \sigma_{Y}^{2})$ and $\rho (X,Y)$. We obtain,
#
# $$d_{\Theta}^{2}(X, Y) = \Theta \frac{1 - \rho_{S}}{2} + (1 - \Theta) (1 - \sqrt{\frac{2 \sigma_{X} \sigma_{Y}}{\sigma_{X}^{2} + \sigma_{Y}^{2}}} e^{ - \frac{1}{4} \frac{(\mu_{X} - \mu_{Y})^{2}}{\sigma_{X}^{2} + \sigma_{Y}^{2}}})$$
#
# The use of this distance is referenced as the generic parametric representation (GPR) approach.
#
# From the paper:
#
# "GPR distance is a fast and good proxy for distance $d_{\Theta}$ when the first two moments $\mu$ and ${\sigma}$ predominate. Nonetheless, for datasets which contain heavy-tailed distributions, GPR fails to capture this information".


# __Property:__ Let $\Theta \in [0,1]$. The distance $d_{\Theta}$verifies $0 \le d_{\Theta} \le 1$.
#
# __Property:__ For $0 < \Theta < 1$, $d_{\Theta}$ is a metric.
#
# __Property:__ [Diffeomorphism](https://en.wikipedia.org/wiki/Diffeomorphism) invariance. Let $h: \nu \rightarrow \nu$ be a diffeomorphism. Let $(X, Y) \in \nu^{2}$. Distance $d_{\Theta}$ is invariant under diffeomorphism, i.e.
#
# $$d_{\Theta}(h(X), h(Y)) = d_{\Theta}(X, Y)$$


# ### GNPR distance


# According to _Marti_:
#
# "To apply the propounded distance $d_{\Theta}$ on sampled data without parametric assumptions, we have to define its statistical estimate $\tilde{d}_{\Theta}$ working on realizations of the i.i.d. random variables.
#
# Distance $d_{1}$ working with continuous uniform distributions can be approximated by normalized rank statistics yielding to discrete uniform distributions.
#
# Distance $d_{0}$ working with densities can be approximated by using its discrete form working on histogram density estimates".


# __Definition:__ (Empirical distance) Let $(X^{t})_{t=1}^{T}$ and $(Y^{t})_{t=1}^{T}$ be $T$ realizations of real-valued random variables $X, Y \in \nu$ respectively. An empirical distance between realizations of random variables can be defined by
#
# $$\tilde{d}_{\Theta}^{2}((X^{t})_{t=1}^{T}, (Y^{t})_{t=1}^{T}) \stackrel{\text{a.s.}}{=} \Theta \tilde{d}_{1}^{2} + (1 - \Theta) \tilde{d}_{0}^{2}$$
#
# where
#
# $$\tilde{d}_{1}^{2} = \frac{3}{T(T^{2} - 1)} \sum_{t = 1}^{T} (X^{(t)} - Y^{(t)}) ^ {2}$$
#
# and
#
# $$\tilde{d}_{0}^{2} = \frac{1}{2} \sum_{k = - \infty}^{+ \infty} (\sqrt{g_{X}^{h}(hk)} - \sqrt{g_{Y}^{h}(hk)})^{2}$$
#
# $h$ being here a suitable bandwidth, and $g_{X}^{h}(x) = \frac{1}{T} \sum_{t = 1}^{T} \mathbf{1}(\lfloor \frac{x}{h} \rfloor h \le X^{t} < (\lfloor \frac{x}{h} \rfloor + 1)h)$ being a density histogram estimating dpf $g_{X}$ from
# $(X^{t})_{t=1}^{T}$ , $T$ realization of a random variable $X \in \nu$.


# The use of this distance is referenced as the generic non-parametric representation (GNPR) approach.
#
# From the paper:
#
# "To use effectively $d_{\Theta}$ and its statistical estimate, it boils down to select a particular value for $\Theta$. We suggest here an exploratory approach where one can test 
#
# - (i) distribution information ($\Theta = 0$),
# - (ii) dependence information ($\Theta = 1$), and
# - (iii) a mix of both information ($\Theta = 0.5$).
#
# Ideally, $\Theta$ should reflect the balance of dependence and distribution information in the data.
#
# In a supervised setting, one could select an estimate $\hat{\Theta}$ of the right balance $\Theta^{*}$ optimizing some loss function by techniques such as cross-validation. Yet, the lack of a clear loss function makes the estimation of $\Theta^{*}$ difficult in an unsupervised setting".


# **Note:** The implementation of GNPR in the ArbitrageLab package was adjusted so that $\tilde{d}_{0}^{2}$
# (dependence information distance) is being calculated using the 1D Optimal Transport Distance following the example in the
# [POT package documentation](https://pythonot.github.io/auto_examples/plot_OT_1D.html#sphx-glr-auto-examples-plot-ot-1d-py).
# This solution was proposed by Marti.
#
# Distributions of random variables are approximated using histograms with a given number of bins as input.
#
# Optimal Transport Distance is then obtained from the Optimal Transportation Matrix (OTM) using
# the Loss Matrix (M) as shown in 
# [Optimal Transport blog post by Marti](https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html):
#
# $$\tilde{d}_{0}^{2} = tr (OT^{T} * M)$$
#
# where $tr( \cdot )$ is trace of a matrix and $\cdot^{T}$ is a transposed matrix.
#
# This approach solves the issue of defining support for underlying distributions and choosing a number of bins.


# ---


# ## Usage of the algorithms


# This part shows how the GPR and the GNPR distances can be used to measure codependence between a set of stocks


import arbitragelab as al
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt


# Loading the dataset of stocks
stocks = pd.read_csv('../../Sample-Data/stock_prices.csv')
stocks.set_index('Date', inplace=True)
stocks.index = pd.to_datetime(stocks.index)

# Taking first 10 stocks for better output visualization
stocks = stocks.iloc[:,0:10]
stocks.head()


# Calculating returns of a given dataset of stocks
stocks_returns = stocks.pct_change()[1:]
stocks_returns.iloc[:,0:10].head()


# Calculating GPR distance between two tickers - EEM and EWG
gpr_dist = al.codependence.gpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=0.5)
print('GPR distance between EEM and EWG measuring a mix of distribution and dependence information is: ', gpr_dist)


# Calculating GPR distance between two tickers - EEM and EWG using only dependence information
gpr_dist = al.codependence.gpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=1)
print('GPR distance between EEM and EWG measuring dependence information is: ', gpr_dist)


# Calculating GNPR distance between two tickers - EEM and EWG
gpr_dist = al.codependence.gnpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=0.5)
print('GNPR distance between EEM and EWG measuring a mix of distribution and dependence information is: ', gpr_dist)


# Calculating GNPR distance between two tickers - EEM and EWG using only distribution information and 100 bins
gpr_dist = al.codependence.gnpr_distance(stocks_returns['EEM'], stocks_returns['EWG'], theta=0, n_bins=100)
print('GNPR distance between EEM and EWG measuring distribution information is: ', gpr_dist)


# These codependence measures can also be used on the whole dataframes, the results will be codependence matrices.


# Calculating GPR distance between all stocks with Θ = 0.5
gpr_matrix = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gpr_distance',theta=0.5)

print('GPR distance matrix measuring a mix of distribution and dependence information:')
gpr_matrix


# Calculating GNPR distance between all stocks with Θ = 1
gnpr_matrix_dep = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gnpr_distance',theta=1)

# Calculating GNPR distance between all stocks with Θ = 0.5
gnpr_matrix_mix = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gnpr_distance',theta=0.5)

# Calculating GNPR distance between all stocks with Θ = 0
gnpr_matrix_dist = al.codependence.get_dependence_matrix(stocks_returns, dependence_method='gnpr_distance',theta=0)

print('GNPR distance matrix measuring distribution information:')
gnpr_matrix_dist


# ### Heatmap of GNPR distribution information distance


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10, 8))
sns.heatmap(gnpr_matrix_dist, ax = ax, cbar_kws={'label': 'GNPR distribution distance'})

ax.set_title('Heatmap of GNPR distribution information distance (Θ = 0)')
plt.show()


# ### Heatmap of GNPR mix of distribution and dependence information distance


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(gnpr_matrix_mix, ax = ax, cbar_kws={'label': 'GNPR mixed distance'})

ax.set_title('Heatmap of GNPR mix of distribution and dependence information distance (Θ = 0.5)')
plt.show()


# ### Heatmap of GNPR dependence information distance


# Plotting the heatmap
fig, ax = plt.subplots(figsize=(10,8))
sns.heatmap(gnpr_matrix_dep, ax = ax, cbar_kws={'label': 'GNPR dependence distance'})

ax.set_title('Heatmap of GNPR dependence information distance (Θ = 1)')
plt.show()


# As seen from the heat maps of GNPR distance matrices, distribution and information distances show different types of codependency. However, when using a mixed approach with $\Theta = 0.5$ the distribution information part is too high, which makes the dependence information adjustment hardly visible, so for a balanced output one may want to increase $\Theta$ in this particular example. 


# ---


# ## Conclusion


# This notebook describes the GPR and the GNPR distances how they may be used in real-life applications.  
#
# These novel distances were originally presented by the _Gautier Marti_ in the work __“Some contributions to the clustering of financial time series and applications to credit default swaps”__  [available here](https://www.researchgate.net/publication/322714557).
#
# Key takeaways from the notebook:
# - Distances can be roughly classified in two families:
#   - Distributional distances, which focus on dissimilarity between probability distributions and quantify divergences in marginal behaviours.
#   - Dependence distances, which focus on the joint behaviours of random variables, generally ignoring their distribution properties.
# - Distance $d_{\Theta}$ between two random variables allows to discriminate random variables both on distribution and dependence.
# - Distance $d_{\Theta}$ is a metric that falls in range $[0, 1]$.
# - GPR distance is a fast and good proxy for distance $d_{\Theta}$ between two Gaussians.
# - GNPR distance is a proxy for distance $d_{\Theta}$ that works on i.i.d. random variables, it requires a declared width of bins for values discretization.
# - $d_{\Theta}$ should be chosen to reflect the balance of dependence and distribution information in the data.
# - The ArbitrageLab implementation has $\tilde{d}_{0}^{2}$ in GNPR is adjusted to using 1D Optimal Transport Distance to solve the issue of defining support for underlying distributions and choosing a number of bins.


# ## References


# 1. Marti, Gautier. (2017). Some contributions to the clustering of financial time series and applications to credit default swaps. Available at: https://www.researchgate.net/publication/322714557
#
# 2. Marti, Gautier. (2020). Measuring non-linear dependence with Optimal Transport. Available at: https://gmarti.gitlab.io/qfin/2020/06/25/copula-optimal-transport-dependence.html



// ---------------------------------------------------

// futures_rollover.py
// arbitrage_research/ML Approach/futures_rollover.py
# Generated from: futures_rollover.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Futures Rolling
#
# ## Introduction to Futures
# Futures are a form of a contract drawn up between two parties to purchase or sell a quantity of an underlying asset at a specified date in the future. This date is known as the delivery or expiration. When this date is reached, the buyer must deliver the physical underlying (or cash equivalent) to the seller for the price agreed at the contract formation date.
#
# In practice, futures are traded on exchanges for standardised quantities and qualities of the underlying. The prices are marked to market every day. Futures are incredibly liquid and are used heavily for speculative purposes. While futures were often utilised to hedge the prices of agricultural or industrial goods, a futures contract can be formed on any tangible or intangible underlying such as stock indices, interest rates of foreign exchange values.
#
# The main difference between a futures contract and equity ownership is the fact that a futures contract has a limited window of availability by virtue of the expiration date. At any one instant, there will be a variety of futures contracts on the same underlying all with varying dates of expiry. The contract with the nearest date of expiry is known as the near contract.
#
# ## Outline
#
# - Contract Rollers
#     - [Crude Oil - WTI](#wti) 
#     - [UK Gas - NBP](#ukgas)
#     - [Gasoline - RBOB](#rbob)
#     - [Soybean - S](#soyb)
#     - [Soy Oil - B0](#soyo)
#     - [Corn - C](#corn)
#     - [Ethanol - EH](#eth)


import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

from arbitragelab.util.rollers import *

import warnings
warnings.filterwarnings('ignore')


# # Crude Oil WTI  <a class="anchor" id="wti"></a>
#
# NYMEX WTI Crude Oil futures (CL) is the world’s most liquid crude oil contract. When traders need the current oil price, they check the WTI Crude Oil price. WTI (West Texas Intermediate, a US light sweet crude oil blend) futures provide direct crude oil exposure and are the most efficient way to trade oil after a sharp rise in US crude oil production. They can also be used to hedge against adverse oil price moves or speculate on whether WTI oil prices will rise or fall.
#
# https://www.cmegroup.com/trading/energy/crude-oil/light-sweet-crude_contract_specifications.html
#
# ### Termination of Trading
#
# Trading terminates 3 business day prior to the 25th calendar day of the month prior to the contract month. If the 25th calendar day is not a business day, trading terminates 4 business days prior to the 25th calendar day of the month prior to the contract month.


# Load contract price data.
cl_df = pd.read_csv('./data/futures_price_data/CL1.csv')
cl_df['Dates'] = pd.to_datetime(cl_df['Dates'])
cl_df.dropna(inplace=True)
cl_df.index = cl_df['Dates']
cl_df = cl_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
wti_roller = CrudeOilFutureRoller().fit(cl_df)
wti_gaps = wti_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(cl_df['PX_LAST'] - wti_gaps).plot(figsize=(15,10))
cl_df['PX_LAST'].plot()
wti_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("WTI future rolling plot");


# Sometimes rolled contracts dip into negative territory. This
# can cause problems when used for ml models, thus there is the
# ability of using the parameter 'handle_negative_roll', which
# will process the price data into positive returns data.
non_negative_cl = wti_roller.transform(handle_negative_roll=True) 
non_negative_cl.to_csv('./data/nonneg_forward_rolled_futures/NonNegative_CL_forward_roll.csv')
non_negative_cl.plot(figsize=(15,10))
plt.title("WTI Non Negative Forward Roll");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
wti_diag_frame = wti_roller.diagnostic_summary()
wti_diag_frame.head(10)


# # NBP UK Natural Gas Futures  <a class="anchor" id="nbp"></a>
#
# Natural gas is the third most important source of energy after oil and coal. The use of natural gas is growing quickly and is expected to overtake coal in the second spot by 2030.
#
# The world’s largest producers of natural gas are currently the United States, Russia, Iran, Qatar, Canada, China and Norway. These countries have excess natural gas that can be exported to other countries around the world, which is either transported through pipelines or as liquefied natural gas (LNG).
#
# In western Europe, gas is the dominant fuel for electricity production. Prices are set at several trading hubs around the region. The two most important hubs in the region are the National Balancing Point or NBP in the UK and the Title Transfer Facility or TTF in the Netherlands.
#
# https://www.theice.com/products/910/UK-Natural-Gas-Futures
#
# ## Termination of Trading
# Trading will cease at the close of business two Business Days prior to the first calendar day of the delivery month, quarter, season, or calendar.


# Load contract price data.
nbp_df = pd.read_csv('./data/futures_price_data/NBP1.csv')
nbp_df['Dates'] = pd.to_datetime(nbp_df['Dates'])
nbp_df.set_index('Dates', inplace=True)


# Fit corresponding roller and retrieve gaps.
nbp_roller = NBPFutureRoller().fit(nbp_df)
nbp_gaps = nbp_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(nbp_df['PX_LAST'] - nbp_gaps).plot(figsize=(15,10))
nbp_df['PX_LAST'].plot()
nbp_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("NBP future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
nbp_diag_frame = nbp_roller.diagnostic_summary()
nbp_diag_frame.head(10)


# # RBOB  <a class="anchor" id="rbob"></a>
#
# RBOB products offer a way for investors to express views on crude oil, weather, consumer behavior and regulatory action in terms of current and future energy consumption. As the primary fuel for most automobiles on the road, gasoline is an integral commodity to the lives of most consumers. 
#
# https://www.cmegroup.com/trading/energy/refined-products/rbob-gasoline_contract_specifications.html
#
# ### Termination of Trading
# Trading terminates on the last business day of the month prior to the contract month.


# Load contract price data.
rb_df = pd.read_csv('./data/futures_price_data/RB1.csv').dropna()
rb_df['Dates'] = pd.to_datetime(rb_df['Dates'])
rb_df.set_index('Dates', inplace=True)
rb_df = rb_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
rbob_roller = RBFutureRoller().fit(rb_df)
rbob_gaps = rbob_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(rb_df['PX_LAST'] - rbob_gaps).plot(figsize=(15,10))
rb_df['PX_LAST'].plot()
rbob_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("RBOB future rolling plot");


# In this case the rolled contract dips into negative territory.
# Thus the 'handle_negative_roll' parameter is used to post process
# the rolled future data.
non_negative_rbob = rbob_roller.transform(handle_negative_roll=True)
non_negative_rbob.to_csv('./data/nonneg_forward_rolled_futures/NonNegative_RB_forward_roll.csv')
non_negative_rbob.plot(figsize=(15,10))
plt.title("RBOB Non Negative Forward Roll");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
rb_diag_frame = rbob_roller.diagnostic_summary()
rb_diag_frame.head(10)


# # Soybeans S1  <a class="anchor" id="soyb"></a>
#
#
#
# ### Termination of Trading
# Trading terminates on the business day prior to the 15th day of the contract month.


# Load contract price data.
soybean_df = pd.read_csv('./data/futures_price_data/S1.csv', index_col='Date', parse_dates=True).dropna()
soybean_df = soybean_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
soy_roller = GrainFutureRoller().fit(soybean_df)
soy_gaps = soy_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(soybean_df['PX_LAST'] - soy_gaps).plot(figsize=(15,10))
soybean_df['PX_LAST'].plot()
soy_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("S future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
soy_diag_frame = soy_roller.diagnostic_summary()
soy_diag_frame.head(10)


# Example on how to analyze and verify gaps using the backwardation/contango plot.

soybean_m1_df = pd.read_csv('./data/futures_price_data/S1.csv', index_col='Date', parse_dates=True).dropna()
soybean_m1_df = soybean_m1_df['2006-01': '2019-12']

soybean_m2_df = pd.read_csv('./data/futures_price_data/S2.csv', index_col='Date', parse_dates=True).dropna()
soybean_m2_df = soybean_m2_df['2006-01': '2019-12']

soy_gaps.plot(figsize=(15,5))
plt.title("Soybean Future Gaps");
plt.show()

plot_historical_future_slope_state(soybean_m1_df['PX_LAST'], soybean_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Soyoil B01  <a class="anchor" id="soyo"></a>
#
# RBOB products offer a way for investors to express views on crude oil, weather, consumer behavior and regulatory action in terms of current and future energy consumption. As the primary fuel for most automobiles on the road, gasoline is an integral commodity to the lives of most consumers. 
#
# https://www.cmegroup.com/trading/energy/refined-products/rbob-gasoline_contract_specifications.html
#
# ### Termination of Trading
# Trading terminates on the business day prior to the 15th day of the contract month.


# Load contract price data.
soyoil_df = pd.read_csv('./data/futures_price_data/B01.csv', index_col='Date', parse_dates=True).dropna()
soyoil_df = soyoil_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
soyo_roller = GrainFutureRoller().fit(soyoil_df*11)
soyo_gaps = soyo_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(soyoil_df['PX_LAST'] - soyo_gaps).plot(figsize=(15,10))
soyoil_df['PX_LAST'].plot()
soyo_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("B0 future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
soyo_diag_frame = soyo_roller.diagnostic_summary()
soyo_diag_frame.head(10)


# Example on how to analyze and verify gaps using the backwardation/contango plot.

soyo_m1_df = pd.read_csv('./data/futures_price_data/B01.csv', index_col='Date', parse_dates=True).dropna()
soyo_m1_df = soyo_m1_df['2006-01': '2019-12']

soyo_m2_df = pd.read_csv('./data/futures_price_data/B02.csv', index_col='Date', parse_dates=True).dropna()
soyo_m2_df = soyo_m2_df['2006-01': '2019-12']

soyo_gaps.plot(figsize=(15,5))
plt.title("Soyoil Future Gaps");
plt.show()

plot_historical_future_slope_state(soyo_m1_df['PX_LAST'], soyo_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Corn C1  <a class="anchor" id="corn"></a>
#
#
# ### Termination of Trading
# Trading terminates on the business day prior to the 15th day of the contract month.


# Load contract price data.
corn_df = pd.read_csv('./data/futures_price_data/C1.csv', index_col='Date', parse_dates=True).dropna()
corn_df = corn_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
corn_roller = GrainFutureRoller().fit(corn_df)
corn_gaps = corn_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(corn_df['PX_LAST'] - corn_gaps).plot(figsize=(15,10))
corn_df['PX_LAST'].plot()
corn_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("Corn future rolling plot");


# The diagnostic summary is a helper function to help the user
# easily double check expiration dates and their respective gap
# calculations.
corn_diag_frame = corn_roller.diagnostic_summary()
corn_diag_frame.head(10)


# Example on how to analyze and verify gaps using the backwardation/contango plot.

corn_m1_df = pd.read_csv('./data/futures_price_data/C1.csv', index_col='Date', parse_dates=True).dropna()
corn_m1_df = corn_m1_df['2006-01': '2019-12']

corn_m2_df = pd.read_csv('./data/futures_price_data/C2.csv', index_col='Date', parse_dates=True).dropna()
corn_m2_df = corn_m2_df['2006-01': '2019-12']

corn_gaps.plot(figsize=(15,5))
plt.title("Corn Future Gaps");
plt.show()

plot_historical_future_slope_state(corn_m1_df['PX_LAST'], corn_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Ethanol EH1 <a class="anchor" id="corn"></a>
#
#
# ### Termination of Trading
# Trading terminates on 3rd business day of the contract month.


# Load contract price data.
ethanol_df = pd.read_csv('./data/futures_price_data/EH1.csv', index_col='Date', parse_dates=True).dropna()
ethanol_df = ethanol_df['2006-01': '2019-12']


# Fit corresponding roller and retrieve gaps.
ethanol_roller = EthanolFutureRoller().fit(ethanol_df)
ethanol_gaps = ethanol_roller.transform()


# Plot the Normal, Rolled and Gaps Series.
(ethanol_df['PX_LAST'] - ethanol_gaps).plot(figsize=(15,10))
ethanol_df['PX_LAST'].plot()
ethanol_gaps.plot()

plt.legend(["Rolled", "Normal", "Gaps"])
plt.title("Ethanol future rolling plot");


# Example on how to analyze and verify gaps using the backwardation/contango plot.

ethanol_m1_df = pd.read_csv('./data/futures_price_data/EH1.csv', index_col='Date', parse_dates=True).dropna()
ethanol_m1_df = ethanol_m1_df['2006-01': '2019-12']

ethanol_m2_df = pd.read_csv('./data/futures_price_data/EH2.csv', index_col='Date', parse_dates=True).dropna()
ethanol_m2_df = ethanol_m2_df['2006-01': '2019-12']

ethanol_gaps.plot(figsize=(15,5))
plt.title("Corn future rolling plot");
plt.show()

plot_historical_future_slope_state(ethanol_m1_df['PX_LAST'], ethanol_m2_df['PX_OPEN'])
plt.title("Contango/Backwardation plot");


# # Conclusion
#
# This notebook describes the methods used to roll futures for different assets. This is done to get a continuous price series for a given set of futures contracts.
#
# Types of contracts covered in this notebooks are:
# * Crude Oil - WTI
# * UK Gas - NBP
# * Gasoline - RBOB
# * Soybean - S
# * Soy Oil - B0
# * Corn - C
# * Ethanol - EH



// ---------------------------------------------------

// fair_value_modeling.py
// arbitrage_research/ML Approach/fair_value_modeling.py
# Generated from: fair_value_modeling.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Fair Value Modeling
#
# ## Abstract
# In [(Dunis et al. 2005)](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.568.7460&rep=rep1&type=pdf) the case is made that the crack spread exhibits asymmetry at the \\$5 dollar mark, with seemingly larger moves occurring on the upside of the long-term 'fair value' than on the downside.
#
#
# The gasoline crack spread can be interpreted as the profit margin gained by processing crude oil into unleaded gasoline. It is simply the monetary difference between West Texas Intermediate crude oil and Unleaded Gasoline, both of which are traded on the New York Mercantile Exchange (NYMEX).
#
# $$ S_{t} = GAS_t - WTI_t $$
#
# $S_{t}$ is the price of the spread at time $t$ (in \\$ per barrel), $GAS_t$ is the price of unleaded gasoline at time $t$ (in \\$ per barrel), and $WTI_t$ is the price of West Texas Intermediate crude oil at time $t$ (in \\$ per
# barrel).
#
# ## Non-linear cointegration
#
# Cointegration was first introduced by [(Engle and Granger. 1987)](https://doi.org/10.2307/1913236). The technique is to test the null hypothesis that any combination of two series contains a unit root. If the null hypothesis is refuted and the conclusion is that a unit root does not exist, the combination of the two series is cointegrated. 
#
# As explained in the previous section, the crack spread may exhibit larger moves in one direction than in the other, this is known as asymmetry. Since the traditional unit root test has only one parameter for the autoregressive estimate, it assumes upside and downside moves to be identical or symmetric.
#
# Non-linear cointegration was first introduced by [(Enders and Granger. 1998)](https://doi.org/10.2307/1392506), who extended the unit root test by considering upside and downside moves separately, thus allowing for the possibility of asymmetric adjustment. 
#
# Enders and Granger extend the Dickey-Fuller test to allow for the unit root hypothesis to be tested against an
# alternative of asymmetric adjustment. Here, this is developed from its simplest form; consider the standard Dickey–Fuller test
#
# $$ \Delta \mu_{t} = p \mu_{t-1} + \epsilon_t $$ 
#
# where $\epsilon_t$ is a white noise process. The null hypothesis of $p=0$ is tested against the alternative of $p \neq 0 $. $p=0$ indicates that there is no unit root, and therefore $\mu_i$ is a stationary series. If the series $\mu_i$ are the residuals of a long-run cointegration relationship as indicated by Johansen, this
# simply results in a test of the validity of the cointegrating vector (the residuals of the cointegration equation should form a stationary series).
#
# The extension provided by Enders and Granger is to consider the upside and downside moves separately, thus allowing for the possibility of asymmetric adjustment. Following this approach;
#
# $$ \Delta \mu_{t} = I_t p_1 \mu_{i-1} + (1 - I_t) p_2 \mu_{i-1} + \epsilon_t  $$
#
# where $I_t$ is the zero-one ‘heaviside’ indicator function. The paper uses the following specification;
#
# $$ I_t = \left \{ {{1, if \mu_{t-1} \geq 0} \over {0, if \mu_{t-1} < 0}} \right. $$
#
# Enders and Granger refer to the model defined above as __threshold autoregressive (TAR)__. The null hypothesis of symmetric adjustment is $(H_0: p_1 = p_2)$, which can be tested using the standard F-test (in this case
# the Wald test), with an additional requirement that both $p_1$ and $p_2$ do not equal zero. If $p_1 \neq p_2$, cointegration between the underlying assets is non-linear.


from IPython.display import Image

import statsmodels.api as sm
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.johansen import JohansenPortfolio
from arbitragelab.ml_approach.tar import TAR


# Load Non Negative Crude and Gasoline futures data.
wti_contract_df = pd.read_csv('./data/NonNegative_CL_forward_roll.csv').set_index('Dates')
rbob_contract_df = pd.read_csv('./data/NonNegative_nRB_forward_roll.csv').set_index('Dates')

working_df = pd.concat([wti_contract_df, rbob_contract_df], axis=1)
working_df.index = pd.to_datetime(working_df.index) 
working_df.columns = ['wti', 'gasoline']

working_df.dropna(inplace=True)
working_df


# Calculate naive spread between gasoline and wti.
sprd = (working_df['gasoline'] - working_df['wti'])

# Plot spread.
plt.figure(figsize=(15,10))
sprd.plot();


# The TAR model expects a Zero mean series.
demeaned_spread = (sprd - sprd.mean())

# Initialize and fit TAR model.
model = TAR(demeaned_spread, False)
tar_results = model.fit()
tar_results.summary()


# # Plotted Residuals of the TAR Model


plt.figure(figsize=(15,10))
plt.plot(tar_results.fittedvalues.values);


# # Results
#
# The results from the original paper (Dunis et al. 2005) which uses the crack spread series from (1995 - 2003), finds statistical significant evidence of the presence of non linearity.  


Image(filename='images/paper_results.png')


# In our current time frame we are using (2005 - 2019) we fail to reject the null in the hypothesis of $p_1 = p_2$.


model.summary()


# # Difference between the Linear and Non Linear Model Residuals


# Initialize the linear fair value model.
jp_df = JohansenPortfolio()
jp_df.fit(working_df[['gasoline', 'wti']])

jp_spread = jp_df.construct_mean_reverting_portfolio(working_df[['gasoline', 'wti']])

# Demean the linear fair value model spread.
demeaned_jp_spread = jp_spread-jp_spread.mean()

plt.figure(figsize=(15,10))

# Plot Johansen Cointegration residuals.
plt.plot(demeaned_jp_spread.values)

# The Tar Fitted Results are multiplied by 1000, to provide 
# parity in measurements between models.
plt.plot(tar_results.fittedvalues.values*1000)
plt.legend(["Linear Fair Value Model", "Non Linear Fair Value Model"])


pd.Series(data=tar_results.fittedvalues.values, index=working_df.index[1:]).to_csv('./data/tar_residual_gas_wti.csv')


# # Conclusion
#
# A major consideration to be taken into account when analyzing these results is that the future contracts data used in the paper went out of service a few months after the paper was released. The data that has been used in this notebook was the new contracts that started back then. 
#
# The results clearly show that at least in the period observed the relationship between WTI and RBOB doesn't fit the Threshold Auto Regressive Model. This is evidenced by the results of the Wald test. However we can deduce, since $p_1$ is smaller in absolute terms than $p_2$, movements below fair value tend, on average, to be larger than movements above fair value. 


# # References
#
# * Dunis et al. (2005) 'Modelling and trading the gasoline crack spread: A non-linear story', Available at <https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.568.7460&rep=rep1&type=pdf>
#
# * Engle, R. F. and Granger, C. W. J. (1987) 'Cointegration and Error Correction: Representation, Estimation and Testing', Available at https://doi.org/10.2307/1913236
#
# * Enders, W. and Granger, C. (1998) 'Unit-root Tests and Asymmetric Adjustment with an Example Using the Term Structure of Interest Rates', Available at https://doi.org/10.2307/1392506



// ---------------------------------------------------

// ml_based_pairs_selection.py
// arbitrage_research/ML Approach/ml_based_pairs_selection.py
# Generated from: ml_based_pairs_selection.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Enhancing a Pairs Trading strategy with the application of Machine Learning__ _by_ Simão Moraes Sarmento and Nuno Horta


from IPython.display import Image
import pandas as pd
import arbitragelab as al
import matplotlib.pyplot as plt

import warnings
warnings.filterwarnings('ignore')


# # Implementation of a Machine Learning based Pairs Selection Framework
#
# ## Abstract
#
# In this paper[1], Horta and Sarmento propose a two-stage solution to applying machine learning to the problem of pairs trading. The first stage involves the application of a clustering algorithm to infer any meaningful clusters and using these groups to generate pairs that will be run through a selection process that will supply a list of pairs that comply with the conditions set. 
#
# The second stage should start by training forecasting algorithms to predict the spreads of the selected pairs. Furthermore, decile-based and quintile-based thresholds should be collected to integrate the trading model. Having fitted the forecasting algorithms and obtained the two combinations for the thresholds, the model can be applied to the validation set. From the validation performance, the best threshold combination is selected. At this point, the model is finally ready to be applied on unseen data, from which the performance may be inferred.
#
# ## Introduction
#
# This notebook will focus on the first stage, which refers to the pairs selection methodology. It will involve the application of PCA to distill the returns universe into a lower dimensional form. Then the OPTICS algorithm will be applied, on the expectation that it infers meaningful clusters of assets from which to select the pairs. The motivation is to let the data explicitly manifest itself, rather than manually defining the groups each security should belong to. The proposed methodology encompasses the following steps:
#
# - Dimensionality reduction - find a compact representation for each security;
# - Unsupervised Learning - apply an appropriate clustering algorithm;
# - Select pairs - define a set of rules to select pairs for trading.


Image(filename='images/prposed_framework_diagram.png') 


# ---


# # Usage of Implementation
#
# To start using this module we first need to set up our asset universe, in this case, the dataset used is the daily price data of every asset in the S&P 500.


prices_df = pd.read_csv('./data/data.csv').set_index('Date').dropna()
prices_df.index = pd.to_datetime(prices_df.index)

prices_df = prices_df.last('10Y')

prices_df.sample(10)


# ## Step 1 - Dimensionality Reduction
#
#
#
# ### Using PCA to find a compact representation for each security
#
# - Extracts common underlying risk factors from securities’ returns;
# - Produces a compact representation for each security;
#
# Before applying PCA, the pricing data needs to be converted to returns and then normalized by subtracting the mean and dividing by the standard deviation, as follows:
#
# Returns 
#
# $$ R_{i, t} = \frac{P_{i,t} - P_{i,t-1}} {P_{i,t-1}} $$
#
#
# Data Normalization
#
# $$ Y_{i} =  \frac{R_{i} - \bar{R_{i}}} {\sigma_{i}} $$
#
# Decomposition
#
# By applying PCA, $A$ is decomposed into the resulting eigenvectors and eigenvalues. An $n$ number of eigenvectors is selected where $n$ represents the number of features to describe the transformed data. The matrix containing the eigenvalues is set as the feature vector. The final dataset is obtained by multiplying the original matrix A by the feature vector.


ps = al.ml_approach.OPTICSDBSCANPairsClustering(prices_df)

# Here the first parameter is the number of features to reduce to.
ps.dimensionality_reduction_by_components(5)

# The following will plot the feature vector from the previous method call.
ps.plot_pca_matrix();


# A quick visual inspection of the feature vector shows a good amount of densely packed groups/clusters. If the points are too sparse, this likely suggests that you don't have enough datapoints.


# ## Step 2 - Unsupervised Learning
# ### Applying OPTICS clustering algorithm
#
# - No need to specify the number of clusters in advance;
# - Robust to outliers;
# - Suitable for clusters with varying density
#
#
# The first method is to use the OPTICS clustering algorithm and letting the built-in automatic 
# procedure to select the most suitable $\epsilon$ for each cluster. 


%matplotlib inline

clustered_pairs = ps.cluster_using_optics(min_samples=3)
ps.plot_clustering_info(method='OPTICS', n_dimensions=3);


# ### Applying DBSCAN clustering algorithm
#
# The second method is to use the DBSCAN clustering algorithm. This is to be used when the user 
# has domain-specific knowledge that can enhance the results given the algorithm's parameter 
# sensitivity. A possible approach to finding $\epsilon$ described in [2] is to inspect the knee plot and fix a 
# suitable $\epsilon$ by observing the global curve turning point.


ps.plot_knee_plot();


# The following are example results of DBSCAN clustering using different $\epsilon$ values, showing the efficacy of the method at different _'k-distance'_ values from the knee plot. 


ps.cluster_using_dbscan(eps=0.1, min_samples=3, metric='euclidean')
ps.plot_clustering_info(method='DBSCAN @ eps: 0.1', figsize=(8,8));

print('-' * 100)

ps.cluster_using_dbscan(eps=0.022, min_samples=3, metric='euclidean')
ps.plot_clustering_info(method='DBSCAN @ eps: 0.022', figsize=(8,8));

print('-' * 100)

ps.cluster_using_dbscan(eps=0.015, min_samples=3, metric='euclidean')
ps.plot_clustering_info(method='DBSCAN @ eps: 0.015', figsize=(8,8));


# The first plot shows the results with the upper bound value of 0.1, which was not sensitive enough to detect any groups. The second plot was set with the optimal value from the knee plot of 0.022 which detected a good amount of structure. The final plot was set with lower bound value of 0.015, which only managed to detect very densely packed clusters. 


# ## Step 3 - Select Pairs
# ### Finding resulting pairs that pass the following set of rules
#
# Sarmento and Horta suggest four criteria to further filter the potential pairs to increase the probability of selecting pairs of securities whose prices will continue to mean revert in the future. 
# - Cointegration using the Engle-Granger Test. 
# - Hurst Exponent $H$: Keep the pairs with (spread) $H<0.5$ for mean-reversion. 
# - Halflife: Keep the pairs with (spread) halflife in between $1$ day and $1$ year.
# - Minimum number of crossing mean in a year: Keep the pairs with (spread) crossing its mean $12$ times a year.
#
#
# These four criteria indicate attractive characteristics for potential tradable pairs of securities. The Engle-Granger tests the pair for cointegration. A Hurst exponent below 0.5 indicates that the pair of prices regresses strongly to the mean. Pairs with extreme half-life values, below 1 or above 356, are excluded from the selected pairs. Extreme half-life values indicate a price series that either reverts too quickly or too slowly to be traded. Finally, the price series must cross the long-term spread mean on average at least 12 times a year.


# <div class="alert alert-warning">
#
# **Warning:** The following pairs selection function is computationally heavy, so execution is going to be long and might slow down your system.
#
# </div>


# Removing duplicates from clustered pairs
clustered_pairs = list(set(clustered_pairs))


# Load data into spread selector
sp = al.spread_selection.CointegrationSpreadSelector(prices_df=prices_df,
                                                     baskets_to_filter=clustered_pairs)
                                                    

# Filtered spreads that passed the criteria by specifying hedge ratio calculation method as well as filtering conditions.
filtered_spreads = sp.select_spreads()


# The module can also work with user-specified spreads to test if a spread passes cointegration selection criterion.
spread = sp.spreads_dict['AAL_FTI'].copy() # Let's take an arbitrary spread.
pairs_selector_custom = al.spread_selection.CointegrationSpreadSelector(prices_df=None, baskets_to_filter=None)
stats = pairs_selector_custom.generate_spread_statistics(spread, log_info=True) # log_info=True to save stats.
print(stats)


filtered_spreads_custom = pairs_selector_custom.apply_filtering_rules(adf_cutoff_threshold=0.9, hurst_exp_threshold=0.5)
print(filtered_spreads_custom)


# Checking the resulting spreads
filtered_spreads


len(filtered_spreads)


# Plot one of the spreads
sp.spreads_dict['AAL_FTI'].plot(figsize=(12,6));


# The following detailed spread statistics can be obtained
sp.selection_logs.loc[['AAL_FTI']].T


# ---


# # Conclusion
#
# This notebook describes the proposed Pairs Selection Framework also shows example usage of the implemented framework to efficiently reduce the search space and select quality trading pairs. 
#
# - Ten years of daily stock price data for 400 securities were reduced to 5 dimensions through PCA. 
# - 1643 spreads from the clusters met the four selection criteria. 
#
# Key takeaways:
# - The number of pairs left for a trader to handle is much less compared to the number of pairs generated through an open combinatorial search of the whole asset universe.
# - (Based on the previous research) Most of the final pairs selected follow expected economic sectoral clusters even though there was no implied industry/sectoral grouping anywhere in the framework.
#
# Solutions to common pitfalls:
# - Dimensionality reduction techniques need a certain amount of data to work reliably, so if instability is encountered at this junction, it is suggested to increase the amount of data.
# - The number of PCA components needs to balance the amount of variance represented with density in euclidean distance. The rule of thumb is a number less than 15 components.
# - When in doubt use OPTICS.
# - For the clustering methods, the _'minimum samples'_ argument needs to be large enough so that the generated clusters are homogeneous. The rule of thumb is a number larger than 3.


# # References
# 1. Sarmento, Simão. & Horta, Nuno. (2020). Enhancing a Pairs Trading strategy with the application of Machine Learning. Available at: http://premio-vidigal.inesc.pt/pdf/SimaoSarmentoMSc-resumo.pdf
#
# 2. Rahmah N, Sitanggang S (2016). Determination of Optimal Epsilon (Eps) Value on DBSCAN Algorithm to Clustering Data on Peatland Hotspots in Sumatra. Available at: https://doi.org/10.1088/1755-1315/31/1/012012



// ---------------------------------------------------

// crack_spread_modeling.py
// arbitrage_research/ML Approach/crack_spread_modeling.py
# Generated from: crack_spread_modeling.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Spread Modeling
#
# * Reference: __Modelling and trading the gasoline crack spread: A non-linear story__ *by* Christian L. Dunis, Jason Laws and Ben Evans
#
# ## Introduction
#
# This notebook follows the works of Dunis et al. in the exploration of various Machine Learning models, with application to fundamental commodity-based spreads. 
#
# A fair value model is developed based on the works of (Enders and Granger 1998). This is used as a benchmark for other non-linear models such as multi-layer perceptron  (MLP), recurrent neural networks (RNN) and higher-order neural networks (HONN). The models are used to forecast $\Delta S_t$, the daily change in the spread. 
#
# Finally, the unfiltered spread is benchmarked against a correlation filter, a time-varying leverage-based volatility filter and, the more traditional threshold filter and, if the cointegration exhibits asymmetry, an asymmetric threshold filter.
#
# ## Outline
#
# - Spreads 
#     - Crack - (Gasoline - WTI) (RB - CL)
#
# - Rollover Method
#     - By Same Expiration Date
#
# - Ensemble
#     - RegressorCommittee
#
# - Models
#     - Fair value model
#         - Johansen Portfolio
#         - [TAR Model](#tar) 
#     - [Multi Layer Perceptron](#mlp) 
#     - [Long Short Term Memory Network](#rnn)
#     - Higher-Order Neural Network
#         - Single Layer
#             - [Feature Expander](#flnn)
#         - Multi layer
#             - [Pi Sigma Neural Network](#pisigma)
#
# - Filters
#     - Unfiltered
#     - Threshold
#     - Correlation
#     - Volatility


from IPython.display import Image

import random
import numpy as np
import pandas as pd 
import tensorflow as tf
import matplotlib.pyplot as plt

from arbitragelab.cointegration_approach.johansen import JohansenPortfolio
from arbitragelab.ml_approach.regressor_committee import RegressorCommittee
from arbitragelab.util.spread_modeling_helper import SpreadModelingHelper
from arbitragelab.ml_approach.neural_networks import (MultiLayerPerceptron, RecurrentNeuralNetwork,
                                                      PiSigmaNeuralNetwork)

import warnings
warnings.filterwarnings('ignore')


# Seed value.
seed_value = 0

# Set the built-in pseudo-random generator at a fixed value.
random.seed(seed_value)

# Set the `numpy` pseudo-random generator at a fixed value.
np.random.seed(seed_value)

# Set the `tensorflow` pseudo-random generator at a fixed value.
tf.random.set_seed(seed_value)

session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)


# # Working with Future Contracts
#
# In this notebook, the spread being modelled is the difference between the Crude oil Future contract and the RBOB Gasoline Future contract. 
#
# The size of the Crude oil future is 1,000 barrels, and the size of the RBOB gasoline future is 42,000 gallons. 
#
# Futures prices need to be handled in a special way, because of their tendency to expire in either of two states; contango or backwardation. This leads to gaps in the price series, that makes it unusable for backtesting purposes. 
#
# The processing of these prices is done through the technique called futures rolling. It involves calculating the gap between the day before expiration and the corresponding day of the new contract, and cumulatively summing these gaps in the direction of the user's choice.
#
# Also, note the resulting series from the rolling procedure will be in absolute terms (price differences) instead of relative terms (percentage difference).
#
# ### __Below is an example of the accumulated differences between a rolled and unrolled crude oil contract__


# Load unrolled futures data.
cl_df = pd.read_csv('./data/futures_price_data/CL1.csv', index_col='Dates', parse_dates=True)
cl_df.dropna(inplace=True)

# Load rolled futures data.
cl_forward_df = pd.read_csv('./data/forward_rolled_futures/CL_rolled_forward.csv', index_col='Dates', parse_dates=True)
cl_forward_df.dropna(inplace=True)

# Plot year of differences for intuition.
cl_forward_df['PX_LAST']['1997-01': '1997-12'].diff().cumsum().plot(figsize=(15,10))
cl_df['PX_LAST']['1997-01': '1997-12'].diff().cumsum().plot(figsize=(15,10))

for month in range(1, 12):
    plt.axvline(pd.datetime(1997, month, 21), color='r')

plt.legend(["Rolled", "Non-Rolled", "Estimated Expiry Dates"]);
plt.title("WTI future rolling plot");


# Now in this notebook, the expiration dates of the contracts are different. This is amended using the methodology described in (Dunis et al. 2006) which entails using the same expiration date for both contracts.
#
# At the same time, the RBOB contract exhibited price negativity after the rolling procedure. In general, we want to use non-negative series, thus a postprocessing method described in (De Prado 2018) was used to convert both contracts to non negative series.
#
# ## Rolling Implementation Details: 
#
# Link to [Notebook](futures_rollover.ipynb)


# Load Non Negative Prices.
wti_contract_df = pd.read_csv('./data/nonneg_forward_rolled_futures/NonNegative_CL_forward_roll.csv', index_col='Dates', parse_dates=True)
rbob_contract_df = pd.read_csv('./data/nonneg_forward_rolled_futures/NonNegative_nRB_forward_roll.csv', index_col='Dates', parse_dates=True)

# Concatenate both price series.
working_df = pd.concat([wti_contract_df, rbob_contract_df], axis=1)
working_df.columns = ['wti', 'gasoline']

working_df.dropna(inplace=True)
working_df


# Using arbitragelab implementation to setup the spread based on the Johansen cointegration method.
johansen_portfolio = JohansenPortfolio()
johansen_portfolio.fit(working_df)
sprd = johansen_portfolio.construct_mean_reverting_portfolio(working_df).diff()


# # Threshold Auto Regressive Model  <a class="anchor" id="tar"></a>
#
# Link to [Notebook](fair_value_modeling.ipynb)


# # Multi Layer Perceptron  <a class="anchor" id="mlp"></a>
#
# The MLP network has three layers; they are the input layer (explanatory variables), the output layer (the model estimation of the time series) and the hidden layer. The number of nodes in the hidden layer defines the amount of complexity that the model can fit.


# Initializing the Helper Class with the linear fair value spread as input
# with unique sampling enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=False,
                              unique_sampling=True)

_, frame_size = helper.input_train.shape

# Here the parameters for a standard Regressor Neural Network are set.
mlp_params = {'frame_size': frame_size, 'hidden_size': 8, 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "sigmoid",
                    'output_layer_act_func': "linear"}

# The RegressorCommittee is initialized with the sample network parameters using the 
# MLP class and a committee of size 10.
committee = RegressorCommittee(mlp_params, regressor_class='MultiLayerPerceptron',
                               num_committee=10, epochs=1000, patience=20, verbose=False)

committee.fit(helper.input_train, helper.target_train, helper.input_test, helper.target_test)

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Functional Link Neural Network <a class="anchor" id="flnn"></a>
#
# Functional Link NN use joint activation functions; this technique reduces the need to establish the relationships between inputs when training. Furthermore, this reduces the number of free weights and means that network can be faster to train than even MLPs. Because the number of inputs can be very large for higher-order architectures,
# however, orders of 4 and over are rarely used. Another advantage of reducing free weights is that the problems of overfitting and local optima affecting the results can be largely avoided.


# Initializing the Helper Class with the linear fair value spread as input
# with both unique sampling and feature expansion enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=True,
                              unique_sampling=True)

_, frame_size = helper.input_train.shape

# Here the parameters for a standard Regressor Neural Network are set.
honn_params = {'frame_size': frame_size, 'hidden_size': 2, 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "relu",
                    'output_layer_act_func': "linear"}

# The RegressorCommittee is initialized with the sample network parameters using the 
# MLP class and a committee of size 10.
committee = RegressorCommittee(honn_params, regressor_class='MultiLayerPerceptron',
                               num_committee=10, epochs=1000, patience=20, verbose=False)

committee.fit(helper.input_train, helper.target_train, helper.input_test, helper.target_test)

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Pi Sigma Neural Network <a class="anchor" id="pisigma"></a>
# Pi Sigma network can be considered as a class of feedforward fully connected HONNs. First introduced by (Shin and Ghosh 1991), the Pi Sigma network utilizes product cells as the output units to indirectly incorporate the capabilities of higher-order networks while using a fewer number of weights and processing units. Their creation
# was motivated by the need to create a network combining the fast learning property of single-layer networks with the powerful mapping capability of HONNs while avoiding the combinatorial increase in the required number of weights. While the order of the more traditional HONN architectures is expressed by the complexity of the inputs, in the context of Pi Sigma, it is represented by the number of hidden nodes.


# Initializing the Helper Class with the linear fair value spread as input
# with unique sampling enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=False,
                              unique_sampling=True)

_, frame_size = helper.input_train.shape

# Here the parameters for the PiSigma Neural Network are set.
ps_params = {'frame_size': frame_size, 'hidden_size': 6, 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "linear",
                    'output_layer_act_func': "tanh"}

# Here the committee class is initialized with PiSigmaNeuralNetwork as the member class
# and a committee size of 10.
committee = RegressorCommittee(ps_params, regressor_class='PiSigmaNeuralNetwork',
                               num_committee=10, epochs=1000, patience=200, verbose=False)

committee.fit(helper.input_train, helper.target_train, helper.input_test, helper.target_test)

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Recurrent Neural Network <a class="anchor" id="rnn"></a>
#
# Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. The main driving factors for the development of LSTM's were to overcome two major technical problems in the classical SimpleRNN. The two technical problems are vanishing gradients and exploding gradients, both related to how the network is trained.
#
# An LSTM layer consists of a set of recurrently connected blocks, known as memory blocks. Each one contains one or more recurrently connected memory cells and three multiplicative units – the input, output and forget gates – that provide continuous analogues of write, read and reset operations for the cells.


# Initializing the Helper Class with the linear fair value spread as input
# with unique sampling enabled.
helper = SpreadModelingHelper(sprd, insample_date_range=('2006', '2016'),
                              oosample_date_range=('2017', None), feat_expansion=False,
                              unique_sampling=True)

# The training variables are reshaped into [Samples, Time Steps, Features]
X_rnn_train = helper.input_train.values.reshape((helper.input_train.shape[0], helper.input_train.shape[1], 1))
X_rnn_test = helper.input_test.values.reshape((helper.input_test.shape[0], helper.input_test.shape[1], 1))
X_rnn_oos = helper.input_oos.values.reshape((helper.input_oos.shape[0], helper.input_oos.shape[1], 1))

_, frame_size, no_features = X_rnn_train.shape

# Here the parameters for the Recurrent Neural Network are set.
rnn_params = {'input_shape': (frame_size, no_features), 'num_outputs': 1, 'loss_fn': "mean_squared_error", 
                    'optmizer': "adam", 'metrics': [], 'hidden_layer_activation_function': "relu",
                    'output_layer_act_func': "linear"}

committee = RegressorCommittee(rnn_params, regressor_class='RecurrentNeuralNetwork',
                               num_committee=10, epochs=500, patience=20, verbose=False)

committee.fit(X_rnn_train, helper.target_train, X_rnn_test, helper.target_test)

helper.input_train = X_rnn_train
helper.input_test = X_rnn_test
helper.input_oos = X_rnn_oos

# Plot predicted values vs actual values.
helper.plot_model_results(committee);


# Plot committee member individual loss metrics.
committee.plot_losses();


# Show performance metrics for the various dataset partitions.
helper.get_metrics(working_df)


# Plot cumulative returns of the out of sample dataset partition.
helper.get_filtering_results(helper.target_oos, helper.oos_pred, helper.test_pred, working_df);


# # Conclusion
#
# Considerations to be taken into account when analyzing these results are;
# - The future contracts data used in the paper went out of service a few months after the paper was released. The data that has been used in this notebook are the new contracts that started back then. 
# - The network training parameters suggested weren't followed. With a modest patience value EarlyStopping detected overfitting at one tenth of the number of epochs that were suggested in the papers.
# - As per the point above, there is quite a high probability that the models used in the paper were overfit.
# - Transaction costs weren't included.
#
# Results produced from each of the models were for the most part mixed with a few outliers. In comparison to earlier work carried out by (Dunis, Laws, and Middleton 2011), it can be concluded that;
#
# - Standard Filters over the model prediction consistently had positive annual returns.   
# - The Correlation Filters showed sporadic performance with periods of massive losses.   
# - The Volatility Filter substantially improved the annual returns when used on top of a Standard Filter.
#
# On the whole, the application of nonlinear methodologies and time-varying volatility leverage filters has proven to be profitable; however, their application will vary depending on market participants. 


# # References
#
# * Dunis, C.L., Laws, J. and Evans, B., 2006. Modelling and trading the gasoline crack spread: A non-linear story. Derivatives Use, Trading & Regulation, 12(1-2), pp.126-145.
# * De Prado, M.L., 2018. Advances in financial machine learning. John Wiley & Sons.
# * Shin, Y. and Ghosh, J. (1991) ‘The Pi-Sigma Network: An Efficient Higher-Order Neural Network for Pattern Classification and Function Approximation’, Proceedings IJCNN, Seattle, July, 13-18.
# * Enders, W. and Granger, C.W.J., 1998. Unit-root tests and asymmetric adjustment with an example using the term structure of interest rates. Journal of Business & Economic Statistics, 16(3), pp.304-311.



// ---------------------------------------------------

// OU_Model_Jurek.py
// arbitrage_research/Stochastic Control Approach/OU_Model_Jurek.py
# Generated from: OU_Model_Jurek.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Implementation of OU Model Jurek
#
# This notebook demonstrates the usage of the `ou_model_jurek` module.
#
# This module contains implementation of the following paper 
#
# - [Dynamic portfolio selection in arbitrage. (2007)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=882536) by Jurek, J.W. and Yang, H.
#
#
# ## Introduction
#
#
#
# In the paper corresponding to this module, the authors derive the optimal dynamic strategy for arbitrageurs with a finite horizon and
# non-myopic preferences facing a mean-reverting arbitrage opportunity (e.g. an equity pairs
# trade).
#
# The law of one price asserts that - in a frictionless market - securities with identical payoffs
# must trade at identical prices. If this were not the case, arbitrageurs could generate a riskless
# profit by supplying (demanding) the expensive (cheap) asset until the mispricing was eliminated.
# Of course, real world markets are not frictionless, and the prices of securities with identical payoffs
# may significantly diverge for extended periods of time. Arbitrageurs can earn potentially attractive
# profits by taking offsetting positions in these relatively mispriced securities, but a worsening of the
# mispricing can result in sizable capital losses.
#
# Unlike textbook arbitrages, which generate riskless profits and require no capital commitments,
# exploiting real-world mispricings requires the assumption of some combination of horizon and divergence risk.
# These two risks represent the uncertainty about whether the mispricing will converge before the positions
# must be closed (or profits reported) and the possibility of a worsening in the mispricing prior to its elimination.
#
# While a complete treatment of optimal arbitrage and price formation requires a general equilibrium framework,
# this paper takes on the more modest goal of examining the arbitrageur's strategy in
# partial equilibrium.
#
# ## Modelling
#
#
# To capture the presence of horizon and divergence risk, the authors model the dynamics of the mispricing
# using a mean-reverting stochastic process. Under this process, although the mispricing is guaranteed
# to be eliminated at some future date, the timing of convergence, as well as the maximum magnitude
# of the mispricing prior to convergence, are uncertain. With this assumption, the authors are able to derive
# the arbitrageur's optimal dynamic portfolio policy for a set of general, non-myopic preference
# specifications, including CRRA utility defined over wealth at a finite horizon and Epstein-Zin
# utility defined over intermediate cash flows (e.g. fees). 
#
# The central assumption of our model is that the arbitrage opportunity is described by an
# Ornstein-Uhlenbeck process (henceforth OU). The OU process captures the two key features of a
# real-world mispricing: the convergence times are uncertain and the mispricing can diverge arbitrarily far
# from its mean prior to convergence.
#
# The optimal trading strategies derived are self-financing and can be interpreted as the optimal
# trading rules for a fund which is not subject to withdrawals but also cannot raise additional assets
# (i.e. a closed-end fund). The dynamics of the optimal allocation to the arbitrage opportunity are
# driven by two factors: the necessity of maintaining wealth (equity) above zero and the proximity
# of the arbitrageur's terminal evaluation date, which affects his appetite for risk.
#
#
# ### Investor Preferences
#
# The authors considered two alternative preferences structures for the arbitrageur in our continuous-time
# model. In the first, the authors assumed that the agent has constant relative risk aversion and maximizes
# the discounted utility of terminal wealth. The arbitrageur's value function at time $t$ - denoted by
# $V_t$ - takes the form:
#
# $$
#     V_{t}=\sup E_{t}\left[e^{-\beta(T-t)} \frac{W_{T}^{1-\gamma}}{1-\gamma}\right]
# $$
#
#
# The second preference structure they considered is the recursive utility of Epstein and Zin (1989, 1991),
# which allows the elasticity of intertemporal substitution and the coefficient of relative risk aversion
# to vary independently. Under this preference specification, the value function of the arbitrageur is
# given by:
#
# $$
#     V_{t}=\sup E_{t}\left[\int_{t}^{T} f\left(C_{s}, J_{s}\right) d s\right]
# $$
#
#
# where $f\left(C_{s}, J_{s}\right)$ is the normalized aggregator for the continuous-time Epstein-Zin utility function:
#
# $$
#     f\left(C_{t}, J_{t}\right)=\beta(1-\gamma) \cdot J_{t} \cdot\left[\log C_{t}-\frac{1}{1-\gamma} \log \left((1-\gamma) J_{t}\right)\right]
# $$
#
#
# Here the authors considered the special case of a unit elasticity of intertemporal substitution ($\psi = 1$).
#
#
# Here $C_t$ denotes the instantaneous consumption (e.g. cash flow). $\beta$ is the rate of time preference, and
# $\gamma$ is the coefficient of relative risk aversion.
#
#
#
# ---


# ## How to use this submodule
#
# This submodule contains seven public methods, of which two methods are necessary to calculate the optimal weights.
# The first method `fit` is for estimating the parameters of the model
# using training data, and the second method `optimal_portfolio_weights` is for calculating the final optimal portfolio weights using evaluation data.


# ### Imports and Loading the dataset


# We use the $GLD$ and $GDX$ tickers daily close prices as our dataset. To get good results for the estimated parameters, using  10 years of training data is recommended.
#
#
# The training data comprises of data from years $2009$ to $2019$. The optimal weights are calculated on data from $2019$ to $2020$.


# Importing required modules
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import yfinance as yf

from arbitragelab.stochastic_control_approach.ou_model_jurek import OUModelJurek

# Importing GLD and GDX daily prices from yfinance
data1 =  yf.download("GLD GDX", start="2009-03-25", end="2019-04-25")
data2 =  yf.download("GLD GDX", start="2019-04-27", end="2020-04-27")

# Using the Adj Close prices for our dataset
data_train_dataframe = data1["Adj Close"][["GLD", "GDX"]]
data_test_dataframe = data2["Adj Close"][["GLD", "GDX"]]


# ### Model fitting
#
#
# We input the training data to the `fit` method which calculates the spread
# and the closed form estimators of the parameters of the model.
#
# We can also specify the time period between steps(delta_t) in the data
# and whether we want to run the ADF statistic test to evaluate the spread.


# #### Spread Construction
#
# To construct the spread for the portfolio, firstly the authors calculated the total return index for each asset $i$ in the spread.
#
# $$
#     P_{i, t}=\left(\frac{1}{P_{i, 1}}\right) \cdot\left(P_{i, 1} \cdot \prod_{j=1}^{t-1}\left(1+R_{i, j+1}\right)\right)
# $$
#
# The price spread is then constructed by taking a linear combination of the total return indices.
# These weights are estimated by using a co-integrating regression technique such as Engle Granger.


# Creating an object of the Jurek class
sc = OUModelJurek()

# Calling the fit method on the train dataset
sc.fit(data_train_dataframe, delta_t = 1/252)


# To view the estimated model parameters from training data, we can use the `describe` function in the class object.


display(pd.DataFrame(sc.describe(), columns=['Values']))


# ### Getting the Optimal Portfolio Weights


# In this step we input the evaluation data and specify the type of investor we are looking for.
# We also need to specify the utility function parameter $\gamma$ and the risk free rate of return $r$.
#
# If we choose the investor with intermediate consumption, we also need to specify the parameter $\beta$.


# #### Optimal Portfolio Strategy
#
# The portfolio consists of a riskless asset and the mean reverting spread. The authors denote
# the prices of the two assets by $B_t$ and $S_t$, respectively. Their dynamics are given by,
#
# $$
#     \begin{aligned}
#     d B_{t} &=r B_{t} d t \\
#     d S_{t} &=\kappa\left(\bar{S}-S_{t}\right) d t+\sigma d Z
#     \end{aligned}
# $$
#
#
# The evolution of wealth which determines the budget constraints is written as,
#
# $$
#     d W_{t}=N_{t} d S_{t}+M_{t} d B_{t}-C_{t} 1\left[C_{t}>0\right] d t
# $$
#
#
# where $N_t$ denotes the number of units of spread, $M_t$ denotes the number of riskless assets and,
# $1[C_{t}>0]$ is an indicator variable for whether intermediate consumption is taking place.
#
# For the terminal wealth problem, the optimal portfolio allocation is given by:
#
# $$
#     N(W, S, \tau)=\left\{\begin{array}{cc}
#     \left(\frac{\kappa(\bar{S}-S)-r S}{\sigma^{2}}\right) W & \gamma=1 \\
#     \left(\frac{\kappa(\bar{S}-S)-r S}{\gamma \sigma^{2}}+\frac{2 A(\tau) S+B(\tau)}{\gamma}\right) W & \gamma \neq 1
#     \end{array}\right.
# $$
#
#
# The functions $A(\tau)$ and $B(\tau)$ depend on the time remaining to the horizon and the parameters of the underlying model.
#
# For the intermediate consumption problem, the optimal portfolio allocation has the same form as the corresponding equation for terminal wealth problem.
#
# Obviously, the functional form of the coefficient functions $A(\tau)$ and $B(\tau)$ are different.
#
#
# $\color{#FF0000}{\textrm{ WARNING }}$ : For utility_type = 2 and low values of $\gamma (< 1)$, the model becomes unstable with respect to the value of $k$.
#
#
#
# ---
#
# The output of the `optimal_portfolio_weights` method is the portfolio weights of the spread scaled w.r.t wealth. i.e, $\frac{N(t)}{W(t)}$
#
# ---


# In this example, we are considering the CRRA investor(utility_type = 1), with gamma = 2

# Plotting the portfolio weights of the spread asset
plt.figure(figsize=(10, 6))
scaled_weights = sc.optimal_portfolio_weights(data_test_dataframe, gamma=2, utility_type=1)
plt.plot(data_test_dataframe.index, scaled_weights, 'c-')
plt.title("Optimal allocation to the spread asset")
plt.ylabel("Weights")
plt.xlabel("Date")
plt.show()


# To calculate the wealth process we make use of the following equations.
#
# For CRRA investor with `utility_type = 1`,
# $$
#     \\
#     d W_{t}=W_{t} \frac{N_{t}}{W_{t}} d S_{t} + r W_{t}(1 - \frac{N_{t}}{W_{t}} S_{t}) d t
#     \\
# $$
#
# For investor with utility defined over intermediate consumption $ C_{t} =  \beta W_{t}$ with `utility_type = 2`,
#
# $$
#     d W_{t}=W_{t} \frac{N_{t}}{W_{t}} d S_{t} + r W_{t}(1 - \frac{N_{t}}{W_{t}} S_{t}) d t - \beta W_{t} d t
# $$
#
#
# where $d W_{t}$ can be written as $W_{t+1} - W_{t}$ and,
#
# $d S_{t}$ can be written as $S_{t+1} - S_{t}$ .


def plot_wealth_process(obj, data, optimal_weights, utility_type):
    """
    Function for plotting the wealth process.
    """
    
    tau, S = obj.spread_calc(data)
    W = np.ones(len(tau))

    if utility_type == 1:
        for i in range(len(data) - 1):
            # Calculating the wealth process for CRRA investor. Follows equation (3) in Appendix A
            W[i + 1] = W[i] + W[i] * optimal_weights[i] * (S[i + 1] - S[i]) + obj.r * W[i] * (
                        1 - optimal_weights[i] * S[i]) * obj.delta_t

    elif utility_type == 2:
        for i in range(len(data) - 1):
            # Calculating the wealth process for investor with intermediate consumption
            # Follows equation (39) in Appendix A
            # Here we add an extra term which denotes consumption
            W[i + 1] = W[i] + W[i] * optimal_weights[i] * (S[i + 1] - S[i]) + obj.r * W[i] * (
                        1 - optimal_weights[i] * S[i]) * obj.delta_t - obj.beta * W[i] * obj.delta_t
    
    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(data.index, W, 'c-')
    plt.title("Wealth process with initial wealth normalized to 1")
    plt.ylabel("Wealth")
    plt.xlabel("Date")
    plt.show()


# Plotting the corresponding wealth process for the calculated weights above.
plot_wealth_process(sc, data_test_dataframe, scaled_weights, utility_type = 1)


# In this example, we are considering the investor with utility defined over intermediate consumption(utility_type = 2), with gamma = 2

# Plotting the portfolio weights of the spread asset
plt.figure(figsize=(10, 6))
scaled_weights = sc.optimal_portfolio_weights(data_test_dataframe, gamma=2, utility_type=2)
plt.plot(data_test_dataframe.index, scaled_weights, 'c-')
plt.title("Optimal allocation to the spread asset")
plt.ylabel("Weights")
plt.xlabel("Date")
plt.show()


# Plotting the corresponding wealth process for the calculated weights above.
plot_wealth_process(sc, data_test_dataframe, scaled_weights, utility_type = 2)


# ### Stabilization Region


# In this optional step, we can calculate the boundaries of the stabilization region for the spread calculated from the data.


#
# In this section, the authors are interested in determining the direction in which an arbitrageur trades in response
# to a shock to the value of the spread asset. If an arbitrageur increases his position in the spread asset
# in response to an adverse shock, his trading is likely to have a stabilizing effect on the mispricing,
# contributing to its elimination in equilibrium. Conversely, if the arbitrageur decreases his position
# in response to the adverse shock, his trading will tend to exacerbate the mispricing.
#
# Sometimes arbitrageurs do not arbitrage. For instance, if the mispricing is sufficiently wide, a divergence
# in the mispricing can result in the decline of the total allocation, as the wealth effect dominates
# the improvement in the investment opportunity set. To characterize the conditions under which arbitrageurs cease
# to trade against the mispricing, the authors derived precise, analytical conditions for the time-varying envelope within which
# arbitrageurs trade against the mispricing.
#
# In the general case when $\bar{S} \neq 0$ the range of values of $S$
# for which the arbitrageur's response to an adverse shock is stabilizing - i.e. the agent trades against
# the spread, increasing his position as the spread widens - is determined by a time-varying envelope
# determined by both $A(\tau)$ and $B(\tau)$. The boundary of the stabilization region is determined by the
# following inequality:
#
# $$
#     \left| \phi(\tau) S+\frac{\kappa \bar{S}+\sigma^{2} B(\tau)}{\gamma \sigma^{2}}\right |<\sqrt{-\phi(\tau)}
# $$
#
# where,
#
# $$
#     \phi(\tau) = \left(\frac{2 A(\tau)}{\gamma}-\frac{\kappa+r}{\gamma \sigma^{2}}\right)
# $$
#
# As long as the spread is within the stabilization region, the improvement in investment opportunities
# from a divergence of the spread away from its long-run mean outweighs the negative
# wealth effect and the arbitrageur increases his position, $N$, in the mean-reverting asset. When
# the spread is outside of the stabilization region, the wealth effect dominates, leading the agent to
# curb his position despite an improvement in investment opportunities.


# In this example, we are considering the CRRA investor(utility_type = 1), with gamma = 2

S, min_bound, max_bound = sc.stabilization_region(data_test_dataframe, beta=0.01, gamma=2, utility_type=1)

# Plotting the stabilization bounds and the spread.
plt.figure(figsize=(10, 6))
plt.plot(data_test_dataframe.index, S , 'c-', label='Spread')
plt.plot(data_test_dataframe.index, min_bound, 'r:')
plt.plot(data_test_dataframe.index, max_bound, 'r:')
plt.title("Evolution of spread with stabilization bounds")
plt.xlabel("Date")
plt.legend()
plt.show()


# ### Optimal Portfolio Weights with Fund Flows


#
# In this optional step, we calculate the optimal portfolio weights with the inclusion of fund flows for the CRRA investor.


#
# #### Fund Flows
#
# This section deals with the inclusion of fund flows. Delegated managers are not only exposed to the
# financial fluctuations of asset prices but also to their client's desires to contribute or withdraw funds.
# Paradoxically, clients are most likely to withdraw funds after performance has been poor
# (i.e. spreads have been widening) and investment opportunities are the best.
#
# In the presence of fund flows the evolution of wealth under management will depend not only
# on performance, denoted by $\Pi_t$, but also on fund flows, $F_t$. Consequently:
#
# $$
#     \begin{aligned}
#     d \Pi &=\tilde{N} d S+(W-\tilde{N} S) r d t \\
#     d F &=f d \Pi+\sigma_{f} W d Z_{f} \\
#     d W &=d \Pi+d F=(1+f) d \Pi+\sigma_{f} W d Z_{f}
#     \end{aligned}
# $$
#
# where $\tilde{N}$ is the optimal policy rule chosen by a fund manager facing fund flows of the type described
# above, and $E[d Z_{f} dZ] = 0$. The fund flow magnifies the effect of performance on wealth
# under management, with each dollar in performance generating a fund flow of $f$ dollars.
#
#
# The optimal portfolio allocation of an agent with constant relative risk aversion with utility
# defined over terminal wealth, in the presence of fund flows is given by:
#
# $$
#     \tilde{N}(S, \tau)=\left(\frac{1}{1+f}\right) \cdot N(S, \tau)
# $$
#
# where $N(S, \tau)$ is the optimal policy function in the problem without fund flows
# and $f$ denotes the proportionality coefficient.
#
# The intuition behind this elegant solution is simple. The performance-chasing component of
# fund flows increases the volatility of wealth by a factor of $(1 + f)$, causing a manager who anticipates
# this flow to commensurately decrease the amount of risk taken on by the underlying strategy.
#
#
# ---
#
# The output of the `optimal_portfolio_weights_fund_flows` method is the portfolio weights of the spread scaled w.r.t wealth.
#
# ---


# In this example, we are considering the CRRA investor(utility_type = 2), with gamma = 2 and f = 0.05

# Plotting the portfolio weights of the spread asset with fund flows
plt.figure(figsize=(10, 6))
plt.plot(data_test_dataframe.index, sc.optimal_portfolio_weights_fund_flows(data_test_dataframe, f=0.05, gamma=2), 'c-')
plt.title("Optimal allocaton to spread asset with fund flows")
plt.ylabel("Weights")
plt.xlabel("Date")
plt.show()


# ### Plotting Results


# In this optional step, we can use the `plot_results` method to plot the out of sample performance of the model on specified number of test windows. This method plots the stabilization bound, optimal portfolio weights and optimal portfolio weights with fund flows.
#
# ---
#
# This method requires the input dataset to be length greater than 10 years.
#
# ---


# Loading the Shell-Royal Dutch Petroleum close prices dataset from 1990 to 2005.
data = pd.read_csv('shell-rdp-close_USD.csv', index_col='Date').ffill()
data.index = pd.to_datetime(data.index, format="%d/%m/%Y")

# Creating an object of class and calling the plotting method on the dataset.
sc = OUModelJurek()
sc.plot_results(data, num_test_windows = 5, figsize=(10, 6), fontsize=10)


# ---


# ## Conclusions
#
# This notebook describes the dynamic optimal portfolio strategy to exploit mispricings using an OU Model. The model and the corresponding implementations closely follow [Dynamic portfolio selection in arbitrage. (2007)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=882536) by Jurek, J.W. and Yang, H.
#
# **Key Takeaways:**
#
# - The portfolio is constructed using two traded assets, a riskless bond and mean reverting spread.
#
# - To construct the mean reverting spread, firstly total return indices are calculated from the price series. Then, the fixed hedge ratio is determined using Engle Granger cointegration. 
#
# - The  dynamics of allocation are driven by two factors : wealth(equity) above zero, and proximity to terminal evaluation date.
#
# - A mean reverting OU model is considered as it models both the horizon and divergence risks.
#
# - Two types of investors and their corresponding utility functions are considered in the model.
#
# - In the case of the investor with utility defined over intermediate consumption, an additional `beta` factor is required as input from the user. This determines the degree of consumption w.r.t wealth.
#
# - The value of `gamma`should always be positive. The greater the value of `gamma`, the more risk averse an investor is . If the value of `gamma` is $< 1$, this signifies an investor who is less risk averse than log utility.
#
# - The `stabilization_region` method derives a bound for the spread, in which the arbitrageur's trading is stabilizing.
#
#


# ## References
#
# * [Dynamic portfolio selection in arbitrage. (2007)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=882536) by Jurek, J.W. and Yang, H.



// ---------------------------------------------------

// OU_Model_Mudchanatongsuk.py
// arbitrage_research/Stochastic Control Approach/OU_Model_Mudchanatongsuk.py
# Generated from: OU_Model_Mudchanatongsuk.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# # Implementation of OU Model Mudchanatongsuk
#
# This notebook demonstrates the usage of the `ou_model_mudchanatongsuk` module.
#
# This module contains implementation of the following paper 
#
# - [Optimal pairs trading: A stochastic control approach. (2008)](http://folk.ntnu.no/skoge/prost/proceedings/acc08/data/papers/0479.pdf) by Mudchanatongsuk, S., Primbs, J.A. and Wong, W.
#
#
# ## Introduction
#
#
# In the paper corresponding to this module, the authors implement a stochastic control-based approach to the problem of pairs trading.
# The paper models the log-relationship between a pair of stock prices as an Ornstein-Uhlenbeck process
# and use this to formulate a portfolio optimization based stochastic control problem.
# This problem is constructed in such a way that one may either
# trade based on the spread (by buying and selling equal amounts of the stocks in the pair) or
# place money in a risk-free asset. Then the optimal solution to this control problem
# is obtained in closed form via the corresponding Hamilton-Jacobi-Bellman equation under a power utility on terminal wealth.
#
#
# ## Modelling
#
#
# Let $A(t)$ and $B(t)$ denote respectively the prices of the
# pair of stocks $A$ and $B$ at time $t$. The authors assume that stock $B$
# follows a geometric Brownian motion,
#
# $$
#     d B(t)=\mu B(t) d t+\sigma B(t) d Z(t)
# $$
#
# where $\mu$ is the drift, $\sigma$ is the volatility, and $Z(t)$ is a standard
# Brownian motion.
#
# Let $X(t)$ denote the spread of the two stocks at time $t$,
# defined as
#
# $$
#     X(t) = \ln(A(t)) − \ln(B(t))
# $$
#
# The authors assume that the spread follows an Ornstein-Uhlenbeck process
#
# $$
#     d X(t)=k(\theta-X(t)) d t+\eta d W(t)
# $$
#
# where $k$ is the rate of reversion, $\eta$ is the standard deviation and
# $\theta$ is the long-term equilibrium level to which the spread reverts.
#
# $\rho$ denotes the instantaneous correlation coefficient between $Z(t)$ and $W(t)$.
#
# Let $V(t)$ be the value of a self-financing pairs-trading portfolio and
# let $h(t)$ and $-h(t)$ denote respectively the
# portfolio weights for stocks $A$ and $B$ at time $t$.
#
#
# The wealth dynamics of the portfolio value is given by,
#
# $$
#     d V(t)= V(t)\left\{\left[h(t)\left(k(\theta-X(t))+\frac{1}{2} \eta^{2}+\rho \sigma \eta\right)+
#     r\right] d t+\eta d W(t)\right\}
# $$
#
#
# Given below is the formulation of the portfolio optimization pair-trading problem
# as a stochastic optimal control problem. The authors assume that an investor’s preference
# can be represented by the utility function $U(x) = \frac{1}{\gamma} x^\gamma$
# with $x ≥ 0$ and $\gamma < 1$. In this formulation, our objective is to maximize expected utility at
# the final time $T$. Thus, the authors seek to solve
#
#
# $$
#     \begin{aligned}
#     \sup _{h(t)} \quad & E\left[\frac{1}{\gamma}(V(T))^{\gamma}\right] \\[0.8em]
#     \text { subject to: } \quad & V(0)=v_{0}, \quad X(0)=x_{0} \\[0.5em]
#     d X(t)=& k(\theta-X(t)) d t+\eta d W(t) \\
#     d V(t)=& V(t)((h(t)(k(\theta-X(t))+\frac{1}{2} \eta^{2}\\
#     &+\rho \sigma \eta)+r) d t+\eta d W(t))
#     \end{aligned}
# $$
#
# Finally, the optimal weights are given by,
#
# $$
#     h^{*}(t, x)=\frac{1}{1-\gamma}\left[\beta(t)+2 x \alpha(t)-\frac{k(x-\theta)}{\eta^{2}}+
#     \frac{\rho \sigma}{\eta}+\frac{1}{2}\right]
# $$


# ---


# ## How to use this submodule
#
# This submodule contains three public methods. One for estimating the parameters of the model using training data,
# and the second method is for calculating the final optimal portfolio weights using evaluation data.


# ### Imports and Loading the dataset


# We use the $GLD$ and $GDX$ tickers daily close prices as our dataset. The training data comprises of data from years $2010$ to $2017$. The optimal weights are calculated on data from $2018$ to $2019$.


# Importing required modules
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import yfinance as yf

from arbitragelab.stochastic_control_approach.ou_model_mudchanatongsuk import OUModelMudchanatongsuk

# Importing GLD and GDX daily prices from yfinance
data1 =  yf.download("GLD GDX", start="2010-01-01", end="2017-12-31")
data2 =  yf.download("GLD GDX", start="2018-01-01", end="2020-01-01")

# Using the Adj Close prices for our dataset
data_train_dataframe = data1["Adj Close"][["GLD", "GDX"]]
data_test_dataframe = data2["Adj Close"][["GLD", "GDX"]]


# ### Fitting the model
#
#
# We input the training data to the fit method which calculates the spread
# and the estimators of the parameters of the model.
#
#
# ***
# Although the paper provides closed form solutions for parameter estimation,
#     this module uses log-likelihood maximization to estimate the parameters as we found the closed form solutions provided to be unstable.
#
# ***


# Creating an object of the Mudchanatongsuk class
sc = OUModelMudchanatongsuk()

# Calling the fit method on the train dataset
sc.fit(data_train_dataframe)


#  To view the estimated model parameters from training data, we can use the `describe` function in the class object.


display(pd.DataFrame(sc.describe(), columns=['Values']))


# ### Generating the Optimal Portfolio Weights
#
#
# In this step we input the out-of-sample test data to calculate the optimal portfolio weights using the fitted model and specify the utility function parameter $\gamma$.
#
# ***
#     As noted in the paper, please make sure the value of gamma is less than 1.
# ***


# In this example, we consider gamma = -10

# Plotting the portfolio weights of the spread asset
plt.figure(figsize=(10, 6))
weights = sc.optimal_portfolio_weights(data_test_dataframe, gamma = -10)
plt.plot(data_test_dataframe.index, weights, 'c-')
plt.title("Optimal allocaton to the spread asset")
plt.ylabel("Weights")
plt.xlabel("Date")
plt.show()


# To calculate the wealth process we use the following equation,
#
#
# $$
#     \\
#     d V(t)= V(t)\left\{\left[h(t)\left(k(\theta-X(t))+\frac{1}{2} \eta^{2}+\rho \sigma \eta\right)+
#     r\right] d t+ d X(t) - k(\theta-X(t)) d t\right\}
#     \\
# $$
#
#
# Here $d V(t)$ can be written as $V(t+1) - V(t)$ and,
#
# $d X(t)$ can be written as $X(t+1) - X(t)$ .


def plot_wealth_process(obj, data, optimal_weights, r):
    """
    Function for plotting the wealth process.
    """
    
    tau, X = obj.spread_calc(data)
    V = np.ones(len(tau))

    for i in range(len(data) - 1):
        # Calculating the wealth process from spread and optimal weights.
        # Follows equation(8) and equation(4) in the paper.
        term_1 = optimal_weights[i] * (obj.k * (obj.theta - X[i]) + 0.5 * obj.eta ** 2
                                      + obj.rho * obj.sigma * obj.eta) + r
        V[i + 1] = V[i] +  V[i] * (term_1 * obj.delta_t + (X[i + 1] - X[i] - obj.k * (obj.theta - X[i]) * obj.delta_t))

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(data.index, V, 'c-')
    plt.title("Wealth process with initial wealth normalized to 1")
    plt.ylabel("Wealth")
    plt.xlabel("Date")
    plt.show()


# Plots the corresponding wealth process for the calculated portolio weights above. 
plot_wealth_process(sc, data_test_dataframe, weights, 0.05)


# Shown below is a plot of the pricing data of the two assets used to calculate the optimal portfolio weights.


data_test_dataframe.plot(figsize=(10, 6));


# ---


# ## Conclusions
#
# This notebook demonstrates an optimal pairs trading model using a stochastic control based approach. This model is detailed in the following paper:
#
# [Optimal pairs trading: A stochastic control approach. (2008)](http://folk.ntnu.no/skoge/prost/proceedings/acc08/data/papers/0479.pdf) by Mudchanatongsuk, S., Primbs, J.A. and Wong, W.
#
# **Key Takeaways:**
#
# - The portfolio consists of the mean reverting spread and a risk free asset.
# - The spread is constructed from the log prices of the two stocks and is delta neutral. This spread is modelled using an OU process.
# - The investor preferences are represented by the power utility function.
# - Here, `gamma` parameter is considered to be $<1$.
# - The optimal portfolio weights generated do not depend on the rate of interest `r`.


# ## References
#
# * [Optimal pairs trading: A stochastic control approach. (2008)](http://folk.ntnu.no/skoge/prost/proceedings/acc08/data/papers/0479.pdf) by Mudchanatongsuk, S., Primbs, J.A. and Wong, W.



// ---------------------------------------------------

// kalman_filter.py
// arbitrage_research/Other Approaches/kalman_filter.py
# Generated from: kalman_filter.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Algorithmic Trading: Winning Strategies and Their Rationale__ _by_ Ernest P. Chan


# # Kalman Filter


# This description of the Kalman filter closely follows the work of _Ernest P. Chan_ __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146). 


# ## Introduction


# While for truly cointegrating price series we can use the tools described in the Cointegration module
# (Johansen test, etc.), for other price series we should use other tools to estimate the
# hedge ratio. For real price series, the hedge ratio can be changing in time. Using a look-back period
# to estimate the parameters of a model has its disadvantages, as a short period can cut a part of the information.
#
# This module describes a scheme that allows using the Kalman filter for hedge ratio updating, as presented in the
# book by Ernest P. Chan **"Algorithmic Trading: Winning Strategies and Their Rationale"**. One of the advantages
# of this approach is that we don't have to pick a weighting scheme for observations in the look-back period.


# ## Kalman Filter
#
# Following the descriptions by Ernest P. Chan:
#
# "Kalman filter is an optimal linear algorithm that updates the expected value of a hidden
# variable based on the latest value of an observable variable.
#
# It is linear because it assumes that the observable variable is a linear function of the hidden
# variable with noise. It also assumes the hidden variable at time $t$ is a linear function
# of itself at time $t - 1$ with noise, and that the noises present in these functions
# have Gaussian distributions (and hence can be specified with an evolving covariance
# matrix, assuming their means to be zero.) Because of all these linear relations, the expected
# value of the hidden variable at time $t$ is also a linear function of its expected value
# prior to the observation at $t$, as well as a linear function of the value of the observed
# variable at $t$.
#
# The Kalman filter is optimal in the sense that it is the best estimator
# available if we assume that the noises are Gaussian, and it minimizes the mean square error of
# the estimated variables."
#
# As we're searching for the hedge ratio, we're using the following linear function:
#
# $$y(t) = x(t) \beta(t) + \epsilon(t)$$
#
# where $y$ and $x$ are price series of the first and the second asset, $\beta$ is the
# hedge ratio that we are searching and $\epsilon$ is the Gaussian noise with variance $V_{\epsilon}$.
#
# Allowing the spread between the $x$ and $y$ to have a nonzero mean, $\beta$
# will be a vector of size $(2, 1)$ denoting both the intercept and the slope of
# the linear relation between $x$ and $y$. For this needs, the $x(t)$
# is augmented with a vector of ones to create an array of size $(N, 2)$.
#
# Next, an assumption is made that the regression coefficient changes in the following way:
#
# $$\beta(t) = \beta(t-1) + \omega(t-1)$$
#
# where $\omega$ is a Gaussian noise with covariance $V_{\omega}$. So the regression
# coefficient at time $t$ is equal to the regression coefficient at time $t-1$ plus
# noise.
#
# With this specification, the Kalman filter can generate the expected value of the hedge ratio
# $\beta$ at each observation $t$.
#
# Kalman filter also generates an estimate of the standard deviation of the forecast error
# of the observable variable. It can be used as the moving standard deviation of a Bollinger band.


# ## Kalman Filter Strategy
#
# Quantities that were computed using the Kalman filter can be utilized to generate trading
# signals.
#
# The forecast error $e(t)$ can be interpreted as the deviation of a pair spread from
# the predicted value. This spread can be bought when it has high negative values and sold
# when it has high positive values.
#
# As a threshold for the $e(t)$, its standard deviation $\sqrt{Q(t)}$ is used:
#
# - If $e(t) < - entry\_std\_score * \sqrt{Q(t)}$, a long position on the spread should be taken: Long $N$
#   units of the $y$ asset and short $N*\beta$ units of the $x$ asset.
#
# - If $e(t) \ge - exit\_std\_score * \sqrt{Q(t)}$, a long position on the spread should be closed.
#
# - If $e(t) > entry\_std\_score * \sqrt{Q(t)}$, a short position on the spread should be taken: Short $N$
#   units of the $y$ asset and long $N*\beta$ units of the $x$ asset.
#
# - If $e(t) \le exit\_std\_score * \sqrt{Q(t)}$, a short position on the spread should be closed.
#
# So it's the same logic as in the Bollinger Band Strategy from the Mean Reversion section of the Cointegration Approach module.


# ## Usage of the Algorithms


# Let's use the above tools on real data. 
#
# First, we will choose a pair of assets to apply the Kalman filter to. Then, from the filter output, we will generate trading signals based on the rules presented in the Kalman Filter Strategy section. Finally, we will analyze the obtained results. 


from IPython.display import Image

import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# As a pair to test the Kalman Filter approach we will use **EWC**(iShares MSCI Canada ETF) - **EWA**(iShares MSCI Australia ETF). This exact pair was used as an example in the book by Ernest P. Chan. We can also check if our results match the original ones for the same dataset.


# List of tickers to use in the analysis
tickers = ['EWC', 'EWA']

# Loading data
data =  yf.download(tickers, start="2006-04-26", end="2012-04-09")

# Taking close prices for chosen instruments
data = data["Adj Close"]

# Looking at the downloaded data
data.head()


# ### Applying Kalman Filter


# Now we'll apply the Kalman filter to our pair. As an output, we'll get the hedge ratios, intercepts, forecast errors, and standard deviations of forecast errors.
#
# Note that when feeding observations to the Kalman Filter _update()_ function, the first parameter is the x (in our case EWA), and y is the second parameter (in our case EWC).


# Covariance parameters used in the example
observation_covariance = 0.001
delta = 0.0001
transition_covariance = delta / (1 - delta)

# Initialising an object containing needed methods
strategy = al.other_approaches.KalmanFilterStrategy(observation_covariance=observation_covariance,
                                                    transition_covariance=transition_covariance)

# Feeding our price series element by element to the strategy
for observations in data.values:
      strategy.update(observations[0], observations[1])

# Getting lists of hedge ratios, intercepts, forecast errors, and their standard deviations
hedge_ratios = strategy.hedge_ratios
intercepts = strategy.intercepts
forecast_errors = strategy.spread_series
error_st_dev = strategy.spread_std_series


# **Note:** We're feeding price observations to the strategy one by one, so when generating internal parameters it has no access to future data.


# Transforming our variables into pandas formats for plotting
hedge_ratios = pd.DataFrame(hedge_ratios[10:],
                            index = data[10:].index,
                            columns =['Hedge Ratios'])

intercepts = pd.DataFrame(intercepts[10:],
                          index = data[10:].index,
                          columns =['Intercepts'])

forecast_errors = pd.DataFrame(forecast_errors[10:],
                               index = data[10:].index,
                               columns =['Forecast Errors'])

error_st_dev = pd.DataFrame(error_st_dev[10:],
                            index = data[10:].index,
                            columns =['St.D. of Forecast Errors'])


# Comparing generated hedge ratios with the ones from the book
hedge_ratios.plot(ylim=(0,1.8),
                  figsize = (9,7),
                  title='Kalman Filter Estimate of the Slope between EWC and EWA');


Image(filename='Kalman/kalman_slope.png')


# _Slope estimated between EWC(y) and EWA(x) using the Kalman Filter. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# Our calculated hedge ratios are pretty similar to the ones shown in the book, however all slightly higher. This can be explained with small differences in datasets that were used.


# Comparing generated hedge ratios with the ones from the book
intercepts[10:].plot(ylim=(0,6), 
                     figsize = (9,7),
                     title='Kalman Filter Estimate of the Intercept between EWC and EWA');


Image(filename='Kalman/kalman_intercept.png')


# _Intercept estimated between EWC(y) and EWA(x) using the Kalman Filter. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# Our calculated intercept values are also very similar to those shown in the book, but are slightly lower. Again, we can assume it's due to differences in datasets.


# ### Kalman Filter Strategy


# First, let's compare the values of forecast errors and their standard deviations that we got using the functions from the ArbitrageLab package with the values from the book.


ax = forecast_errors.plot(figsize = (9,7))

error_st_dev.plot(ax = ax,
                  figsize = (9,7),
                  title='Measurement Prediction Error - e(t) and its Standard Deviation sqrt(Q(t))');


Image(filename='Kalman/kalman_forecast_errors.png')


# _Measurement Prediction Error $e(t)$ and Standard Deviation of $e(t)$. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# Both values of forecast errors and the standard deviation are similar to the values from the book.
#
# We can say that our implementation works on test data as expected.


# Now let's run a function to generate trading signals for our dataset. As for the parameters of the Kalman Filter Strategy ($entry\_std\_score$ and $exit\_std\_score$) we can use the same values as in the book, so $entry\_std\_score = 1$ and $entry\_std\_score = 1$.


# Generating trading signals
trading_signals = strategy.trading_signals(entry_std_score=1, exit_std_score=1)[10:]

# Setting index for the Dataframe with trading signals
trading_signals = trading_signals.set_index(data[10:].index)


# Plotting obtained target quantities of portfolio to hold
trading_signals['target_quantity'].plot(figsize = (20,7),
                                        title='Kalman Filter Strategy portfolio target quantity values');


# As seen, trading signals change frequently as values of errors often surpass the 3 standard deviation mark, especially during the 2008-2009 period. This should be taken into account as we are able to obtain trading signals (target quantities) only after the observations for that period is known. So using them to trade the same day results in a lookahead bias.
#
# We can build an equity curve for this strategy on this dataset to see if it's profitable.


# Returns of elemrnts in our dataset
data_returns = (data / data.shift(1) - 1)[10:]

data_returns.head()


# Now calculating weights for X and Y in a portfolio.
#
# In a long position on spread we long $N$ units of the $y$ asset and short $N*\beta$ units of the $x$ asset.
# As we want our weights to sum up to 1, we long $\frac{N}{N+N*\beta}$ of the $y$ asset and short $\frac{N*\beta}{N+N*\beta}$ of the $x$ asset.
#
# When we short the spread, we short $N$ units of the $y$ asset and long $N*\beta$ units of the $x$ asset.
# So we short $\frac{N}{N+N*\beta}$ of the $y$ asset and long $\frac{N*\beta}{N+N*\beta}$ of the $x$ asset.


# Weights to use when opening trades
weights_x = pd.Series(strategy.hedge_ratios[10:],
                      index = data[10:].index,
                      name = 'weight_x')

weights_x = weights_x / (weights_x + 1)
weights_y = 1 - weights_x


# Looking at weights series - in the case of a Kalman Filter, they are dynamic
weights_x.head()


# Now constructing the equity curve of a portfolio.
#
# Our asset $x$ is **EWA** and asset $y$ is **EWC**.


# Portfolio returns - in our case it's spread returns
portfolio_returns = data_returns['EWC'] * weights_x - data_returns['EWA'] * weights_y

# Returns of our investment portfolio - using the generated signals on portfolio returns
# Shifting our trading signals one observation ahead to avoid the lookahead bias
investment_portfolio_returns = portfolio_returns * trading_signals['target_quantity'].shift(1)

# Price of our investment portfolio
portfolio_price = (investment_portfolio_returns + 1).cumprod()


# Calculating the equity curve of our investemnt portfolio
equity_curve = portfolio_price - 1


# And plotting it
equity_curve.plot(figsize = (9,7),
                  title='Kalman Filter Strategy investment portfolio equity curve');


Image(filename='Kalman/kalman_cumulative_returns.png')


# _Cumulative Returns of Kalman Filter Strategy on EWA-EWC. An example from ["Algorithmic Trading: Winning Strategies and Their Rationale"](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146) by Ernest P. Chan._


# We are getting results very close to those shows in the book. The equity curve being flattened along the Y axis can be explained with
# that we are normalizing our weights for them to be summed up to one.
#
# These results look good, over the 6 year period equity curve shows an increase in the investment portfolio value from 1 to around 2,38. 
#
# We can further test this strategy by choosing different $entry\_std\_score$ and $exit\_std\_score$ values, or adding transaction costs to see if the strategy is robust.


# ## Conclusion


# This notebook describes the Kalman Filter Strategy class and its functionality. Also, it shows how the tools can be used on real data.
#
# The algorithms and the descriptions used in this notebook were described by _Ernest P. Chan_ in the book __Algorithmic Trading: Winning Strategies and Their Rationale__  [available here](https://www.wiley.com/en-us/Algorithmic+Trading%3A+Winning+Strategies+and+Their+Rationale-p-9781118460146).
#
# Key takeaways from the notebook:
# - Kalman Filter Strategy is applicable when we don't have a truly cointegrating pair of price series.
# - No need to pick the look-backward window for the means and the standard deviation estimation, as in strategies from the Mean Reversion approach.
# - Kalman Filter approach allows estimation of changing hedge ratio between elements, whereas tools from the Cointegration Approach module offered a fixed hedge ratio.
# - Kalman Filter Strategy generates trading signals based on forecast errors and their standard deviations. The logic is similar to the one from the Bollinger Bands Strategy from the Mean Reversion section of the Cointegration Approach module.
# - Parameters available for optimization in the Kalman Filter Strategy are the ender and exit standard deviation scores.



// ---------------------------------------------------

// pca_approach.py
// arbitrage_research/Other Approaches/pca_approach.py
# Generated from: pca_approach.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * Reference: __Statistical Arbitrage in the U.S. Equities Market__ _by_ Marco Avellaneda and Jeong-Hyun Lee


# # PCA Approach


# This description of the PCA approach closely follows the work of _Marco Avellaneda_ and _Jeong-Hyun Lee_ __Statistical Arbitrage in the U.S. Equities Market__  [available here](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf). 


# ## Introduction


# This research notebook shows how the Principal Component Analysis can be used to create mean-reverting portfolios
# and generate trading signals. It's done by considering residuals or idiosyncratic components
# of returns and modeling them as mean-reverting processes.
#
# The original paper presents the following description:
#
# The returns for different stocks are denoted as $\{ R_{i} \}^{N}_{i=1}$. The $F$ represents
# the return of a "market portfolio" over the same period. For each stock in the universe:
#
# $$R_{i} = \beta_{i} F + \tilde{R_{i}}$$
#
# which is a regression, decomposing stock returns into a systematic component $\beta_{i} F$ and
# an (uncorrelated) idiosyncratic component $\tilde{R_{i}}$.
#
# This can also be extended to a multi-factor model with $m$ systematic factors:
#
# $$R_{i} = \sum^{m}_{j=1} \beta_{ij} F_{j} + \tilde{R_{i}}$$
#
# A trading portfolio is a market-neutral one if the amounts $\{ Q_{i} \}^{N}_{i=1}$ invested in
# each of the stocks are such that:
#
# $$\bar{\beta}_{j} = \sum^{N}_{i=1} \beta_{ij} Q_{i} = 0, j = 1, 2,, ..., m.$$
#
# where $\bar{\beta}_{j}$ correspond to the portfolio betas - projections of the
# portfolio returns on different factors.
#
# As derived in the original paper,
#
# $$\sum^{N}_{i=1} Q_{i} R_{i} = \sum^{N}_{i=1} Q_{i} \tilde{R_{i}}$$
#
# So, a market-neutral portfolio is only affected by idiosyncratic returns.


# ## PCA Approach
#
#
# This approach was originally proposed by Jolliffe (2002). It is using a historical share price data
# on a cross-section of $N$ stocks going back $M$ days in history. The stocks return data
# on a date $t_{0}$ going back $M + 1$ days can be represented as a matrix:
#
# $$R_{ik} = \frac{S_{i(t_{0} - (k - 1) \Delta t)} - S_{i(t_{0} - k \Delta t)}}{S_{i(t_{0} - k \Delta t)}}; k = 1, ..., M; i = 1, ..., N.$$
#
# where $S_{it}$ is the price of stock $i$ at time $t$ adjusted for dividends. For
# daily observations $\Delta t = 1 / 252$.
#
# Returns are standardized, as some assets may have greater volatility than others:
#
# $$Y_{ik} = \frac{R_{ik} - \bar{R_{i}}}{\bar{\sigma_{i}}}$$
#
# where
#
# $$\bar{R_{i}} = \frac{1}{M} \sum^{M}_{k=1}R_{ik}$$
#
# and
#
# $$\bar{\sigma_{i}}^{2} = \frac{1}{M-1} \sum^{M}_{k=1} (R_{ik} - \bar{R_{i}})^{2}$$
#
# And the empirical correlation matrix is defined by
#
# $$\rho_{ij} = \frac{1}{M-1} \sum^{M}_{k=1} Y_{ik} Y_{jk}$$
#
# **Note:** It's important to standardize data before inputting it to PCA, as the PCA seeks to maximize the
# variance of each component. Using unstandardized input data will result in worse results.
# The *get_signals()* function in this module automatically standardizes input returns before
# feeding them to PCA.
#
# The original paper mentions that picking long estimation windows for the correlation matrix
# ($M \gg N$, $M$ is the estimation window, $N$ is the number of assets in a portfolio)
# don't make sense because they take into account the distant past which is economically irrelevant.
# The estimation windows used by the authors is fixed at 1 year (252 trading days) prior to the trading date.
#
# The eigenvalues of the correlation matrix are ranked in the decreasing order:
#
# $$N \ge \lambda_{1} \ge \lambda_{2} \ge \lambda_{3} \ge ... \ge \lambda_{N} \ge 0.$$
#
# And the corresponding eigenvectors:
#
# $$v^{(j)} = ( v^{(j)}_{1}, ..., v^{(j)}_{N} ); j = 1, ..., N.$$
#
# Now, for each index $j$ we consider a corresponding "eigenportfolio", in which we
# invest the respective amounts invested in each of the stocks as:
#
# $$Q^{(j)}_{i} = \frac{v^{(j)}_{i}}{\bar{\sigma_{i}}}$$
#
# And the eigenportfolio returns are:
#
# $$F_{jk} = \sum^{N}_{i=1} \frac{v^{(j)}_{i}}{\bar{\sigma_{i}}} R_{ik}; j = 1, 2, ..., m.$$


from IPython.display import Image
Image(filename='PCA/pca_approach_portfolio.png')


# *Performance of a portfolio composed using the PCA approach in comparison to the market cap portfolio.
# An example from ["Statistical Arbitrage in the U.S. Equities Market"](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf)
# by Marco Avellaneda and Jeong-Hyun Lee.*


# In a multi-factor model we assume that stock returns satisfy the system of stochastic
# differential equations:
#
# $$\frac{dS_{i}(t)}{S_{i}(t)} = \alpha_{i} dt + \sum^{N}_{j=1} \beta_{ij} \frac{dI_{j}(t)}{I_{j}(t)} + dX_{i}(t),$$
#
# where $\beta_{ij}$ are the factor loadings.
#
# The idiosyncratic component of the return with drift $\alpha_{i}$ is:
#
# $$d \widetilde{X_{i}}(t) = \alpha_{i} dt + d X_{i} (t).$$
#
# Based on the previous descriptions, a model for $X_{i}(t)$ is estimated as the Ornstein-Uhlenbeck
# process:
#
# $$dX_{i}(t) = \kappa_{i} (m_{i} - X_{i}(t))dt + \sigma_{i} dW_{i}(t), \kappa_{i} > 0.$$
#
# which is stationary and auto-regressive with lag 1.
#
# The parameters $\alpha_{i}, \kappa_{i}, m_{i}, \sigma_{i}$ are specific for each stock.
# They are assumed to *de facto* vary slowly in relation to Brownian motion increments $dW_{i}(t)$,
# in the chosen time-window. The authors of the paper were using a 60-day window to estimate the residual
# processes for each stock and assumed that these parameters were constant over the window.
#
# However, the hypothesis of parameters being constant over the time-window is being accepted
# for stocks which mean reversion (the estimate of $\kappa$) is sufficiently high and is
# rejected for stocks with a slow speed of mean-reversion.
#
# An investment in a market long-short portfolio is being constructed by going long 1 dollar on the stock and
# short $\beta_{ij}$ dollars on the $j$ -th factor. Expected 1-day return of such portfolio
# is:
#
# $$\alpha_{i} dt + \kappa_{i} (m_{i} - X_{i}(t))dt$$
#
# The parameter $\kappa_{i}$ is called the speed of mean-reversion. If $\kappa \gg 1$ the
# stock reverts quickly to its means and the effect of drift is negligible. As we are assuming that
# the parameters of our model are constant, we are interested in stocks with fast mean-reversion,
# such that:
#
# $$\frac{1}{\kappa_{i}} \ll T_{1}$$
#
# where $T_{1}$ is the estimation window to estimate residuals in years.


# ## PCA Trading Strategy
#
# The strategy implemented in the ArbitrageLab module sets a default estimation window for the correlation
# matrix as 252 days, a window for residuals estimation of 60 days ($T_{1} = 60/252$) and the
# threshold for the mean reversion speed of an eigenportfolio for it to be traded so that the reversion time
# is less than $1/2$ period ($\kappa > 252/30 = 8.4$).
#
# For the process $X_{i}(t)$ the equilibrium variance is defined as:
#
# $$\sigma_{eq,i} = \frac{\sigma_{i}}{\sqrt{2 \kappa_{i}}}$$
#
# And the following variable is defined:
#
# $$s_{i} = \frac{X_{i}(t)-m_{i}}{\sigma_{eq,i}}$$
#
# This variable is called the S-score. The S-score measures the distance to the equilibrium of the cointegrated
# residual in units standard deviations, i.e. how far away a given asset eigenportfolio is from the theoretical
# equilibrium value associated with the model.


Image(filename='PCA/pca_approach_s_score.png')


# *Evolution of the S-score of JPM ( vs. XLF ) from January 2006 to December 2007.
# An example from  ["Statistical Arbitrage in the U.S. Equities Market"](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf)
# by Marco Avellaneda and Jeong-Hyun Lee.*


# If the eigenportfolio shows a mean reversion speed above the set threshold ($\kappa$), the
# S-score based on the values from the residual estimation window is being calculated.
#
# The trading signals are generated from the S-scores using the following rules:
#
# - Open a long position if $s_{i} < - \bar{s_{bo}}$
#
# - Close a long position if $s_{i} < + \bar{s_{bc}}$
#
# - Open a short position if $s_{i} > + \bar{s_{so}}$
#
# - Close a short position if $s_{i} > - \bar{s_{sc}}$
#
# Opening a long position means buying 1 dollar of the corresponding stock (of the asset eigenportfolio)
# and selling $\beta_{i1}$ dollars of assets from the first scaled eigenvector ($Q^{(1)}_{i}$),
# $\beta_{i2}$ from the second scaled eigenvector ($Q^{(2)}_{i}$) and so on.
#
# Opening a short position, on the other hand, means selling 1 dollar of the corresponding stock and buying
# respective beta values of stocks from scaled eigenvectors.
#
# Authors of the paper, based on empirical analysis chose the following cutoffs. They were selected
# based on simulating strategies from 2000 to 2004 in the case of ETF factors:
#
# - $\bar{s_{bo}} = \bar{s_{so}} = 1.25$
#
# - $\bar{s_{bc}} = 0.75$, $\bar{s_{sc}} = 0.50$
#
# The rationale behind this strategy is that we open trades when the eigenportfolio shows good mean
# reversion speed and its S-score is far from the equilibrium, as we think that we detected an anomalous
# excursion of the co-integration residual. We expect most of the assets in our portfolio to be near
# equilibrium most of the time, so we are closing trades at values close to zero.
#
# The signal generating function implemented in the ArbitrageLab package outputs target weights for each
# asset in our portfolio for each observation time - target weights here are the sum of weights of all
# eigenportfolios that show high mean reversion speed and have needed S-score value at a given time.


# ## Usage of the Algorithms


# Let's use the above tools on real data. 
#
# First, we will choose a set of stocks to apply the PCA approach to. Then we will go through the steps of the PCA approach and we'll generate trading signals using the PCA Strategy. Finally, we will analyze the obtained results. 


import arbitragelab as al
import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt


# ### Loading data


# We picked a set containing 176 stocks to apply the PCA Approach to. We'll be looking at a period of years 2018-2019. We'll eventually get trading signals for the year 2019, as the observations from the year 2018 will be needed to estimate the correlation matrix to get the PCA components.  


# List of tickers to use in the analysis
tickers = ['MMM', 'ABT', 'ANF', 'ACN', 'ADBE', 'AMD', 'AES', 'AFL', 'A', 'APD',
           'AKAM', 'AA', 'ALXN', 'ATI', 'ALL', 'MO', 'AMZN', 'AEE',
           'AEP', 'AXP', 'AIG', 'AMT', 'AMP', 'ABC', 'AMGN', 'APH', 'ADI', 'AON',
           'APA', 'AIV', 'AAPL', 'AMAT', 'ADM', 'AIZ', 'T', 'ADSK', 'ADP', 'AN',
           'AZO', 'AVB', 'AVY', 'BLL', 'BAC', 'BK', 'BAX', 'BDX', 'BBBY', 'BIG',
           'BIIB', 'BLK', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'CHRW', 'COG',
           'CPB', 'COF', 'CAH', 'KMX', 'CCL', 'CAT', 'CNP', 'CERN', 'CF', 'SCHW',
           'CVX', 'CMG', 'CB', 'CI', 'CINF', 'CTAS', 'CSCO', 'C', 'CTXS', 'CLF',
           'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP',
           'CNX', 'ED', 'STZ', 'GLW', 'COST', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI',
           'DHR', 'DRI', 'DVA', 'DE', 'XRAY', 'DVN', 'DFS', 'DISCA',
           'DLTR', 'D', 'RRD', 'DOV', 'DTE', 'DD', 'DUK', 'ETFC', 'EMN',
           'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EOG', 'EQT',
           'EFX', 'EQR', 'EL', 'EXC', 'EXPE', 'EXPD', 'XOM', 'FFIV', 'FAST', 'FDX',
           'FIS', 'FITB', 'FHN', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC',
           'FTI', 'F', 'FOSL', 'BEN', 'FCX', 'GME', 'GPS', 'GD', 'GE', 'GIS',
           'GPC', 'GNW', 'GILD', 'GS', 'GT', 'GOOG', 'GWW', 'HAL', 'HOG', 'HIG',
           'HAS', 'HP', 'HES', 'HPQ', 'HD', 'HON', 'HRL', 'HST', 'HUM', 'HBAN',
           'ITW']

# Loading data
data =  yf.download(tickers, start="2018-01-03", end="2019-12-31")

# Taking close prices for chosen instruments
data = data["Adj Close"]

# Looking at the downloaded data
data.head()


# As our method takes in returns, we'll calculate them from our dataframe of prices
returns = data.pct_change()[1:]

# Looking at the obtrined returns series
returns.head()


# We will go now through the steps of calculating S-scores for a window of observations to explain how the PCA approach works and will then generate treading signals.


# Starting with setting a PCAStrategy class with 15 principal components
pca_strategy = al.other_approaches.PCAStrategy(n_components=15)


# To get the factor weights from PCA we'll be using a window with 252 observations as in the original paper
data_252days = returns[:252]

# We can standardize our data
standardized_252days = pca_strategy.standardize_data(data_252days)

# But the function for calculation of the factor weights using PCA will do it by itself
factorweights = pca_strategy.get_factorweights(data_252days)

# Looking at the factor weights
factorweights


# Our factor weights are 15 first components from the PCA divided by the standard deviations of returns of our assets.
#
# Now we can get a 60-day window of observations to calculate residuals and coefficients. The calculation is done by fitting a linear regression on the returns from this window and factor returns from this window.


# Getting a 60-day window of observations to calculate factor returns
data_60days = returns[(252-60):252]

# Last day in our window
data_60days.index[-1]


# Calculating factor returns from our returns - multiplying them by factor weights
factorret = pd.DataFrame(np.dot(data_60days, factorweights.transpose()), index=data_60days.index)

# Looking at the obtained factor returns
factorret.head()


# So for each component from PCA and for each observation we get a return value. 


# Now fitting the linear regression to get residuals and coefficients of the regression
residual, coefficient = pca_strategy.get_residuals(data_60days, factorret)

# Residuals dataframe
residual.head()


# Coefficients dataframe
coefficient


# Using each column in the residuals dataframe we'll decide whether to trade an eigenportfolio related to that asset.
# The eigenportfolio will be used if:
#
#     a) the mean reversion speed of the OU process composed from the residuals is high enough,
#
#     b) the OU process deviates enough from its mean value.
#
# So, in our example, we can have up to 176 eigenportfolios. After the checks for the mean reversion speed and the S-scores are
# made we might end up with about 10 eigenportfolios that are suitable to be traded. 
#
# Using the betas - values from the coefficients dataframe the eigenportfolio will be constructed. 
#
# For example, if we have a signal to go long on the **AA** asset eigenportfolio and we use a scaling parameter equal to one, we
# will:
# - go long one dollar of the **AA** asset;
# - go short the **A** stock with the sum of betas from the **A** column (*coefficient* dataframe);
# - go short the **AA** stock with the sum of betas from the **AA** column (*coefficient* dataframe);
# - go short the **AAPL** stock with the sum of betas from the **AAPL** column (*coefficient* dataframe);
# - and so on..
#
# for each stock in a portfolio.


# Now, calculating the S-scores
s_scores = pca_strategy.get_sscores(residual, k=8.4)

# Picking parameters to trade based on the S-score
sbo = 1.25
sso = 1.25
ssc = 0.5
sbc = 0.75

# Stock eigenportfolios that we should long
s_scores[s_scores < -sbo]


# Stock eigenportfolios that we should short
s_scores[s_scores > sso]


# So these are the S-scores for each eigenportfolio. We've printed those of them that are either above or below critical values
# to enter a trade.
#
# We should go long on 19 eigneportfolios and short on 15 eigenportfolios for this observation.
#
# To calculate these S-scores we estimated the PCA components on the 252 trading days of the year 2018 and
# calculated the residuals on the last 60 days of the year 2018. The last observation used for the estimation was
# 2019.01.02. So these trading signals can be used on 2019.01.03.
#
# By moving the 60-day window one observation ahead and recalculating the residuals and the regression coefficients
# we would get new S-scores and based on them we would have a trading signal for 2019.01.04.
#
# As the estimated correlation matrix used to get the PCA factors doesn't change much, it's calculated again only once
# we generated the whole residual window of signals. So in our example, it would be recalculated every 60 days using the
# last 252 observations.


# Now we can simply use the PCA Strategy with a single function and given input parameters to generate trading signals
#
# **Note:** This function can be used on the raw returns dataframe. The previous steps are presented in the notebook
# to explain the idea behind the PCA Strategy.
#
# This function might take a long time to generate output, especially if given a dataframe with a big number of observations, as
# to generate trading signals for a single day we have to go through the all steps mentioned above.


# Simply applying the PCAStrategy with standard parameters
target_weights = pca_strategy.get_signals(returns, k=8.4, corr_window=252,
                                          residual_window=60, sbo=1.25,
                                          sso=1.25, ssc=0.5, sbc=0.75,
                                          size=1)



# Looking at generated trading signals
target_weights.head()


# Now, let's normalize these weights - so that on each day we'd be long or short a sum of 1 asset units. 


# Normalizing weights
norm_weights = target_weights.divide(abs(target_weights).sum(axis=1), axis=0)

# Checking if the sum of absolute weights for each date is 1
abs(norm_weights).head().sum(axis=1)


# Returns dataframe
returns_test = returns[(252 - 1):-1]

# Checking that our returns dataframe have the same index as the trading signals dataframe
returns_test.head()


# Shifting the trading signal dataframe one observation further,
# as we would be able to use those signals on the following day
investment_portfolio_returns = (returns_test * norm_weights.shift(1)).sum(axis=1)

# Calculating the portfolio price of our investment portfolio
investment_portfolio_price = (investment_portfolio_returns + 1).cumprod()

# And calculating the equity curve of our investment portfolio
equity_curve = investment_portfolio_price  - 1


# Plotting the equity curve
equity_curve.plot(figsize = (20,7),
                  title='PCA Strategy investment portfolio equity curve');


# These results look good, over the year 2019 equity curve shows an increase in the investment portfolio value from 1 to around 1,1. 
#
# We can further test this strategy by choosing different critical values for the S-score, increasing the mean reversion speed threshold, or adding transaction costs to see if the strategy is robust.


# ## Conclusion


# This notebook describes the PCA Strategy class and its functionality. Also, it shows how the tools can be used on real data.
#
# The algorithms used in this notebook were described by _Marco Avellaneda_ and _Jeong-Hyun Lee_ in the paper __Statistical Arbitrage in the U.S. Equities Market__  [available here](https://math.nyu.edu/faculty/avellane/AvellanedaLeeStatArb20090616.pdf).
#
# Key takeaways from the notebook:
# - Principal Component Analysis can be used to create mean-reverting portfolios and generate trading signals.
# - First, a window of 252 observations is used to get an empirical correlation matrix and use the PCA to get N top components.
# - It’s important to standardize data before inputting it to PCA, as the PCA seeks to maximize the variance of each component.
# - A separate market-neutral eigenportfolio can be calculated for each stock in our portfolio.
# - Next, we pick a window to calculate residuals (60 days in the example).
# - By using linear regression on the second window (60-days) and factor returns for this window we get residuals and the coefficients of regression.
# - These residuals are used to construct an OU process for each eigenportfolio.
# - If the OU process shows a high (above the $\kappa$ threshold) speed of mean reversion, we calculate the s-score for it.
# - S-score is measuring how far away a given asset eigenportfolio is from the theoretical equilibrium value associated with the model.
# - If the S-score is too high or too low we generate a signal to sell or buy this eigenportfolio.
# - Resulting trading signal is the sum of all eigenportfolio weights that satisfy the requirements to be traded.
# - Parameters to optimize in this strategy are the mean reversion speed threshold, the windows for PCA and residual calculation, and the S-score thresholds to enter or exit positions. 



// ---------------------------------------------------

// hedge_ratios.py
// arbitrage_research/Hedge Ratios/hedge_ratios.py
# Generated from: hedge_ratios.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from arbitragelab.hedge_ratios import (construct_spread, get_box_tiao_hedge_ratio,
                                       get_johansen_hedge_ratio, get_minimum_hl_hedge_ratio, 
                                       get_ols_hedge_ratio, get_tls_hedge_ratio,
                                       get_adf_optimal_hedge_ratio)


# # Hedge Ratio Methods


# In order to test our hedge ratios module, let's first create a spread that is mean-reverting by its nature and see how different methods perform.


# Generating a series of returns X
rs = np.random.RandomState(42)
X_returns = rs.normal(0, 1, 100)
X = pd.Series(np.cumsum(X_returns), name='X') + 50

# And series of cointegrated returns Y
noise = rs.normal(0, 1, 100)
Y = 5 * X + noise
Y.name = 'Y'

cointegrated_series = pd.concat([Y, X], axis=1)


# By default, the first columns ticker is used as a **dependent variable**. This can be adjusted in hedge ratio functions by providing an alternative ticker to be used as a dependent variable.


# Check columns in a generated dataframe
cointegrated_series.columns


# Use the construct_spread function with provided hedge ratios
theoretical_spread = construct_spread(price_data=cointegrated_series,
                                      hedge_ratios={'Y': 1, 'X': 5})


# Plot resulting spread
theoretical_spread.plot(figsize=(12, 10), grid=True, title='Cointegrated spread series.');


# One can also decide on `dependent variable` in `construct_spread` function.


# Spread construction with given dependent variable
theoretical_spread_reverted = construct_spread(price_data=cointegrated_series,
                                               hedge_ratios={'Y': 1, 'X': 5},
                                               dependent_variable='X')

theoretical_spread_reverted.plot(figsize=(12, 10), grid=True, title='Cointegrated spread series with X being dependent variable.');


# Now we can compare these spreads and see that one is a reverted version of the other.


theoretical_spread_reverted.plot(figsize=(12, 10), grid=True, title='Spreads comparison.')
theoretical_spread.plot(figsize=(12, 10), grid=True, title='Spreads comparison.');


# Mean values of two generated spreads
theoretical_spread.mean(), theoretical_spread_reverted.mean()


# ## OLS and TLS hedge ratios


# Now let's use Ordinary Least Squares and Total Least Squares to estimate hedge ratios and see how far the results are from theoretical values.


ols_hedge_ratio, X, y, spread_ols = get_ols_hedge_ratio(price_data=cointegrated_series,
                                                        dependent_variable='Y',
                                                        add_constant=False) # One can add constant if needed.

tls_hedge_ratio, X, y, spread_tls = get_tls_hedge_ratio(price_data=cointegrated_series,
                                                        dependent_variable='Y',
                                                        add_constant=False) # One can add constant if needed.


# Hedge ratios methods return:
# - Hedge ratio dictionary such that the `dependent variable` has a hedge ratio of 1. 
# - X and y datasets used in model valuation. It means that the ratio was found by regressing $y \sim X + eps$.
# - Resulting spread obtained by calculated hedge ratio. It can be also obtained by using `contruct_spread` method.
# - All hedge ratio and spread calculations assume that: $spread = 1.0*Price_{dependent variable} - hedge_{1}*Price_{1}-...hedge_{n}*Price_{n}$


# Checking hedge reatios for both methods
ols_hedge_ratio, tls_hedge_ratio


# As we can see, OLS and TLS managed to capture the hedge ratio correctly.


# Plotting theoretical, OLS, and TLS spreads

theoretical_spread.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='Theoretical spread.')
spread_ols.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='OLS spread')
spread_tls.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='TLS spread.')
plt.legend(loc='best');


# ## Johansen hedge ratio


johansen_hedge_ratio, _, _, spread_joh = get_johansen_hedge_ratio(price_data=cointegrated_series,
                                                                  dependent_variable='Y')


# Checking hedge reatio of the Johansen method
johansen_hedge_ratio


# ## Box-Tiao hedge ratio


bt_hedge_ratio, _, _, spread_bt = get_box_tiao_hedge_ratio(price_data=cointegrated_series,
                                                           dependent_variable='Y')


# Checking hedge reatio of the Box-Tiao method
bt_hedge_ratio


# ## Minimum half-life, Minimum ADF t-statistic hedge ratio


# Min HL, Min ADF methods use numerical optimization techniques to find a spread with minimum half-life of mean-reversion, minimum ADF
# t-statistic, respectively.


min_hl_hedge_ratio, _, _, spread_min_hl, opt_object_hl = get_minimum_hl_hedge_ratio(price_data=cointegrated_series, 
                                                                                    dependent_variable='Y')

min_adf_hedge_ratio, _, _, spread_min_adf, opt_object_adf = get_adf_optimal_hedge_ratio(price_data=cointegrated_series, 
                                                                                        dependent_variable='Y')


# Checking hedge reatio of the Minimum half-life method
min_hl_hedge_ratio


# Checking hedge reatio of the Minimum ADF t-statistic method
min_adf_hedge_ratio


# As both methods rely on numerical optimization, sometimes methods fail to converge yielding unstable, wrong results. That is why both methods return scipy optimization objects which can be used to check the status of optimization.


# Optimization status of the Minimum half-life method
opt_object_hl.message


# Optimization status of the Minimum ADF t-statistic method
opt_object_adf.message


# ## Example of diverging min HL/ADF hedge ratios


diverging_series = cointegrated_series.copy()

# Creating two constant assets.
diverging_series['Y'] = 1.0
diverging_series['X'] = 2.0


min_hl_hedge_ratio, _, _, spread_min_hl, opt_object_hl = get_minimum_hl_hedge_ratio(price_data=diverging_series, 
                                                                                    dependent_variable='Y')

min_adf_hedge_ratio, _, _, spread_min_adf, opt_object_adf = get_adf_optimal_hedge_ratio(price_data=diverging_series, 
                                                                                        dependent_variable='Y')


# Optimization status of both methods
opt_object_hl.message, opt_object_adf.message


# ## Example on real data


# Let's apply functions on a real dataset and construct a spread from 3 assets.


# Loading data
price_data = pd.read_csv('data/data.csv', index_col=0, parse_dates=[0])
price_data = price_data[['GOOG', 'AAPL', 'AMZN']].dropna()


# Calculating hedge ratios using all mehtods

ols_hedge_ratio, _, _, spread_ols = get_ols_hedge_ratio(price_data=price_data, dependent_variable='GOOG', add_constant=False) 
tls_hedge_ratio, _, _, spread_tls = get_tls_hedge_ratio(price_data=price_data, dependent_variable='GOOG', add_constant=False)
joh_hedge_ratio, _, _, spread_joh = get_johansen_hedge_ratio(price_data=price_data, dependent_variable='GOOG')
bt_hedge_ratio, _, _, spread_bt = get_box_tiao_hedge_ratio(price_data=price_data, dependent_variable='GOOG')
min_hl_hedge_ratio, _, _, spread_min_hl, _ = get_minimum_hl_hedge_ratio(price_data=price_data, dependent_variable='GOOG')
min_adf_hedge_ratio, _, _, spread_min_adf, _ = get_adf_optimal_hedge_ratio(price_data=price_data, dependent_variable='GOOG')


# Plotting spreads from each method
spread_ols.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='OLS spread')
spread_tls.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='TLS spread.')
spread_joh.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='Johansen spread')
spread_bt.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='Box-Tiao spread')
spread_min_hl.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='Min HL spread')
spread_min_adf.plot(figsize=(12, 10), grid=True, title='Spreads comparison', label='Min ADF spread')
plt.legend(loc='best');


# ## Conclusion
#
# This notebook presents the use of the hedge ratios methods available in the MlFinLab package. 
#
# When applied to synthetically generated data, they perform similarly. However, on real data, the results may differ, as methods have different underlying logic.
#
# It should be noted that some methods may fail to converge, and the output may be unstable.



// ---------------------------------------------------

// xou_model.py
// arbitrage_research/Optimal Mean Reversion/xou_model.py
# Generated from: xou_model.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * References:  **Tim Leung and Xin Li** - *Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications (2015)*


# ## Abstract
#
# An Exponential Ornstein-Uhlenbeck process is a modification of the geometric Ornstein–Uhlenbeck process
# developed previously by Dixit and Pyndick in a paper titled:
# _"The stochastic behavior of commodity prices: Implications for valuation and hedging."_
#
# Tim Leung, Xin Li in _"Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications" (2015)_ present the solution to the optimal timing problems for one-time entering and liquidating the position and also provide the 
# ability to find optimal levels for infinite amount of trades based on Exponential Ornstein-Uhlenbeck process.


# The following implementations and descriptions closely follow the work of Tim Leung: [Tim Leung and Xin Li Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919). Data used to showcase the module was chosen based on the example in the publication.


# ## Exponential Ornstein-Uhlenbeck process
# The Exponential Ornstein-Uhlenbeck (XOU) process is defined the following way:
#
#
#    $$ \xi_t = e^{X_t}, t \geq 0$$
#
# where $X$ is the Ornstein-Uhlenbeck process.


# >The definition of the OU process and the fitting procedure details are presented in **OU model notebook** (ou_model.ipynb)
#
# In other words, $X$ is a *log-price* of a positive XOU
# process $\xi$.


from IPython.display import Image
Image(filename='XOU_model/xou_vs_ou.png')


# _Simulated OU ($\theta$ = 1, $\mu$ = 0.6, $\sigma$ = 0.2) and XOU ($\theta$ = 0, $\mu$ = 0.6, $\sigma$ = 0.2) processes paths._


#
#
# The main parameters of the XOU model coincide with the parameters of the OU model:
#
# * $\theta$ − long term mean level, all future trajectories of 𝑋 will evolve around a mean level 𝜃 in the long run.
#
# * $\mu$ - the speed of reversion, characterizes the velocity at which such trajectories will regroup around $\theta$ in time.
#
# * $\sigma$ - instantaneous volatility, measures the amplitude of randomness entering the system. Higher values imply more randomness.
#
#
#
# To fit the XOU process to our data and find the optimal ratio between the two assets we
# are using the same approach as we utilized for the OU process:
# firstly, we are maximizing the average log-likelihood function with respect to model parameters, and secondly
# choosing the $\beta^*$ that provides the maximum value of the said max log-likelihood function.


# ## Optimal stopping problem
#
# > This approach presumes that the investor wants to commit only two trades: entering the position, and
#     liquidating it.
#
# First of all, let's assume that the investor already has a position the value of which follows the XOU process. When
# the investor closes his position at the time $\tau$ he receives the value $\xi_{\tau}=e^{X_{\tau}}$ and pays a
# constant transaction cost $c_s > 0$. To maximize the expected discounted value we need to solve
# the optimal stopping problem:
#


# $$V^{\xi}(x) = \underset{\tau \in T}{\sup} \mathbb{E}({e^{-r \tau} (e^{X_{\tau}} - c_s)| X_0 = x}),$$


# where $T$ denotes the set of all possible stopping times and $r > 0$ is our subjective constant
# discount rate. $V^{\xi}(x)$ represents the expected liquidation value accounted with $\xi$.
#
# Current price plus transaction cost constitute the cost of entering the trade. After subtracting the found cost from
# the expected optimal value of liquidation - $V(x)$ we can formalize the optimal entry problem:


# $$ J^{\xi}(x) = \underset{\nu \in T}{\sup} \mathbb{E}({e^{-\hat{r} \tau} (V^{\xi}(X_{\nu}) - e^{X_{\nu}} - c_b)| X_0 = x})$$


# $$\hat{r}>0, c_b > 0$$


# To sum up this problem, we, as an investor, want to maximize the expected discounted difference between the current price
# of the position - $e^{x_{\nu}}$ and its expected liquidation value $V^{\xi}(X_{\nu})$ minus transaction cost
# $c_b$.


# ### The solutions:


# Theorem 3.2 (p.54):
#
# **The optimal liquidation problem admits the solution:**
#
# $$V^{\xi}(x) = \begin{cases} (e^{b^{\xi*}} - c_s) \frac{F(x)}{F(b^{\xi*})} , & \mbox{if } x \in (-\infty,b^{\xi*})\\
#   \\ e^x - c_s, &  \mbox{ otherwise}  \end{cases}$$
#
# The optimal liquidation level $b^*$ is found from the equation:
#
# $$e^b F(b) - (e^b - c_s)F'(b) = 0$$


# Corresponding optimal liquidation time is given by
#
# $$\tau^{\xi*} = \inf [t\geq0:X_t \geq b^{\xi*}] = \inf [t\geq0:\xi \geq e^{b^{\xi*}}]$$
#
#
# Theorem 3.4 (p.54):
#
# **The optimal entry timing problem admits the solution:**
#
#
# $$J^{\xi}(x) = \begin{cases} P^{\xi}{F}(x),  & \mbox{if } x \in (-\infty,a^{\xi*})\\
#     \\ V^{\xi}(x) - e^x - c_b, & \mbox{if } x \in [a^{\xi*}, d^{\xi*}]\\
#     \\ Q^{\xi}{G}(x), & \mbox{if } x \in (d^{\xi*}, \infty)\end{cases}$$
#
# The optimal entry interval $(a^{\xi*},d^{\xi*})$ is found using the respective equations:
#
# $$G(d)(V^{\xi'}(d) - e^d) - G'(d)(V^{\xi}(d) - e^d - c_b) = 0$$
# $$F(a)(V^{\xi'}(a) - e^a) - F'(a)(V^{\xi}(a) - e^a - c_b) = 0$$
#
#
# Corresponding optimal entry time is given by
#
#
# $$\nu_{a^{\xi*}, d^{\xi*}} = \inf [t\geq0:X_t \in [a^{\xi*}, d^{\xi*}]]$$
#
#
# To summarize: the investor should enter the market when the price enters the interval
# $[e^{a^{\xi*}}, e^{d^{\xi*}}]$ for the first time, and exit as soon as it reaches the price level
# $e^{b{\xi*}}$.


# ## Optimal switching problem
#
# > This approach presumes that the investor can commit an infinite number of trades.
#
#
# If there is no limit on the number of times the investor will open or close the position, the sequential trading times
# are modelled by the stopping times $\nu_1,\tau_1,\nu_2,\tau_2,... \in T$ such that
#
# $$0\leq\nu_1\leq\tau_1\leq\nu_2\leq\tau_2\leq...$$
#
# Where $\nu_i$ are times when the share of a risky asset was bought and $\tau_i$ - when it was sold. In the case
# of pairs trading, we consider our spread as such an asset.


# ### The solutions:
#
# In case of optimal switching the formulation of the problem will depend on the position we start with, as an investor, and subsequently - our first trading decision. 
#
# **Zero position - Buy**
#
# If we start from the zero position, our first action would be to decide when to buy the share of our asset. In our calculation we would
# like to account for all our possible market re-entries and exits , so the problem is formulated in the following way:


# $$\tilde{J}^{\xi}(x) = \underset{\Lambda_0}{\sup} \{ \mathbb{E}_x \sum^{\infty}_{n=1}[e^{-r\tau_n}h^{\xi}_s(X_{\tau_n}) - e^{-r\nu_n}h^{\xi}_b(X_{\nu_n})]\}$$


# Where $\Lambda_0$ is the set of admissible times, and helper functions denoted as such:
#
# $$h^{\xi}_s=e^x-c_s$$


# $$and$$


# $$h^{\xi}_s=e^x+c_b$$


# **Existing position - Sell**
#
# Vice-versa, if we start with pre-existing position in said risky asset our firt decision would be to decide on liquidation timing, therefore we have to account our first optimal "sell" value and then all of the possible market re-entries and exits:


# $$\tilde{V}^{\xi}(x) = \underset{\Lambda_1}{\sup} \{ \mathbb{E}_x{e^{-r\tau_1}h^{\xi}_s(X_{\tau_1})+\sum^{\infty}_{n=2}[e^{-r\tau_n}h^{\xi}_s(X_{\tau_n}) - e^{-r\nu_n}h^{\xi}_b(X_{\nu_n})]} \}$$


# With $\Lambda_1$ as the set of admissible times.


# ### The solution:
#
# To find both optimal entry and liquidation switching levels we need to ensure that re-entering the market(or entering at all) is optimal. 
#
# For that we need to establish the helper fumctions correlated with the entry and liquidation processes:
#
# $$f_s(x):=(\mu\theta+\frac{1}{2}\sigma^2-r) - \mu x + r c_s e^{-x}$$
# $$f_b(x):=(\mu\theta+\frac{1}{2}\sigma^2-r) - \mu x - r c_b e^{-x}$$


# Theorem 3.7 (p.56):
#
# **Under optimal switching approach it is optimal to re-enter the market if and only if all of the following conditions hold true:**
#
# a) There are two distinct roots to $f_b:\ x_{b1},x_{b2}$
#
# b) $\exists \tilde{a}^* \in (x_{b1},x_{b2})$ satisfying $F(\tilde{a}^*)e^{\tilde{a}^*}=F'(\tilde{a}^*)(e^{\tilde{a}^*}+c_b)$
#
# c) The following inequality must hold true:
#
# $$\frac{e^{\tilde{a}^*}+c_b}{F(\tilde{a}^*)}\geq\frac{b^{\xi*}-c_s}{F(b^{\xi*})}$$
#
# In case any of the conditions are not met - re-entering the market is deemed not optimal it would be advised to exit
# at the optimal liquidation price without re-entering in the case when the investor had already entered the market beforehand,
# or don't enter the market at all in the case when he or she starts with a zero position.


# ## How to use the XOU module
#
# For this module the most suitable input would be a logarithmized pre-built mean-reverting portfolio prices.
#
# Both optimal stopping and optimal switching levels are used alike in determining the rules of our trading strategy:
#
#
# * If not already entered, enter when the price reaches optimal entry level.
#
# * If not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# However, the differenece lies in the applications of the two approaches. The optimal stopping usually 
# has a much longer delay between the buy and sell levels are reached and offers a bigger gain than a one-time entry and liquidation using the optimal switching levels. The optimal switching levels, on the other hand, may provide a bigger cumulative gain by performing the trades multiple times during the same time period.


import arbitragelab.optimal_mean_reversion as omr
import numpy as np
import matplotlib.pyplot as plt


# Creating a class instance
example = omr.ExponentialOrnsteinUhlenbeck()


# We are able to create the training data sample using the module itself:


# We establish our training sample
delta_t = 1/252
np.random.seed(31)
xou_example =  example.ou_model_simulation(n=1000, theta_given=1, mu_given=0.6,
                                           sigma_given=0.2, delta_t_given=delta_t)


# Model fitting uses the same function structure as the OU module:


# Model fitting
example.fit(xou_example, data_frequency="D", discount_rate=0.05,
            transaction_cost=[0.02, 0.02])


# Optimal stopping levels can be found separately via respective functions.


# Solving the optimal stopping problem
b = example.xou_optimal_liquidation_level()

a,d = example.xou_optimal_entry_interval()


print("Optimal liquidation level:", round(b,5),
      "\nOptimal entry interval:[",round(a, 5),",",round(d, 5),"]")



# Both optimal levels can be found via the `optimal_switching_levels` function.


# Solving the optimal switching problem
d_switch, b_switch = example.optimal_switching_levels()


print ("Optimal switching liquidation level:", round(b_switch,5),
       "\nOptimal switching entry interval:[", round(np.exp(example.a_tilde), 5),",",round(d_switch, 5),"]")



# To test the obtained results let's simulate the XOU process using the respective function.


np.random.seed(31)
xou_plot_data =  example.xou_model_simulation(n=1000, theta_given=1, mu_given=0.6,
                                              sigma_given=0.2, delta_t_given=delta_t)


# To visualize the results we use the `xou_plot_levels` function. 


# Showcasing the results on the training data (pd.DataFrame)
fig = example.xou_plot_levels(xou_plot_data, switching=True)

# Adjusting the size of the plot
fig.set_figheight(7)
fig.set_figwidth(12)


# Or you can view the model statistics
example.xou_description(switching=True)


# ## Conclusion


# This notebook describes the Exponential Ornstein-Uhlenbeck (XOU) model and how it is applied to mean reverting portfolios. The main goal of the notebook is to show the usage of the optimal stopping and optimal switching problems.
#
# Both techniques were introduced by *Tim Leung and Xin Li* in [**Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications**](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919)
#
# Key takeaways from the notebook:
#
# * Exponential Ornstein-Uhlenbeck model is a great tool used to model the behavior of mean-reverting assets.
#
# * Main idea behind the use of the  optimal levels is:
#
#     * If position is not already entered, enter when the price reaches optimal entry level.
#
#     * If position is not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# * An optimal stopping problem formulated for the XOU process allows us to maximize the expected discounted value of one-time entering or liquidating the position by finding the optimal price levels at which trades should be committed.
#
# * An optimal switching problem, on the other hand, allows us to maximize the expected discounted value of infinite amount of trades(entering or liquidating the position). By finding the optimal price levels at which the repeated trades should be committed for the maximum overall gain. 
#
# * The model should be considered as a building block for the problem with any finite number of sequential trades.
#


# ## Reference


# 1. Leung, Tim, and Xin Li. Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications. World Scientific Publishing Company, 2451. 



// ---------------------------------------------------

// heat_potentials.py
// arbitrage_research/Optimal Mean Reversion/heat_potentials.py
# Generated from: heat_potentials.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# References:
#
# **Alexandr Lipton and Marcos Lopez de Prado:** [ _"A closed-form solution for optimal mean-reverting trading strategies"_](https://ssrn.com/abstract=3534445)
#
# **Marcos Lopez de Prado:**  [_"Advances in Financial Machine Learning"_](https://www.amazon.com/Advances-Financial-Machine-Learning-Marcos/dp/1119482089)


# ## Abstract


# An Ornstein-Uhlenbeck process is a great tool for modeling the behavior of mean-reverting portfolio prices. Alexandr Lipton and Marcos Lopez de Prado present a methodology that allows to obtain a closed-form solution for the optimal mean-reverting trading strategies based on the OU model characteristics and the heat potential approach. The optimal trading rule, namely, optimal stop-loss and optimal profit-taking level are found by maximizing the approximated value of the Sharpe ratio.


# ## Data scaling


# **NOTE:**
# >In this approach we use the volume clock metric instead of the time-based metric. More on that in the paper
# [**"The Volume Clock: Insights into the High Frequency Paradigm"**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2034858) by David Easley, Marcos Lopez de Prado, and Maureen O'Hara


# Let's presume an investment strategy S is a long investment strategy with p/l driven by the OU process:
#
# $$dx' = \mu'(\theta'-x')dt'+\sigma'dW_{t'}, x'(0) = 0$$


# and a trading rule $R = \{ \bar{\pi}',\underline{\pi}',T' \}$. To make the application of the method of heat potentials possible we transform it to use its steady-state by performing scaling to remove superfluous parameters.
#
# $$t = \mu't',\ T = \mu'T',\ x = \frac{\sqrt{\mu'}}{\sigma'} x',\ \theta = \frac{\sqrt{\mu'}}{\sigma'} \theta',\ \bar{\pi} = \frac{\sqrt{\mu'}}{\sigma'} \bar{\pi}',
# \ \underline{\pi} = \frac{\sqrt{\mu'}}{\sigma'} \underline{\pi}'$$


# And get:
# $$dx = (\theta-x)dt + dW_t, \ \bar{\pi}' \leq x \leq \underline{\pi},\ 0 \leq t \leq T$$


# **NOTE:**
#
# >Short strategy reverses the roles of ${\bar{\pi}',\underline{\pi}'}$:
# >
# >$-\underline{\pi}$ equals the profit taken when the price hits $\underline{\pi}$ and
# >
# >$-\bar{\pi}$ losses are incurred while price hits $-\bar{\pi}$
#
#


# Hence, we can restrict ourself to case with $\theta \geq 0$.


# ## Sharpe ratio calculation
#
# The calculation of the Sharpe ratio follows the four-step algorithm:
# ***
# **Step 1: Define a calculation grid**
#
# First of all we define the grid $\upsilon$ based on which we will perform our numerical calculation:
#
# $$ 0=\upsilon_0<\upsilon_1<...<\upsilon_n=\Upsilon,\  \upsilon(t) = \frac{1 - e^{-2(T-t)}}{2}$$
#
# **Step 2: Numerically calculate helper functions** $\bar{\epsilon}, \underline{\epsilon}, \bar{\phi}, \underline{\phi}$
#
# We are going to use the classical method of heat potentials to calculate the SR.
# As a preparation, in this step, we solve the two sets of Volterra equations by using the trapezoidal rule of integral calculation.
#
# **Step 3: Calculate the values of** $\hat{E}(\Upsilon,\bar{\omega})$ **and** $\hat{F}(\Upsilon,\bar{\omega})$
#
# We need to compute these functions at one point, which can be done by approximation of the integrals using the
# trapezoidal rule:
#
# $$\hat{E}(\Upsilon,\bar{\omega}) = \frac{1}{2} \sum_{i=1}^k(\underline{w}_{n,i}\underline{\epsilon}_i + \underline{w}_{n,i-1}\underline{\epsilon}_{i-1} + \bar{w}_{n,i}\bar{\epsilon}_i + \bar{w}_{n,i-1}\bar{\epsilon}_{i-1})(\upsilon_i - \upsilon_{i-1})$$
#
# $$ \hat{F}(\Upsilon,\bar{\omega}) = \frac{1}{2} \sum_{i=1}^k(\underline{w}_{n,i}\underline{\phi}_i + \underline{w}_{n,i-1}\underline{\phi}_{i-1} + \bar{w}_{n,i}\bar{\phi}_i + \bar{w}_{n,i-1}\bar{\phi}_{i-1})(\upsilon_i - \upsilon_{i-1})$$
#
# Where *w* are the weights.
#
# **Step 4: calculate the SR using the obtained values**
#
# The previously computed functions $\hat{E}(\Upsilon,\bar{\omega})$ and $\hat{F}(\Upsilon,\bar{\omega})$
# are substituted into the following formula to calculate the Sharpe ratio.
#
# $$SR = \frac{\hat{E}(\Upsilon,\bar{\omega}) - \frac{2(\bar{\omega}-\theta)}{ln(1-2\Upsilon)}}{\sqrt{\hat{F}(\Upsilon,\bar{\omega}) - (\hat{E}(\Upsilon,\bar{\omega}))^2 + \frac{4(\Upsilon + ln(1-2\Upsilon)(\bar{\omega}-\theta)\hat{E}(\Upsilon,\bar{\omega})}{(ln(1-2\Upsilon))^2}}}$$
#
#
# ***


# To find the optimal thresholds for the data provided by the user we maximize the calculated SR with respect to
# $\bar{\pi}\geq0,\underline{\pi}\leq0$**


# ## How to use the heat potentials module
#
# This module gives the ability to calculate optimal values for stop-loss and profit-taking level to construct the trading rule.


# Imports


from arbitragelab.optimal_mean_reversion import OrnsteinUhlenbeck
from arbitragelab.optimal_mean_reversion import HeatPotentials
import matplotlib.pyplot as plt
import numpy as np


# Data preparation:
#
# Let's generate the OU data sample to example the whole model usage cycle.


# Generating the sample OU data
ou_data = OrnsteinUhlenbeck()

data = ou_data.ou_model_simulation(n=1000, theta_given=0.03711, mu_given=65.3333,
                            sigma_given=0.3, delta_t_given=1/255)


# Let's plot our data example
plt.figure(figsize=(12, 7))
plt.plot(data);


# The next step would be to fit the OU model and obtain its parameters:


# To get the model parameters we need to fit the OU model to the data

# Assign the delta value
ou_data.delta_t = 1/252

# Model fitting
ou_data.fit_to_portfolio(data)

# Now we obtained the parameters to use for our optimization procedure
theta,mu,sigma = ou_data.theta,ou_data.mu,np.sqrt(ou_data.sigma_square)


# Printing out fitted parameters
print('theta:', round(theta,5), '\nmu:', round(mu,5), '\nsigma:', round(sigma,5))


# Now we can perform the optimization process:


# To fit the model and calculate optimal thresholds we need to provide:
#
# * OU-model parameters that represent the data
# * The grid density
# * Maximum duration of the trade


# Establish the instance of the class
example = HeatPotentials()

# Fit the model and establish the maximum duration of the trade
example.fit(ou_params=(theta, mu, sigma), delta_grid=0.1, max_trade_duration=0.03)


# Let's calculate the optimal levels and SR separately. 
#
# To do so we can use the respective functions: 


# Calculate the initial optimal levels
levels = example.optimal_levels()


print('profit-taking threshold:', levels[0],
      '\nstop-loss threshold:', levels[1],
      '\nSharpe ratio:', levels[2])


# While calculating the Sharpe ratio separately we will use the rounded values of the optimal threshold to showcase the sensitivity of the approach to small differences in values.


# We can also calculate the Sharpe ratio for given scaled parameters
sr = example.sharpe_calculation(max_trade_duration=1.9599, optimal_profit=5.07525, optimal_stop_loss=-3.41002)

print(sr,'\ndelta:', levels[2]-sr)


# To get scaled back results that correspond to our model parameters we use the description function.


# To get the results scaled back to our initial model we call the description function
example.description()


# ## Conclusion
#
# This notebook describes the heat-potential approach to finding optimal trading rules for mean-reverting strategies.
#
# Key takeaways:
# * Ornstein-Uhlenbeck model is used to model the behavior of mean-reverting assets.
#
# * It is possible to use the heat potential approach on the financial data represented by an OU model.
#
# * We formulate the problem for the long strategy, short strategy results can be obtained by reflection.
#
# * The closed-form solution allows to formulate the optimization problem for calculating the optimal stop-loss and profit-taking thresholds.
#
# * The model is sensitive to small changes in the given data.


# ## Reference
# 1.  Lipton, Alex and López de Prado, Marcos, A Closed-Form Solution for Optimal Mean-Reverting Trading Strategies (February 8, 2020). Available at SSRN: https://ssrn.com/abstract=3534445 or http://dx.doi.org/10.2139/ssrn.3534445 
# 2. Prado, Marcos Lopez de. Advances in Financial Machine Learning. Wiley, 2018.



// ---------------------------------------------------

// ou_model.py
// arbitrage_research/Optimal Mean Reversion/ou_model.py
# Generated from: ou_model.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * References:  **Tim Leung and Xin Li** - *Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications (2015)*


# ## Abstract
#
# An Ornstein-Uhlenbeck process is a great tool for modeling the behavior of mean-reverting portfolio prices. Tim Leung, Xin Li in "Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications" (2015) present the solution to the optimal timing problems for entering and liquidating the position and the method of creating an optimal mean-reverting portfolio of two assets based on the Ornstein-Uhlenbeck model. Their findings also provide optimal solutions with respect to the stop-loss level if they are provided as an extension of a base problem.


# The following implementations and descriptions closely follow the work of Tim Leung: [Tim Leung and Xin Li Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919). Assets used to showcase the module were chosen based on the example in the publication.


# ## Mean-reverting portfolio
#
# To create a mean-reverting portfolio we *hold* $\alpha = \frac{A}{S_0^{(1)}}$ of a risky asset $S^{(1)}$ and *short* $\beta = \frac{B}{S_0^{(2)}}$, yielding a portfolio value:
# $$X_t^{\alpha,\beta} = \alpha S^{(1)} - \beta S^{(2)}, t \geq 0$$
# Both assets chosen should be correlated or co-moving. Since in terms of mean-reversion we care only about the ratio between $\alpha$ and $\beta$, without the loss of generality we can set $\alpha=const$ and A =  1 (that represents the amount of investment), while varying $\beta$ to find the optimal strategy $(\alpha,\beta^*)$
#


from IPython.display import Image


Image(filename='OU_model/GLD_GDX.png')


# _Historical price paths of gold (GLD) and VanEck Vectors Gold Miners ETF (GDX). An example of assets that can be used for mean-reverting portfolio creation._


# ## Ornstein-Uhlenbeck process
#
# We establish Ornstein-Uhlenbeck process driven by the SDE:
# $$dX_t = \mu(\theta - X_t)dt + \sigma dB_t,$$
# $$\mu, \sigma > 0,$$
# $$\theta \in \mathbb{R},$$
# $$B\ -\text{a standard Brownian motion}$$
#


# Where:
# * $\theta$ − long term mean level, all future trajectories of 𝑋 will evolve around a mean level 𝜃 in the long run.
#
# * $\mu$ - speed of reversion, characterizes the velocity at which such trajectories will regroup around $\theta$ in time.
#
# * $\sigma$ - instantaneous volatility, measures instant by instant the amplitude of randomness entering the system. Higher values imply more randomness.


# ## Model fitting
#
# To fit the OU model to the portfolio and also find the optimal $\beta^*$ we also have to use the probability density function of $X_t$ with increment  $\delta t = t_i - t_{i-1}$ :
#
#
# $$f^{OU} (x_i|x_{i-1};\theta,\mu,\sigma) = \frac{1}{\sqrt{2\pi\tilde{\sigma}^2}}exp(-\frac{(x_i - x_{i-1} e^{-\mu\Delta t} - \theta (1 - e^{-\mu \Delta t})^2)} {2 \tilde{\sigma}^2})$$


#
# $$\text{with the constant } \tilde{\sigma}^2 = \sigma^2 \frac{1 - e^{-2\mu\Delta t}}{2\mu}$$
#
# We observe the resulting portfolio values $(x_i^\beta)_{i = 0,1,\cdots,n}$ for every strategy $\beta$
# realized over an *n*-day period. To fit the model to our data and find optimal parameters we define the average log-likelihood function:
#
# $$\ell (\theta,\mu,\sigma|x_0^{\alpha\beta},x_1^{\alpha\beta},\cdots,x_n^{\alpha\beta}) := \frac{1}{n}\sum_{i=1}^{n} ln f^{OU}(x_i|x_{i-1};\theta,\mu,\sigma)$$


# $$= -\frac{1}{2} ln(2 \pi) - ln(\tilde{\sigma}) - \frac{1}{2\pi\tilde{\sigma}^2}\sum_{i=1}^{n} [x_i^{\alpha\beta} - x_{i-1}^{\alpha\beta} e^{-\mu \Delta t} - \theta (1 - e^{-\mu \Delta t})]^2$$
#
# Maximizing the log-likelihood function by applying maximum likelihood estimation(MLE) we are able to determine
# the parameters of the model and fit the observed portfolio prices to an OU process. Let's denote the maximized average
# log-likelihood by $\hat{\ell}(\theta^*,\mu^*,\sigma^*)$. Then for every $\alpha$ we choose
# $\beta^*$, where:
#
# $$\beta^* = \underset{\beta}{\arg\max}\ \hat{\ell}(\theta^*,\mu^*,\sigma^*|x_0^{\alpha\beta},x_1^{\alpha\beta},\cdots,x_n^{\alpha\beta})$$


Image(filename='OU_model/Optimal_mean_reverting_portfolio.png')


# _An optimal mean-reverting portfolio constructed with GLD and GDX assets using the average log-likelihood method._


# ## Optimal stopping problems
#
# Let's establish an optimal stopping problem. Suppose the investor already has a position with a **value process** $(X_t)_{t>0}$ that follows the OU process. When the investor closes his position at the time $\tau$ he receives the value $(X_{\tau})$ and pays a **constant transaction cost** $c_s \in \mathbb{R}$ Our goal is to maximize the expected discounted value, where $r > 0$ is the **subjective discount rate of liquidation**. To achieve that we need to solve the optimal stopping problem:
#
# $$V(x) = \underset{\tau \in T}{\sup} \mathbb{E}_x{e^{-r \tau} (X_{\tau} - c_s)| X_0 = x}$$
#
# $V(x)$ represents the expected liquidation value accounted with X.
#
# Current price plus transaction cost constitute the cost of entering the trade and in combination with $V(x)$ we can formalize the optimal entry problem:
#
# $$J(x) = \underset{\nu \in T}{\sup} \mathbb{E}_x{e^{-\hat{r} \tau} (V(X_{\nu}) - X_{\nu} - c_b)| X_0 = x}$$


# $$r,\hat{r}>0 \text{ - discount rates}$$


# $$c_s,c_b \in \mathbb{R} \text{ - transaction costs}$$


#
# ### The analytical solution to the optimal stopping problems:
#
# We denote the OU process infinitesimal generator:
#
# $$L = \frac{\sigma^2}{2} \frac{d^2}{dx^2} + \mu(\theta - x) \frac{d}{dx}$$
#
# and recall the classical solution of the differential equation
#
# $$L u(x) = ru(x)$$


# $$F(x) = \int_{0}^{\infty} u^{ \frac{r}{\mu} - 1} e^{\sqrt{\frac{2\mu}{\sigma^2}}(x - \theta)u - \frac{u^2}{2}}du$$


# $$G(x) = \int_{0}^{\infty} u^{\frac{r}{\mu} - 1} e^{\sqrt{\frac{2\mu}{\sigma^2}} (\theta - x)u - \frac{u^2}{2}}du$$


# #### The analytical solutions can be divided into two parts:
#
# * The default solutions
#
#     **Theorem 2.6 (p.23).**
#     The *optimal liquidation problem* admits the solution:
#
#     $$ V(x) = \begin{cases} (b^* - c_s) \frac{F(x)}{F(b^*)} , & \mbox{if } x \in (-\infty,b^*) \\ x - c_s, &  \mbox{ otherwise}  \end{cases}$$
#
#     The optimal liquidation level $b^*$ is found from the equation:
#
#     $$F(b) - (b - c_s)F'(b) = 0$$
#
#     **Theorem 2.10 (p.27).**
#     The *optimal entry timing problem* admits the solution:
#
#     $$ J(x) = \begin{cases} V(x) - x - c_b, & \mbox{if } x \in (-\infty,d^*)\\ \frac{V(d^*) - d^* - c_b}{\hat{G}(d^*)}, & \mbox{if } x \in (d^*, \infty)  \end{cases}$$
#
#     The optimal entry level $d^*$ is found from the equation:
#
#     $$\hat{G}(d)(V'(d) - 1) - \hat{G}'(d)(V(d) - d - c_b) = 0$$
#
#


# * The solutions with the inclusion of stop-loss
#
#     **Theorem 2.13 (p.31):**
#     The *optimal liquidation problem with respect to stop-loss level* admits the solution:
#
#     $$V(x) = \begin{cases} C F(x)+D G(x) , & \mbox{if } x \in (-\infty,b^*)\\ x - c_s, & \mbox{ otherwise}  \end{cases}$$
#
#     The **optimal liquidation level** $b_L^*$ is found from the equation:
#
#     $$F'(b) [(L - c_s) G(b) - (b - c_s) G(L)] + G'(b) [(b - c_s) F(L) - (L - c_s) F(b)] - G(b) F(L) - G(L)F(b) = 0$$
#
#     Helper functions C and D defined as following:
#
#     $$C = \frac{(b_L^* - c_s) G(L) - ( L - c_s) G(b^*)}{F(b_L^*)G(L) - F(L)G(b_L^*)}$$
#
#     $$D = \frac{(L - c_s) F(L) - ( b_L^* - c_s) F(b^*)}{F(b_L^*)G(L) - F(L)G(b_L^*)}$$
#
#     **Theorem 2.42 (p.35):**
#     The *optimal entry timing problem with respect to stop-loss level* admits the solution:
#
#     $$J_L(x) = \begin{cases} P\hat{F}(x),  & \mbox{if } x \in (-\infty,a_L^*)\\ V_L(x) - x - c_b, & \mbox{if } x \in (a_L^*, d_L^*)\\  Q\hat{G}(x), & \mbox{if } x \in (d_L^*, \infty)\end{cases}$$
#
#     The **optimal entry interval** $(a_L^*,d_L^*)$ is found using the respective equations:
#
#     $$\hat{G}(d)(V_L'(d) - 1) - \hat{G}'(d)(V_L(d) - d - c_b) = 0$$
#
#     $$\hat{F}(a)(V_L'(a) - 1) - \hat{F}'(a)(V_L(a) - a - c_b) = 0$$
#


# ## How to use the OU module
#
# This module gives the ability to calculate optimal values for entering and liquidating the position for your portfolio. 
#
# **Note:** It is important that the model returns the *single* best pair of one-time entry/liquidation values. 


# Our main incentive in using this model is fairly simple:
#
# * If not already entered, enter when the price reaches optimal entry level.
#
# * If not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# In the case of the optimal timing solutions with the inclusion of the stop-loss level, the level of entry becomes the first bound of the entry interval to be reached by the portfolio price.
#
# This module also can be used on already constructed mean-reverting portfolios by providing the one-dimensional array or dataframe as input data.


import arbitragelab.optimal_mean_reversion as omr
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt


# ### Data preparation:


# To showcase the module functionality we will use GLD and GDX data from Yahoo Finance:


# Import data from Yahoo finance
data1 =  yf.download("GLD GDX", start="2012-03-25", end="2013-12-09")
data2 =  yf.download("GLD GDX", start="2015-12-10", end="2016-02-20")
data3 =  yf.download("GLD GDX", start="2016-02-21", end="2020-08-20")

data1.head()


# To build the optimal portfolio, first we create a *pd.DataFrame* or a *np.array* of two asset prices that we are using.  In the following cell we separate the data into 3 types of arrays for *initial training*, *testing and retraining the model* and *testing the retrained model*.


# You can use the pd.DataFrame of two asset prices
data_train_dataframe = data1["Adj Close"][["GLD", "GDX"]]

# And also we can create training dataset as an array of two asset prices
data_train = np.array(data1["Adj Close"][["GLD", "GDX"]])

# Create an out-of-sample dataset
data_test_and_retrain = data2["Adj Close"][["GLD", "GDX"]]

data_test_the_retrained = np.array(data3["Adj Close"][["GLD", "GDX"]])

data_train.shape


# **NOTE:** It is important which one of your assets you decide to long and which you decide to short since the outcome of the model and existence of the solution depend on that. Therefore, we have to be mindful of the order of asset prices in our array or dataframe used for training. The one that you intend to long has to be put first, and the one you are shorting - second.


# We are longing the GLD and shorting the GDX
data_train_dataframe.head()


# Logically we can divide the module usage proces into four steps: 
# * model fitting
# * optimal levels calculation
# * showcasing the result
# * model retraining


# ### Step 1: Data training


# First of all, we use a `fit` function to find the optimal ratio between the assets for our portfolio to achieve maximum mean reversion and then fit the OU model to our optimal portfolio. During this step, we also set the discount ratios, transaction costs for entering/exiting the position and data frequency. Adding the stop loss level is optional and can be added or changed along the way.


# Set up a class object
example = omr.OrnsteinUhlenbeck()


# After that we fit the model to the training data and allocate data frequency,
# transaction costs, discount rates and stop-loss level


# You can use the *np.array* as an input data: 


# You can input the np.array as data 
example.fit(data_train, data_frequency="D", discount_rate=[0.05, 0.05],
            transaction_cost=[0.02, 0.02], stop_loss=0.2)

# The parameters can be allocated in an alternative way
example.fit(data_train_dataframe, data_frequency="D", discount_rate=0.05,
            transaction_cost=0.02, stop_loss=0.2)


# You also can use the *pd.DataFrame* as an input data: 


# Chosen data type can be pd.DataFrame
example.fit(data_train_dataframe, data_frequency="D", discount_rate=[0.05, 0.05],
            transaction_cost=[0.02, 0.02], stop_loss=0.2)

# In this case we can also specify the interval we want to use for training
example.fit(data_train_dataframe, data_frequency="D", discount_rate=[0.05, 0.05],
            start="2012-03-27", end="2013-12-08",
            transaction_cost=[0.02, 0.02], stop_loss=0.2)


# Stop-loss level, transaction costs and discount rates
# can be changed along the way


example.L = 0.3


# The crucial point is to understand how well is your input data represented by the fitted model. Is it suitable to fit an OU process to it?
#
# To check we are using the `check_fit` function. The main incentive is to have simulated *max log-likelihood* (mll) function close to a fitted one. If the simulated mll is much greater it means that data provided to the model is not good enough to be modeled by an OU process. 
#
# The check can occasionally return numbers significantly different from the fitted mll due to the random nature of the simulated process and the possibility of outliers. We advise to perform the check multiple times and if it's consistently showing very different values of *mll* you might suspect the unsuitability of your data.


example.check_fit()


# Our data passes the fitness check since the difference between mll's is relatively small.


# ### Step 2: Optimal levels calculation


# If you need to calculate the optimal levels separately to use for your trading strategy you should call the function for the respective optimal level.


# To calculate the optimal entry of liquidation levels separately
# you need to use following functions


# Calculate the optimal liquidation level
b = example.optimal_liquidation_level()

# Calculate the optimal entry level
d = example.optimal_entry_level()

# Calculate the optimal liquidation level accounting for stop-loss
b_L = example.optimal_liquidation_level_stop_loss()

# Calculate the optimal entry interval accounting for stop-loss
interval_L = example.optimal_entry_interval_stop_loss()


print("b*=",np.round(b, 4),"\nd*=",np.round(d, 4),"\nb_L*=",np.round(b_L, 4),"\n[a_L*,d_L*]=",np.round(interval_L, 4))


# ### Step 3: Showcasing the results


# To showcase all the parameters of the fitted model and found optimal levels we use the `description` function.


# If the stop-loss level is not set all the functions that are using it will not be calculated.


# Setting the stop-loss level to "None"
example.L = None

# Call the description function to see all the model's parameters and optimal levels
example.description()


# We deleted our stop-loss level in the previous cell, lets set it back to showcase the how the description function works wth the stop-loss.


# Setting the stop-loss level back to previous value
example.L = 0.3

# Call the description function to see all the model's parameters and optimal levels
example.description()


# We can also showcase our results on any data we choose by calling the `plot_levels` function. When provided an np.array or pd.DataFrame for two asset prices it uses the previously found optimal coefficient to create a portfolio out of them and plots it and the found optimal levels.


# Showcasing the results on the training data (pd.DataFrame)
fig = example.plot_levels(data=data_train_dataframe, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# Showcasing the results on the test data (np.array)
fig = example.plot_levels(data=data_test_and_retrain, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# During the first test period none of the optimal levels are reached yet.


# ### Step 4: Retraining the model


# The next possiple step to take is to retrain the model.For that we are using `fit_to_assets` or `fit_to_data` depending on the input data.  
# As input, you can either use new data or if you used pd.DataFrame during the fitting process, you can retrain the model on a different time interval of your already provided data.


# Retrain the model on the different training interval from already provided data
example.fit_to_assets(start="2012-05-27", end="2013-05-08")

# Showcase the results of retraining on the different training period of already provided data:
example.L = None
example.description()


# Lets use a new training data:


# Retrain the model on new data
example.fit_to_assets(data=data_test_and_retrain)

# Showcase the results of retraining on the new data:
example.L = 0.5
example.description()


# And now plotting our results on the data from the `data_test_retrained` dataset.


fig = example.plot_levels(data_test_the_retrained, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# Here we can see that both optimal entry and exit levels are reached at some point of time during this observation period. After that moment it is advised to retrain the model and continue the process.


# ### Additional functionality


# **Half-life**
#
# Since the half-life parameter of the OU model is widely used in researches we also implemented the function that calculates it. 
#
# *Half-life represents the average time that it takes for the process to revert to it's long term mean on a half of its initial deviation.*


example.half_life()


# **Generating synthetic mean-reverting data**


# We can also use `ou_model_simulation` to generate synthetic data to test our model on or for other purposes. The function uses the parameter values of previously fitted model by default but it can be used with your own OU model paramenters. 


# Syntetic data generated with fitted model parameters
ou_fitted = example.ou_model_simulation(n=400)

plt.plot(ou_fitted)


# Presuming we are using daily data
delta_t = 1/252
# Syntetic data generated with given model parameters
ou_given = example.ou_model_simulation(n=400, theta_given=0.7, mu_given=21,
                                       sigma_given=0.3, delta_t_given=delta_t)

plt.plot(ou_given)


# You can plot the previously found optimal levels on generated data

fig = example.plot_levels(ou_given, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# As an another application you can train and test the model on the simulated data.


# Creating the dataset
ou_train_given = example.ou_model_simulation(n=400, theta_given=0.7, mu_given=12,
                                       sigma_given=0.1, delta_t_given=delta_t)

# Training our model on simulated data
example.fit(ou_train_given, data_frequency="D", discount_rate=[0.05, 0.05],
            transaction_cost=[0.02, 0.02], stop_loss=0.55)

# Showcasing the model's details
example.description()


# Creating a testing dataset
ou_test_given = example.ou_model_simulation(n=1000, theta_given=0.7, mu_given=12,
                                            sigma_given=0.15, delta_t_given=delta_t)

# Plotting found optimal levels on a testing dataset
fig = example.plot_levels(ou_test_given, stop_loss=True)
fig.set_figheight(15)
fig.set_figwidth(10)


# *“Our model can be considered as the building block for the problem with any finite number of sequential trades”* – Professor Tim Leung and Xin Li, 2015.


# ## Conclusion


# This notebook describes the Ornstein-Uhlenbeck (OU) model and how it is applied to mean reverting portfolios. The main focus was portfolio optimization for pairs trading and the optimal timing of trades.
#
# Both techniques were introduced by *Tim Leung and Xin Li* in [**Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications**](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919)
#
# Key takeaways from the notebook:
#
# * Ornstein-Uhlenbeck model is used to model the behavior of mean-reverting assets.
#
# * Using the OU model we can create an optimal mean-reverting portfolio with a maximum level of mean reversion and the best fit. We achieve both goals using the average loglikelihood function.
#
# * For the OU process, we can formulate an optimal stopping problem that allows us to maximize the expected discounted value of entering or liquidating the position.
#
# * The problem can be solved for the case with a defined stop-loss level or without it.
#
# * The model should be considered as a building block for the problem with any finite number of sequential trades.
#
# * Main idea behind the use of the model is:
#
#     * If position is not already entered, enter when the price reaches optimal entry level.
#
#     * If position is not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# * The model is built on a premise of one entry and one exit point during the observation period.
#
#
#


# ## Reference


# 1. Leung, Tim, and Xin Li. Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications. World Scientific Publishing Company, 2451. 



// ---------------------------------------------------

// cir_model.py
// arbitrage_research/Optimal Mean Reversion/cir_model.py
# Generated from: cir_model.ipynb
# Warning: This is an auto-generated file. Changes may be overwritten.

# * References:  **Tim Leung and Xin Li** - *Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications (2015)*


# ## Abstract
#
# The Cox-Ingersoll-Ross model or CIR model is a process mostly used in mathematical finance to describe the evolution of interest rates. It was first introduced in 1985 by John C. Cox, Jonathan E. Ingersoll and Stephen A. Ross as an extension of the Vasicek model. 
#
# Tim Leung, Xin Li in "Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications" (2015) present the solution to the optimal timing problems for one-time entering and liquidating the position and also provide the ability to find optimal levels for an infinite amount of trades based on CIR process.
#
# The following implementations and descriptions closely follow the work of Tim Leung: Tim Leung and Xin Li Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications. Data used to showcase the module was chosen based on the example in the publication.


# ## Cox-Ingersoll-Ross model


# A CIR process satisfies the following stochastic differential equation:
# $$dY_t = \mu(\theta - Y_t)dt + \sigma \sqrt{Y_t} dB_t,$$
#
# $$\theta, \mu, \sigma > 0,$$
# $$B\ -\text{a standard Brownian motion}$$


# * $\theta$ − long term mean level, all future trajectories of Y will evolve around a mean level 𝜃 in the long run.
#
# * $\mu$ - speed of reversion, characterizes the velocity at which such trajectories will regroup around $\theta$ in time.
#
# * $\sigma$ - instantaneous volatility, measures instant by instant the amplitude of randomness entering the system. Higher values imply more randomness.


# The standard deviation factor, $\sigma \sqrt {Y_{t}}$, avoids the possibility of negative $Y_{t}$ values for all positive values of a $\theta$ and $\mu$.


# To fit the model to given data we define the log-likelihood function as in Borodin and Salminen (2002):
#
# $$ \ell (\theta,\mu,\sigma|y_0,y_1,\cdots,y_n) := \frac{1}{n}\sum_{i=1}^{n} ln
#     f^{CIR}(y_i|y_{i-1};\theta,\mu,\sigma)$$


# $$= -ln(\tilde{\sigma}) - \frac{1}{n\tilde{\sigma}}\sum_{i=1}^{n} [y_i +y_{i-1}e^{-\mu \Delta t}] - \frac{1}{n} \sum_{i=1}^{n} [\frac{q}{2}ln(\frac{y_i}{y_{i-1}e^{-\mu\Delta t}}) - ln I_q(\frac{2}{\tilde{\sigma}^2}\sqrt{y_i y_{i-1}e^{-\mu\Delta t}})$$


# By applying the maximum likelihood estimation(MLE) method we are able to determine
# the parameters of the model and fit the observed portfolio prices to a CIR process.


# ## Optimal stopping problem
#
# > This approach presumes that the investor wants to commit only two trades: entering the position, and
#     liquidating it.
#
# Suppose the investor already has the position the value process of which $(Y_t)_{t>0}$ follows the CIR process. After liquidating the position at the time $\tau$ e receives the value $(Y_{\tau})$ and pays a
# constant transaction cost $c_s \in \mathbb{R}$ To maximize the expected discounted value we need to solve
# the optimal stopping problem:


# $$ V^{\chi}(y) = \underset{\tau \in T}{\sup} \mathbb{E}({e^{-r \tau} (Y_{\tau} - c_s)| Y_0 = y})$$
#
# where $T$ denotes the set of all possible stopping times and $r > 0$ is our subjective constant
# discount rate. $V^{\chi}(y)$ represents the expected liquidation value accounted with y.
#
# Current price plus transaction cost  constitute the cost of entering the trade and in combination with $V^{\chi}(y)$
# we can formalize the optimal entry problem:
#
#
# $$J^{\chi}(y) = \underset{\nu \in T}{\sup} \mathbb{E}({e^{-\hat{r} \tau} (V^{\chi}(Y_{\nu}) - Y_{\nu} - c_b)| Y_0 = y})$$
#
# with
#
# $$\hat{r}>0,\ c_b \in \mathbb{R}$$
#
# As an investor our goal is to maximize the expected difference between the current price
# of the position - $Y_{\nu}$ and its' expected liquidation value $V^{\chi}(Y_{\nu})$ minus transaction cost
# $c_b$


# ### The solutions:


#
# Theorem 4.2 (p.85):
#
# **The optimal liquidation problem admits the solution:**
#
# $$
#     V^{\chi}(x) = \begin{cases} (b^{\chi*} - c_s) \frac{F^{\chi}(x)}{F^{\chi}(b^{\chi*})} , & \mbox{if } x \in [0,b^{\chi*})\\
#     \\x - c_s, &  \mbox{ otherwise}  \end{cases}\\
# $$
#
# The optimal liquidation level $b^{\chi*}$ is found from the equation:
#
# $$F^{\chi} (b^{\chi}) - (b^{\chi} - c_s)F'^{\chi}(b^{\chi}) = 0$$
#
# Corresponding optimal liquidation time is given by
#
# $$\tau^{\chi*} = inf [t\geq0:Y_t \geq b^{\chi*}]$$
#
# Theorem 4.4 (p.86):
#
# **The optimal entry timing problem admits the solution:**
#
# $$
#     J(x) = \begin{cases} V^{\chi}(x) - x - c_b, & \mbox{if } x \in [0,d^{\chi*}) \\
#     \\\frac{V^{\chi}(d^{\chi*}) - d^{\chi*} - c_b}{\hat{G^{\chi}}(d^{\chi*})}, & \mbox{if } x \in (d^{\chi*}, \infty)  \end{cases}
# $$
#
# The optimal entry level $d^{\chi*}$ is found from the equation:
#
# $$ \hat{G}^{\chi}(d^{\chi})(V'^{\chi}(d^{\chi}) - 1) - \hat{G}'^{\chi}(d^{\chi})(V^{\chi}(d^{\chi}) - d^{\chi} - c_b) = 0$$
#
# Where "$\hat{\ }$" represents the use of transaction cost and discount rate of entering.


# ## Optimal switching problem
#
# > This approach presumes that the investor can commit an infinite number of trades.
#
#
# If there is no limit on the number of times the investor will open or close the position, the sequential trading times
# are modelled by the stopping times $\nu_1,\tau_1,\nu_2,\tau_2,... \in T$ such that
#
# $$0\leq\nu_1\leq\tau_1\leq\nu_2\leq\tau_2\leq...$$
#
# Where $\nu_i$ are times when the share of a risky asset was bought and $\tau_i$ - when it was sold. In the case
# of pairs trading, we consider our spread as such an asset.


# To find the optimal levels, first, two critical constants have to be denoted:
#
# $$y_s:=\frac{\mu\theta+rc_s}{\mu+r} \\
#     y_b:=\frac{\mu\theta-rc_b}{\mu+r}$$


# Theorem 4.7 (p.56):
#
# **Under optimal switching approach it is optimal to re-enter the market if and only if all of the following conditions
# hold true:**
#
# a) If $y_b>0$
#
# b) The following inequality must hold true:
#
# $$ c_b < \frac{b^{\chi*}-c_s}{F^{\chi}(b^{\chi*})}$$
#
# In case any of the conditions are not met - re-entering the market is deemed not optimal. It would be advised to exit
# at the optimal liquidation price without re-entering, or not enter the position at all. The difference between the
# options depends on whether the investor had already entered the market beforehand,
# or did he or she start with a zero position.


# ## How to use the CIR module
#
# For this module, the most suitable input would be a mean-reverting portfolio or an array of two correlated or co-moving asset prices.
#
# Both optimal stopping and optimal switching levels are used alike in determining the rules of our trading strategy:
#
#
# * If not already entered, enter when the price reaches optimal entry level.
#
# * If not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# However, the difference lies in the applications of the two approaches. The optimal stopping usually 
# has a much longer delay between the buy and sell levels are reached and offers a bigger gain than a one-time entry and liquidation using the optimal switching levels. The optimal switching levels, on the other hand, may provide a bigger cumulative gain by performing the trades multiple times during the same time period.


# Imports:


import numpy as np
import matplotlib.pyplot as plt

from arbitragelab.optimal_mean_reversion import CoxIngersollRoss


# Let's establish a testing sample. Alongside with the numerical example from the book we use generated CIR data with the following parameters:
#
# $\theta$=0.2; $\mu$=0.2; $\sigma$=0.3 


# During this step, we also demonstrate the ability of our model to generate artificial CIR data based on given parameters


# Creating a class instance
example = CoxIngersollRoss()

# We establish our training sample
delta_t = 1/252
np.random.seed(30)
cir_example =  example.cir_model_simulation(n=1000, theta_given=0.2, mu_given=0.2,
                                            sigma_given=0.3, delta_t_given=delta_t)


# Plotting the generated CIR process
plt.figure(figsize=(12, 7))
plt.plot(cir_example);


# Model fitting uses the same functional structure as the OU and XOU modules:


# Model fitting
example.fit(cir_example, data_frequency="D", discount_rate=0.05,
            transaction_cost=[0.001, 0.001])


# Optimal stopping levels can be found separately via respective functions.


# You can separately solve optimal stopping
# and optimal switching problems

# Solving the optimal stopping problem
b = example.optimal_liquidation_level()

d = example.optimal_entry_level()


print("Optimal liquidation level:", round(b,5),
      "\nOptimal entry level:", round(d, 5))


# For the optimal switching level exists a separate function. 


# Solving the optimal switching problem
d_switch, b_switch = example.optimal_switching_levels()


print("Optimal liquidation level:", round(b_switch,5),
      "\nOptimal entry level:", round(d_switch, 5))


# To test the obtained results let's simulate the CIR process based on the fitted model using the respective function.


# Generating a CIR process to calculate the optimal levels
np.random.seed(30)
cir_test = example.cir_model_simulation(n=1000)


# To visualize all the obtained results we use the `cir_plot_levels` function. 


# You can display the results using the plot
fig = example.cir_plot_levels(cir_test, switching=True)

# Adjusting the size of the plot
fig.set_figheight(7)
fig.set_figwidth(12)


# Or you can view the model statistics


# Or you can view the model statistics
example.cir_description(switching=True)


# ## Conclusion


# This notebook describes the Cox-Ingersoll-Ross (CIR) model and how it is applied to mean reverting portfolios. The main goal of the notebook is to show the usage of the module to obtain solutions to the optimal stopping and optimal switching problems.
#
# Both techniques were introduced by *Tim Leung and Xin Li* in [**Optimal Mean reversion Trading: Mathematical Analysis and Practical Applications**](https://www.amazon.com/Optimal-Mean-Reversion-Trading-Mathematical/dp/9814725919)
#
# Key takeaways from the notebook:
#
# * Cox-Ingersoll-Ross model is a tool used to model the behavior of mean-reverting assets.
#
# * Main idea behind the use of the  optimal levels is:
#
#     * If the position is not already entered, enter when the price reaches the optimal entry level.
#
#     * If the position is not already liquidated, liquidate when the price reaches optimal liquidation level.
#
# * An optimal stopping problem formulated for the CIR process allows us to maximize the expected discounted value of one-time entering or liquidating the position by finding the optimal price levels at which trades should be committed.
#
# * An optimal switching problem, on the other hand, allows us to maximize the expected discounted value of an infinite amount of trades(entering or liquidating the position). By finding the optimal price levels at which the repeated trades should be committed for the maximum overall gain. 
#
# * The model should be considered as a building block for the problem with any finite number of sequential trades.


# ## Reference


# 1. Leung, Tim, and Xin Li. Optimal Mean Reversion Trading: Mathematical Analysis and Practical Applications. World Scientific Publishing Company, 2451. 



// ---------------------------------------------------

