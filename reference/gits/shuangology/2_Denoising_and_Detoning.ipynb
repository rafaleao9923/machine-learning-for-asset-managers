{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 Denoising and Detoning\n",
    "\n",
    "reduce the noise and enhance the signal included in an empirical covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:25.763346Z",
     "start_time": "2020-08-27T11:16:24.827832Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np,pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Marcenko-Pastur Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNIPPET 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:25.775379Z",
     "start_time": "2020-08-27T11:16:25.766174Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "def mpPDF(var, q, pts):\n",
    "    # Marcenko-Pastur pdf\n",
    "    # q=T/N\n",
    "    # when var= 1, C = T^-1 X'X  is the correlation matrix associated with X\n",
    "    # lambda+ =,lambda- = eMax, eMin\n",
    "    eMin, eMax = var*(1-(1./q)**.5)**2, var*(1+(1./q)**.5)**2\n",
    "    eVal = np.linspace(eMin, eMax, pts)\n",
    "    pdf = q/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5\n",
    "    # pdf = pdf.ravel()\n",
    "    pdf = pd.Series(pdf, index=eVal)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SNIPPET 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:27.094305Z",
     "start_time": "2020-08-27T11:16:26.323828Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def getPCA(matrix):\n",
    "    # Get eVal,eVec from a Hermitian matrix\n",
    "    eVal, eVec = np.linalg.eigh(matrix)\n",
    "    indices = eVal.argsort()[::-1]  # arguments for sorting eVal desc\n",
    "    eVal, eVec = eVal[indices], eVec[:, indices]\n",
    "    eVal = np.diagflat(eVal)\n",
    "    return eVal, eVec\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def fitKDE(obs, bWidth=.25, kernel='gaussian', x=None):\n",
    "    # Fit kernel to a series of obs, and derive the prob of obs\n",
    "    # x is the array of values on which the fit KDE will be evaluated\n",
    "    if len(obs.shape) == 1:\n",
    "        obs = obs.reshape(-1, 1)\n",
    "    kde = KernelDensity(kernel=kernel, bandwidth=bWidth).fit(obs)\n",
    "    if x is None:\n",
    "        x = np.unique(obs).reshape(-1, 1)\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.reshape(-1, 1)\n",
    "    logProb = kde.score_samples(x)  # log(density)\n",
    "    pdf = pd.Series(np.exp(logProb), index=x.flatten())\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:28.409053Z",
     "start_time": "2020-08-27T11:16:27.101220Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "x = np.random.normal(size=(10000, 1000))\n",
    "eVal0, eVec0 = getPCA(np.corrcoef(x, rowvar=False)\n",
    "                      )  # each column is a variable\n",
    "pdf0 = mpPDF(1., q=x.shape[0]/float(x.shape[1]), pts=1000)\n",
    "pdf1 = fitKDE(np.diag(eVal0), bWidth=.01)  # empirical pdf\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(pdf0, label='Marcenko-Pastur')\n",
    "ax.plot(pdf1, linestyle='--', label='Empirical:KDE')\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "ax.set_ylabel(r'prob[$\\lambda$]')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T13:21:47.400283Z",
     "start_time": "2020-08-18T13:21:47.397362Z"
    }
   },
   "source": [
    "\n",
    "## Random Matrix with Signal (not perfectly random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:29.216047Z",
     "start_time": "2020-08-27T11:16:28.412271Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.3 ADD SIGNAL TO A RANDOM COVARIANCE MATRIX\n",
    "def getRndCov(nCols, nFacts):\n",
    "    w = np.random.normal(size=(nCols, nFacts))\n",
    "    cov = np.dot(w, w.T)  # random cov matrix, however not full rank\n",
    "    cov += np.diag(np.random.uniform(size=nCols))  # full rank cov\n",
    "    return cov\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def cov2corr(cov):\n",
    "    # Derive the correlation matrix from a covariance matrix\n",
    "    std = np.sqrt(np.diag(cov))\n",
    "    corr = cov/np.outer(std, std)\n",
    "    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error\n",
    "    return corr\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "alpha, nCols, nFact, q = .995, 1000, 100, 10\n",
    "cov = np.cov(np.random.normal(size=(nCols*q, nCols)), rowvar=False)\n",
    "cov = alpha*cov+(1-alpha)*getRndCov(nCols, nFact)  # noise+signal\n",
    "corr0 = cov2corr(cov)\n",
    "eVal0, eVec0 = getPCA(corr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:30.137094Z",
     "start_time": "2020-08-27T11:16:29.220414Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.4 FITTING THE MARCENKO–PASTUR PDF\n",
    "from scipy.optimize import minimize\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def errPDFs(var, eVal, q, bWidth, pts=1000):\n",
    "    # Fit error\n",
    "    var = var[0]\n",
    "    pdf0 = mpPDF(var, q, pts)  # theoretical pdf\n",
    "    pdf1 = fitKDE(eVal, bWidth, x=pdf0.index.values)  # empirical pdf\n",
    "    # import pdb; pdb.set_trace()\n",
    "    sse = np.sum((pdf1-pdf0)**2)\n",
    "    return sse\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def findMaxEval(eVal, q, bWidth):\n",
    "    # Find max random eVal by fitting Marcenko’s dist\n",
    "    out = minimize(lambda *x: errPDFs(*x), .5,\n",
    "                   args=(eVal, q, bWidth), bounds=((1E-5, 1-1E-5),))\n",
    "    if out['success']:\n",
    "        var = out['x'][0]\n",
    "    else:\n",
    "        var = 1\n",
    "    eMax = var*(1+(1./q)**.5)**2\n",
    "    return eMax, var\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth=.01)\n",
    "nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)\n",
    "\n",
    "# nFacts0 gives the number of the eigenvalue is assumed to be important \n",
    "# (cutoff level lambda+ adjusted for the presence of nonrandom eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:33.439928Z",
     "start_time": "2020-08-27T11:16:30.139726Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Fitting the Marcenko–Pastur PDF on a noisy covariance matrix.\n",
    "# estimate the sigma for Marcenko-Pastur dist\n",
    "bWidth = 0.01\n",
    "out = minimize(lambda *x: errPDFs(*x), .5,\n",
    "               args=(np.diag(eVal0), q, bWidth), bounds=((1E-5, 1-1E-5),))\n",
    "if out['success']:\n",
    "    var = out['x'][0]\n",
    "else:\n",
    "    var = 1\n",
    "\n",
    "pdf0 = mpPDF(var, q, pts=1000)  # Marcenko-Pastur dist\n",
    "pdf1 = fitKDE(np.diag(eVal0), bWidth=.01)  # empirical pdf\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(pdf0, label='Marcenko-Pastur dist')\n",
    "ax.bar(pdf1.index, pdf1.values, width=bWidth,\n",
    "       label='Empirical dist', color='darkorange')\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "ax.set_ylabel(r'prob[$\\lambda$]')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T14:53:35.920500Z",
     "start_time": "2020-08-18T14:53:35.899509Z"
    }
   },
   "source": [
    "## 2.5 Denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 Constant Residual Eigenvalue Method\n",
    "\n",
    "setting a constant eigenvalue for all random eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:33.682854Z",
     "start_time": "2020-08-27T11:16:33.442273Z"
    }
   },
   "outputs": [],
   "source": [
    "def denoisedCorr(eVal, eVec, nFacts):\n",
    "    # Remove noise from corr by fixing random eigenvalues\n",
    "    eVal_ = np.diag(eVal).copy()\n",
    "    eVal_[nFacts:] = eVal_[nFacts:].sum()/float(eVal_.shape[0] -\n",
    "                                                nFacts)  # average the rest\n",
    "    eVal_ = np.diag(eVal_)\n",
    "    corr1 = np.dot(eVec, eVal_).dot(eVec.T)\n",
    "    corr1 = cov2corr(corr1)\n",
    "    return corr1\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "corr1 = denoisedCorr(eVal0, eVec0, nFacts0)\n",
    "eVal1, eVec1 = getPCA(corr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:33.972914Z",
     "start_time": "2020-08-27T11:16:33.685496Z"
    }
   },
   "outputs": [],
   "source": [
    "# A comparison of eigenvalues before and after applying the residual eigenvalue method.\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(np.diagonal(eVal0), label='Original eigen-function')\n",
    "ax.plot(np.diagonal(eVal1),\n",
    "        label='Denoised eigen-function (Constant Residual)', linestyle='--')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Eigenvalue number')\n",
    "ax.set_ylabel('Eigenvalue (log-scale)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 Targeted Shrinkage\n",
    "$\\alpha$ regulates the amount fo shrinkage among the eigen vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:34.218679Z",
     "start_time": "2020-08-27T11:16:33.976086Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.6 DENOISING BY TARGETED SHRINKAGE\n",
    "def denoisedCorr2(eVal, eVec, nFacts, alpha=0):\n",
    "    # Remove noise from corr through targeted shrinkage\n",
    "    eValL, eVecL = eVal[:nFacts, :nFacts], eVec[:, :nFacts]\n",
    "    eValR, eVecR = eVal[nFacts:, nFacts:], eVec[:, nFacts:]\n",
    "    corr0 = np.dot(eVecL, eValL).dot(eVecL.T)\n",
    "    corr1 = np.dot(eVecR, eValR).dot(eVecR.T)\n",
    "    corr2 = corr0+alpha*corr1+(1-alpha)*np.diag(np.diag(corr1))\n",
    "    return corr2\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "corr1 = denoisedCorr2(eVal0, eVec0, nFacts0, alpha=.5)\n",
    "eVal1, eVec1 = getPCA(corr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:34.531019Z",
     "start_time": "2020-08-27T11:16:34.220304Z"
    }
   },
   "outputs": [],
   "source": [
    "# A comparison of eigenvalues before and after applying the residual eigenvalue method.\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(np.diagonal(eVal0), label='Original eigen-function')\n",
    "ax.plot(np.diagonal(eVal1),\n",
    "        label='Denoised eigen-function (targeted shrinkage)', linestyle='--')\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Eigenvalue number')\n",
    "ax.set_ylabel('Eigenvalue (log-scale)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results\n",
    "## 2.7.1 Minimum Variance Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:34.541046Z",
     "start_time": "2020-08-27T11:16:34.533046Z"
    }
   },
   "outputs": [],
   "source": [
    "def corr2cov(corr, std):\n",
    "    # Derive the covariance matrix from a correlation matrix\n",
    "    corr[corr < -1], corr[corr > 1] = -1, 1  # numerical error\n",
    "    cov = np.outer(std, std)*corr\n",
    "    return cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:34.828989Z",
     "start_time": "2020-08-27T11:16:34.546026Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# SNIPPET 2.7 GENERATING A BLOCK-DIAGONAL COVARIANCE MATRIX AND A VECTOR OF MEANS\n",
    "from sklearn.covariance import LedoitWolf\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "\n",
    "def formBlockMatrix(nBlocks, bSize, bCorr):\n",
    "    block = np.ones((bSize, bSize))*bCorr\n",
    "    block[range(bSize), range(bSize)] = 1\n",
    "    corr = block_diag(*([block]*nBlocks))\n",
    "    return corr\n",
    "# ---------------------------------------------------\n",
    "\n",
    "\n",
    "def formTrueMatrix(nBlocks, bSize, bCorr):\n",
    "    # In each block, the variances are drawn from a uniform distribution bounded between 5% and 20%; the vector of means is drawn from a Normal distribution with mean and standard deviation equal to the standard deviation from the covariance matrix\n",
    "    corr0 = formBlockMatrix(nBlocks, bSize, bCorr)\n",
    "    corr0 = pd.DataFrame(corr0)\n",
    "    cols = corr0.columns.tolist()\n",
    "    np.random.shuffle(cols)\n",
    "    corr0 = corr0[cols].loc[cols].copy(deep=True)\n",
    "    std0 = np.random.uniform(.05, .2, corr0.shape[0])\n",
    "    cov0 = corr2cov(corr0, std0)\n",
    "    mu0 = np.random.normal(std0, std0, cov0.shape[0]).reshape(-1, 1)\n",
    "    return mu0, cov0\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "nBlocks, bSize, bCorr = 10, 50, .5\n",
    "np.random.seed(0)\n",
    "mu0, cov0 = formTrueMatrix(nBlocks, bSize, bCorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:34.836036Z",
     "start_time": "2020-08-27T11:16:34.831071Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.8 GENERATING THE EMPIRICAL COVARIANCE MATRIX\n",
    "def simCovMu(mu0, cov0, nObs, shrink=False):\n",
    "    x = np.random.multivariate_normal(mu0.flatten(), cov0, size=nObs)\n",
    "    mu1 = x.mean(axis=0).reshape(-1, 1)\n",
    "    if shrink:\n",
    "        cov1 = LedoitWolf().fit(x).covariance_\n",
    "    else:\n",
    "        cov1 = np.cov(x, rowvar=0)\n",
    "    return mu1, cov1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:16:34.849299Z",
     "start_time": "2020-08-27T11:16:34.841606Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.9 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX\n",
    "def deNoiseCov(cov0, q, bWidth):\n",
    "    corr0 = cov2corr(cov0)\n",
    "    eVal0, eVec0 = getPCA(corr0)\n",
    "    eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth)\n",
    "    nFacts0 = eVal0.shape[0]-np.diag(eVal0)[::-1].searchsorted(eMax0)\n",
    "    corr1 = denoisedCorr(eVal0, eVec0, nFacts0)\n",
    "    cov1 = corr2cov(corr1, np.diag(cov0)**.5)\n",
    "    return cov1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:18:00.869121Z",
     "start_time": "2020-08-27T11:16:34.852844Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.10 DENOISING OF THE EMPIRICAL COVARIANCE MATRIX\n",
    "def optPort(cov, mu=None):  # optimal portfolio for minimum variance\n",
    "    inv = np.linalg.inv(cov)\n",
    "    ones = np.ones(shape=(inv.shape[0], 1))\n",
    "    if mu is None:\n",
    "        mu = ones\n",
    "    w = np.dot(inv, mu)\n",
    "    w /= np.dot(ones.T, w)\n",
    "    return w\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "nObs, nTrials, bWidth, shrink, minVarPortf = 1000, 100, .01, False, True\n",
    "w1 = w1_s = pd.DataFrame(columns=range(cov0.shape[0]),\n",
    "                         index=range(nTrials), dtype=float)\n",
    "w1_d = w1.copy(deep=True)\n",
    "w1_s_d = w1_s.copy(deep=True)\n",
    "np.random.seed(0)\n",
    "for i in tqdm(range(nTrials)):\n",
    "    mu1, cov1 = simCovMu(mu0, cov0, nObs, shrink=True)\n",
    "    if minVarPortf:\n",
    "        mu1 = None\n",
    "    cov1_d = deNoiseCov(cov1, nObs*1./cov1.shape[1], bWidth)\n",
    "    w1_s.loc[i] = optPort(cov1, mu1).flatten()\n",
    "    w1_s_d.loc[i] = optPort(cov1_d, mu1).flatten()\n",
    "\n",
    "\n",
    "for i in tqdm(range(nTrials)):\n",
    "    mu1, cov1 = simCovMu(mu0, cov0, nObs, shrink=False)\n",
    "    if minVarPortf:\n",
    "        mu1 = None\n",
    "    cov1_d = deNoiseCov(cov1, nObs*1./cov1.shape[1], bWidth)\n",
    "    w1.loc[i] = optPort(cov1, mu1).flatten()\n",
    "    w1_d.loc[i] = optPort(cov1_d, mu1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:18:36.190247Z",
     "start_time": "2020-08-27T11:18:36.144340Z"
    }
   },
   "outputs": [],
   "source": [
    "# SNIPPET 2.11 ROOT-MEAN-SQUARE ERRORS\n",
    "w0 = optPort(cov0, None if minVarPortf else mu0)\n",
    "w0 = np.repeat(w0.T, w1.shape[0], axis=0)\n",
    "# RMSE  not shrunk not denoised\n",
    "rmsd = np.mean((w1-w0).values.flatten()**2)**.5\n",
    "rmsd_d = np.mean((w1_d-w0).values.flatten()**2)**.5  # RMSE not shrunk denoised\n",
    "rmsd_s = np.mean((w1_s-w0).values.flatten()**2)**.5  # RMSE shrunk not denoised\n",
    "rmsd_s_d = np.mean((w1_s_d-w0).values.flatten()**2)**.5  # RMSE shrunk denoised\n",
    "\n",
    "res_tab = pd.DataFrame(columns=['Note denoised', 'Denoised'], index=[\n",
    "                       'Not shrunk', 'Shrunk'], data=np.array([[rmsd, rmsd_d], [rmsd_s, rmsd_s_d]]))\n",
    "res_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T11:20:44.214819Z",
     "start_time": "2020-08-27T11:18:38.201356Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.9 Exercise\n",
    "# Download the historical SP500 stocks daily closing price for the most recent year\n",
    "import bs4 as bs\n",
    "import requests\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "\n",
    "resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "tickers = []\n",
    "for row in table.findAll('tr')[1:]:\n",
    "    ticker = row.findAll('td')[0].text\n",
    "    tickers.append(ticker)\n",
    "\n",
    "tickers = [s.replace('\\n', '') for s in tickers]\n",
    "popular_tickers = ['AAPL', 'AMZN', 'FB', 'GOOGL', 'SPY']\n",
    "data = yf.download(tickers, period=\"ytd\", auto_adjust=True, threads=True)\n",
    "data = data.T.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:46:11.549016Z",
     "start_time": "2020-08-25T10:46:11.538669Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the log return time series and calculate the covariance matrix for the SP 500 stocks\n",
    "cls = data[data.level_0 == 'Close']\n",
    "cls = cls.drop(['level_0'], axis='columns')\n",
    "\n",
    "cls = cls.set_index(['level_1'])\n",
    "cls = cls.T\n",
    "cls.index = pd.to_datetime(cls.index)\n",
    "\n",
    "logret = cls.apply(np.log, axis=0).diff().drop(\n",
    "    pd.to_datetime(['2020-01-02']), axis=0).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:46:12.832651Z",
     "start_time": "2020-08-25T10:46:12.824213Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.a Covaraince matrix ; condition number of the correlation matrix\n",
    "\n",
    "covmat = np.cov(logret, rowvar=False)  # compute the covariance matrix\n",
    "Codnum = np.linalg.cond(covmat)  # condition number of correlation matrix\n",
    "Codnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:52.221232Z",
     "start_time": "2020-08-25T10:31:52.211704Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2.b one hundread efficient frontiers\n",
    "\n",
    "def portfolio_annualised_performance(weights, day_returns, cov_matrix):\n",
    "    returns = np.sum(day_returns.T.dot(weights)) * 252\n",
    "    std = np.sqrt(np.dot(weights, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "    return std, returns\n",
    "\n",
    "\n",
    "def random_portfolios(num_portfolios, day_returns, cov_matrix):\n",
    "    '''\n",
    "    Return performance of required number of random portfolios\n",
    "    '''\n",
    "    results = np.zeros((2, num_portfolios))\n",
    "    weights_record = []\n",
    "    for i in range(num_portfolios):\n",
    "        # mean and standard deviation of the alternative vectors of expected returns\n",
    "        mu, sigma = 0.1, 0.1\n",
    "        weights = np.random.normal(mu, sigma, len(day_returns))\n",
    "        weights /= np.sum(weights)\n",
    "        weights_record.append(weights)\n",
    "        portfolio_std_dev, portfolio_return = portfolio_annualised_performance(\n",
    "            weights, day_returns, cov_matrix)\n",
    "        results[0, i] = portfolio_std_dev\n",
    "        results[1, i] = portfolio_return\n",
    "    return results, weights_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:52.938392Z",
     "start_time": "2020-08-25T10:31:52.929942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Way 1 to get the efficient frontier: using optimization techniques to solve the efficient frontier\n",
    "# Time consuming\n",
    "import scipy.optimize as opt\n",
    "\n",
    "\n",
    "# target is the target value for returning the efficient frontier axis (similiar to a y-axis value )\n",
    "def efficient_return(day_returns, cov_matrix, target):\n",
    "    num_assets = len(day_returns)\n",
    "    args = (day_returns, cov_matrix)\n",
    "\n",
    "    def portfolio_return(weights):\n",
    "        return portfolio_annualised_performance(weights, day_returns, cov_matrix)[1]\n",
    "\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: portfolio_return(x) - target},\n",
    "                   {'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for asset in range(num_assets))\n",
    "    result = opt.minimize(portfolio_volatility, num_assets*[\n",
    "                          1./num_assets,], args=args, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return result\n",
    "\n",
    "\n",
    "def efficient_frontier(day_returns, cov_matrix, returns_range):  # return efficient frontier\n",
    "    efficients = []\n",
    "    for ret in tqdm(returns_range, desc='calculating efficient frontier using optimization method:'):\n",
    "        efficients.append(efficient_return(day_returns, cov_matrix, ret))\n",
    "    return efficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:53.620703Z",
     "start_time": "2020-08-25T10:31:53.612942Z"
    }
   },
   "outputs": [],
   "source": [
    "def ef_with_random_portfolio_opt(day_returns, cov_matrix, num_portfolios, ax=None, return_plot=True):\n",
    "    results, weights = random_portfolios(\n",
    "        num_portfolios, day_returns, cov_matrix)\n",
    "\n",
    "    target = np.linspace(\n",
    "        max(np.min(results[1]), 0), np.quantile(results[1], 0.7), 30)\n",
    "    efficient_portfolios = efficient_frontier(day_returns, cov_matrix, target)\n",
    "    frontier = [p['fun'] for p in efficient_portfolios]\n",
    "\n",
    "    if return_plot:\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            # ax.legend(labelspacing=0.8)\n",
    "        ax.plot(frontier, target, color='black',\n",
    "                linewidth=2, label='efficient frontier')\n",
    "\n",
    "        ax.scatter(results[0, :], results[1, :], marker='o', s=10, alpha=0.3)\n",
    "        ax.set_title(\n",
    "            'Calculated Portfolio Optimization based on Efficient Frontier')\n",
    "        ax.set_xlabel('annualised volatility')\n",
    "        ax.set_ylabel('annualised returns')\n",
    "        ax.legend()\n",
    "\n",
    "    return ax, [frontier, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:56.205701Z",
     "start_time": "2020-08-25T10:31:54.274405Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "target_ret = pd.DataFrame(logret.T)  # select 50 stocks to construct portfolio\n",
    "target_cov = np.cov(target_ret)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax, frontier_ret = ef_with_random_portfolio_opt(\n",
    "    target_ret, target_cov, 1000, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:57.545865Z",
     "start_time": "2020-08-25T10:31:57.537820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Way 2: monte-carlo method, direct infer from the efficient frontier from the simulated data\n",
    "\n",
    "\n",
    "def efficient_return_simu(results, target):\n",
    "\n",
    "    # return efficient frontier\n",
    "    # use +-5% area of the target, return the nearest min\n",
    "    results = pd.DataFrame(results.T).sort_values(by=1)\n",
    "    closiest_idx = np.argmin(np.abs(results[1]-target))\n",
    "    data_target = results[1][closiest_idx]\n",
    "    target_range_min = min(data_target*0.95, data_target*1.05)\n",
    "    target_range_max = max(data_target*0.95, data_target*1.05)\n",
    "    sub_results = results.loc[(results[1] <= target_range_max) & (\n",
    "        results[1] >= target_range_min), 0:2]\n",
    "\n",
    "    return min(sub_results[0])\n",
    "\n",
    "\n",
    "def efficient_frontier_emp(day_returns, cov_matrix, num_portfolios,  returns_range, random_seed=0):\n",
    "    efficients = []\n",
    "    np.random.seed(random_seed)\n",
    "    results, weights = random_portfolios(\n",
    "        num_portfolios, day_returns, cov_matrix)\n",
    "    for ret in returns_range:\n",
    "        efficients.append(efficient_return_simu(results, ret))\n",
    "    return efficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:58.116582Z",
     "start_time": "2020-08-25T10:31:58.107128Z"
    }
   },
   "outputs": [],
   "source": [
    "def ef_with_random_portfolio_simu(day_returns, cov_matrix, num_portfolios, ax=None, return_plot=True, random_seed=0, mean_frontier=None):\n",
    "    results, weights = random_portfolios(\n",
    "        num_portfolios, day_returns, cov_matrix)\n",
    "\n",
    "    # target_start = max(results[1][results[0]==np.min(results[0])][0],0)\n",
    "    target_start = 0\n",
    "    if True:  # not return_plot:\n",
    "        target_end = 400\n",
    "    else:\n",
    "        target_end = results[1][results[0] == np.max(results[0])][0]\n",
    "\n",
    "    target = np.linspace(target_start, target_end, 30)\n",
    "    efficient_portfolios = None\n",
    "    efficient_portfolios = efficient_frontier_emp(\n",
    "        day_returns, cov_matrix, num_portfolios, target, random_seed=random_seed)\n",
    "\n",
    "    if return_plot:\n",
    "        if not ax:\n",
    "            fig = plt.figure(figsize=(10, 7))\n",
    "            ax = fig.add_subplot(111)\n",
    "\n",
    "            # ax.legend(labelspacing=0.8)\n",
    "        if mean_frontier:\n",
    "            ax.plot(mean_frontier[0], mean_frontier[1], color='black',\n",
    "                    linewidth=2, label='mean efficient frontier')\n",
    "        else:\n",
    "            ax.plot(efficient_portfolios, target, color='black',\n",
    "                    linewidth=2, label='efficient frontier')\n",
    "\n",
    "        ax.scatter(results[0, :], results[1, :], marker='o', s=10, alpha=0.3)\n",
    "        ax.set_title(\n",
    "            'Calculated Portfolio Optimization based on Efficient Frontier')\n",
    "        ax.set_xlabel('annualised volatility')\n",
    "        ax.set_ylabel('annualised returns')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax = None\n",
    "\n",
    "    return ax, [efficient_portfolios, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:31:59.608513Z",
     "start_time": "2020-08-25T10:31:58.695290Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "target_ret = pd.DataFrame(logret.T)\n",
    "target_cov = np.cov(target_ret)\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax, frontier_ret = ef_with_random_portfolio_simu(\n",
    "    target_ret, target_cov, 1000, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:32:00.936400Z",
     "start_time": "2020-08-25T10:32:00.930360Z"
    }
   },
   "outputs": [],
   "source": [
    "def MC_ef_frontier(day_returns, cov_matrix, itertimes=100, random_seed=3):\n",
    "    np.random.seed(random_seed)\n",
    "    target_ret = pd.DataFrame(day_returns.T)\n",
    "    target_cov = cov_matrix\n",
    "    frontier = []\n",
    "    for i in tqdm(range(itertimes)):\n",
    "        ax, frontier_ret = ef_with_random_portfolio_simu(\n",
    "            target_ret, target_cov, 1000, return_plot=False, random_seed=i)\n",
    "        frontier.append(frontier_ret[0])\n",
    "    mean_frontier_vol = np.mean(frontier, axis=0)\n",
    "    mean_frontier_ret = frontier_ret[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:33:14.168650Z",
     "start_time": "2020-08-25T10:32:01.466001Z"
    }
   },
   "outputs": [],
   "source": [
    "MC_ef_frontier(logret,covmat,itertimes = 100,random_seed = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:33:22.201839Z",
     "start_time": "2020-08-25T10:33:21.288213Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax, _ = ef_with_random_portfolio_simu(target_ret, target_cov, 1000, ax=ax,\n",
    "                                      return_plot=True, random_seed=0, mean_frontier=[mean_frontier, frontier_ret[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:33:23.283570Z",
     "start_time": "2020-08-25T10:33:23.278944Z"
    }
   },
   "outputs": [],
   "source": [
    "# variance of the errors against the mean efficient frontier\n",
    "def error_mean_ef_frontier(frontiers, mean_frontier):\n",
    "    err = []\n",
    "    for frontier in frontiers:\n",
    "        err.append(np.std(frontier-mean_frontier))\n",
    "    return np.var(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:33:23.974255Z",
     "start_time": "2020-08-25T10:33:23.965134Z"
    }
   },
   "outputs": [],
   "source": [
    "error_mean_ef_frontier(frontier_ret, mean_frontier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-25T10:33:25.048219Z",
     "start_time": "2020-08-25T10:33:24.822552Z"
    }
   },
   "outputs": [],
   "source": [
    "# exercise 3\n",
    "corr3 = cov2corr(covmat)\n",
    "eVal3, eVec3 = getPCA(corr3)\n",
    "# Fitting the Marcenko–Pastur PDF on a noisy covariance matrix.\n",
    "# estimate the sigma for Marcenko-Pastur dist\n",
    "bWidth = 0.01\n",
    "out = minimize(lambda *x: errPDFs(*x), .5,\n",
    "               args=(np.diag(eVal3), q, bWidth), bounds=((1E-5, 1-1E-5),))\n",
    "if out['success']:\n",
    "    var = out['x'][0]\n",
    "else:\n",
    "    var = 1\n",
    "print('-'*10)\n",
    "print(r'value of $\\sigma^2$ implied by Marcenko-Pastur distribution: ')\n",
    "pdf0 = mpPDF(var, q, pts=1000)  # Marcenko-Pastur dist\n",
    "pdf3 = fitKDE(np.diag(eVal3), bWidth=.01)  # empirical pdf\n",
    "ax = plt.figure().add_subplot(111)\n",
    "ax.plot(pdf0, label='Marcenko-Pastur dist')\n",
    "ax.bar(pdf3.index, pdf3.values, width=bWidth,\n",
    "       label='Empirical dist', color='darkorange')\n",
    "ax.set_xlabel(r'$\\lambda$')\n",
    "ax.set_ylabel(r'prob[$\\lambda$]')\n",
    "ax.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
